{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Bri7e8L6oHrJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!sudo echo -ne '\\n' | sudo add-apt-repository ppa:alessandro-strada/ppa >/dev/null 2>&1 # note: >/dev/null 2>&1 is used to supress printing\n",
        "!sudo apt update >/dev/null 2>&1\n",
        "!sudo apt install google-drive-ocamlfuse >/dev/null 2>&1\n",
        "!google-drive-ocamlfuse\n",
        "!sudo apt-get install w3m >/dev/null 2>&1 # to act as web browser \n",
        "!xdg-settings set default-web-browser w3m.desktop >/dev/null 2>&1 # to set default browser \n",
        "%cd /content\n",
        "!mkdir gdrive\n",
        "%cd gdrive\n",
        "!mkdir \"MyDrive\"\n",
        "!google-drive-ocamlfuse \"/content/gdrive/MyDrive\""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pguRZ5AIiYTG",
        "outputId": "46b55b7c-aa26-4e46-df28-5abd2ef73d76"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/usr/bin/xdg-open: 869: www-browser: not found\n",
            "/usr/bin/xdg-open: 869: links2: not found\n",
            "/usr/bin/xdg-open: 869: elinks: not found\n",
            "/usr/bin/xdg-open: 869: links: not found\n",
            "/usr/bin/xdg-open: 869: lynx: not found\n",
            "/usr/bin/xdg-open: 869: w3m: not found\n",
            "xdg-open: no method available for opening 'https://accounts.google.com/o/oauth2/auth?client_id=564921029129.apps.googleusercontent.com&redirect_uri=https%3A%2F%2Fgd-ocaml-auth.appspot.com%2Foauth2callback&scope=https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive&response_type=code&access_type=offline&approval_prompt=force&state=0Z1LUFUDwnr-tUj4MxDx9HN-QiFwxs%2FNNWJgRE3NPHc'\n",
            "/bin/sh: 1: firefox: not found\n",
            "/bin/sh: 1: google-chrome: not found\n",
            "/bin/sh: 1: chromium-browser: not found\n",
            "/bin/sh: 1: open: not found\n",
            "Cannot retrieve auth tokens.\n",
            "Failure(\"Error opening URL:https://accounts.google.com/o/oauth2/auth?client_id=564921029129.apps.googleusercontent.com&redirect_uri=https%3A%2F%2Fgd-ocaml-auth.appspot.com%2Foauth2callback&scope=https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive&response_type=code&access_type=offline&approval_prompt=force&state=0Z1LUFUDwnr-tUj4MxDx9HN-QiFwxs%2FNNWJgRE3NPHc\")\n",
            "/content\n",
            "/content/gdrive\n",
            "Access token retrieved correctly.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "8lj1aAj5sYCx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "urbQ3rY7hKPw",
        "outputId": "33e0aaf8-99f0-4766-d68a-b12c4e6e0d99",
        "collapsed": true
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2023-04-15 13:06:31--  https://repo.anaconda.com/miniconda/Miniconda3-py37_4.8.2-Linux-x86_64.sh\n",
            "Resolving repo.anaconda.com (repo.anaconda.com)... 104.16.131.3, 104.16.130.3, 2606:4700::6810:8203, ...\n",
            "Connecting to repo.anaconda.com (repo.anaconda.com)|104.16.131.3|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 85055499 (81M) [application/x-sh]\n",
            "Saving to: ‘Miniconda3-py37_4.8.2-Linux-x86_64.sh’\n",
            "\n",
            "Miniconda3-py37_4.8 100%[===================>]  81.12M   207MB/s    in 0.4s    \n",
            "\n",
            "2023-04-15 13:06:31 (207 MB/s) - ‘Miniconda3-py37_4.8.2-Linux-x86_64.sh’ saved [85055499/85055499]\n",
            "\n",
            "PREFIX=/usr/local\n",
            "Unpacking payload ...\n",
            "Collecting package metadata (current_repodata.json): - \b\bdone\n",
            "Solving environment: | \b\b/ \b\bdone\n",
            "\n",
            "## Package Plan ##\n",
            "\n",
            "  environment location: /usr/local\n",
            "\n",
            "  added / updated specs:\n",
            "    - _libgcc_mutex==0.1=main\n",
            "    - asn1crypto==1.3.0=py37_0\n",
            "    - ca-certificates==2020.1.1=0\n",
            "    - certifi==2019.11.28=py37_0\n",
            "    - cffi==1.14.0=py37h2e261b9_0\n",
            "    - chardet==3.0.4=py37_1003\n",
            "    - conda-package-handling==1.6.0=py37h7b6447c_0\n",
            "    - conda==4.8.2=py37_0\n",
            "    - cryptography==2.8=py37h1ba5d50_0\n",
            "    - idna==2.8=py37_0\n",
            "    - ld_impl_linux-64==2.33.1=h53a641e_7\n",
            "    - libedit==3.1.20181209=hc058e9b_0\n",
            "    - libffi==3.2.1=hd88cf55_4\n",
            "    - libgcc-ng==9.1.0=hdf63c60_0\n",
            "    - libstdcxx-ng==9.1.0=hdf63c60_0\n",
            "    - ncurses==6.2=he6710b0_0\n",
            "    - openssl==1.1.1d=h7b6447c_4\n",
            "    - pip==20.0.2=py37_1\n",
            "    - pycosat==0.6.3=py37h7b6447c_0\n",
            "    - pycparser==2.19=py37_0\n",
            "    - pyopenssl==19.1.0=py37_0\n",
            "    - pysocks==1.7.1=py37_0\n",
            "    - python==3.7.6=h0371630_2\n",
            "    - readline==7.0=h7b6447c_5\n",
            "    - requests==2.22.0=py37_1\n",
            "    - ruamel_yaml==0.15.87=py37h7b6447c_0\n",
            "    - setuptools==45.2.0=py37_0\n",
            "    - six==1.14.0=py37_0\n",
            "    - sqlite==3.31.1=h7b6447c_0\n",
            "    - tk==8.6.8=hbc83047_0\n",
            "    - tqdm==4.42.1=py_0\n",
            "    - urllib3==1.25.8=py37_0\n",
            "    - wheel==0.34.2=py37_0\n",
            "    - xz==5.2.4=h14c3975_4\n",
            "    - yaml==0.1.7=had09818_2\n",
            "    - zlib==1.2.11=h7b6447c_3\n",
            "\n",
            "\n",
            "The following NEW packages will be INSTALLED:\n",
            "\n",
            "  _libgcc_mutex      pkgs/main/linux-64::_libgcc_mutex-0.1-main\n",
            "  asn1crypto         pkgs/main/linux-64::asn1crypto-1.3.0-py37_0\n",
            "  ca-certificates    pkgs/main/linux-64::ca-certificates-2020.1.1-0\n",
            "  certifi            pkgs/main/linux-64::certifi-2019.11.28-py37_0\n",
            "  cffi               pkgs/main/linux-64::cffi-1.14.0-py37h2e261b9_0\n",
            "  chardet            pkgs/main/linux-64::chardet-3.0.4-py37_1003\n",
            "  conda              pkgs/main/linux-64::conda-4.8.2-py37_0\n",
            "  conda-package-han~ pkgs/main/linux-64::conda-package-handling-1.6.0-py37h7b6447c_0\n",
            "  cryptography       pkgs/main/linux-64::cryptography-2.8-py37h1ba5d50_0\n",
            "  idna               pkgs/main/linux-64::idna-2.8-py37_0\n",
            "  ld_impl_linux-64   pkgs/main/linux-64::ld_impl_linux-64-2.33.1-h53a641e_7\n",
            "  libedit            pkgs/main/linux-64::libedit-3.1.20181209-hc058e9b_0\n",
            "  libffi             pkgs/main/linux-64::libffi-3.2.1-hd88cf55_4\n",
            "  libgcc-ng          pkgs/main/linux-64::libgcc-ng-9.1.0-hdf63c60_0\n",
            "  libstdcxx-ng       pkgs/main/linux-64::libstdcxx-ng-9.1.0-hdf63c60_0\n",
            "  ncurses            pkgs/main/linux-64::ncurses-6.2-he6710b0_0\n",
            "  openssl            pkgs/main/linux-64::openssl-1.1.1d-h7b6447c_4\n",
            "  pip                pkgs/main/linux-64::pip-20.0.2-py37_1\n",
            "  pycosat            pkgs/main/linux-64::pycosat-0.6.3-py37h7b6447c_0\n",
            "  pycparser          pkgs/main/linux-64::pycparser-2.19-py37_0\n",
            "  pyopenssl          pkgs/main/linux-64::pyopenssl-19.1.0-py37_0\n",
            "  pysocks            pkgs/main/linux-64::pysocks-1.7.1-py37_0\n",
            "  python             pkgs/main/linux-64::python-3.7.6-h0371630_2\n",
            "  readline           pkgs/main/linux-64::readline-7.0-h7b6447c_5\n",
            "  requests           pkgs/main/linux-64::requests-2.22.0-py37_1\n",
            "  ruamel_yaml        pkgs/main/linux-64::ruamel_yaml-0.15.87-py37h7b6447c_0\n",
            "  setuptools         pkgs/main/linux-64::setuptools-45.2.0-py37_0\n",
            "  six                pkgs/main/linux-64::six-1.14.0-py37_0\n",
            "  sqlite             pkgs/main/linux-64::sqlite-3.31.1-h7b6447c_0\n",
            "  tk                 pkgs/main/linux-64::tk-8.6.8-hbc83047_0\n",
            "  tqdm               pkgs/main/noarch::tqdm-4.42.1-py_0\n",
            "  urllib3            pkgs/main/linux-64::urllib3-1.25.8-py37_0\n",
            "  wheel              pkgs/main/linux-64::wheel-0.34.2-py37_0\n",
            "  xz                 pkgs/main/linux-64::xz-5.2.4-h14c3975_4\n",
            "  yaml               pkgs/main/linux-64::yaml-0.1.7-had09818_2\n",
            "  zlib               pkgs/main/linux-64::zlib-1.2.11-h7b6447c_3\n",
            "\n",
            "\n",
            "Preparing transaction: \\ \b\b| \b\b/ \b\bdone\n",
            "Executing transaction: \\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\bdone\n",
            "installation finished.\n",
            "WARNING:\n",
            "    You currently have a PYTHONPATH environment variable set. This may cause\n",
            "    unexpected behavior when running the Python interpreter in Miniconda3.\n",
            "    For best results, please verify that your PYTHONPATH only points to\n",
            "    directories of packages that are compatible with the Python interpreter\n",
            "    in Miniconda3: /usr/local\n"
          ]
        }
      ],
      "source": [
        "################################################################################\n",
        "# INSTALL CONDA ON GOOGLE COLAB\n",
        "################################################################################\n",
        "! wget https://repo.anaconda.com/miniconda/Miniconda3-py37_4.8.2-Linux-x86_64.sh\n",
        "! chmod +x Miniconda3-py37_4.8.2-Linux-x86_64.sh\n",
        "! bash ./Miniconda3-py37_4.8.2-Linux-x86_64.sh -b -f -p /usr/local\n",
        "import sys\n",
        "sys.path.append('/usr/local/lib/python3.7/site-packages/')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install torch==1.8.1+cu101 torchvision==0.9.1+cu101 -f https://download.pytorch.org/whl/torch_stable.html"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZiArhBX0hkPF",
        "outputId": "cc8a0203-7749-4f46-9bb7-b6eb207e5361",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Looking in links: https://download.pytorch.org/whl/torch_stable.html\n",
            "Collecting torch==1.8.1+cu101\n",
            "  Downloading https://download.pytorch.org/whl/cu101/torch-1.8.1%2Bcu101-cp37-cp37m-linux_x86_64.whl (763.7 MB)\n",
            "\u001b[K     |████████████████████████████████| 763.7 MB 9.9 kB/s \n",
            "\u001b[?25hCollecting torchvision==0.9.1+cu101\n",
            "  Downloading https://download.pytorch.org/whl/cu101/torchvision-0.9.1%2Bcu101-cp37-cp37m-linux_x86_64.whl (17.3 MB)\n",
            "\u001b[K     |████████████████████████████████| 17.3 MB 60 kB/s \n",
            "\u001b[?25hCollecting typing-extensions\n",
            "  Downloading typing_extensions-4.5.0-py3-none-any.whl (27 kB)\n",
            "Collecting numpy\n",
            "  Downloading numpy-1.21.6-cp37-cp37m-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (15.7 MB)\n",
            "\u001b[K     |████████████████████████████████| 15.7 MB 8.0 MB/s \n",
            "\u001b[?25hCollecting pillow>=4.1.1\n",
            "  Downloading Pillow-9.5.0-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.3 MB)\n",
            "\u001b[K     |████████████████████████████████| 3.3 MB 56.2 MB/s \n",
            "\u001b[?25hInstalling collected packages: typing-extensions, numpy, torch, pillow, torchvision\n",
            "Successfully installed numpy-1.21.6 pillow-9.5.0 torch-1.8.1+cu101 torchvision-0.9.1+cu101 typing-extensions-4.5.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!python -m pip install detectron2==0.4 -f https://dl.fbaipublicfiles.com/detectron2/wheels/cu101/torch1.8/index.html"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fJ0SZhH6hydV",
        "outputId": "39524862-288e-4b4a-8147-fc491d8577a4",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Looking in links: https://dl.fbaipublicfiles.com/detectron2/wheels/cu101/torch1.8/index.html\n",
            "Collecting detectron2==0.4\n",
            "  Downloading https://dl.fbaipublicfiles.com/detectron2/wheels/cu101/torch1.8/detectron2-0.4%2Bcu101-cp37-cp37m-linux_x86_64.whl (6.2 MB)\n",
            "\u001b[K     |████████████████████████████████| 6.2 MB 4.9 MB/s \n",
            "\u001b[?25hCollecting tabulate\n",
            "  Downloading tabulate-0.9.0-py3-none-any.whl (35 kB)\n",
            "Collecting matplotlib\n",
            "  Downloading matplotlib-3.5.3-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.whl (11.2 MB)\n",
            "\u001b[K     |████████████████████████████████| 11.2 MB 6.9 MB/s \n",
            "\u001b[?25hCollecting fvcore<0.1.4,>=0.1.3\n",
            "  Downloading fvcore-0.1.3.post20210317.tar.gz (47 kB)\n",
            "\u001b[K     |████████████████████████████████| 47 kB 4.6 MB/s \n",
            "\u001b[?25hRequirement already satisfied: Pillow>=7.1 in /usr/local/lib/python3.7/site-packages (from detectron2==0.4) (9.5.0)\n",
            "Collecting omegaconf>=2\n",
            "  Downloading omegaconf-2.3.0-py3-none-any.whl (79 kB)\n",
            "\u001b[K     |████████████████████████████████| 79 kB 8.7 MB/s \n",
            "\u001b[?25hCollecting yacs>=0.1.6\n",
            "  Downloading yacs-0.1.8-py3-none-any.whl (14 kB)\n",
            "Requirement already satisfied: tqdm>4.29.0 in /usr/local/lib/python3.7/site-packages (from detectron2==0.4) (4.42.1)\n",
            "Collecting termcolor>=1.1\n",
            "  Downloading termcolor-2.2.0-py3-none-any.whl (6.6 kB)\n",
            "Collecting future\n",
            "  Downloading future-0.18.3.tar.gz (840 kB)\n",
            "\u001b[K     |████████████████████████████████| 840 kB 60.3 MB/s \n",
            "\u001b[?25hCollecting cloudpickle\n",
            "  Downloading cloudpickle-2.2.1-py3-none-any.whl (25 kB)\n",
            "Collecting tensorboard\n",
            "  Downloading tensorboard-2.11.2-py3-none-any.whl (6.0 MB)\n",
            "\u001b[K     |████████████████████████████████| 6.0 MB 35.8 MB/s \n",
            "\u001b[?25hCollecting pycocotools>=2.0.2\n",
            "  Downloading pycocotools-2.0.6.tar.gz (24 kB)\n",
            "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "    Preparing wheel metadata ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting pydot\n",
            "  Downloading pydot-1.4.2-py2.py3-none-any.whl (21 kB)\n",
            "Collecting iopath>=0.1.2\n",
            "  Downloading iopath-0.1.10.tar.gz (42 kB)\n",
            "\u001b[K     |████████████████████████████████| 42 kB 1.1 MB/s \n",
            "\u001b[?25hCollecting pyparsing>=2.2.1\n",
            "  Downloading pyparsing-3.0.9-py3-none-any.whl (98 kB)\n",
            "\u001b[K     |████████████████████████████████| 98 kB 9.1 MB/s \n",
            "\u001b[?25hCollecting fonttools>=4.22.0\n",
            "  Downloading fonttools-4.38.0-py3-none-any.whl (965 kB)\n",
            "\u001b[K     |████████████████████████████████| 965 kB 53.6 MB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/site-packages (from matplotlib->detectron2==0.4) (1.21.6)\n",
            "Collecting cycler>=0.10\n",
            "  Downloading cycler-0.11.0-py3-none-any.whl (6.4 kB)\n",
            "Collecting python-dateutil>=2.7\n",
            "  Downloading python_dateutil-2.8.2-py2.py3-none-any.whl (247 kB)\n",
            "\u001b[K     |████████████████████████████████| 247 kB 53.2 MB/s \n",
            "\u001b[?25hCollecting packaging>=20.0\n",
            "  Downloading packaging-23.1-py3-none-any.whl (48 kB)\n",
            "\u001b[K     |████████████████████████████████| 48 kB 304 kB/s \n",
            "\u001b[?25hCollecting kiwisolver>=1.0.1\n",
            "  Downloading kiwisolver-1.4.4-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.whl (1.1 MB)\n",
            "\u001b[K     |████████████████████████████████| 1.1 MB 65.6 MB/s \n",
            "\u001b[?25hCollecting pyyaml>=5.1\n",
            "  Downloading PyYAML-6.0-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (596 kB)\n",
            "\u001b[K     |████████████████████████████████| 596 kB 53.0 MB/s \n",
            "\u001b[?25hCollecting antlr4-python3-runtime==4.9.*\n",
            "  Downloading antlr4-python3-runtime-4.9.3.tar.gz (117 kB)\n",
            "\u001b[K     |████████████████████████████████| 117 kB 68.9 MB/s \n",
            "\u001b[?25hCollecting tensorboard-data-server<0.7.0,>=0.6.0\n",
            "  Downloading tensorboard_data_server-0.6.1-py3-none-manylinux2010_x86_64.whl (4.9 MB)\n",
            "\u001b[K     |████████████████████████████████| 4.9 MB 59.5 MB/s \n",
            "\u001b[?25hCollecting google-auth<3,>=1.6.3\n",
            "  Downloading google_auth-2.17.3-py2.py3-none-any.whl (178 kB)\n",
            "\u001b[K     |████████████████████████████████| 178 kB 60.2 MB/s \n",
            "\u001b[?25hCollecting absl-py>=0.4\n",
            "  Downloading absl_py-1.4.0-py3-none-any.whl (126 kB)\n",
            "\u001b[K     |████████████████████████████████| 126 kB 48.3 MB/s \n",
            "\u001b[?25hRequirement already satisfied: wheel>=0.26 in /usr/local/lib/python3.7/site-packages (from tensorboard->detectron2==0.4) (0.34.2)\n",
            "Collecting werkzeug>=1.0.1\n",
            "  Downloading Werkzeug-2.2.3-py3-none-any.whl (233 kB)\n",
            "\u001b[K     |████████████████████████████████| 233 kB 49.3 MB/s \n",
            "\u001b[?25hCollecting protobuf<4,>=3.9.2\n",
            "  Downloading protobuf-3.20.3-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.whl (1.0 MB)\n",
            "\u001b[K     |████████████████████████████████| 1.0 MB 63.3 MB/s \n",
            "\u001b[?25hCollecting markdown>=2.6.8\n",
            "  Downloading Markdown-3.4.3-py3-none-any.whl (93 kB)\n",
            "\u001b[K     |████████████████████████████████| 93 kB 3.0 MB/s \n",
            "\u001b[?25hRequirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.7/site-packages (from tensorboard->detectron2==0.4) (2.22.0)\n",
            "Collecting tensorboard-plugin-wit>=1.6.0\n",
            "  Downloading tensorboard_plugin_wit-1.8.1-py3-none-any.whl (781 kB)\n",
            "\u001b[K     |████████████████████████████████| 781 kB 64.7 MB/s \n",
            "\u001b[?25hCollecting google-auth-oauthlib<0.5,>=0.4.1\n",
            "  Downloading google_auth_oauthlib-0.4.6-py2.py3-none-any.whl (18 kB)\n",
            "Collecting grpcio>=1.24.3\n",
            "  Downloading grpcio-1.53.0-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (5.0 MB)\n",
            "\u001b[K     |████████████████████████████████| 5.0 MB 62.4 MB/s \n",
            "\u001b[?25hRequirement already satisfied: setuptools>=41.0.0 in /usr/local/lib/python3.7/site-packages (from tensorboard->detectron2==0.4) (45.2.0.post20200210)\n",
            "Requirement already satisfied: typing_extensions in /usr/local/lib/python3.7/site-packages (from iopath>=0.1.2->detectron2==0.4) (4.5.0)\n",
            "Collecting portalocker\n",
            "  Downloading portalocker-2.7.0-py2.py3-none-any.whl (15 kB)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.7/site-packages (from python-dateutil>=2.7->matplotlib->detectron2==0.4) (1.14.0)\n",
            "Collecting pyasn1-modules>=0.2.1\n",
            "  Downloading pyasn1_modules-0.2.8-py2.py3-none-any.whl (155 kB)\n",
            "\u001b[K     |████████████████████████████████| 155 kB 69.1 MB/s \n",
            "\u001b[?25hCollecting rsa<5,>=3.1.4; python_version >= \"3.6\"\n",
            "  Downloading rsa-4.9-py3-none-any.whl (34 kB)\n",
            "Collecting cachetools<6.0,>=2.0.0\n",
            "  Downloading cachetools-5.3.0-py3-none-any.whl (9.3 kB)\n",
            "Collecting MarkupSafe>=2.1.1\n",
            "  Downloading MarkupSafe-2.1.2-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (25 kB)\n",
            "Collecting importlib-metadata>=4.4; python_version < \"3.10\"\n",
            "  Downloading importlib_metadata-6.3.0-py3-none-any.whl (22 kB)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/site-packages (from requests<3,>=2.21.0->tensorboard->detectron2==0.4) (1.25.8)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/site-packages (from requests<3,>=2.21.0->tensorboard->detectron2==0.4) (2019.11.28)\n",
            "Requirement already satisfied: idna<2.9,>=2.5 in /usr/local/lib/python3.7/site-packages (from requests<3,>=2.21.0->tensorboard->detectron2==0.4) (2.8)\n",
            "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /usr/local/lib/python3.7/site-packages (from requests<3,>=2.21.0->tensorboard->detectron2==0.4) (3.0.4)\n",
            "Collecting requests-oauthlib>=0.7.0\n",
            "  Downloading requests_oauthlib-1.3.1-py2.py3-none-any.whl (23 kB)\n",
            "Collecting pyasn1<0.5.0,>=0.4.6\n",
            "  Downloading pyasn1-0.4.8-py2.py3-none-any.whl (77 kB)\n",
            "\u001b[K     |████████████████████████████████| 77 kB 8.0 MB/s \n",
            "\u001b[?25hCollecting zipp>=0.5\n",
            "  Downloading zipp-3.15.0-py3-none-any.whl (6.8 kB)\n",
            "Collecting oauthlib>=3.0.0\n",
            "  Downloading oauthlib-3.2.2-py3-none-any.whl (151 kB)\n",
            "\u001b[K     |████████████████████████████████| 151 kB 63.9 MB/s \n",
            "\u001b[?25hBuilding wheels for collected packages: fvcore, future, pycocotools, iopath, antlr4-python3-runtime\n",
            "  Building wheel for fvcore (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for fvcore: filename=fvcore-0.1.3.post20210317-py3-none-any.whl size=58540 sha256=2feb54bd379a0261a7bc53cfc7518d58d18f25401f718a78aab4d2a11c5b1bc8\n",
            "  Stored in directory: /root/.cache/pip/wheels/a6/02/09/10e3a0150eb92e5ecbee3677a813bffc32a8ec6f876bfe4adf\n",
            "  Building wheel for future (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for future: filename=future-0.18.3-py3-none-any.whl size=492025 sha256=2f3bea196c3bfed6604e7aeec71f8c75d6761f547ab2fe6274a5ea4ed54dd5dd\n",
            "  Stored in directory: /root/.cache/pip/wheels/fa/cd/1f/c6b7b50b564983bf3011e8fc75d06047ddc50c07f6e3660b00\n",
            "  Building wheel for pycocotools (PEP 517) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pycocotools: filename=pycocotools-2.0.6-cp37-cp37m-linux_x86_64.whl size=374040 sha256=0cd802449d6a29e97aa029a7d8919321e91d9754724a2fbd59358772d16e64ea\n",
            "  Stored in directory: /root/.cache/pip/wheels/06/f6/f9/9cc49c6de8e3cf27dfddd91bf46595a057141d4583a2adaf03\n",
            "  Building wheel for iopath (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for iopath: filename=iopath-0.1.10-py3-none-any.whl size=31542 sha256=7c6cf4b91db15cfc8cb640c26c6c365cca683b3b58275ffcad8fa5f87392949c\n",
            "  Stored in directory: /root/.cache/pip/wheels/aa/cc/ed/ca4e88beef656b01c84b9185196513ef2faf74a5a379b043a7\n",
            "  Building wheel for antlr4-python3-runtime (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for antlr4-python3-runtime: filename=antlr4_python3_runtime-4.9.3-py3-none-any.whl size=144573 sha256=a28719393975609d65ac03f0512a656c859ed4dd7edba5da061e140f5fc32202\n",
            "  Stored in directory: /root/.cache/pip/wheels/8b/8d/53/2af8772d9aec614e3fc65e53d4a993ad73c61daa8bbd85a873\n",
            "Successfully built fvcore future pycocotools iopath antlr4-python3-runtime\n",
            "Installing collected packages: tabulate, pyparsing, fonttools, cycler, python-dateutil, packaging, kiwisolver, matplotlib, pyyaml, yacs, termcolor, portalocker, iopath, fvcore, antlr4-python3-runtime, omegaconf, future, cloudpickle, tensorboard-data-server, pyasn1, pyasn1-modules, rsa, cachetools, google-auth, absl-py, MarkupSafe, werkzeug, protobuf, zipp, importlib-metadata, markdown, tensorboard-plugin-wit, oauthlib, requests-oauthlib, google-auth-oauthlib, grpcio, tensorboard, pycocotools, pydot, detectron2\n",
            "Successfully installed MarkupSafe-2.1.2 absl-py-1.4.0 antlr4-python3-runtime-4.9.3 cachetools-5.3.0 cloudpickle-2.2.1 cycler-0.11.0 detectron2-0.4+cu101 fonttools-4.38.0 future-0.18.3 fvcore-0.1.3.post20210317 google-auth-2.17.3 google-auth-oauthlib-0.4.6 grpcio-1.53.0 importlib-metadata-6.3.0 iopath-0.1.10 kiwisolver-1.4.4 markdown-3.4.3 matplotlib-3.5.3 oauthlib-3.2.2 omegaconf-2.3.0 packaging-23.1 portalocker-2.7.0 protobuf-3.20.3 pyasn1-0.4.8 pyasn1-modules-0.2.8 pycocotools-2.0.6 pydot-1.4.2 pyparsing-3.0.9 python-dateutil-2.8.2 pyyaml-6.0 requests-oauthlib-1.3.1 rsa-4.9 tabulate-0.9.0 tensorboard-2.11.2 tensorboard-data-server-0.6.1 tensorboard-plugin-wit-1.8.1 termcolor-2.2.0 werkzeug-2.2.3 yacs-0.1.8 zipp-3.15.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install spconv-cu102==2.1.25"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a4Zc-AariJLW",
        "outputId": "1ebcf0ec-c3d1-471e-be92-a81cc57dd430",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting spconv-cu102==2.1.25\n",
            "  Downloading spconv_cu102-2.1.25-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (20.8 MB)\n",
            "\u001b[K     |████████████████████████████████| 20.8 MB 1.2 MB/s \n",
            "\u001b[?25hCollecting pybind11>=2.6.0\n",
            "  Downloading pybind11-2.10.4-py3-none-any.whl (222 kB)\n",
            "\u001b[K     |████████████████████████████████| 222 kB 66.8 MB/s \n",
            "\u001b[?25hCollecting fire\n",
            "  Downloading fire-0.5.0.tar.gz (88 kB)\n",
            "\u001b[K     |████████████████████████████████| 88 kB 9.2 MB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy in /usr/local/lib/python3.7/site-packages (from spconv-cu102==2.1.25) (1.21.6)\n",
            "Collecting cumm-cu102<0.3.0,>=0.2.9\n",
            "  Downloading cumm_cu102-0.2.9-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.1 MB)\n",
            "\u001b[K     |████████████████████████████████| 1.1 MB 62.3 MB/s \n",
            "\u001b[?25hCollecting pccm<0.4.0\n",
            "  Downloading pccm-0.3.4-py3-none-any.whl (64 kB)\n",
            "\u001b[K     |████████████████████████████████| 64 kB 3.8 MB/s \n",
            "\u001b[?25hCollecting ccimport<0.4.0\n",
            "  Downloading ccimport-0.3.7-py3-none-any.whl (27 kB)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/site-packages (from fire->spconv-cu102==2.1.25) (1.14.0)\n",
            "Requirement already satisfied: termcolor in /usr/local/lib/python3.7/site-packages (from fire->spconv-cu102==2.1.25) (2.2.0)\n",
            "Requirement already satisfied: portalocker>=2.3.2 in /usr/local/lib/python3.7/site-packages (from pccm<0.4.0->spconv-cu102==2.1.25) (2.7.0)\n",
            "Collecting lark>=1.0.0\n",
            "  Downloading lark-1.1.5-py3-none-any.whl (107 kB)\n",
            "\u001b[K     |████████████████████████████████| 107 kB 65.8 MB/s \n",
            "\u001b[?25hRequirement already satisfied: requests in /usr/local/lib/python3.7/site-packages (from ccimport<0.4.0->spconv-cu102==2.1.25) (2.22.0)\n",
            "Collecting ninja\n",
            "  Downloading ninja-1.11.1-py2.py3-none-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (145 kB)\n",
            "\u001b[K     |████████████████████████████████| 145 kB 63.6 MB/s \n",
            "\u001b[?25hRequirement already satisfied: importlib-metadata>=2.0; python_version < \"3.8\" in /usr/local/lib/python3.7/site-packages (from ccimport<0.4.0->spconv-cu102==2.1.25) (6.3.0)\n",
            "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /usr/local/lib/python3.7/site-packages (from requests->ccimport<0.4.0->spconv-cu102==2.1.25) (3.0.4)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/site-packages (from requests->ccimport<0.4.0->spconv-cu102==2.1.25) (1.25.8)\n",
            "Requirement already satisfied: idna<2.9,>=2.5 in /usr/local/lib/python3.7/site-packages (from requests->ccimport<0.4.0->spconv-cu102==2.1.25) (2.8)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/site-packages (from requests->ccimport<0.4.0->spconv-cu102==2.1.25) (2019.11.28)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/site-packages (from importlib-metadata>=2.0; python_version < \"3.8\"->ccimport<0.4.0->spconv-cu102==2.1.25) (3.15.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.4; python_version < \"3.8\" in /usr/local/lib/python3.7/site-packages (from importlib-metadata>=2.0; python_version < \"3.8\"->ccimport<0.4.0->spconv-cu102==2.1.25) (4.5.0)\n",
            "Building wheels for collected packages: fire\n",
            "  Building wheel for fire (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for fire: filename=fire-0.5.0-py2.py3-none-any.whl size=116936 sha256=d91af57f3fa61ce2e2997b9065070b700c45be6d7c2a657af762e53d1c71a05a\n",
            "  Stored in directory: /root/.cache/pip/wheels/20/97/e1/dd2c472bebcdcaa85fdc07d0f19020299f1c86773028860c53\n",
            "Successfully built fire\n",
            "Installing collected packages: pybind11, fire, ninja, ccimport, lark, pccm, cumm-cu102, spconv-cu102\n",
            "Successfully installed ccimport-0.3.7 cumm-cu102-0.2.9 fire-0.5.0 lark-1.1.5 ninja-1.11.1 pccm-0.3.4 pybind11-2.10.4 spconv-cu102-2.1.25\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install opencv-python"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_bWH_byzopud",
        "outputId": "7108a990-f86e-4bc1-a108-c1740b982f0b",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting opencv-python\n",
            "  Downloading opencv_python-4.7.0.72-cp37-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (61.8 MB)\n",
            "\u001b[K     |████████████████████████████████| 61.8 MB 1.2 MB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.17.0; python_version >= \"3.7\" in /usr/local/lib/python3.7/site-packages (from opencv-python) (1.21.6)\n",
            "Installing collected packages: opencv-python\n",
            "Successfully installed opencv-python-4.7.0.72\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "os.chdir('/content/gdrive/MyDrive/QueryDet-PyTorch')"
      ],
      "metadata": {
        "id": "lIdvezYwidTc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!python train_visdrone.py --resume"
      ],
      "metadata": {
        "id": "HjnxqH1neoBg",
        "outputId": "57d328b1-dd67-4953-a29d-4723b4ed6438",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "** fvcore version of PathManager will be deprecated soon. **\n",
            "** Please migrate to the version in iopath repo. **\n",
            "https://github.com/facebookresearch/iopath \n",
            "\n",
            "Command Line Args: Namespace(config_file='', dist_url='tcp://127.0.0.1:49152', eval_only=False, machine_rank=0, no_pretrain=False, num_gpus=1, num_machines=1, opts=[], resume=True)\n",
            "Traceback (most recent call last):\n",
            "  File \"train_visdrone.py\", line 18, in <module>\n",
            "    args=(args,),\n",
            "  File \"/usr/local/lib/python3.7/site-packages/detectron2/engine/launch.py\", line 62, in launch\n",
            "    main_func(*args)\n",
            "  File \"/content/gdrive/MyDrive/.shared/QueryDet-PyTorch/train_tools/visdrone_train.py\", line 237, in start_train\n",
            "    cfg = setup(args)\n",
            "  File \"/content/gdrive/MyDrive/.shared/QueryDet-PyTorch/train_tools/visdrone_train.py\", line 229, in setup\n",
            "    cfg.merge_from_file(args.config_file)\n",
            "  File \"/usr/local/lib/python3.7/site-packages/detectron2/config/config.py\", line 30, in merge_from_file\n",
            "    assert PathManager.isfile(cfg_filename), f\"Config file '{cfg_filename}' does not exist!\"\n",
            "AssertionError: Config file '' does not exist!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "p3ZZUJQXErpw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!python train_visdrone.py --resume --config /content/gdrive/MyDrive/QueryDet-PyTorch/work_dirs/visdrone_querydet/config.yaml --num-gpu 1 OUTPUT_DIR work_dirs/visdrone_querydet"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7tippXx_4Qx7",
        "outputId": "99419ff8-e4ce-4fe5-9054-7bcd217a6682"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "** fvcore version of PathManager will be deprecated soon. **\n",
            "** Please migrate to the version in iopath repo. **\n",
            "https://github.com/facebookresearch/iopath \n",
            "\n",
            "Command Line Args: Namespace(config_file='/content/gdrive/MyDrive/QueryDet-PyTorch/work_dirs/visdrone_querydet/config.yaml', dist_url='tcp://127.0.0.1:49152', eval_only=False, machine_rank=0, no_pretrain=False, num_gpus=1, num_machines=1, opts=['OUTPUT_DIR', 'work_dirs/visdrone_querydet'], resume=True)\n",
            "\u001b[32m[04/15 07:54:55 detectron2]: \u001b[0mRank of current process: 0. World size: 1\n",
            "\u001b[32m[04/15 07:54:58 detectron2]: \u001b[0mEnvironment info:\n",
            "----------------------  ---------------------------------------------------------------\n",
            "sys.platform            linux\n",
            "Python                  3.7.6 (default, Jan  8 2020, 19:59:22) [GCC 7.3.0]\n",
            "numpy                   1.21.6\n",
            "detectron2              0.4 @/usr/local/lib/python3.7/site-packages/detectron2\n",
            "Compiler                GCC 7.3\n",
            "CUDA compiler           CUDA 10.1\n",
            "detectron2 arch flags   3.7, 5.0, 5.2, 6.0, 6.1, 7.0, 7.5\n",
            "DETECTRON2_ENV_MODULE   <not set>\n",
            "PyTorch                 1.8.1+cu101 @/usr/local/lib/python3.7/site-packages/torch\n",
            "PyTorch debug build     False\n",
            "GPU available           True\n",
            "GPU 0                   Tesla T4 (arch=7.5)\n",
            "CUDA_HOME               /usr/local/cuda\n",
            "Pillow                  9.5.0\n",
            "torchvision             0.9.1+cu101 @/usr/local/lib/python3.7/site-packages/torchvision\n",
            "torchvision arch flags  3.5, 5.0, 6.0, 7.0, 7.5\n",
            "fvcore                  0.1.3.post20210317\n",
            "cv2                     4.7.0\n",
            "----------------------  ---------------------------------------------------------------\n",
            "PyTorch built with:\n",
            "  - GCC 7.3\n",
            "  - C++ Version: 201402\n",
            "  - Intel(R) Math Kernel Library Version 2020.0.0 Product Build 20191122 for Intel(R) 64 architecture applications\n",
            "  - Intel(R) MKL-DNN v1.7.0 (Git Hash 7aed236906b1f7a05c0917e5257a1af05e9ff683)\n",
            "  - OpenMP 201511 (a.k.a. OpenMP 4.5)\n",
            "  - NNPACK is enabled\n",
            "  - CPU capability usage: AVX2\n",
            "  - CUDA Runtime 10.1\n",
            "  - NVCC architecture flags: -gencode;arch=compute_37,code=sm_37;-gencode;arch=compute_50,code=sm_50;-gencode;arch=compute_60,code=sm_60;-gencode;arch=compute_70,code=sm_70\n",
            "  - CuDNN 7.6.3\n",
            "  - Magma 2.5.2\n",
            "  - Build settings: BLAS_INFO=mkl, BUILD_TYPE=Release, CUDA_VERSION=10.1, CUDNN_VERSION=7.6.3, CXX_COMPILER=/opt/rh/devtoolset-7/root/usr/bin/c++, CXX_FLAGS= -Wno-deprecated -fvisibility-inlines-hidden -DUSE_PTHREADPOOL -fopenmp -DNDEBUG -DUSE_KINETO -DUSE_FBGEMM -DUSE_QNNPACK -DUSE_PYTORCH_QNNPACK -DUSE_XNNPACK -O2 -fPIC -Wno-narrowing -Wall -Wextra -Werror=return-type -Wno-missing-field-initializers -Wno-type-limits -Wno-array-bounds -Wno-unknown-pragmas -Wno-sign-compare -Wno-unused-parameter -Wno-unused-variable -Wno-unused-function -Wno-unused-result -Wno-unused-local-typedefs -Wno-strict-overflow -Wno-strict-aliasing -Wno-error=deprecated-declarations -Wno-stringop-overflow -Wno-psabi -Wno-error=pedantic -Wno-error=redundant-decls -Wno-error=old-style-cast -fdiagnostics-color=always -faligned-new -Wno-unused-but-set-variable -Wno-maybe-uninitialized -fno-math-errno -fno-trapping-math -Werror=format -Wno-stringop-overflow, LAPACK_INFO=mkl, PERF_WITH_AVX=1, PERF_WITH_AVX2=1, PERF_WITH_AVX512=1, TORCH_VERSION=1.8.1, USE_CUDA=ON, USE_CUDNN=ON, USE_EXCEPTION_PTR=1, USE_GFLAGS=OFF, USE_GLOG=OFF, USE_MKL=ON, USE_MKLDNN=ON, USE_MPI=OFF, USE_NCCL=ON, USE_NNPACK=ON, USE_OPENMP=ON, \n",
            "\n",
            "\u001b[32m[04/15 07:54:58 detectron2]: \u001b[0mCommand line arguments: Namespace(config_file='/content/gdrive/MyDrive/QueryDet-PyTorch/work_dirs/visdrone_querydet/config.yaml', dist_url='tcp://127.0.0.1:49152', eval_only=False, machine_rank=0, no_pretrain=False, num_gpus=1, num_machines=1, opts=['OUTPUT_DIR', 'work_dirs/visdrone_querydet'], resume=True)\n",
            "\u001b[32m[04/15 07:54:58 detectron2]: \u001b[0mContents of args.config_file=/content/gdrive/MyDrive/QueryDet-PyTorch/work_dirs/visdrone_querydet/config.yaml:\n",
            "CUDNN_BENCHMARK: false\n",
            "DATALOADER:\n",
            "  ASPECT_RATIO_GROUPING: true\n",
            "  FILTER_EMPTY_ANNOTATIONS: true\n",
            "  NUM_WORKERS: 4\n",
            "  REPEAT_THRESHOLD: 0.0\n",
            "  SAMPLER_TRAIN: TrainingSampler\n",
            "DATASETS:\n",
            "  PRECOMPUTED_PROPOSAL_TOPK_TEST: 1000\n",
            "  PRECOMPUTED_PROPOSAL_TOPK_TRAIN: 2000\n",
            "  PROPOSAL_FILES_TEST: []\n",
            "  PROPOSAL_FILES_TRAIN: []\n",
            "  TEST:\n",
            "  - coco_2017_val\n",
            "  TRAIN:\n",
            "  - coco_2017_train\n",
            "GLOBAL:\n",
            "  HACK: 1.0\n",
            "INPUT:\n",
            "  CROP:\n",
            "    ENABLED: false\n",
            "    SIZE:\n",
            "    - 0.9\n",
            "    - 0.9\n",
            "    TYPE: relative_range\n",
            "  FORMAT: BGR\n",
            "  MASK_FORMAT: polygon\n",
            "  MAX_SIZE_TEST: 1333\n",
            "  MAX_SIZE_TRAIN: 1333\n",
            "  MIN_SIZE_TEST: 800\n",
            "  MIN_SIZE_TRAIN:\n",
            "  - 640\n",
            "  - 672\n",
            "  - 704\n",
            "  - 736\n",
            "  - 768\n",
            "  - 800\n",
            "  MIN_SIZE_TRAIN_SAMPLING: choice\n",
            "  RANDOM_FLIP: horizontal\n",
            "META_INFO:\n",
            "  EVAL_AP: true\n",
            "  EVAL_GPU_TIME: true\n",
            "  VIS_ROOT: ''\n",
            "MODEL:\n",
            "  ANCHOR_GENERATOR:\n",
            "    ANGLES:\n",
            "    - - -90\n",
            "      - 0\n",
            "      - 90\n",
            "    ASPECT_RATIOS:\n",
            "    - - 0.5\n",
            "      - 1.0\n",
            "      - 2.0\n",
            "    NAME: AnchorGeneratorWithCenter\n",
            "    OFFSET: 0.0\n",
            "    SIZES:\n",
            "    - - 16\n",
            "      - 20.15873679831797\n",
            "      - 25.39841683149119\n",
            "    - - 32\n",
            "      - 40.31747359663594\n",
            "      - 50.79683366298238\n",
            "    - - 64\n",
            "      - 80.63494719327188\n",
            "      - 101.59366732596476\n",
            "    - - 128\n",
            "      - 161.26989438654377\n",
            "      - 203.18733465192952\n",
            "    - - 256\n",
            "      - 322.53978877308754\n",
            "      - 406.37466930385904\n",
            "    - - 512\n",
            "      - 645.0795775461751\n",
            "      - 812.7493386077181\n",
            "  BACKBONE:\n",
            "    FREEZE_AT: 2\n",
            "    NAME: build_retinanet_resnet_fpn_backbone\n",
            "  CUSTOM:\n",
            "    CLEAR_CUDA_CACHE: false\n",
            "    CLS_WEIGHTS:\n",
            "    - 1.0\n",
            "    - 1.4\n",
            "    - 1.8\n",
            "    - 2.2\n",
            "    - 2.6\n",
            "    - 2.6\n",
            "    FOCAL_LOSS_ALPHAS:\n",
            "    - 0.25\n",
            "    - 0.25\n",
            "    - 0.25\n",
            "    - 0.25\n",
            "    - 0.25\n",
            "    - 0.25\n",
            "    FOCAL_LOSS_GAMMAS:\n",
            "    - 2.0\n",
            "    - 2.0\n",
            "    - 2.0\n",
            "    - 2.0\n",
            "    - 2.0\n",
            "    - 2.0\n",
            "    GIOU_LOSS: false\n",
            "    GRADIENT_CHECKPOINT: false\n",
            "    HEAD_BN: false\n",
            "    REG_WEIGHTS:\n",
            "    - 1.0\n",
            "    - 1.4\n",
            "    - 1.8\n",
            "    - 2.2\n",
            "    - 2.6\n",
            "    - 2.6\n",
            "    SOFT_NMS_METHOD: linear\n",
            "    SOFT_NMS_PRUND: 0.001\n",
            "    SOFT_NMS_SIGMA: 0.5\n",
            "    SOFT_NMS_THRESHOLD: 0.5\n",
            "    USE_LOOP_MATCHER: true\n",
            "    USE_SOFT_NMS: false\n",
            "  DEVICE: cuda\n",
            "  FPN:\n",
            "    FUSE_TYPE: sum\n",
            "    IN_FEATURES:\n",
            "    - res2\n",
            "    - res3\n",
            "    - res4\n",
            "    - res5\n",
            "    NORM: ''\n",
            "    OUT_CHANNELS: 256\n",
            "    TOP_LEVELS: 2\n",
            "  KEYPOINT_ON: false\n",
            "  LOAD_PROPOSALS: false\n",
            "  MASK_ON: false\n",
            "  META_ARCHITECTURE: RetinaNetQueryDet\n",
            "  PANOPTIC_FPN:\n",
            "    COMBINE:\n",
            "      ENABLED: true\n",
            "      INSTANCES_CONFIDENCE_THRESH: 0.5\n",
            "      OVERLAP_THRESH: 0.5\n",
            "      STUFF_AREA_LIMIT: 4096\n",
            "    INSTANCE_LOSS_WEIGHT: 1.0\n",
            "  PIXEL_MEAN:\n",
            "  - 103.53\n",
            "  - 116.28\n",
            "  - 123.675\n",
            "  PIXEL_STD:\n",
            "  - 1.0\n",
            "  - 1.0\n",
            "  - 1.0\n",
            "  PROPOSAL_GENERATOR:\n",
            "    MIN_SIZE: 0\n",
            "    NAME: RPN\n",
            "  QUERY:\n",
            "    CONTEXT: 2\n",
            "    ENCODE_CENTER_DIS_COEFF:\n",
            "    - 1.0\n",
            "    - 1.0\n",
            "    ENCODE_SMALL_OBJ_SCALE:\n",
            "    - - 0\n",
            "      - 32\n",
            "    - - 0\n",
            "      - 64\n",
            "    FEATURES_VALUE_TEST:\n",
            "    - 0\n",
            "    - 1\n",
            "    FEATURES_VALUE_TRAIN:\n",
            "    - 0\n",
            "    - 1\n",
            "    FEATURES_WHOLE_TEST:\n",
            "    - 2\n",
            "    - 3\n",
            "    - 4\n",
            "    - 5\n",
            "    FEATURES_WHOLE_TRAIN:\n",
            "    - 2\n",
            "    - 3\n",
            "    - 4\n",
            "    - 5\n",
            "    QUERY_INFER: false\n",
            "    QUERY_LOSS_GAMMA:\n",
            "    - 1.3\n",
            "    - 1.3\n",
            "    QUERY_LOSS_WEIGHT:\n",
            "    - 10.0\n",
            "    - 10.0\n",
            "    Q_FEATURE_TEST:\n",
            "    - 1\n",
            "    - 2\n",
            "    Q_FEATURE_TRAIN:\n",
            "    - 1\n",
            "    - 2\n",
            "    THRESHOLD: 0.12\n",
            "  RESNETS:\n",
            "    DEFORM_MODULATED: false\n",
            "    DEFORM_NUM_GROUPS: 1\n",
            "    DEFORM_ON_PER_STAGE:\n",
            "    - false\n",
            "    - false\n",
            "    - false\n",
            "    - false\n",
            "    DEPTH: 50\n",
            "    NORM: FrozenBN\n",
            "    NUM_GROUPS: 1\n",
            "    OUT_FEATURES:\n",
            "    - res2\n",
            "    - res3\n",
            "    - res4\n",
            "    - res5\n",
            "    RES2_OUT_CHANNELS: 256\n",
            "    RES5_DILATION: 1\n",
            "    STEM_OUT_CHANNELS: 64\n",
            "    STRIDE_IN_1X1: true\n",
            "    WIDTH_PER_GROUP: 64\n",
            "  RETINANET:\n",
            "    BBOX_REG_LOSS_TYPE: smooth_l1\n",
            "    BBOX_REG_WEIGHTS:\n",
            "    - 1.0\n",
            "    - 1.0\n",
            "    - 1.0\n",
            "    - 1.0\n",
            "    FOCAL_LOSS_ALPHA: 0.25\n",
            "    FOCAL_LOSS_GAMMA: 2.0\n",
            "    IN_FEATURES:\n",
            "    - p2\n",
            "    - p3\n",
            "    - p4\n",
            "    - p5\n",
            "    - p6\n",
            "    - p7\n",
            "    IOU_LABELS:\n",
            "    - 0\n",
            "    - -1\n",
            "    - 1\n",
            "    IOU_THRESHOLDS:\n",
            "    - 0.4\n",
            "    - 0.5\n",
            "    NMS_THRESH_TEST: 0.5\n",
            "    NORM: ''\n",
            "    NUM_CLASSES: 10\n",
            "    NUM_CONVS: 4\n",
            "    PRIOR_PROB: 0.01\n",
            "    SCORE_THRESH_TEST: 0.05\n",
            "    SMOOTH_L1_LOSS_BETA: 0.1\n",
            "    TOPK_CANDIDATES_TEST: 1000\n",
            "  ROI_BOX_CASCADE_HEAD:\n",
            "    BBOX_REG_WEIGHTS:\n",
            "    - - 10.0\n",
            "      - 10.0\n",
            "      - 5.0\n",
            "      - 5.0\n",
            "    - - 20.0\n",
            "      - 20.0\n",
            "      - 10.0\n",
            "      - 10.0\n",
            "    - - 30.0\n",
            "      - 30.0\n",
            "      - 15.0\n",
            "      - 15.0\n",
            "    IOUS:\n",
            "    - 0.5\n",
            "    - 0.6\n",
            "    - 0.7\n",
            "  ROI_BOX_HEAD:\n",
            "    BBOX_REG_LOSS_TYPE: smooth_l1\n",
            "    BBOX_REG_LOSS_WEIGHT: 1.0\n",
            "    BBOX_REG_WEIGHTS:\n",
            "    - 10.0\n",
            "    - 10.0\n",
            "    - 5.0\n",
            "    - 5.0\n",
            "    CLS_AGNOSTIC_BBOX_REG: false\n",
            "    CONV_DIM: 256\n",
            "    FC_DIM: 1024\n",
            "    NAME: ''\n",
            "    NORM: ''\n",
            "    NUM_CONV: 0\n",
            "    NUM_FC: 0\n",
            "    POOLER_RESOLUTION: 14\n",
            "    POOLER_SAMPLING_RATIO: 0\n",
            "    POOLER_TYPE: ROIAlignV2\n",
            "    SMOOTH_L1_BETA: 0.0\n",
            "    TRAIN_ON_PRED_BOXES: false\n",
            "  ROI_HEADS:\n",
            "    BATCH_SIZE_PER_IMAGE: 512\n",
            "    IN_FEATURES:\n",
            "    - res4\n",
            "    IOU_LABELS:\n",
            "    - 0\n",
            "    - 1\n",
            "    IOU_THRESHOLDS:\n",
            "    - 0.5\n",
            "    NAME: Res5ROIHeads\n",
            "    NMS_THRESH_TEST: 0.5\n",
            "    NUM_CLASSES: 80\n",
            "    POSITIVE_FRACTION: 0.25\n",
            "    PROPOSAL_APPEND_GT: true\n",
            "    SCORE_THRESH_TEST: 0.05\n",
            "  ROI_KEYPOINT_HEAD:\n",
            "    CONV_DIMS:\n",
            "    - 512\n",
            "    - 512\n",
            "    - 512\n",
            "    - 512\n",
            "    - 512\n",
            "    - 512\n",
            "    - 512\n",
            "    - 512\n",
            "    LOSS_WEIGHT: 1.0\n",
            "    MIN_KEYPOINTS_PER_IMAGE: 1\n",
            "    NAME: KRCNNConvDeconvUpsampleHead\n",
            "    NORMALIZE_LOSS_BY_VISIBLE_KEYPOINTS: true\n",
            "    NUM_KEYPOINTS: 17\n",
            "    POOLER_RESOLUTION: 14\n",
            "    POOLER_SAMPLING_RATIO: 0\n",
            "    POOLER_TYPE: ROIAlignV2\n",
            "  ROI_MASK_HEAD:\n",
            "    CLS_AGNOSTIC_MASK: false\n",
            "    CONV_DIM: 256\n",
            "    NAME: MaskRCNNConvUpsampleHead\n",
            "    NORM: ''\n",
            "    NUM_CONV: 0\n",
            "    POOLER_RESOLUTION: 14\n",
            "    POOLER_SAMPLING_RATIO: 0\n",
            "    POOLER_TYPE: ROIAlignV2\n",
            "  RPN:\n",
            "    BATCH_SIZE_PER_IMAGE: 256\n",
            "    BBOX_REG_LOSS_TYPE: smooth_l1\n",
            "    BBOX_REG_LOSS_WEIGHT: 1.0\n",
            "    BBOX_REG_WEIGHTS:\n",
            "    - 1.0\n",
            "    - 1.0\n",
            "    - 1.0\n",
            "    - 1.0\n",
            "    BOUNDARY_THRESH: -1\n",
            "    HEAD_NAME: StandardRPNHead\n",
            "    IN_FEATURES:\n",
            "    - res4\n",
            "    IOU_LABELS:\n",
            "    - 0\n",
            "    - -1\n",
            "    - 1\n",
            "    IOU_THRESHOLDS:\n",
            "    - 0.3\n",
            "    - 0.7\n",
            "    LOSS_WEIGHT: 1.0\n",
            "    NMS_THRESH: 0.7\n",
            "    POSITIVE_FRACTION: 0.5\n",
            "    POST_NMS_TOPK_TEST: 1000\n",
            "    POST_NMS_TOPK_TRAIN: 2000\n",
            "    PRE_NMS_TOPK_TEST: 6000\n",
            "    PRE_NMS_TOPK_TRAIN: 12000\n",
            "    SMOOTH_L1_BETA: 0.0\n",
            "  SEM_SEG_HEAD:\n",
            "    COMMON_STRIDE: 4\n",
            "    CONVS_DIM: 128\n",
            "    IGNORE_VALUE: 255\n",
            "    IN_FEATURES:\n",
            "    - p2\n",
            "    - p3\n",
            "    - p4\n",
            "    - p5\n",
            "    LOSS_WEIGHT: 1.0\n",
            "    NAME: SemSegFPNHead\n",
            "    NORM: GN\n",
            "    NUM_CLASSES: 54\n",
            "  WEIGHTS: /content/gdrive/MyDrive/QueryDet-PyTorch/work_dirs/visdrone_querydet/model_0005999.pth\n",
            "OUTPUT_DIR: work_dirs/visdrone_querydet\n",
            "SEED: -1\n",
            "SOLVER:\n",
            "  AMP:\n",
            "    ENABLED: true\n",
            "  BASE_LR: 0.006\n",
            "  BIAS_LR_FACTOR: 1.0\n",
            "  CHECKPOINT_PERIOD: 2000\n",
            "  CLIP_GRADIENTS:\n",
            "    CLIP_TYPE: value\n",
            "    CLIP_VALUE: 35.0\n",
            "    ENABLED: true\n",
            "    NORM_TYPE: 2.0\n",
            "  GAMMA: 0.1\n",
            "  IMS_PER_BATCH: 3\n",
            "  LR_SCHEDULER_NAME: WarmupMultiStepLR\n",
            "  MAX_ITER: 60000\n",
            "  MOMENTUM: 0.9\n",
            "  NESTEROV: false\n",
            "  REFERENCE_WORLD_SIZE: 0\n",
            "  STEPS:\n",
            "  - 15000\n",
            "  - 20000\n",
            "  WARMUP_FACTOR: 0.001\n",
            "  WARMUP_ITERS: 1000\n",
            "  WARMUP_METHOD: linear\n",
            "  WEIGHT_DECAY: 0.0001\n",
            "  WEIGHT_DECAY_BIAS: 0.0001\n",
            "  WEIGHT_DECAY_NORM: 0.0\n",
            "TEST:\n",
            "  AUG:\n",
            "    ENABLED: false\n",
            "    FLIP: true\n",
            "    MAX_SIZE: 4000\n",
            "    MIN_SIZES:\n",
            "    - 400\n",
            "    - 500\n",
            "    - 600\n",
            "    - 700\n",
            "    - 800\n",
            "    - 900\n",
            "    - 1000\n",
            "    - 1100\n",
            "    - 1200\n",
            "  DETECTIONS_PER_IMAGE: 500\n",
            "  EVAL_PERIOD: 0\n",
            "  EXPECTED_RESULTS: []\n",
            "  KEYPOINT_OKS_SIGMAS: []\n",
            "  PRECISE_BN:\n",
            "    ENABLED: false\n",
            "    NUM_ITER: 200\n",
            "VERSION: 2\n",
            "VISDRONE:\n",
            "  MAX_LENGTH: 1999\n",
            "  SHORT_LENGTH:\n",
            "  - 1200\n",
            "  TEST_IMG_ROOT: data/visdrone/coco_format/val_images\n",
            "  TEST_JSON: data/visdrone/coco_format/annotations/val_label.json\n",
            "  TEST_LENGTH: 3999\n",
            "  TRAIN_JSON: data/visdrone/coco_format/annotations/train_label.json\n",
            "  TRING_IMG_ROOT: data//visdrone/coco_format/train_images\n",
            "VIS_PERIOD: 0\n",
            "\n",
            "\u001b[32m[04/15 07:54:58 detectron2]: \u001b[0mRunning with full config:\n",
            "CUDNN_BENCHMARK: False\n",
            "DATALOADER:\n",
            "  ASPECT_RATIO_GROUPING: True\n",
            "  FILTER_EMPTY_ANNOTATIONS: True\n",
            "  NUM_WORKERS: 4\n",
            "  REPEAT_THRESHOLD: 0.0\n",
            "  SAMPLER_TRAIN: TrainingSampler\n",
            "DATASETS:\n",
            "  PRECOMPUTED_PROPOSAL_TOPK_TEST: 1000\n",
            "  PRECOMPUTED_PROPOSAL_TOPK_TRAIN: 2000\n",
            "  PROPOSAL_FILES_TEST: ()\n",
            "  PROPOSAL_FILES_TRAIN: ()\n",
            "  TEST: ('coco_2017_val',)\n",
            "  TRAIN: ('coco_2017_train',)\n",
            "GLOBAL:\n",
            "  HACK: 1.0\n",
            "INPUT:\n",
            "  CROP:\n",
            "    ENABLED: False\n",
            "    SIZE: [0.9, 0.9]\n",
            "    TYPE: relative_range\n",
            "  FORMAT: BGR\n",
            "  MASK_FORMAT: polygon\n",
            "  MAX_SIZE_TEST: 1333\n",
            "  MAX_SIZE_TRAIN: 1333\n",
            "  MIN_SIZE_TEST: 800\n",
            "  MIN_SIZE_TRAIN: (640, 672, 704, 736, 768, 800)\n",
            "  MIN_SIZE_TRAIN_SAMPLING: choice\n",
            "  RANDOM_FLIP: horizontal\n",
            "META_INFO:\n",
            "  EVAL_AP: True\n",
            "  EVAL_GPU_TIME: True\n",
            "  VIS_ROOT: \n",
            "MODEL:\n",
            "  ANCHOR_GENERATOR:\n",
            "    ANGLES: [[-90, 0, 90]]\n",
            "    ASPECT_RATIOS: [[0.5, 1.0, 2.0]]\n",
            "    NAME: AnchorGeneratorWithCenter\n",
            "    OFFSET: 0.0\n",
            "    SIZES: [[16, 20.15873679831797, 25.39841683149119], [32, 40.31747359663594, 50.79683366298238], [64, 80.63494719327188, 101.59366732596476], [128, 161.26989438654377, 203.18733465192952], [256, 322.53978877308754, 406.37466930385904], [512, 645.0795775461751, 812.7493386077181]]\n",
            "  BACKBONE:\n",
            "    FREEZE_AT: 2\n",
            "    NAME: build_retinanet_resnet_fpn_backbone\n",
            "  CUSTOM:\n",
            "    CLEAR_CUDA_CACHE: False\n",
            "    CLS_WEIGHTS: [1.0, 1.4, 1.8, 2.2, 2.6, 2.6]\n",
            "    FOCAL_LOSS_ALPHAS: [0.25, 0.25, 0.25, 0.25, 0.25, 0.25]\n",
            "    FOCAL_LOSS_GAMMAS: [2.0, 2.0, 2.0, 2.0, 2.0, 2.0]\n",
            "    GIOU_LOSS: False\n",
            "    GRADIENT_CHECKPOINT: False\n",
            "    HEAD_BN: False\n",
            "    REG_WEIGHTS: [1.0, 1.4, 1.8, 2.2, 2.6, 2.6]\n",
            "    SOFT_NMS_METHOD: linear\n",
            "    SOFT_NMS_PRUND: 0.001\n",
            "    SOFT_NMS_SIGMA: 0.5\n",
            "    SOFT_NMS_THRESHOLD: 0.5\n",
            "    USE_LOOP_MATCHER: True\n",
            "    USE_SOFT_NMS: False\n",
            "  DEVICE: cuda\n",
            "  FPN:\n",
            "    FUSE_TYPE: sum\n",
            "    IN_FEATURES: ['res2', 'res3', 'res4', 'res5']\n",
            "    NORM: \n",
            "    OUT_CHANNELS: 256\n",
            "    TOP_LEVELS: 2\n",
            "  KEYPOINT_ON: False\n",
            "  LOAD_PROPOSALS: False\n",
            "  MASK_ON: False\n",
            "  META_ARCHITECTURE: RetinaNetQueryDet\n",
            "  PANOPTIC_FPN:\n",
            "    COMBINE:\n",
            "      ENABLED: True\n",
            "      INSTANCES_CONFIDENCE_THRESH: 0.5\n",
            "      OVERLAP_THRESH: 0.5\n",
            "      STUFF_AREA_LIMIT: 4096\n",
            "    INSTANCE_LOSS_WEIGHT: 1.0\n",
            "  PIXEL_MEAN: [103.53, 116.28, 123.675]\n",
            "  PIXEL_STD: [1.0, 1.0, 1.0]\n",
            "  PROPOSAL_GENERATOR:\n",
            "    MIN_SIZE: 0\n",
            "    NAME: RPN\n",
            "  QUERY:\n",
            "    CONTEXT: 2\n",
            "    ENCODE_CENTER_DIS_COEFF: [1.0, 1.0]\n",
            "    ENCODE_SMALL_OBJ_SCALE: [[0, 32], [0, 64]]\n",
            "    FEATURES_VALUE_TEST: [0, 1]\n",
            "    FEATURES_VALUE_TRAIN: [0, 1]\n",
            "    FEATURES_WHOLE_TEST: [2, 3, 4, 5]\n",
            "    FEATURES_WHOLE_TRAIN: [2, 3, 4, 5]\n",
            "    QUERY_INFER: False\n",
            "    QUERY_LOSS_GAMMA: [1.3, 1.3]\n",
            "    QUERY_LOSS_WEIGHT: [10.0, 10.0]\n",
            "    Q_FEATURE_TEST: [1, 2]\n",
            "    Q_FEATURE_TRAIN: [1, 2]\n",
            "    THRESHOLD: 0.12\n",
            "  RESNETS:\n",
            "    DEFORM_MODULATED: False\n",
            "    DEFORM_NUM_GROUPS: 1\n",
            "    DEFORM_ON_PER_STAGE: [False, False, False, False]\n",
            "    DEPTH: 50\n",
            "    NORM: FrozenBN\n",
            "    NUM_GROUPS: 1\n",
            "    OUT_FEATURES: ['res2', 'res3', 'res4', 'res5']\n",
            "    RES2_OUT_CHANNELS: 256\n",
            "    RES5_DILATION: 1\n",
            "    STEM_OUT_CHANNELS: 64\n",
            "    STRIDE_IN_1X1: True\n",
            "    WIDTH_PER_GROUP: 64\n",
            "  RETINANET:\n",
            "    BBOX_REG_LOSS_TYPE: smooth_l1\n",
            "    BBOX_REG_WEIGHTS: (1.0, 1.0, 1.0, 1.0)\n",
            "    FOCAL_LOSS_ALPHA: 0.25\n",
            "    FOCAL_LOSS_GAMMA: 2.0\n",
            "    IN_FEATURES: ['p2', 'p3', 'p4', 'p5', 'p6', 'p7']\n",
            "    IOU_LABELS: [0, -1, 1]\n",
            "    IOU_THRESHOLDS: [0.4, 0.5]\n",
            "    NMS_THRESH_TEST: 0.5\n",
            "    NORM: \n",
            "    NUM_CLASSES: 10\n",
            "    NUM_CONVS: 4\n",
            "    PRIOR_PROB: 0.01\n",
            "    SCORE_THRESH_TEST: 0.05\n",
            "    SMOOTH_L1_LOSS_BETA: 0.1\n",
            "    TOPK_CANDIDATES_TEST: 1000\n",
            "  ROI_BOX_CASCADE_HEAD:\n",
            "    BBOX_REG_WEIGHTS: ([10.0, 10.0, 5.0, 5.0], [20.0, 20.0, 10.0, 10.0], [30.0, 30.0, 15.0, 15.0])\n",
            "    IOUS: (0.5, 0.6, 0.7)\n",
            "  ROI_BOX_HEAD:\n",
            "    BBOX_REG_LOSS_TYPE: smooth_l1\n",
            "    BBOX_REG_LOSS_WEIGHT: 1.0\n",
            "    BBOX_REG_WEIGHTS: (10.0, 10.0, 5.0, 5.0)\n",
            "    CLS_AGNOSTIC_BBOX_REG: False\n",
            "    CONV_DIM: 256\n",
            "    FC_DIM: 1024\n",
            "    NAME: \n",
            "    NORM: \n",
            "    NUM_CONV: 0\n",
            "    NUM_FC: 0\n",
            "    POOLER_RESOLUTION: 14\n",
            "    POOLER_SAMPLING_RATIO: 0\n",
            "    POOLER_TYPE: ROIAlignV2\n",
            "    SMOOTH_L1_BETA: 0.0\n",
            "    TRAIN_ON_PRED_BOXES: False\n",
            "  ROI_HEADS:\n",
            "    BATCH_SIZE_PER_IMAGE: 512\n",
            "    IN_FEATURES: ['res4']\n",
            "    IOU_LABELS: [0, 1]\n",
            "    IOU_THRESHOLDS: [0.5]\n",
            "    NAME: Res5ROIHeads\n",
            "    NMS_THRESH_TEST: 0.5\n",
            "    NUM_CLASSES: 80\n",
            "    POSITIVE_FRACTION: 0.25\n",
            "    PROPOSAL_APPEND_GT: True\n",
            "    SCORE_THRESH_TEST: 0.05\n",
            "  ROI_KEYPOINT_HEAD:\n",
            "    CONV_DIMS: (512, 512, 512, 512, 512, 512, 512, 512)\n",
            "    LOSS_WEIGHT: 1.0\n",
            "    MIN_KEYPOINTS_PER_IMAGE: 1\n",
            "    NAME: KRCNNConvDeconvUpsampleHead\n",
            "    NORMALIZE_LOSS_BY_VISIBLE_KEYPOINTS: True\n",
            "    NUM_KEYPOINTS: 17\n",
            "    POOLER_RESOLUTION: 14\n",
            "    POOLER_SAMPLING_RATIO: 0\n",
            "    POOLER_TYPE: ROIAlignV2\n",
            "  ROI_MASK_HEAD:\n",
            "    CLS_AGNOSTIC_MASK: False\n",
            "    CONV_DIM: 256\n",
            "    NAME: MaskRCNNConvUpsampleHead\n",
            "    NORM: \n",
            "    NUM_CONV: 0\n",
            "    POOLER_RESOLUTION: 14\n",
            "    POOLER_SAMPLING_RATIO: 0\n",
            "    POOLER_TYPE: ROIAlignV2\n",
            "  RPN:\n",
            "    BATCH_SIZE_PER_IMAGE: 256\n",
            "    BBOX_REG_LOSS_TYPE: smooth_l1\n",
            "    BBOX_REG_LOSS_WEIGHT: 1.0\n",
            "    BBOX_REG_WEIGHTS: (1.0, 1.0, 1.0, 1.0)\n",
            "    BOUNDARY_THRESH: -1\n",
            "    HEAD_NAME: StandardRPNHead\n",
            "    IN_FEATURES: ['res4']\n",
            "    IOU_LABELS: [0, -1, 1]\n",
            "    IOU_THRESHOLDS: [0.3, 0.7]\n",
            "    LOSS_WEIGHT: 1.0\n",
            "    NMS_THRESH: 0.7\n",
            "    POSITIVE_FRACTION: 0.5\n",
            "    POST_NMS_TOPK_TEST: 1000\n",
            "    POST_NMS_TOPK_TRAIN: 2000\n",
            "    PRE_NMS_TOPK_TEST: 6000\n",
            "    PRE_NMS_TOPK_TRAIN: 12000\n",
            "    SMOOTH_L1_BETA: 0.0\n",
            "  SEM_SEG_HEAD:\n",
            "    COMMON_STRIDE: 4\n",
            "    CONVS_DIM: 128\n",
            "    IGNORE_VALUE: 255\n",
            "    IN_FEATURES: ['p2', 'p3', 'p4', 'p5']\n",
            "    LOSS_WEIGHT: 1.0\n",
            "    NAME: SemSegFPNHead\n",
            "    NORM: GN\n",
            "    NUM_CLASSES: 54\n",
            "  WEIGHTS: /content/gdrive/MyDrive/QueryDet-PyTorch/work_dirs/visdrone_querydet/model_0005999.pth\n",
            "OUTPUT_DIR: work_dirs/visdrone_querydet\n",
            "SEED: -1\n",
            "SOLVER:\n",
            "  AMP:\n",
            "    ENABLED: True\n",
            "  BASE_LR: 0.006\n",
            "  BIAS_LR_FACTOR: 1.0\n",
            "  CHECKPOINT_PERIOD: 2000\n",
            "  CLIP_GRADIENTS:\n",
            "    CLIP_TYPE: value\n",
            "    CLIP_VALUE: 35.0\n",
            "    ENABLED: True\n",
            "    NORM_TYPE: 2.0\n",
            "  GAMMA: 0.1\n",
            "  IMS_PER_BATCH: 3\n",
            "  LR_SCHEDULER_NAME: WarmupMultiStepLR\n",
            "  MAX_ITER: 60000\n",
            "  MOMENTUM: 0.9\n",
            "  NESTEROV: False\n",
            "  REFERENCE_WORLD_SIZE: 0\n",
            "  STEPS: (15000, 20000)\n",
            "  WARMUP_FACTOR: 0.001\n",
            "  WARMUP_ITERS: 1000\n",
            "  WARMUP_METHOD: linear\n",
            "  WEIGHT_DECAY: 0.0001\n",
            "  WEIGHT_DECAY_BIAS: 0.0001\n",
            "  WEIGHT_DECAY_NORM: 0.0\n",
            "TEST:\n",
            "  AUG:\n",
            "    ENABLED: False\n",
            "    FLIP: True\n",
            "    MAX_SIZE: 4000\n",
            "    MIN_SIZES: (400, 500, 600, 700, 800, 900, 1000, 1100, 1200)\n",
            "  DETECTIONS_PER_IMAGE: 500\n",
            "  EVAL_PERIOD: 0\n",
            "  EXPECTED_RESULTS: []\n",
            "  KEYPOINT_OKS_SIGMAS: []\n",
            "  PRECISE_BN:\n",
            "    ENABLED: False\n",
            "    NUM_ITER: 200\n",
            "VERSION: 2\n",
            "VISDRONE:\n",
            "  MAX_LENGTH: 1999\n",
            "  SHORT_LENGTH: [1200]\n",
            "  TEST_IMG_ROOT: data/visdrone/coco_format/val_images\n",
            "  TEST_JSON: data/visdrone/coco_format/annotations/val_label.json\n",
            "  TEST_LENGTH: 3999\n",
            "  TRAIN_JSON: data/visdrone/coco_format/annotations/train_label.json\n",
            "  TRING_IMG_ROOT: data//visdrone/coco_format/train_images\n",
            "VIS_PERIOD: 0\n",
            "\u001b[32m[04/15 07:54:59 detectron2]: \u001b[0mFull config saved to work_dirs/visdrone_querydet/config.yaml\n",
            "\u001b[32m[04/15 07:54:59 d2.utils.env]: \u001b[0mUsing a generated random seed 59788880\n",
            "\u001b[32m[04/15 07:55:05 d2.engine.defaults]: \u001b[0mModel:\n",
            "RetinaNetQueryDet(\n",
            "  (backbone): FPN(\n",
            "    (fpn_lateral2): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1))\n",
            "    (fpn_output2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (fpn_lateral3): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1))\n",
            "    (fpn_output3): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (fpn_lateral4): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1))\n",
            "    (fpn_output4): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (fpn_lateral5): Conv2d(2048, 256, kernel_size=(1, 1), stride=(1, 1))\n",
            "    (fpn_output5): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (top_block): LastLevelP6P7(\n",
            "      (p6): Conv2d(2048, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
            "      (p7): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
            "    )\n",
            "    (bottom_up): ResNet(\n",
            "      (stem): BasicStem(\n",
            "        (conv1): Conv2d(\n",
            "          3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)\n",
            "        )\n",
            "      )\n",
            "      (res2): Sequential(\n",
            "        (0): BottleneckBlock(\n",
            "          (shortcut): Conv2d(\n",
            "            64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
            "          )\n",
            "          (conv1): Conv2d(\n",
            "            64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)\n",
            "          )\n",
            "          (conv2): Conv2d(\n",
            "            64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)\n",
            "          )\n",
            "          (conv3): Conv2d(\n",
            "            64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
            "          )\n",
            "        )\n",
            "        (1): BottleneckBlock(\n",
            "          (conv1): Conv2d(\n",
            "            256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)\n",
            "          )\n",
            "          (conv2): Conv2d(\n",
            "            64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)\n",
            "          )\n",
            "          (conv3): Conv2d(\n",
            "            64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
            "          )\n",
            "        )\n",
            "        (2): BottleneckBlock(\n",
            "          (conv1): Conv2d(\n",
            "            256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)\n",
            "          )\n",
            "          (conv2): Conv2d(\n",
            "            64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)\n",
            "          )\n",
            "          (conv3): Conv2d(\n",
            "            64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
            "          )\n",
            "        )\n",
            "      )\n",
            "      (res3): Sequential(\n",
            "        (0): BottleneckBlock(\n",
            "          (shortcut): Conv2d(\n",
            "            256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
            "          )\n",
            "          (conv1): Conv2d(\n",
            "            256, 128, kernel_size=(1, 1), stride=(2, 2), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)\n",
            "          )\n",
            "          (conv2): Conv2d(\n",
            "            128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)\n",
            "          )\n",
            "          (conv3): Conv2d(\n",
            "            128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
            "          )\n",
            "        )\n",
            "        (1): BottleneckBlock(\n",
            "          (conv1): Conv2d(\n",
            "            512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)\n",
            "          )\n",
            "          (conv2): Conv2d(\n",
            "            128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)\n",
            "          )\n",
            "          (conv3): Conv2d(\n",
            "            128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
            "          )\n",
            "        )\n",
            "        (2): BottleneckBlock(\n",
            "          (conv1): Conv2d(\n",
            "            512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)\n",
            "          )\n",
            "          (conv2): Conv2d(\n",
            "            128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)\n",
            "          )\n",
            "          (conv3): Conv2d(\n",
            "            128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
            "          )\n",
            "        )\n",
            "        (3): BottleneckBlock(\n",
            "          (conv1): Conv2d(\n",
            "            512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)\n",
            "          )\n",
            "          (conv2): Conv2d(\n",
            "            128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)\n",
            "          )\n",
            "          (conv3): Conv2d(\n",
            "            128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
            "          )\n",
            "        )\n",
            "      )\n",
            "      (res4): Sequential(\n",
            "        (0): BottleneckBlock(\n",
            "          (shortcut): Conv2d(\n",
            "            512, 1024, kernel_size=(1, 1), stride=(2, 2), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n",
            "          )\n",
            "          (conv1): Conv2d(\n",
            "            512, 256, kernel_size=(1, 1), stride=(2, 2), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
            "          )\n",
            "          (conv2): Conv2d(\n",
            "            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
            "          )\n",
            "          (conv3): Conv2d(\n",
            "            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n",
            "          )\n",
            "        )\n",
            "        (1): BottleneckBlock(\n",
            "          (conv1): Conv2d(\n",
            "            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
            "          )\n",
            "          (conv2): Conv2d(\n",
            "            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
            "          )\n",
            "          (conv3): Conv2d(\n",
            "            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n",
            "          )\n",
            "        )\n",
            "        (2): BottleneckBlock(\n",
            "          (conv1): Conv2d(\n",
            "            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
            "          )\n",
            "          (conv2): Conv2d(\n",
            "            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
            "          )\n",
            "          (conv3): Conv2d(\n",
            "            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n",
            "          )\n",
            "        )\n",
            "        (3): BottleneckBlock(\n",
            "          (conv1): Conv2d(\n",
            "            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
            "          )\n",
            "          (conv2): Conv2d(\n",
            "            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
            "          )\n",
            "          (conv3): Conv2d(\n",
            "            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n",
            "          )\n",
            "        )\n",
            "        (4): BottleneckBlock(\n",
            "          (conv1): Conv2d(\n",
            "            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
            "          )\n",
            "          (conv2): Conv2d(\n",
            "            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
            "          )\n",
            "          (conv3): Conv2d(\n",
            "            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n",
            "          )\n",
            "        )\n",
            "        (5): BottleneckBlock(\n",
            "          (conv1): Conv2d(\n",
            "            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
            "          )\n",
            "          (conv2): Conv2d(\n",
            "            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
            "          )\n",
            "          (conv3): Conv2d(\n",
            "            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n",
            "          )\n",
            "        )\n",
            "      )\n",
            "      (res5): Sequential(\n",
            "        (0): BottleneckBlock(\n",
            "          (shortcut): Conv2d(\n",
            "            1024, 2048, kernel_size=(1, 1), stride=(2, 2), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=2048, eps=1e-05)\n",
            "          )\n",
            "          (conv1): Conv2d(\n",
            "            1024, 512, kernel_size=(1, 1), stride=(2, 2), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
            "          )\n",
            "          (conv2): Conv2d(\n",
            "            512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
            "          )\n",
            "          (conv3): Conv2d(\n",
            "            512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=2048, eps=1e-05)\n",
            "          )\n",
            "        )\n",
            "        (1): BottleneckBlock(\n",
            "          (conv1): Conv2d(\n",
            "            2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
            "          )\n",
            "          (conv2): Conv2d(\n",
            "            512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
            "          )\n",
            "          (conv3): Conv2d(\n",
            "            512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=2048, eps=1e-05)\n",
            "          )\n",
            "        )\n",
            "        (2): BottleneckBlock(\n",
            "          (conv1): Conv2d(\n",
            "            2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
            "          )\n",
            "          (conv2): Conv2d(\n",
            "            512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
            "          )\n",
            "          (conv3): Conv2d(\n",
            "            512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=2048, eps=1e-05)\n",
            "          )\n",
            "        )\n",
            "      )\n",
            "    )\n",
            "  )\n",
            "  (det_head): RetinaNetHead_3x3(\n",
            "    (cls_layer_0): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (bbox_layer_0): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (cls_layer_1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (bbox_layer_1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (cls_layer_2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (bbox_layer_2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (cls_layer_3): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (bbox_layer_3): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (cls_score): Conv2d(256, 90, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (bbox_pred): Conv2d(256, 36, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "  )\n",
            "  (query_head): Head_3x3(\n",
            "    (layer_0): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (layer_1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (layer_2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (layer_3): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (pred_net): Conv2d(256, 1, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "  )\n",
            "  (anchor_generator): AnchorGeneratorWithCenter(\n",
            "    (cell_anchors): BufferList()\n",
            "  )\n",
            "  (query_anchor_generator): AnchorGeneratorWithCenter(\n",
            "    (cell_anchors): BufferList()\n",
            "  )\n",
            ")\n",
            "\u001b[32m[04/15 07:55:05 fvcore.common.checkpoint]: \u001b[0mLoading checkpoint from /content/gdrive/MyDrive/QueryDet-PyTorch/work_dirs/visdrone_querydet/model_0005999.pth\n",
            "\u001b[32m[04/15 07:55:15 d2.data.common]: \u001b[0mSerializing 25884 elements to byte tensors and concatenating them all ...\n",
            "\u001b[32m[04/15 07:55:16 d2.data.common]: \u001b[0mSerialized dataset takes 15.78 MiB\n",
            "/usr/local/lib/python3.7/site-packages/torch/utils/data/dataloader.py:477: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  cpuset_checked))\n",
            "\u001b[32m[04/15 07:55:18 d2.engine.train_loop]: \u001b[0mStarting training from iteration 0\n",
            "\u001b[32m[04/15 07:56:15 d2.utils.events]: \u001b[0m eta: 1 day, 12:11:37  iter: 19  total_loss: 0.8142  loss_cls: 0.5384  loss_box_reg: 0.3331  loss_query: 0.02639  time: 2.3439  data_time: 0.0839  lr: 0.00011989  max_mem: 10555M\n",
            "\u001b[32m[04/15 07:57:01 d2.utils.events]: \u001b[0m eta: 1 day, 12:28:42  iter: 39  total_loss: 0.4689  loss_cls: 0.3334  loss_box_reg: 0.1439  loss_query: 0.02255  time: 2.2693  data_time: 0.0069  lr: 0.00023977  max_mem: 10555M\n",
            "\u001b[32m[04/15 07:57:50 d2.utils.events]: \u001b[0m eta: 1 day, 13:42:50  iter: 59  total_loss: 0.4931  loss_cls: 0.288  loss_box_reg: 0.1789  loss_query: 0.03067  time: 2.3144  data_time: 0.0070  lr: 0.00035965  max_mem: 10555M\n",
            "\u001b[32m[04/15 07:58:37 d2.utils.events]: \u001b[0m eta: 1 day, 14:02:52  iter: 79  total_loss: 0.5977  loss_cls: 0.3357  loss_box_reg: 0.2174  loss_query: 0.03485  time: 2.3091  data_time: 0.0063  lr: 0.00047953  max_mem: 10555M\n",
            "\u001b[32m[04/15 07:59:25 d2.utils.events]: \u001b[0m eta: 1 day, 14:08:07  iter: 99  total_loss: 0.5618  loss_cls: 0.3388  loss_box_reg: 0.1963  loss_query: 0.02227  time: 2.3098  data_time: 0.0068  lr: 0.00059941  max_mem: 10555M\n",
            "\u001b[32m[04/15 08:00:12 d2.utils.events]: \u001b[0m eta: 1 day, 14:07:56  iter: 119  total_loss: 0.5643  loss_cls: 0.3258  loss_box_reg: 0.2011  loss_query: 0.0181  time: 2.3061  data_time: 0.0071  lr: 0.00071929  max_mem: 10555M\n",
            "\u001b[32m[04/15 08:00:58 d2.utils.events]: \u001b[0m eta: 1 day, 14:09:48  iter: 139  total_loss: 0.4876  loss_cls: 0.2875  loss_box_reg: 0.1555  loss_query: 0.01873  time: 2.2966  data_time: 0.0055  lr: 0.00083917  max_mem: 10555M\n",
            "\u001b[32m[04/15 08:01:45 d2.utils.events]: \u001b[0m eta: 1 day, 14:11:58  iter: 159  total_loss: 0.6197  loss_cls: 0.4006  loss_box_reg: 0.2167  loss_query: 0.03062  time: 2.2965  data_time: 0.0074  lr: 0.00095905  max_mem: 10555M\n",
            "\u001b[32m[04/15 08:02:31 d2.utils.events]: \u001b[0m eta: 1 day, 14:08:16  iter: 179  total_loss: 0.539  loss_cls: 0.336  loss_box_reg: 0.1852  loss_query: 0.02902  time: 2.2919  data_time: 0.0060  lr: 0.0010789  max_mem: 10555M\n",
            "\u001b[32m[04/15 08:03:17 d2.utils.events]: \u001b[0m eta: 1 day, 14:07:30  iter: 199  total_loss: 0.7498  loss_cls: 0.4062  loss_box_reg: 0.2654  loss_query: 0.03466  time: 2.2882  data_time: 0.0079  lr: 0.0011988  max_mem: 10555M\n",
            "\u001b[32m[04/15 08:04:05 d2.utils.events]: \u001b[0m eta: 1 day, 14:10:35  iter: 219  total_loss: 0.5399  loss_cls: 0.321  loss_box_reg: 0.1831  loss_query: 0.02404  time: 2.2912  data_time: 0.0071  lr: 0.0013187  max_mem: 10555M\n",
            "\u001b[32m[04/15 08:04:52 d2.utils.events]: \u001b[0m eta: 1 day, 14:10:19  iter: 239  total_loss: 0.5024  loss_cls: 0.3003  loss_box_reg: 0.1724  loss_query: 0.02973  time: 2.2899  data_time: 0.0057  lr: 0.0014386  max_mem: 10555M\n",
            "\u001b[32m[04/15 08:05:38 d2.utils.events]: \u001b[0m eta: 1 day, 14:09:03  iter: 259  total_loss: 0.7623  loss_cls: 0.4432  loss_box_reg: 0.2844  loss_query: 0.0323  time: 2.2870  data_time: 0.0069  lr: 0.0015584  max_mem: 10555M\n",
            "\u001b[32m[04/15 08:06:25 d2.utils.events]: \u001b[0m eta: 1 day, 14:10:25  iter: 279  total_loss: 0.5525  loss_cls: 0.3028  loss_box_reg: 0.205  loss_query: 0.03513  time: 2.2893  data_time: 0.0083  lr: 0.0016783  max_mem: 10555M\n",
            "\u001b[32m[04/15 08:07:12 d2.utils.events]: \u001b[0m eta: 1 day, 14:10:10  iter: 299  total_loss: 0.6015  loss_cls: 0.3536  loss_box_reg: 0.227  loss_query: 0.02985  time: 2.2879  data_time: 0.0090  lr: 0.0017982  max_mem: 10555M\n",
            "\u001b[32m[04/15 08:07:58 d2.utils.events]: \u001b[0m eta: 1 day, 14:11:15  iter: 319  total_loss: 0.5089  loss_cls: 0.312  loss_box_reg: 0.1987  loss_query: 0.03378  time: 2.2857  data_time: 0.0072  lr: 0.0019181  max_mem: 10555M\n",
            "\u001b[32m[04/15 08:08:45 d2.utils.events]: \u001b[0m eta: 1 day, 14:10:32  iter: 339  total_loss: 0.517  loss_cls: 0.302  loss_box_reg: 0.1779  loss_query: 0.02848  time: 2.2853  data_time: 0.0055  lr: 0.002038  max_mem: 10555M\n",
            "\u001b[32m[04/15 08:09:32 d2.utils.events]: \u001b[0m eta: 1 day, 14:10:47  iter: 359  total_loss: 0.4533  loss_cls: 0.2733  loss_box_reg: 0.174  loss_query: 0.02155  time: 2.2850  data_time: 0.0067  lr: 0.0021578  max_mem: 10555M\n",
            "\u001b[32m[04/15 08:10:19 d2.utils.events]: \u001b[0m eta: 1 day, 14:09:50  iter: 379  total_loss: 0.6784  loss_cls: 0.4208  loss_box_reg: 0.2347  loss_query: 0.02874  time: 2.2871  data_time: 0.0062  lr: 0.0022777  max_mem: 10555M\n",
            "\u001b[32m[04/15 08:11:06 d2.utils.events]: \u001b[0m eta: 1 day, 14:08:46  iter: 399  total_loss: 0.5454  loss_cls: 0.3244  loss_box_reg: 0.2087  loss_query: 0.03037  time: 2.2864  data_time: 0.0083  lr: 0.0023976  max_mem: 10555M\n",
            "\u001b[32m[04/15 08:11:52 d2.utils.events]: \u001b[0m eta: 1 day, 14:08:28  iter: 419  total_loss: 0.5463  loss_cls: 0.3356  loss_box_reg: 0.2101  loss_query: 0.03059  time: 2.2857  data_time: 0.0063  lr: 0.0025175  max_mem: 10555M\n",
            "\u001b[32m[04/15 08:12:39 d2.utils.events]: \u001b[0m eta: 1 day, 14:07:42  iter: 439  total_loss: 0.6613  loss_cls: 0.4009  loss_box_reg: 0.2328  loss_query: 0.03498  time: 2.2845  data_time: 0.0066  lr: 0.0026374  max_mem: 10555M\n",
            "\u001b[32m[04/15 08:13:26 d2.utils.events]: \u001b[0m eta: 1 day, 14:06:46  iter: 459  total_loss: 0.6548  loss_cls: 0.3302  loss_box_reg: 0.2386  loss_query: 0.03548  time: 2.2840  data_time: 0.0068  lr: 0.0027572  max_mem: 10555M\n",
            "\u001b[32m[04/15 08:14:12 d2.utils.events]: \u001b[0m eta: 1 day, 14:05:18  iter: 479  total_loss: 0.6147  loss_cls: 0.3633  loss_box_reg: 0.2187  loss_query: 0.03021  time: 2.2839  data_time: 0.0071  lr: 0.0028771  max_mem: 10555M\n",
            "\u001b[32m[04/15 08:14:59 d2.utils.events]: \u001b[0m eta: 1 day, 14:04:55  iter: 499  total_loss: 0.3976  loss_cls: 0.237  loss_box_reg: 0.1511  loss_query: 0.02281  time: 2.2832  data_time: 0.0078  lr: 0.002997  max_mem: 10555M\n",
            "\u001b[32m[04/15 08:15:46 d2.utils.events]: \u001b[0m eta: 1 day, 14:04:28  iter: 519  total_loss: 0.4548  loss_cls: 0.2571  loss_box_reg: 0.155  loss_query: 0.0292  time: 2.2837  data_time: 0.0074  lr: 0.0031169  max_mem: 10555M\n",
            "\u001b[32m[04/15 08:16:31 d2.utils.events]: \u001b[0m eta: 1 day, 14:02:11  iter: 539  total_loss: 0.5164  loss_cls: 0.3215  loss_box_reg: 0.1664  loss_query: 0.02939  time: 2.2799  data_time: 0.0073  lr: 0.0032368  max_mem: 10555M\n",
            "\u001b[32m[04/15 08:17:19 d2.utils.events]: \u001b[0m eta: 1 day, 14:02:37  iter: 559  total_loss: 0.606  loss_cls: 0.3247  loss_box_reg: 0.2255  loss_query: 0.03487  time: 2.2818  data_time: 0.0068  lr: 0.0033566  max_mem: 10555M\n",
            "\u001b[32m[04/15 08:18:05 d2.utils.events]: \u001b[0m eta: 1 day, 14:01:28  iter: 579  total_loss: 0.6157  loss_cls: 0.3623  loss_box_reg: 0.2336  loss_query: 0.03797  time: 2.2806  data_time: 0.0062  lr: 0.0034765  max_mem: 10555M\n",
            "\u001b[32m[04/15 08:18:52 d2.utils.events]: \u001b[0m eta: 1 day, 14:01:23  iter: 599  total_loss: 0.437  loss_cls: 0.2718  loss_box_reg: 0.1343  loss_query: 0.02928  time: 2.2811  data_time: 0.0071  lr: 0.0035964  max_mem: 10555M\n",
            "\u001b[32m[04/15 08:19:37 d2.utils.events]: \u001b[0m eta: 1 day, 14:00:40  iter: 619  total_loss: 0.6031  loss_cls: 0.3401  loss_box_reg: 0.2299  loss_query: 0.03038  time: 2.2788  data_time: 0.0071  lr: 0.0037163  max_mem: 10555M\n",
            "\u001b[32m[04/15 08:20:23 d2.utils.events]: \u001b[0m eta: 1 day, 13:59:54  iter: 639  total_loss: 0.6327  loss_cls: 0.3588  loss_box_reg: 0.2058  loss_query: 0.02712  time: 2.2774  data_time: 0.0074  lr: 0.0038362  max_mem: 10555M\n",
            "\u001b[32m[04/15 08:21:10 d2.utils.events]: \u001b[0m eta: 1 day, 13:59:18  iter: 659  total_loss: 0.5514  loss_cls: 0.3386  loss_box_reg: 0.1801  loss_query: 0.0247  time: 2.2783  data_time: 0.0064  lr: 0.003956  max_mem: 10555M\n",
            "\u001b[32m[04/15 08:21:57 d2.utils.events]: \u001b[0m eta: 1 day, 13:58:32  iter: 679  total_loss: 0.5695  loss_cls: 0.3422  loss_box_reg: 0.1834  loss_query: 0.02782  time: 2.2782  data_time: 0.0062  lr: 0.0040759  max_mem: 10555M\n",
            "\u001b[32m[04/15 08:22:45 d2.utils.events]: \u001b[0m eta: 1 day, 13:57:46  iter: 699  total_loss: 0.4128  loss_cls: 0.2476  loss_box_reg: 0.1447  loss_query: 0.02874  time: 2.2799  data_time: 0.0066  lr: 0.0041958  max_mem: 10555M\n",
            "\u001b[32m[04/15 08:23:32 d2.utils.events]: \u001b[0m eta: 1 day, 13:56:49  iter: 719  total_loss: 0.4318  loss_cls: 0.2591  loss_box_reg: 0.1476  loss_query: 0.02439  time: 2.2799  data_time: 0.0072  lr: 0.0043157  max_mem: 10555M\n",
            "\u001b[32m[04/15 08:24:18 d2.utils.events]: \u001b[0m eta: 1 day, 13:56:44  iter: 739  total_loss: 0.8753  loss_cls: 0.4862  loss_box_reg: 0.3024  loss_query: 0.04189  time: 2.2800  data_time: 0.0076  lr: 0.0044356  max_mem: 10555M\n",
            "\u001b[32m[04/15 08:25:04 d2.utils.events]: \u001b[0m eta: 1 day, 13:55:58  iter: 759  total_loss: 0.4896  loss_cls: 0.2867  loss_box_reg: 0.1712  loss_query: 0.02532  time: 2.2789  data_time: 0.0077  lr: 0.0045554  max_mem: 10555M\n",
            "\u001b[32m[04/15 08:25:51 d2.utils.events]: \u001b[0m eta: 1 day, 13:54:55  iter: 779  total_loss: 0.5409  loss_cls: 0.3336  loss_box_reg: 0.2067  loss_query: 0.04363  time: 2.2785  data_time: 0.0072  lr: 0.0046753  max_mem: 10555M\n",
            "\u001b[32m[04/15 08:26:39 d2.utils.events]: \u001b[0m eta: 1 day, 13:55:02  iter: 799  total_loss: 0.5798  loss_cls: 0.3423  loss_box_reg: 0.206  loss_query: 0.03726  time: 2.2802  data_time: 0.0072  lr: 0.0047952  max_mem: 10555M\n",
            "\u001b[32m[04/15 08:27:25 d2.utils.events]: \u001b[0m eta: 1 day, 13:54:38  iter: 819  total_loss: 0.599  loss_cls: 0.3321  loss_box_reg: 0.2177  loss_query: 0.02527  time: 2.2796  data_time: 0.0068  lr: 0.0049151  max_mem: 10555M\n",
            "\u001b[32m[04/15 08:28:12 d2.utils.events]: \u001b[0m eta: 1 day, 13:54:28  iter: 839  total_loss: 0.8054  loss_cls: 0.448  loss_box_reg: 0.2991  loss_query: 0.0469  time: 2.2797  data_time: 0.0086  lr: 0.005035  max_mem: 10555M\n",
            "\u001b[32m[04/15 08:29:01 d2.utils.events]: \u001b[0m eta: 1 day, 13:55:11  iter: 859  total_loss: 0.5393  loss_cls: 0.2744  loss_box_reg: 0.2008  loss_query: 0.03754  time: 2.2817  data_time: 0.0075  lr: 0.0051548  max_mem: 10555M\n",
            "\u001b[32m[04/15 08:29:48 d2.utils.events]: \u001b[0m eta: 1 day, 13:54:51  iter: 879  total_loss: 0.6099  loss_cls: 0.3342  loss_box_reg: 0.2115  loss_query: 0.03444  time: 2.2821  data_time: 0.0074  lr: 0.0052747  max_mem: 10555M\n",
            "\u001b[32m[04/15 08:30:34 d2.utils.events]: \u001b[0m eta: 1 day, 13:53:52  iter: 899  total_loss: 0.5492  loss_cls: 0.3225  loss_box_reg: 0.1838  loss_query: 0.03375  time: 2.2809  data_time: 0.0083  lr: 0.0053946  max_mem: 10555M\n",
            "\u001b[32m[04/15 08:31:21 d2.utils.events]: \u001b[0m eta: 1 day, 13:53:15  iter: 919  total_loss: 0.6077  loss_cls: 0.3828  loss_box_reg: 0.1745  loss_query: 0.02319  time: 2.2818  data_time: 0.0067  lr: 0.0055145  max_mem: 10555M\n",
            "\u001b[32m[04/15 08:32:07 d2.utils.events]: \u001b[0m eta: 1 day, 13:51:44  iter: 939  total_loss: 0.771  loss_cls: 0.428  loss_box_reg: 0.3184  loss_query: 0.02686  time: 2.2811  data_time: 0.0083  lr: 0.0056344  max_mem: 10555M\n",
            "\u001b[32m[04/15 08:32:54 d2.utils.events]: \u001b[0m eta: 1 day, 13:50:40  iter: 959  total_loss: 0.6388  loss_cls: 0.3807  loss_box_reg: 0.2164  loss_query: 0.03229  time: 2.2808  data_time: 0.0068  lr: 0.0057542  max_mem: 10555M\n",
            "\u001b[32m[04/15 08:33:41 d2.utils.events]: \u001b[0m eta: 1 day, 13:50:57  iter: 979  total_loss: 0.6279  loss_cls: 0.356  loss_box_reg: 0.2236  loss_query: 0.03841  time: 2.2817  data_time: 0.0067  lr: 0.0058741  max_mem: 10555M\n",
            "\u001b[32m[04/15 08:34:29 d2.utils.events]: \u001b[0m eta: 1 day, 13:50:20  iter: 999  total_loss: 0.5939  loss_cls: 0.321  loss_box_reg: 0.2283  loss_query: 0.03253  time: 2.2828  data_time: 0.0083  lr: 0.005994  max_mem: 10555M\n",
            "\u001b[32m[04/15 08:35:17 d2.utils.events]: \u001b[0m eta: 1 day, 13:51:19  iter: 1019  total_loss: 0.5831  loss_cls: 0.34  loss_box_reg: 0.2075  loss_query: 0.03195  time: 2.2835  data_time: 0.0081  lr: 0.006  max_mem: 10555M\n",
            "\u001b[32m[04/15 08:36:05 d2.utils.events]: \u001b[0m eta: 1 day, 13:51:39  iter: 1039  total_loss: 0.4761  loss_cls: 0.2698  loss_box_reg: 0.1785  loss_query: 0.03217  time: 2.2847  data_time: 0.0084  lr: 0.006  max_mem: 10555M\n",
            "\u001b[32m[04/15 08:36:51 d2.utils.events]: \u001b[0m eta: 1 day, 13:50:41  iter: 1059  total_loss: 0.6121  loss_cls: 0.3469  loss_box_reg: 0.2209  loss_query: 0.03443  time: 2.2845  data_time: 0.0068  lr: 0.006  max_mem: 10555M\n",
            "\u001b[32m[04/15 08:37:38 d2.utils.events]: \u001b[0m eta: 1 day, 13:49:55  iter: 1079  total_loss: 0.5322  loss_cls: 0.2566  loss_box_reg: 0.2249  loss_query: 0.03048  time: 2.2848  data_time: 0.0065  lr: 0.006  max_mem: 10555M\n",
            "\u001b[32m[04/15 08:38:26 d2.utils.events]: \u001b[0m eta: 1 day, 13:48:35  iter: 1099  total_loss: 0.579  loss_cls: 0.3011  loss_box_reg: 0.2084  loss_query: 0.03837  time: 2.2856  data_time: 0.0085  lr: 0.006  max_mem: 10555M\n",
            "\u001b[32m[04/15 08:39:13 d2.utils.events]: \u001b[0m eta: 1 day, 13:47:41  iter: 1119  total_loss: 0.4372  loss_cls: 0.2645  loss_box_reg: 0.1571  loss_query: 0.01993  time: 2.2853  data_time: 0.0067  lr: 0.006  max_mem: 10555M\n",
            "\u001b[32m[04/15 08:40:01 d2.utils.events]: \u001b[0m eta: 1 day, 13:48:05  iter: 1139  total_loss: 0.534  loss_cls: 0.2989  loss_box_reg: 0.2295  loss_query: 0.02755  time: 2.2865  data_time: 0.0065  lr: 0.006  max_mem: 10555M\n",
            "\u001b[32m[04/15 08:40:48 d2.utils.events]: \u001b[0m eta: 1 day, 13:46:59  iter: 1159  total_loss: 0.5327  loss_cls: 0.3266  loss_box_reg: 0.1939  loss_query: 0.01961  time: 2.2868  data_time: 0.0074  lr: 0.006  max_mem: 10555M\n",
            "\u001b[32m[04/15 08:41:34 d2.utils.events]: \u001b[0m eta: 1 day, 13:46:12  iter: 1179  total_loss: 0.4857  loss_cls: 0.3139  loss_box_reg: 0.175  loss_query: 0.02219  time: 2.2860  data_time: 0.0081  lr: 0.006  max_mem: 10555M\n",
            "\u001b[32m[04/15 08:42:21 d2.utils.events]: \u001b[0m eta: 1 day, 13:45:40  iter: 1199  total_loss: 0.7471  loss_cls: 0.4529  loss_box_reg: 0.2485  loss_query: 0.03438  time: 2.2861  data_time: 0.0088  lr: 0.006  max_mem: 10555M\n",
            "\u001b[32m[04/15 08:43:06 d2.utils.events]: \u001b[0m eta: 1 day, 13:44:04  iter: 1219  total_loss: 0.4855  loss_cls: 0.2856  loss_box_reg: 0.1672  loss_query: 0.02479  time: 2.2849  data_time: 0.0068  lr: 0.006  max_mem: 10555M\n",
            "\u001b[32m[04/15 08:43:53 d2.utils.events]: \u001b[0m eta: 1 day, 13:42:53  iter: 1239  total_loss: 0.59  loss_cls: 0.3288  loss_box_reg: 0.2103  loss_query: 0.03391  time: 2.2845  data_time: 0.0068  lr: 0.006  max_mem: 10555M\n",
            "\u001b[32m[04/15 08:44:37 d2.utils.events]: \u001b[0m eta: 1 day, 13:41:50  iter: 1259  total_loss: 0.373  loss_cls: 0.2452  loss_box_reg: 0.1373  loss_query: 0.02464  time: 2.2827  data_time: 0.0073  lr: 0.006  max_mem: 10555M\n",
            "\u001b[32m[04/15 08:45:22 d2.utils.events]: \u001b[0m eta: 1 day, 13:40:03  iter: 1279  total_loss: 0.6959  loss_cls: 0.4058  loss_box_reg: 0.2624  loss_query: 0.01956  time: 2.2812  data_time: 0.0068  lr: 0.006  max_mem: 10555M\n",
            "\u001b[32m[04/15 08:46:09 d2.utils.events]: \u001b[0m eta: 1 day, 13:38:54  iter: 1299  total_loss: 0.538  loss_cls: 0.3779  loss_box_reg: 0.1749  loss_query: 0.01595  time: 2.2812  data_time: 0.0079  lr: 0.006  max_mem: 10555M\n",
            "\u001b[32m[04/15 08:46:54 d2.utils.events]: \u001b[0m eta: 1 day, 13:37:29  iter: 1319  total_loss: 0.6842  loss_cls: 0.416  loss_box_reg: 0.221  loss_query: 0.02898  time: 2.2797  data_time: 0.0078  lr: 0.006  max_mem: 10555M\n",
            "\u001b[32m[04/15 08:47:40 d2.utils.events]: \u001b[0m eta: 1 day, 13:37:07  iter: 1339  total_loss: 0.7163  loss_cls: 0.3968  loss_box_reg: 0.3025  loss_query: 0.03554  time: 2.2797  data_time: 0.0085  lr: 0.006  max_mem: 10555M\n",
            "\u001b[32m[04/15 08:48:26 d2.utils.events]: \u001b[0m eta: 1 day, 13:36:21  iter: 1359  total_loss: 0.6888  loss_cls: 0.425  loss_box_reg: 0.226  loss_query: 0.03078  time: 2.2792  data_time: 0.0070  lr: 0.006  max_mem: 10555M\n",
            "\u001b[32m[04/15 08:49:12 d2.utils.events]: \u001b[0m eta: 1 day, 13:35:11  iter: 1379  total_loss: 0.5694  loss_cls: 0.3268  loss_box_reg: 0.1988  loss_query: 0.02351  time: 2.2784  data_time: 0.0082  lr: 0.006  max_mem: 10555M\n",
            "\u001b[32m[04/15 08:49:58 d2.utils.events]: \u001b[0m eta: 1 day, 13:34:25  iter: 1399  total_loss: 0.5087  loss_cls: 0.311  loss_box_reg: 0.1876  loss_query: 0.0273  time: 2.2777  data_time: 0.0077  lr: 0.006  max_mem: 10555M\n",
            "\u001b[32m[04/15 08:50:44 d2.utils.events]: \u001b[0m eta: 1 day, 13:33:26  iter: 1419  total_loss: 0.6981  loss_cls: 0.4035  loss_box_reg: 0.2565  loss_query: 0.04398  time: 2.2772  data_time: 0.0068  lr: 0.006  max_mem: 10555M\n",
            "\u001b[32m[04/15 08:51:31 d2.utils.events]: \u001b[0m eta: 1 day, 13:32:40  iter: 1439  total_loss: 0.544  loss_cls: 0.3517  loss_box_reg: 0.1661  loss_query: 0.03445  time: 2.2776  data_time: 0.0076  lr: 0.006  max_mem: 10555M\n",
            "\u001b[32m[04/15 08:52:18 d2.utils.events]: \u001b[0m eta: 1 day, 13:32:30  iter: 1459  total_loss: 0.8563  loss_cls: 0.487  loss_box_reg: 0.2777  loss_query: 0.04257  time: 2.2782  data_time: 0.0076  lr: 0.006  max_mem: 10555M\n",
            "\u001b[32m[04/15 08:53:05 d2.utils.events]: \u001b[0m eta: 1 day, 13:33:00  iter: 1479  total_loss: 0.4821  loss_cls: 0.2873  loss_box_reg: 0.1757  loss_query: 0.02881  time: 2.2781  data_time: 0.0072  lr: 0.006  max_mem: 10555M\n",
            "\u001b[32m[04/15 08:53:52 d2.utils.events]: \u001b[0m eta: 1 day, 13:32:27  iter: 1499  total_loss: 0.5844  loss_cls: 0.3522  loss_box_reg: 0.2106  loss_query: 0.02669  time: 2.2783  data_time: 0.0082  lr: 0.006  max_mem: 10555M\n",
            "\u001b[32m[04/15 08:54:38 d2.utils.events]: \u001b[0m eta: 1 day, 13:31:44  iter: 1519  total_loss: 0.559  loss_cls: 0.3442  loss_box_reg: 0.1811  loss_query: 0.02821  time: 2.2781  data_time: 0.0065  lr: 0.006  max_mem: 10555M\n",
            "\u001b[32m[04/15 08:55:24 d2.utils.events]: \u001b[0m eta: 1 day, 13:31:31  iter: 1539  total_loss: 0.615  loss_cls: 0.3783  loss_box_reg: 0.2226  loss_query: 0.02874  time: 2.2776  data_time: 0.0075  lr: 0.006  max_mem: 10555M\n",
            "\u001b[32m[04/15 08:56:11 d2.utils.events]: \u001b[0m eta: 1 day, 13:30:11  iter: 1559  total_loss: 0.7355  loss_cls: 0.4545  loss_box_reg: 0.2371  loss_query: 0.02878  time: 2.2780  data_time: 0.0061  lr: 0.006  max_mem: 10555M\n",
            "\u001b[32m[04/15 08:56:58 d2.utils.events]: \u001b[0m eta: 1 day, 13:29:29  iter: 1579  total_loss: 0.5555  loss_cls: 0.3112  loss_box_reg: 0.1783  loss_query: 0.03084  time: 2.2777  data_time: 0.0076  lr: 0.006  max_mem: 10555M\n",
            "\u001b[32m[04/15 08:57:44 d2.utils.events]: \u001b[0m eta: 1 day, 13:27:44  iter: 1599  total_loss: 0.4083  loss_cls: 0.2387  loss_box_reg: 0.119  loss_query: 0.01302  time: 2.2774  data_time: 0.0070  lr: 0.006  max_mem: 10555M\n",
            "\u001b[32m[04/15 08:58:29 d2.utils.events]: \u001b[0m eta: 1 day, 13:26:18  iter: 1619  total_loss: 0.5699  loss_cls: 0.3859  loss_box_reg: 0.1914  loss_query: 0.02952  time: 2.2767  data_time: 0.0056  lr: 0.006  max_mem: 10555M\n",
            "\u001b[32m[04/15 08:59:15 d2.utils.events]: \u001b[0m eta: 1 day, 13:25:32  iter: 1639  total_loss: 0.5326  loss_cls: 0.3282  loss_box_reg: 0.1874  loss_query: 0.02308  time: 2.2759  data_time: 0.0072  lr: 0.006  max_mem: 10555M\n",
            "\u001b[32m[04/15 09:00:01 d2.utils.events]: \u001b[0m eta: 1 day, 13:24:05  iter: 1659  total_loss: 0.6252  loss_cls: 0.3703  loss_box_reg: 0.2108  loss_query: 0.02564  time: 2.2755  data_time: 0.0062  lr: 0.006  max_mem: 10555M\n",
            "\u001b[32m[04/15 09:00:48 d2.utils.events]: \u001b[0m eta: 1 day, 13:23:18  iter: 1679  total_loss: 0.6812  loss_cls: 0.3807  loss_box_reg: 0.2439  loss_query: 0.04112  time: 2.2759  data_time: 0.0082  lr: 0.006  max_mem: 10555M\n",
            "\u001b[32m[04/15 09:01:34 d2.utils.events]: \u001b[0m eta: 1 day, 13:23:13  iter: 1699  total_loss: 0.6433  loss_cls: 0.3522  loss_box_reg: 0.1987  loss_query: 0.0374  time: 2.2754  data_time: 0.0071  lr: 0.006  max_mem: 10555M\n",
            "\u001b[32m[04/15 09:02:22 d2.utils.events]: \u001b[0m eta: 1 day, 13:23:34  iter: 1719  total_loss: 0.5624  loss_cls: 0.3428  loss_box_reg: 0.2087  loss_query: 0.04877  time: 2.2763  data_time: 0.0090  lr: 0.006  max_mem: 10555M\n",
            "\u001b[32m[04/15 09:03:09 d2.utils.events]: \u001b[0m eta: 1 day, 13:22:30  iter: 1739  total_loss: 0.4546  loss_cls: 0.28  loss_box_reg: 0.1457  loss_query: 0.02522  time: 2.2762  data_time: 0.0070  lr: 0.006  max_mem: 10555M\n",
            "\u001b[32m[04/15 09:03:55 d2.utils.events]: \u001b[0m eta: 1 day, 13:21:46  iter: 1759  total_loss: 0.6217  loss_cls: 0.3791  loss_box_reg: 0.1946  loss_query: 0.02698  time: 2.2760  data_time: 0.0077  lr: 0.006  max_mem: 10555M\n",
            "\u001b[32m[04/15 09:04:43 d2.utils.events]: \u001b[0m eta: 1 day, 13:21:48  iter: 1779  total_loss: 0.6853  loss_cls: 0.4055  loss_box_reg: 0.2427  loss_query: 0.04873  time: 2.2766  data_time: 0.0093  lr: 0.006  max_mem: 10555M\n",
            "\u001b[32m[04/15 09:05:29 d2.utils.events]: \u001b[0m eta: 1 day, 13:19:40  iter: 1799  total_loss: 0.4557  loss_cls: 0.2479  loss_box_reg: 0.1925  loss_query: 0.03294  time: 2.2764  data_time: 0.0073  lr: 0.006  max_mem: 10555M\n",
            "\u001b[32m[04/15 09:06:16 d2.utils.events]: \u001b[0m eta: 1 day, 13:18:44  iter: 1819  total_loss: 0.6698  loss_cls: 0.404  loss_box_reg: 0.2517  loss_query: 0.03353  time: 2.2763  data_time: 0.0072  lr: 0.006  max_mem: 10555M\n",
            "\u001b[32m[04/15 09:07:02 d2.utils.events]: \u001b[0m eta: 1 day, 13:17:29  iter: 1839  total_loss: 0.6646  loss_cls: 0.3719  loss_box_reg: 0.2267  loss_query: 0.03263  time: 2.2760  data_time: 0.0079  lr: 0.006  max_mem: 10555M\n",
            "\u001b[32m[04/15 09:07:48 d2.utils.events]: \u001b[0m eta: 1 day, 13:15:48  iter: 1859  total_loss: 0.5154  loss_cls: 0.3084  loss_box_reg: 0.1831  loss_query: 0.02098  time: 2.2758  data_time: 0.0060  lr: 0.006  max_mem: 10555M\n",
            "\u001b[32m[04/15 09:08:34 d2.utils.events]: \u001b[0m eta: 1 day, 13:14:37  iter: 1879  total_loss: 0.6597  loss_cls: 0.3863  loss_box_reg: 0.2419  loss_query: 0.03063  time: 2.2758  data_time: 0.0085  lr: 0.006  max_mem: 10555M\n",
            "\u001b[32m[04/15 09:09:20 d2.utils.events]: \u001b[0m eta: 1 day, 13:14:01  iter: 1899  total_loss: 0.6062  loss_cls: 0.3479  loss_box_reg: 0.2263  loss_query: 0.03256  time: 2.2750  data_time: 0.0060  lr: 0.006  max_mem: 10555M\n",
            "\u001b[32m[04/15 09:10:05 d2.utils.events]: \u001b[0m eta: 1 day, 13:13:05  iter: 1919  total_loss: 0.7201  loss_cls: 0.3844  loss_box_reg: 0.238  loss_query: 0.03636  time: 2.2745  data_time: 0.0064  lr: 0.006  max_mem: 10555M\n",
            "\u001b[32m[04/15 09:10:52 d2.utils.events]: \u001b[0m eta: 1 day, 13:12:57  iter: 1939  total_loss: 0.6074  loss_cls: 0.3346  loss_box_reg: 0.2213  loss_query: 0.0436  time: 2.2744  data_time: 0.0079  lr: 0.006  max_mem: 10555M\n",
            "\u001b[32m[04/15 09:11:38 d2.utils.events]: \u001b[0m eta: 1 day, 13:12:03  iter: 1959  total_loss: 0.4806  loss_cls: 0.275  loss_box_reg: 0.158  loss_query: 0.02766  time: 2.2742  data_time: 0.0078  lr: 0.006  max_mem: 10555M\n",
            "\u001b[32m[04/15 09:12:27 d2.utils.events]: \u001b[0m eta: 1 day, 13:11:24  iter: 1979  total_loss: 0.691  loss_cls: 0.3818  loss_box_reg: 0.2667  loss_query: 0.03395  time: 2.2752  data_time: 0.0073  lr: 0.006  max_mem: 10555M\n",
            "\u001b[32m[04/15 09:13:14 fvcore.common.checkpoint]: \u001b[0mSaving checkpoint to work_dirs/visdrone_querydet/model_0001999.pth\n",
            "\u001b[32m[04/15 09:13:24 d2.utils.events]: \u001b[0m eta: 1 day, 13:08:34  iter: 1999  total_loss: 0.4555  loss_cls: 0.2867  loss_box_reg: 0.158  loss_query: 0.03325  time: 2.2752  data_time: 0.0061  lr: 0.006  max_mem: 10555M\n",
            "\u001b[32m[04/15 09:14:17 d2.utils.events]: \u001b[0m eta: 1 day, 13:07:45  iter: 2019  total_loss: 0.6076  loss_cls: 0.3989  loss_box_reg: 0.2103  loss_query: 0.04194  time: 2.2779  data_time: 0.1297  lr: 0.006  max_mem: 10555M\n",
            "\u001b[32m[04/15 09:15:04 d2.utils.events]: \u001b[0m eta: 1 day, 13:06:59  iter: 2039  total_loss: 0.7352  loss_cls: 0.4487  loss_box_reg: 0.2507  loss_query: 0.03868  time: 2.2776  data_time: 0.0076  lr: 0.006  max_mem: 10555M\n",
            "\u001b[32m[04/15 09:15:51 d2.utils.events]: \u001b[0m eta: 1 day, 13:05:57  iter: 2059  total_loss: 0.4464  loss_cls: 0.247  loss_box_reg: 0.1564  loss_query: 0.02273  time: 2.2779  data_time: 0.0074  lr: 0.006  max_mem: 10555M\n",
            "\u001b[32m[04/15 09:16:38 d2.utils.events]: \u001b[0m eta: 1 day, 13:05:27  iter: 2079  total_loss: 0.8591  loss_cls: 0.4851  loss_box_reg: 0.3061  loss_query: 0.03612  time: 2.2778  data_time: 0.0087  lr: 0.006  max_mem: 10555M\n",
            "\u001b[32m[04/15 09:17:25 d2.utils.events]: \u001b[0m eta: 1 day, 13:03:47  iter: 2099  total_loss: 0.7679  loss_cls: 0.4245  loss_box_reg: 0.2744  loss_query: 0.0399  time: 2.2781  data_time: 0.0080  lr: 0.006  max_mem: 10555M\n",
            "\u001b[32m[04/15 09:18:12 d2.utils.events]: \u001b[0m eta: 1 day, 13:03:46  iter: 2119  total_loss: 0.6375  loss_cls: 0.37  loss_box_reg: 0.2238  loss_query: 0.03111  time: 2.2780  data_time: 0.0064  lr: 0.006  max_mem: 10555M\n",
            "\u001b[32m[04/15 09:18:59 d2.utils.events]: \u001b[0m eta: 1 day, 13:02:53  iter: 2139  total_loss: 0.7634  loss_cls: 0.4414  loss_box_reg: 0.2785  loss_query: 0.03563  time: 2.2781  data_time: 0.0070  lr: 0.006  max_mem: 10555M\n",
            "\u001b[32m[04/15 09:19:49 d2.utils.events]: \u001b[0m eta: 1 day, 13:03:51  iter: 2159  total_loss: 0.3698  loss_cls: 0.2383  loss_box_reg: 0.1185  loss_query: 0.03035  time: 2.2793  data_time: 0.0068  lr: 0.006  max_mem: 10555M\n",
            "\u001b[32m[04/15 09:20:36 d2.utils.events]: \u001b[0m eta: 1 day, 13:03:34  iter: 2179  total_loss: 0.4534  loss_cls: 0.2859  loss_box_reg: 0.1334  loss_query: 0.03398  time: 2.2797  data_time: 0.0067  lr: 0.006  max_mem: 10555M\n",
            "\u001b[32m[04/15 09:21:23 d2.utils.events]: \u001b[0m eta: 1 day, 13:02:42  iter: 2199  total_loss: 0.5672  loss_cls: 0.369  loss_box_reg: 0.1741  loss_query: 0.04281  time: 2.2796  data_time: 0.0075  lr: 0.006  max_mem: 10555M\n",
            "\u001b[32m[04/15 09:22:10 d2.utils.events]: \u001b[0m eta: 1 day, 13:02:49  iter: 2219  total_loss: 0.6052  loss_cls: 0.3741  loss_box_reg: 0.2121  loss_query: 0.03378  time: 2.2795  data_time: 0.0066  lr: 0.006  max_mem: 10555M\n",
            "\u001b[32m[04/15 09:22:53 d2.utils.events]: \u001b[0m eta: 1 day, 13:02:02  iter: 2239  total_loss: 0.5139  loss_cls: 0.3124  loss_box_reg: 0.2095  loss_query: 0.01946  time: 2.2780  data_time: 0.0068  lr: 0.006  max_mem: 10555M\n",
            "\u001b[32m[04/15 09:23:38 d2.utils.events]: \u001b[0m eta: 1 day, 13:03:08  iter: 2259  total_loss: 0.6668  loss_cls: 0.4075  loss_box_reg: 0.2579  loss_query: 0.02708  time: 2.2773  data_time: 0.0082  lr: 0.006  max_mem: 10555M\n",
            "\u001b[32m[04/15 09:24:23 d2.utils.events]: \u001b[0m eta: 1 day, 13:02:26  iter: 2279  total_loss: 0.6248  loss_cls: 0.4067  loss_box_reg: 0.1961  loss_query: 0.03488  time: 2.2767  data_time: 0.0063  lr: 0.006  max_mem: 10555M\n",
            "\u001b[32m[04/15 09:25:09 d2.utils.events]: \u001b[0m eta: 1 day, 13:00:49  iter: 2299  total_loss: 0.5886  loss_cls: 0.3581  loss_box_reg: 0.2017  loss_query: 0.03207  time: 2.2762  data_time: 0.0062  lr: 0.006  max_mem: 10555M\n",
            "\u001b[32m[04/15 09:25:55 d2.utils.events]: \u001b[0m eta: 1 day, 13:00:54  iter: 2319  total_loss: 0.7628  loss_cls: 0.4592  loss_box_reg: 0.2717  loss_query: 0.04065  time: 2.2759  data_time: 0.0066  lr: 0.006  max_mem: 10555M\n",
            "\u001b[32m[04/15 09:26:42 d2.utils.events]: \u001b[0m eta: 1 day, 13:00:08  iter: 2339  total_loss: 0.5684  loss_cls: 0.3198  loss_box_reg: 0.203  loss_query: 0.02794  time: 2.2760  data_time: 0.0074  lr: 0.006  max_mem: 10555M\n",
            "\u001b[32m[04/15 09:27:28 d2.utils.events]: \u001b[0m eta: 1 day, 12:59:36  iter: 2359  total_loss: 0.6798  loss_cls: 0.3991  loss_box_reg: 0.2417  loss_query: 0.03422  time: 2.2760  data_time: 0.0074  lr: 0.006  max_mem: 10555M\n",
            "\u001b[32m[04/15 09:28:15 d2.utils.events]: \u001b[0m eta: 1 day, 12:59:58  iter: 2379  total_loss: 0.6239  loss_cls: 0.3586  loss_box_reg: 0.2207  loss_query: 0.03991  time: 2.2759  data_time: 0.0071  lr: 0.006  max_mem: 10555M\n",
            "\u001b[32m[04/15 09:29:02 d2.utils.events]: \u001b[0m eta: 1 day, 12:59:17  iter: 2399  total_loss: 0.6035  loss_cls: 0.3618  loss_box_reg: 0.256  loss_query: 0.0286  time: 2.2761  data_time: 0.0066  lr: 0.006  max_mem: 10555M\n",
            "\u001b[32m[04/15 09:29:48 d2.utils.events]: \u001b[0m eta: 1 day, 12:58:00  iter: 2419  total_loss: 0.7388  loss_cls: 0.409  loss_box_reg: 0.2555  loss_query: 0.03317  time: 2.2761  data_time: 0.0076  lr: 0.006  max_mem: 10555M\n",
            "\u001b[32m[04/15 09:30:36 d2.utils.events]: \u001b[0m eta: 1 day, 12:57:14  iter: 2439  total_loss: 0.6499  loss_cls: 0.3745  loss_box_reg: 0.2129  loss_query: 0.03005  time: 2.2762  data_time: 0.0065  lr: 0.006  max_mem: 10555M\n",
            "\u001b[32m[04/15 09:31:23 d2.utils.events]: \u001b[0m eta: 1 day, 12:56:03  iter: 2459  total_loss: 0.6385  loss_cls: 0.4014  loss_box_reg: 0.229  loss_query: 0.03017  time: 2.2764  data_time: 0.0062  lr: 0.006  max_mem: 10555M\n",
            "\u001b[32m[04/15 09:32:10 d2.utils.events]: \u001b[0m eta: 1 day, 12:54:40  iter: 2479  total_loss: 0.7357  loss_cls: 0.4539  loss_box_reg: 0.271  loss_query: 0.04339  time: 2.2764  data_time: 0.0077  lr: 0.006  max_mem: 10555M\n",
            "\u001b[32m[04/15 09:32:57 d2.utils.events]: \u001b[0m eta: 1 day, 12:53:54  iter: 2499  total_loss: 0.5489  loss_cls: 0.3343  loss_box_reg: 0.1762  loss_query: 0.03158  time: 2.2766  data_time: 0.0090  lr: 0.006  max_mem: 10555M\n",
            "\u001b[32m[04/15 09:33:43 d2.utils.events]: \u001b[0m eta: 1 day, 12:52:40  iter: 2519  total_loss: 0.5313  loss_cls: 0.3523  loss_box_reg: 0.1951  loss_query: 0.02011  time: 2.2763  data_time: 0.0064  lr: 0.006  max_mem: 10555M\n",
            "\u001b[32m[04/15 09:34:28 d2.utils.events]: \u001b[0m eta: 1 day, 12:51:04  iter: 2539  total_loss: 0.7184  loss_cls: 0.4219  loss_box_reg: 0.26  loss_query: 0.02306  time: 2.2757  data_time: 0.0077  lr: 0.006  max_mem: 10555M\n",
            "\u001b[32m[04/15 09:35:15 d2.utils.events]: \u001b[0m eta: 1 day, 12:50:36  iter: 2559  total_loss: 0.6359  loss_cls: 0.3837  loss_box_reg: 0.2158  loss_query: 0.03498  time: 2.2758  data_time: 0.0068  lr: 0.006  max_mem: 10555M\n",
            "\u001b[32m[04/15 09:36:02 d2.utils.events]: \u001b[0m eta: 1 day, 12:49:34  iter: 2579  total_loss: 0.3662  loss_cls: 0.2398  loss_box_reg: 0.1437  loss_query: 0.01629  time: 2.2759  data_time: 0.0059  lr: 0.006  max_mem: 10555M\n",
            "\u001b[32m[04/15 09:36:49 d2.utils.events]: \u001b[0m eta: 1 day, 12:50:39  iter: 2599  total_loss: 0.7229  loss_cls: 0.4578  loss_box_reg: 0.244  loss_query: 0.03016  time: 2.2762  data_time: 0.0083  lr: 0.006  max_mem: 10555M\n",
            "\u001b[32m[04/15 09:37:37 d2.utils.events]: \u001b[0m eta: 1 day, 12:51:07  iter: 2619  total_loss: 0.6579  loss_cls: 0.3903  loss_box_reg: 0.2518  loss_query: 0.03815  time: 2.2766  data_time: 0.0066  lr: 0.006  max_mem: 10555M\n",
            "\u001b[32m[04/15 09:38:24 d2.utils.events]: \u001b[0m eta: 1 day, 12:50:38  iter: 2639  total_loss: 0.5019  loss_cls: 0.2841  loss_box_reg: 0.1919  loss_query: 0.02542  time: 2.2768  data_time: 0.0062  lr: 0.006  max_mem: 10555M\n",
            "\u001b[32m[04/15 09:39:11 d2.utils.events]: \u001b[0m eta: 1 day, 12:50:40  iter: 2659  total_loss: 0.6562  loss_cls: 0.3967  loss_box_reg: 0.2256  loss_query: 0.03508  time: 2.2770  data_time: 0.0092  lr: 0.006  max_mem: 10555M\n",
            "\u001b[32m[04/15 09:39:59 d2.utils.events]: \u001b[0m eta: 1 day, 12:49:50  iter: 2679  total_loss: 0.7438  loss_cls: 0.4352  loss_box_reg: 0.2877  loss_query: 0.02921  time: 2.2773  data_time: 0.0067  lr: 0.006  max_mem: 10555M\n",
            "\u001b[32m[04/15 09:40:46 d2.utils.events]: \u001b[0m eta: 1 day, 12:48:19  iter: 2699  total_loss: 0.554  loss_cls: 0.3097  loss_box_reg: 0.2056  loss_query: 0.02581  time: 2.2772  data_time: 0.0072  lr: 0.006  max_mem: 10555M\n",
            "\u001b[32m[04/15 09:41:31 d2.utils.events]: \u001b[0m eta: 1 day, 12:47:06  iter: 2719  total_loss: 0.6361  loss_cls: 0.3945  loss_box_reg: 0.2308  loss_query: 0.02857  time: 2.2766  data_time: 0.0081  lr: 0.006  max_mem: 10555M\n",
            "\u001b[32m[04/15 09:42:18 d2.utils.events]: \u001b[0m eta: 1 day, 12:46:25  iter: 2739  total_loss: 0.6594  loss_cls: 0.3745  loss_box_reg: 0.2398  loss_query: 0.04274  time: 2.2769  data_time: 0.0071  lr: 0.006  max_mem: 10555M\n",
            "\u001b[32m[04/15 09:43:05 d2.utils.events]: \u001b[0m eta: 1 day, 12:46:00  iter: 2759  total_loss: 0.6314  loss_cls: 0.3431  loss_box_reg: 0.2328  loss_query: 0.02769  time: 2.2769  data_time: 0.0076  lr: 0.006  max_mem: 10555M\n",
            "\u001b[32m[04/15 09:43:53 d2.utils.events]: \u001b[0m eta: 1 day, 12:45:14  iter: 2779  total_loss: 0.6132  loss_cls: 0.3454  loss_box_reg: 0.207  loss_query: 0.02987  time: 2.2774  data_time: 0.0064  lr: 0.006  max_mem: 10555M\n",
            "\u001b[32m[04/15 09:44:39 d2.utils.events]: \u001b[0m eta: 1 day, 12:44:06  iter: 2799  total_loss: 0.4594  loss_cls: 0.2823  loss_box_reg: 0.1292  loss_query: 0.0214  time: 2.2771  data_time: 0.0072  lr: 0.006  max_mem: 10555M\n",
            "\u001b[32m[04/15 09:45:25 d2.utils.events]: \u001b[0m eta: 1 day, 12:43:20  iter: 2819  total_loss: 0.5275  loss_cls: 0.2939  loss_box_reg: 0.2014  loss_query: 0.0356  time: 2.2770  data_time: 0.0076  lr: 0.006  max_mem: 10555M\n",
            "\u001b[32m[04/15 09:46:12 d2.utils.events]: \u001b[0m eta: 1 day, 12:42:26  iter: 2839  total_loss: 0.6509  loss_cls: 0.3766  loss_box_reg: 0.2401  loss_query: 0.02771  time: 2.2769  data_time: 0.0072  lr: 0.006  max_mem: 10555M\n",
            "\u001b[32m[04/15 09:46:58 d2.utils.events]: \u001b[0m eta: 1 day, 12:41:39  iter: 2859  total_loss: 0.5974  loss_cls: 0.3451  loss_box_reg: 0.2034  loss_query: 0.0296  time: 2.2769  data_time: 0.0072  lr: 0.006  max_mem: 10555M\n",
            "\u001b[32m[04/15 09:47:45 d2.utils.events]: \u001b[0m eta: 1 day, 12:40:53  iter: 2879  total_loss: 0.7664  loss_cls: 0.4605  loss_box_reg: 0.2622  loss_query: 0.03044  time: 2.2770  data_time: 0.0071  lr: 0.006  max_mem: 10555M\n",
            "\u001b[32m[04/15 09:48:32 d2.utils.events]: \u001b[0m eta: 1 day, 12:40:10  iter: 2899  total_loss: 0.6809  loss_cls: 0.3631  loss_box_reg: 0.2761  loss_query: 0.02967  time: 2.2770  data_time: 0.0068  lr: 0.006  max_mem: 10555M\n",
            "\u001b[32m[04/15 09:49:18 d2.utils.events]: \u001b[0m eta: 1 day, 12:40:00  iter: 2919  total_loss: 0.579  loss_cls: 0.3386  loss_box_reg: 0.2232  loss_query: 0.03229  time: 2.2770  data_time: 0.0070  lr: 0.006  max_mem: 10555M\n",
            "\u001b[32m[04/15 09:50:04 d2.utils.events]: \u001b[0m eta: 1 day, 12:38:42  iter: 2939  total_loss: 0.5902  loss_cls: 0.3485  loss_box_reg: 0.2004  loss_query: 0.02513  time: 2.2767  data_time: 0.0078  lr: 0.006  max_mem: 10555M\n",
            "\u001b[32m[04/15 09:50:50 d2.utils.events]: \u001b[0m eta: 1 day, 12:37:56  iter: 2959  total_loss: 0.491  loss_cls: 0.2906  loss_box_reg: 0.1897  loss_query: 0.02535  time: 2.2765  data_time: 0.0072  lr: 0.006  max_mem: 10555M\n",
            "\u001b[32m[04/15 09:51:38 d2.utils.events]: \u001b[0m eta: 1 day, 12:37:01  iter: 2979  total_loss: 0.4791  loss_cls: 0.2878  loss_box_reg: 0.1558  loss_query: 0.02647  time: 2.2769  data_time: 0.0074  lr: 0.006  max_mem: 10555M\n",
            "\u001b[32m[04/15 09:52:26 d2.utils.events]: \u001b[0m eta: 1 day, 12:36:32  iter: 2999  total_loss: 0.6412  loss_cls: 0.3859  loss_box_reg: 0.2163  loss_query: 0.02966  time: 2.2773  data_time: 0.0064  lr: 0.006  max_mem: 10555M\n",
            "\u001b[32m[04/15 09:53:13 d2.utils.events]: \u001b[0m eta: 1 day, 12:35:22  iter: 3019  total_loss: 0.4662  loss_cls: 0.2781  loss_box_reg: 0.1637  loss_query: 0.01801  time: 2.2775  data_time: 0.0072  lr: 0.006  max_mem: 10555M\n",
            "\u001b[32m[04/15 09:54:00 d2.utils.events]: \u001b[0m eta: 1 day, 12:32:56  iter: 3039  total_loss: 0.6036  loss_cls: 0.3152  loss_box_reg: 0.2236  loss_query: 0.02503  time: 2.2776  data_time: 0.0074  lr: 0.006  max_mem: 10555M\n",
            "\u001b[32m[04/15 09:54:46 d2.utils.events]: \u001b[0m eta: 1 day, 12:32:10  iter: 3059  total_loss: 0.6504  loss_cls: 0.3693  loss_box_reg: 0.2336  loss_query: 0.04045  time: 2.2773  data_time: 0.0071  lr: 0.006  max_mem: 10555M\n",
            "\u001b[32m[04/15 09:55:32 d2.utils.events]: \u001b[0m eta: 1 day, 12:31:16  iter: 3079  total_loss: 0.5483  loss_cls: 0.3237  loss_box_reg: 0.1924  loss_query: 0.02694  time: 2.2771  data_time: 0.0076  lr: 0.006  max_mem: 10555M\n",
            "\u001b[32m[04/15 09:56:19 d2.utils.events]: \u001b[0m eta: 1 day, 12:30:30  iter: 3099  total_loss: 0.5538  loss_cls: 0.3037  loss_box_reg: 0.2032  loss_query: 0.03451  time: 2.2772  data_time: 0.0068  lr: 0.006  max_mem: 10555M\n",
            "\u001b[32m[04/15 09:57:05 d2.utils.events]: \u001b[0m eta: 1 day, 12:29:51  iter: 3119  total_loss: 0.6005  loss_cls: 0.355  loss_box_reg: 0.214  loss_query: 0.02965  time: 2.2768  data_time: 0.0086  lr: 0.006  max_mem: 10555M\n",
            "\u001b[32m[04/15 09:57:52 d2.utils.events]: \u001b[0m eta: 1 day, 12:29:05  iter: 3139  total_loss: 0.6582  loss_cls: 0.3692  loss_box_reg: 0.2265  loss_query: 0.0378  time: 2.2769  data_time: 0.0099  lr: 0.006  max_mem: 10555M\n",
            "\u001b[32m[04/15 09:58:38 d2.utils.events]: \u001b[0m eta: 1 day, 12:27:17  iter: 3159  total_loss: 0.725  loss_cls: 0.4414  loss_box_reg: 0.2321  loss_query: 0.03452  time: 2.2766  data_time: 0.0069  lr: 0.006  max_mem: 10555M\n",
            "\u001b[32m[04/15 09:59:24 d2.utils.events]: \u001b[0m eta: 1 day, 12:26:07  iter: 3179  total_loss: 0.5487  loss_cls: 0.319  loss_box_reg: 0.2115  loss_query: 0.02517  time: 2.2765  data_time: 0.0073  lr: 0.006  max_mem: 10555M\n",
            "\u001b[32m[04/15 10:00:12 d2.utils.events]: \u001b[0m eta: 1 day, 12:25:26  iter: 3199  total_loss: 0.6943  loss_cls: 0.3935  loss_box_reg: 0.2416  loss_query: 0.03643  time: 2.2769  data_time: 0.0080  lr: 0.006  max_mem: 10555M\n",
            "\u001b[32m[04/15 10:00:57 d2.utils.events]: \u001b[0m eta: 1 day, 12:24:26  iter: 3219  total_loss: 0.4524  loss_cls: 0.2761  loss_box_reg: 0.1595  loss_query: 0.03188  time: 2.2763  data_time: 0.0079  lr: 0.006  max_mem: 10555M\n",
            "\u001b[32m[04/15 10:01:45 d2.utils.events]: \u001b[0m eta: 1 day, 12:24:12  iter: 3239  total_loss: 0.5548  loss_cls: 0.3179  loss_box_reg: 0.217  loss_query: 0.02272  time: 2.2767  data_time: 0.0066  lr: 0.006  max_mem: 10555M\n",
            "\u001b[32m[04/15 10:02:31 d2.utils.events]: \u001b[0m eta: 1 day, 12:23:15  iter: 3259  total_loss: 0.619  loss_cls: 0.3426  loss_box_reg: 0.2035  loss_query: 0.02258  time: 2.2765  data_time: 0.0071  lr: 0.006  max_mem: 10555M\n",
            "\u001b[32m[04/15 10:03:18 d2.utils.events]: \u001b[0m eta: 1 day, 12:23:41  iter: 3279  total_loss: 0.6498  loss_cls: 0.3852  loss_box_reg: 0.2503  loss_query: 0.03175  time: 2.2767  data_time: 0.0082  lr: 0.006  max_mem: 10555M\n",
            "\u001b[32m[04/15 10:04:05 d2.utils.events]: \u001b[0m eta: 1 day, 12:24:33  iter: 3299  total_loss: 0.7285  loss_cls: 0.4293  loss_box_reg: 0.2565  loss_query: 0.05666  time: 2.2767  data_time: 0.0096  lr: 0.006  max_mem: 10555M\n",
            "\u001b[32m[04/15 10:04:51 d2.utils.events]: \u001b[0m eta: 1 day, 12:22:09  iter: 3319  total_loss: 0.4466  loss_cls: 0.2596  loss_box_reg: 0.1499  loss_query: 0.03198  time: 2.2765  data_time: 0.0069  lr: 0.006  max_mem: 10555M\n",
            "\u001b[32m[04/15 10:05:37 d2.utils.events]: \u001b[0m eta: 1 day, 12:20:54  iter: 3339  total_loss: 0.6444  loss_cls: 0.3718  loss_box_reg: 0.2318  loss_query: 0.04033  time: 2.2763  data_time: 0.0080  lr: 0.006  max_mem: 10555M\n",
            "\u001b[32m[04/15 10:06:22 d2.utils.events]: \u001b[0m eta: 1 day, 12:19:42  iter: 3359  total_loss: 0.6696  loss_cls: 0.3739  loss_box_reg: 0.2475  loss_query: 0.01889  time: 2.2760  data_time: 0.0064  lr: 0.006  max_mem: 10555M\n",
            "\u001b[32m[04/15 10:07:08 d2.utils.events]: \u001b[0m eta: 1 day, 12:18:38  iter: 3379  total_loss: 0.6921  loss_cls: 0.3921  loss_box_reg: 0.2539  loss_query: 0.03637  time: 2.2757  data_time: 0.0070  lr: 0.006  max_mem: 10555M\n",
            "\u001b[32m[04/15 10:07:56 d2.utils.events]: \u001b[0m eta: 1 day, 12:18:36  iter: 3399  total_loss: 0.6794  loss_cls: 0.3897  loss_box_reg: 0.2425  loss_query: 0.03676  time: 2.2759  data_time: 0.0081  lr: 0.006  max_mem: 10555M\n",
            "\u001b[32m[04/15 10:08:44 d2.utils.events]: \u001b[0m eta: 1 day, 12:17:39  iter: 3419  total_loss: 0.6099  loss_cls: 0.3668  loss_box_reg: 0.2219  loss_query: 0.03833  time: 2.2762  data_time: 0.0065  lr: 0.006  max_mem: 10555M\n",
            "\u001b[32m[04/15 10:09:30 d2.utils.events]: \u001b[0m eta: 1 day, 12:16:38  iter: 3439  total_loss: 0.7039  loss_cls: 0.3626  loss_box_reg: 0.2421  loss_query: 0.03176  time: 2.2762  data_time: 0.0070  lr: 0.006  max_mem: 10555M\n",
            "\u001b[32m[04/15 10:10:15 d2.utils.events]: \u001b[0m eta: 1 day, 12:15:26  iter: 3459  total_loss: 0.525  loss_cls: 0.3293  loss_box_reg: 0.1922  loss_query: 0.0225  time: 2.2756  data_time: 0.0065  lr: 0.006  max_mem: 10555M\n",
            "\u001b[32m[04/15 10:11:02 d2.utils.events]: \u001b[0m eta: 1 day, 12:14:49  iter: 3479  total_loss: 0.6703  loss_cls: 0.399  loss_box_reg: 0.2113  loss_query: 0.03075  time: 2.2759  data_time: 0.0066  lr: 0.006  max_mem: 10555M\n",
            "\u001b[32m[04/15 10:11:49 d2.utils.events]: \u001b[0m eta: 1 day, 12:14:34  iter: 3499  total_loss: 0.5731  loss_cls: 0.3367  loss_box_reg: 0.1816  loss_query: 0.04007  time: 2.2757  data_time: 0.0076  lr: 0.006  max_mem: 10555M\n",
            "\u001b[32m[04/15 10:12:36 d2.utils.events]: \u001b[0m eta: 1 day, 12:14:27  iter: 3519  total_loss: 0.5159  loss_cls: 0.3657  loss_box_reg: 0.1836  loss_query: 0.02705  time: 2.2759  data_time: 0.0068  lr: 0.006  max_mem: 10555M\n",
            "\u001b[32m[04/15 10:13:24 d2.utils.events]: \u001b[0m eta: 1 day, 12:15:18  iter: 3539  total_loss: 0.5201  loss_cls: 0.3102  loss_box_reg: 0.1827  loss_query: 0.03441  time: 2.2763  data_time: 0.0092  lr: 0.006  max_mem: 10555M\n",
            "\u001b[32m[04/15 10:14:11 d2.utils.events]: \u001b[0m eta: 1 day, 12:13:37  iter: 3559  total_loss: 0.5902  loss_cls: 0.363  loss_box_reg: 0.222  loss_query: 0.02873  time: 2.2764  data_time: 0.0081  lr: 0.006  max_mem: 10555M\n",
            "\u001b[32m[04/15 10:14:57 d2.utils.events]: \u001b[0m eta: 1 day, 12:12:09  iter: 3579  total_loss: 0.6701  loss_cls: 0.4153  loss_box_reg: 0.2084  loss_query: 0.02943  time: 2.2762  data_time: 0.0055  lr: 0.006  max_mem: 10555M\n",
            "\u001b[32m[04/15 10:15:45 d2.utils.events]: \u001b[0m eta: 1 day, 12:11:23  iter: 3599  total_loss: 0.6331  loss_cls: 0.3354  loss_box_reg: 0.2505  loss_query: 0.04246  time: 2.2765  data_time: 0.0077  lr: 0.006  max_mem: 10555M\n",
            "\u001b[32m[04/15 10:16:31 d2.utils.events]: \u001b[0m eta: 1 day, 12:09:18  iter: 3619  total_loss: 0.5349  loss_cls: 0.3134  loss_box_reg: 0.2117  loss_query: 0.04587  time: 2.2762  data_time: 0.0073  lr: 0.006  max_mem: 10555M\n",
            "\u001b[32m[04/15 10:17:18 d2.utils.events]: \u001b[0m eta: 1 day, 12:08:32  iter: 3639  total_loss: 0.7346  loss_cls: 0.3826  loss_box_reg: 0.3055  loss_query: 0.01937  time: 2.2764  data_time: 0.0065  lr: 0.006  max_mem: 10555M\n",
            "\u001b[32m[04/15 10:18:06 d2.utils.events]: \u001b[0m eta: 1 day, 12:07:31  iter: 3659  total_loss: 0.5921  loss_cls: 0.3841  loss_box_reg: 0.2188  loss_query: 0.0196  time: 2.2766  data_time: 0.0077  lr: 0.006  max_mem: 10555M\n",
            "\u001b[32m[04/15 10:18:53 d2.utils.events]: \u001b[0m eta: 1 day, 12:07:24  iter: 3679  total_loss: 0.6901  loss_cls: 0.4165  loss_box_reg: 0.2584  loss_query: 0.02877  time: 2.2769  data_time: 0.0081  lr: 0.006  max_mem: 10555M\n",
            "\u001b[32m[04/15 10:19:40 d2.utils.events]: \u001b[0m eta: 1 day, 12:06:21  iter: 3699  total_loss: 0.496  loss_cls: 0.305  loss_box_reg: 0.1726  loss_query: 0.02018  time: 2.2768  data_time: 0.0072  lr: 0.006  max_mem: 10555M\n",
            "\u001b[32m[04/15 10:20:27 d2.utils.events]: \u001b[0m eta: 1 day, 12:05:27  iter: 3719  total_loss: 0.7082  loss_cls: 0.4121  loss_box_reg: 0.2802  loss_query: 0.01863  time: 2.2767  data_time: 0.0067  lr: 0.006  max_mem: 10555M\n",
            "\u001b[32m[04/15 10:21:14 d2.utils.events]: \u001b[0m eta: 1 day, 12:04:41  iter: 3739  total_loss: 0.7148  loss_cls: 0.4032  loss_box_reg: 0.2674  loss_query: 0.03837  time: 2.2769  data_time: 0.0071  lr: 0.006  max_mem: 10555M\n",
            "\u001b[32m[04/15 10:22:01 d2.utils.events]: \u001b[0m eta: 1 day, 12:03:48  iter: 3759  total_loss: 0.6275  loss_cls: 0.3654  loss_box_reg: 0.2093  loss_query: 0.02974  time: 2.2770  data_time: 0.0083  lr: 0.006  max_mem: 10555M\n",
            "\u001b[32m[04/15 10:22:47 d2.utils.events]: \u001b[0m eta: 1 day, 12:02:29  iter: 3779  total_loss: 0.5541  loss_cls: 0.3251  loss_box_reg: 0.1814  loss_query: 0.03879  time: 2.2768  data_time: 0.0086  lr: 0.006  max_mem: 10555M\n",
            "\u001b[32m[04/15 10:23:33 d2.utils.events]: \u001b[0m eta: 1 day, 12:01:43  iter: 3799  total_loss: 0.5502  loss_cls: 0.3227  loss_box_reg: 0.1924  loss_query: 0.02688  time: 2.2764  data_time: 0.0064  lr: 0.006  max_mem: 10555M\n",
            "\u001b[32m[04/15 10:24:20 d2.utils.events]: \u001b[0m eta: 1 day, 12:00:57  iter: 3819  total_loss: 0.6985  loss_cls: 0.4101  loss_box_reg: 0.2171  loss_query: 0.02642  time: 2.2765  data_time: 0.0067  lr: 0.006  max_mem: 10555M\n",
            "\u001b[32m[04/15 10:25:08 d2.utils.events]: \u001b[0m eta: 1 day, 12:00:21  iter: 3839  total_loss: 0.4432  loss_cls: 0.2452  loss_box_reg: 0.1825  loss_query: 0.01673  time: 2.2767  data_time: 0.0063  lr: 0.006  max_mem: 10555M\n",
            "\u001b[32m[04/15 10:25:56 d2.utils.events]: \u001b[0m eta: 1 day, 12:01:02  iter: 3859  total_loss: 0.578  loss_cls: 0.3398  loss_box_reg: 0.198  loss_query: 0.03887  time: 2.2769  data_time: 0.0079  lr: 0.006  max_mem: 10555M\n",
            "\u001b[32m[04/15 10:26:42 d2.utils.events]: \u001b[0m eta: 1 day, 11:58:49  iter: 3879  total_loss: 0.6826  loss_cls: 0.3853  loss_box_reg: 0.2367  loss_query: 0.02783  time: 2.2766  data_time: 0.0073  lr: 0.006  max_mem: 10555M\n",
            "\u001b[32m[04/15 10:27:29 d2.utils.events]: \u001b[0m eta: 1 day, 11:57:52  iter: 3899  total_loss: 0.6071  loss_cls: 0.3439  loss_box_reg: 0.2224  loss_query: 0.01793  time: 2.2767  data_time: 0.0087  lr: 0.006  max_mem: 10555M\n",
            "\u001b[32m[04/15 10:28:16 d2.utils.events]: \u001b[0m eta: 1 day, 11:56:40  iter: 3919  total_loss: 0.7243  loss_cls: 0.3746  loss_box_reg: 0.2696  loss_query: 0.03349  time: 2.2770  data_time: 0.0057  lr: 0.006  max_mem: 10555M\n",
            "\u001b[32m[04/15 10:29:04 d2.utils.events]: \u001b[0m eta: 1 day, 11:55:54  iter: 3939  total_loss: 0.5611  loss_cls: 0.3223  loss_box_reg: 0.2092  loss_query: 0.03249  time: 2.2771  data_time: 0.0073  lr: 0.006  max_mem: 10555M\n",
            "\u001b[32m[04/15 10:29:49 d2.utils.events]: \u001b[0m eta: 1 day, 11:54:45  iter: 3959  total_loss: 0.4688  loss_cls: 0.2764  loss_box_reg: 0.1645  loss_query: 0.02278  time: 2.2767  data_time: 0.0067  lr: 0.006  max_mem: 10555M\n",
            "\u001b[32m[04/15 10:30:35 d2.utils.events]: \u001b[0m eta: 1 day, 11:53:59  iter: 3979  total_loss: 0.6741  loss_cls: 0.3939  loss_box_reg: 0.2442  loss_query: 0.02704  time: 2.2766  data_time: 0.0076  lr: 0.006  max_mem: 10555M\n",
            "\u001b[32m[04/15 10:31:20 fvcore.common.checkpoint]: \u001b[0mSaving checkpoint to work_dirs/visdrone_querydet/model_0003999.pth\n",
            "\u001b[32m[04/15 10:31:35 d2.utils.events]: \u001b[0m eta: 1 day, 11:53:13  iter: 3999  total_loss: 0.6467  loss_cls: 0.3613  loss_box_reg: 0.2048  loss_query: 0.02506  time: 2.2762  data_time: 0.0067  lr: 0.006  max_mem: 10555M\n",
            "\u001b[32m[04/15 10:32:22 d2.utils.events]: \u001b[0m eta: 1 day, 11:52:58  iter: 4019  total_loss: 0.4607  loss_cls: 0.3057  loss_box_reg: 0.157  loss_query: 0.02177  time: 2.2763  data_time: 0.0067  lr: 0.006  max_mem: 10555M\n",
            "\u001b[32m[04/15 10:33:09 d2.utils.events]: \u001b[0m eta: 1 day, 11:53:01  iter: 4039  total_loss: 0.6869  loss_cls: 0.3956  loss_box_reg: 0.2659  loss_query: 0.02939  time: 2.2764  data_time: 0.0074  lr: 0.006  max_mem: 10555M\n",
            "\u001b[32m[04/15 10:33:56 d2.utils.events]: \u001b[0m eta: 1 day, 11:54:16  iter: 4059  total_loss: 0.6046  loss_cls: 0.3671  loss_box_reg: 0.2319  loss_query: 0.03024  time: 2.2765  data_time: 0.0077  lr: 0.006  max_mem: 10555M\n",
            "\u001b[32m[04/15 10:34:42 d2.utils.events]: \u001b[0m eta: 1 day, 11:51:12  iter: 4079  total_loss: 0.5026  loss_cls: 0.2841  loss_box_reg: 0.1882  loss_query: 0.02293  time: 2.2763  data_time: 0.0065  lr: 0.006  max_mem: 10555M\n",
            "\u001b[32m[04/15 10:35:29 d2.utils.events]: \u001b[0m eta: 1 day, 11:50:11  iter: 4099  total_loss: 0.5185  loss_cls: 0.3144  loss_box_reg: 0.1786  loss_query: 0.02248  time: 2.2762  data_time: 0.0061  lr: 0.006  max_mem: 10555M\n",
            "\u001b[32m[04/15 10:36:15 d2.utils.events]: \u001b[0m eta: 1 day, 11:49:55  iter: 4119  total_loss: 0.674  loss_cls: 0.3883  loss_box_reg: 0.2316  loss_query: 0.03517  time: 2.2762  data_time: 0.0069  lr: 0.006  max_mem: 10555M\n",
            "\u001b[32m[04/15 10:37:02 d2.utils.events]: \u001b[0m eta: 1 day, 11:48:48  iter: 4139  total_loss: 0.5204  loss_cls: 0.2957  loss_box_reg: 0.2091  loss_query: 0.03012  time: 2.2762  data_time: 0.0075  lr: 0.006  max_mem: 10555M\n",
            "\u001b[32m[04/15 10:37:50 d2.utils.events]: \u001b[0m eta: 1 day, 11:49:45  iter: 4159  total_loss: 0.8315  loss_cls: 0.4545  loss_box_reg: 0.3414  loss_query: 0.04845  time: 2.2764  data_time: 0.0069  lr: 0.006  max_mem: 10555M\n",
            "\u001b[32m[04/15 10:38:37 d2.utils.events]: \u001b[0m eta: 1 day, 11:50:12  iter: 4179  total_loss: 0.3532  loss_cls: 0.2122  loss_box_reg: 0.1196  loss_query: 0.02798  time: 2.2764  data_time: 0.0066  lr: 0.006  max_mem: 10555M\n",
            "\u001b[32m[04/15 10:39:23 d2.utils.events]: \u001b[0m eta: 1 day, 11:47:21  iter: 4199  total_loss: 0.4241  loss_cls: 0.2478  loss_box_reg: 0.1539  loss_query: 0.01843  time: 2.2763  data_time: 0.0097  lr: 0.006  max_mem: 10555M\n",
            "\u001b[32m[04/15 10:40:10 d2.utils.events]: \u001b[0m eta: 1 day, 11:48:36  iter: 4219  total_loss: 0.5655  loss_cls: 0.3328  loss_box_reg: 0.208  loss_query: 0.03841  time: 2.2764  data_time: 0.0076  lr: 0.006  max_mem: 10555M\n",
            "\u001b[32m[04/15 10:40:57 d2.utils.events]: \u001b[0m eta: 1 day, 11:47:25  iter: 4239  total_loss: 0.473  loss_cls: 0.2881  loss_box_reg: 0.1488  loss_query: 0.0202  time: 2.2765  data_time: 0.0078  lr: 0.006  max_mem: 10555M\n",
            "\u001b[32m[04/15 10:41:43 d2.utils.events]: \u001b[0m eta: 1 day, 11:47:13  iter: 4259  total_loss: 0.6242  loss_cls: 0.3677  loss_box_reg: 0.224  loss_query: 0.0291  time: 2.2764  data_time: 0.0075  lr: 0.006  max_mem: 10555M\n",
            "\u001b[32m[04/15 10:42:30 d2.utils.events]: \u001b[0m eta: 1 day, 11:45:52  iter: 4279  total_loss: 0.5108  loss_cls: 0.3038  loss_box_reg: 0.1764  loss_query: 0.02083  time: 2.2764  data_time: 0.0061  lr: 0.006  max_mem: 10555M\n",
            "\u001b[32m[04/15 10:43:16 d2.utils.events]: \u001b[0m eta: 1 day, 11:45:06  iter: 4299  total_loss: 0.6452  loss_cls: 0.4058  loss_box_reg: 0.2228  loss_query: 0.03639  time: 2.2763  data_time: 0.0065  lr: 0.006  max_mem: 10555M\n",
            "\u001b[32m[04/15 10:44:02 d2.utils.events]: \u001b[0m eta: 1 day, 11:44:15  iter: 4319  total_loss: 0.5074  loss_cls: 0.3121  loss_box_reg: 0.1563  loss_query: 0.01605  time: 2.2762  data_time: 0.0068  lr: 0.006  max_mem: 10555M\n",
            "\u001b[32m[04/15 10:44:48 d2.utils.events]: \u001b[0m eta: 1 day, 11:43:28  iter: 4339  total_loss: 0.6855  loss_cls: 0.4105  loss_box_reg: 0.2599  loss_query: 0.0324  time: 2.2760  data_time: 0.0077  lr: 0.006  max_mem: 10555M\n",
            "\u001b[32m[04/15 10:45:35 d2.utils.events]: \u001b[0m eta: 1 day, 11:41:33  iter: 4359  total_loss: 0.4888  loss_cls: 0.2856  loss_box_reg: 0.1791  loss_query: 0.03275  time: 2.2761  data_time: 0.0066  lr: 0.006  max_mem: 10555M\n",
            "\u001b[32m[04/15 10:46:22 d2.utils.events]: \u001b[0m eta: 1 day, 11:40:34  iter: 4379  total_loss: 0.6318  loss_cls: 0.396  loss_box_reg: 0.2128  loss_query: 0.02417  time: 2.2762  data_time: 0.0078  lr: 0.006  max_mem: 10555M\n",
            "\u001b[32m[04/15 10:47:10 d2.utils.events]: \u001b[0m eta: 1 day, 11:39:15  iter: 4399  total_loss: 0.5462  loss_cls: 0.3006  loss_box_reg: 0.2343  loss_query: 0.03215  time: 2.2763  data_time: 0.0068  lr: 0.006  max_mem: 10555M\n",
            "\u001b[32m[04/15 10:47:54 d2.utils.events]: \u001b[0m eta: 1 day, 11:38:02  iter: 4419  total_loss: 0.5749  loss_cls: 0.3295  loss_box_reg: 0.2358  loss_query: 0.01873  time: 2.2757  data_time: 0.0070  lr: 0.006  max_mem: 10555M\n",
            "\u001b[32m[04/15 10:48:42 d2.utils.events]: \u001b[0m eta: 1 day, 11:38:03  iter: 4439  total_loss: 0.5907  loss_cls: 0.3851  loss_box_reg: 0.1828  loss_query: 0.02712  time: 2.2761  data_time: 0.0068  lr: 0.006  max_mem: 10555M\n",
            "\u001b[32m[04/15 10:49:31 d2.utils.events]: \u001b[0m eta: 1 day, 11:37:42  iter: 4459  total_loss: 0.5325  loss_cls: 0.3241  loss_box_reg: 0.1679  loss_query: 0.02555  time: 2.2766  data_time: 0.1511  lr: 0.006  max_mem: 10555M\n",
            "\u001b[32m[04/15 10:50:17 d2.utils.events]: \u001b[0m eta: 1 day, 11:36:20  iter: 4479  total_loss: 0.5408  loss_cls: 0.3153  loss_box_reg: 0.1927  loss_query: 0.02319  time: 2.2764  data_time: 0.0072  lr: 0.006  max_mem: 10555M\n",
            "\u001b[32m[04/15 10:51:03 d2.utils.events]: \u001b[0m eta: 1 day, 11:34:57  iter: 4499  total_loss: 0.6087  loss_cls: 0.3641  loss_box_reg: 0.2212  loss_query: 0.02573  time: 2.2763  data_time: 0.0058  lr: 0.006  max_mem: 10555M\n",
            "\u001b[32m[04/15 10:51:50 d2.utils.events]: \u001b[0m eta: 1 day, 11:32:10  iter: 4519  total_loss: 0.6149  loss_cls: 0.3852  loss_box_reg: 0.2285  loss_query: 0.02005  time: 2.2763  data_time: 0.0072  lr: 0.006  max_mem: 10555M\n",
            "\u001b[32m[04/15 10:52:37 d2.utils.events]: \u001b[0m eta: 1 day, 11:31:40  iter: 4539  total_loss: 0.5053  loss_cls: 0.2912  loss_box_reg: 0.1981  loss_query: 0.02481  time: 2.2765  data_time: 0.0060  lr: 0.006  max_mem: 10555M\n",
            "\u001b[32m[04/15 10:53:23 d2.utils.events]: \u001b[0m eta: 1 day, 11:30:42  iter: 4559  total_loss: 0.6508  loss_cls: 0.3482  loss_box_reg: 0.2463  loss_query: 0.01679  time: 2.2763  data_time: 0.0067  lr: 0.006  max_mem: 10555M\n",
            "\u001b[32m[04/15 10:54:16 d2.utils.events]: \u001b[0m eta: 1 day, 11:31:37  iter: 4579  total_loss: 0.638  loss_cls: 0.3787  loss_box_reg: 0.2236  loss_query: 0.03495  time: 2.2777  data_time: 0.3565  lr: 0.006  max_mem: 10555M\n",
            "\u001b[32m[04/15 10:55:02 d2.utils.events]: \u001b[0m eta: 1 day, 11:30:25  iter: 4599  total_loss: 0.7444  loss_cls: 0.4229  loss_box_reg: 0.2833  loss_query: 0.02891  time: 2.2775  data_time: 0.0063  lr: 0.006  max_mem: 10555M\n",
            "\u001b[32m[04/15 10:55:49 d2.utils.events]: \u001b[0m eta: 1 day, 11:30:26  iter: 4619  total_loss: 0.7275  loss_cls: 0.3891  loss_box_reg: 0.2444  loss_query: 0.03386  time: 2.2775  data_time: 0.0079  lr: 0.006  max_mem: 10555M\n",
            "\u001b[32m[04/15 10:56:35 d2.utils.events]: \u001b[0m eta: 1 day, 11:28:52  iter: 4639  total_loss: 0.6995  loss_cls: 0.3942  loss_box_reg: 0.2573  loss_query: 0.03406  time: 2.2774  data_time: 0.0060  lr: 0.006  max_mem: 10555M\n",
            "\u001b[32m[04/15 10:57:22 d2.utils.events]: \u001b[0m eta: 1 day, 11:27:23  iter: 4659  total_loss: 0.4991  loss_cls: 0.3264  loss_box_reg: 0.1496  loss_query: 0.02842  time: 2.2775  data_time: 0.0065  lr: 0.006  max_mem: 10555M\n",
            "\u001b[32m[04/15 10:58:09 d2.utils.events]: \u001b[0m eta: 1 day, 11:25:46  iter: 4679  total_loss: 0.7283  loss_cls: 0.4539  loss_box_reg: 0.2448  loss_query: 0.03539  time: 2.2776  data_time: 0.0076  lr: 0.006  max_mem: 10555M\n",
            "\u001b[32m[04/15 10:58:54 d2.utils.events]: \u001b[0m eta: 1 day, 11:24:56  iter: 4699  total_loss: 0.5517  loss_cls: 0.336  loss_box_reg: 0.194  loss_query: 0.03419  time: 2.2771  data_time: 0.0066  lr: 0.006  max_mem: 10555M\n",
            "\u001b[32m[04/15 10:59:42 d2.utils.events]: \u001b[0m eta: 1 day, 11:25:48  iter: 4719  total_loss: 0.7159  loss_cls: 0.4285  loss_box_reg: 0.2585  loss_query: 0.03661  time: 2.2773  data_time: 0.0072  lr: 0.006  max_mem: 10555M\n",
            "\u001b[32m[04/15 11:00:27 d2.utils.events]: \u001b[0m eta: 1 day, 11:23:14  iter: 4739  total_loss: 0.5834  loss_cls: 0.3097  loss_box_reg: 0.2123  loss_query: 0.01967  time: 2.2769  data_time: 0.0085  lr: 0.006  max_mem: 10555M\n",
            "\u001b[32m[04/15 11:01:13 d2.utils.events]: \u001b[0m eta: 1 day, 11:21:38  iter: 4759  total_loss: 0.4872  loss_cls: 0.3144  loss_box_reg: 0.1803  loss_query: 0.03104  time: 2.2769  data_time: 0.0064  lr: 0.006  max_mem: 10555M\n",
            "\u001b[32m[04/15 11:02:00 d2.utils.events]: \u001b[0m eta: 1 day, 11:21:42  iter: 4779  total_loss: 0.6131  loss_cls: 0.3578  loss_box_reg: 0.2188  loss_query: 0.02956  time: 2.2768  data_time: 0.0061  lr: 0.006  max_mem: 10555M\n",
            "\u001b[32m[04/15 11:02:45 d2.utils.events]: \u001b[0m eta: 1 day, 11:20:56  iter: 4799  total_loss: 0.6961  loss_cls: 0.3877  loss_box_reg: 0.2143  loss_query: 0.02105  time: 2.2766  data_time: 0.0063  lr: 0.006  max_mem: 10555M\n",
            "\u001b[32m[04/15 11:03:33 d2.utils.events]: \u001b[0m eta: 1 day, 11:20:40  iter: 4819  total_loss: 0.7878  loss_cls: 0.4827  loss_box_reg: 0.257  loss_query: 0.03604  time: 2.2769  data_time: 0.0077  lr: 0.006  max_mem: 10555M\n",
            "\u001b[32m[04/15 11:04:20 d2.utils.events]: \u001b[0m eta: 1 day, 11:19:24  iter: 4839  total_loss: 0.4903  loss_cls: 0.3374  loss_box_reg: 0.1607  loss_query: 0.02352  time: 2.2769  data_time: 0.0059  lr: 0.006  max_mem: 10555M\n",
            "\u001b[32m[04/15 11:05:06 d2.utils.events]: \u001b[0m eta: 1 day, 11:17:28  iter: 4859  total_loss: 0.6661  loss_cls: 0.3714  loss_box_reg: 0.2466  loss_query: 0.04124  time: 2.2768  data_time: 0.0065  lr: 0.006  max_mem: 10555M\n",
            "\u001b[32m[04/15 11:05:53 d2.utils.events]: \u001b[0m eta: 1 day, 11:16:47  iter: 4879  total_loss: 0.4438  loss_cls: 0.2657  loss_box_reg: 0.1399  loss_query: 0.02833  time: 2.2767  data_time: 0.0087  lr: 0.006  max_mem: 10555M\n",
            "\u001b[32m[04/15 11:06:40 d2.utils.events]: \u001b[0m eta: 1 day, 11:15:56  iter: 4899  total_loss: 0.7424  loss_cls: 0.4229  loss_box_reg: 0.2387  loss_query: 0.03081  time: 2.2767  data_time: 0.0063  lr: 0.006  max_mem: 10555M\n",
            "\u001b[32m[04/15 11:07:26 d2.utils.events]: \u001b[0m eta: 1 day, 11:14:04  iter: 4919  total_loss: 0.6315  loss_cls: 0.3476  loss_box_reg: 0.2427  loss_query: 0.0286  time: 2.2767  data_time: 0.0072  lr: 0.006  max_mem: 10555M\n",
            "\u001b[32m[04/15 11:08:12 d2.utils.events]: \u001b[0m eta: 1 day, 11:13:50  iter: 4939  total_loss: 0.5553  loss_cls: 0.3595  loss_box_reg: 0.1856  loss_query: 0.03864  time: 2.2766  data_time: 0.0064  lr: 0.006  max_mem: 10555M\n",
            "\u001b[32m[04/15 11:08:59 d2.utils.events]: \u001b[0m eta: 1 day, 11:13:55  iter: 4959  total_loss: 0.4376  loss_cls: 0.2773  loss_box_reg: 0.1452  loss_query: 0.02388  time: 2.2766  data_time: 0.0068  lr: 0.006  max_mem: 10555M\n",
            "\u001b[32m[04/15 11:09:46 d2.utils.events]: \u001b[0m eta: 1 day, 11:14:10  iter: 4979  total_loss: 0.6298  loss_cls: 0.3849  loss_box_reg: 0.2208  loss_query: 0.03112  time: 2.2767  data_time: 0.0071  lr: 0.006  max_mem: 10555M\n",
            "\u001b[32m[04/15 11:10:32 d2.utils.events]: \u001b[0m eta: 1 day, 11:12:11  iter: 4999  total_loss: 0.4306  loss_cls: 0.288  loss_box_reg: 0.1574  loss_query: 0.0165  time: 2.2765  data_time: 0.0070  lr: 0.006  max_mem: 10555M\n",
            "\u001b[32m[04/15 11:11:17 d2.utils.events]: \u001b[0m eta: 1 day, 11:10:11  iter: 5019  total_loss: 0.5174  loss_cls: 0.3037  loss_box_reg: 0.1927  loss_query: 0.02316  time: 2.2762  data_time: 0.0069  lr: 0.006  max_mem: 10555M\n",
            "\u001b[32m[04/15 11:12:04 d2.utils.events]: \u001b[0m eta: 1 day, 11:09:01  iter: 5039  total_loss: 0.6196  loss_cls: 0.3627  loss_box_reg: 0.2123  loss_query: 0.02247  time: 2.2762  data_time: 0.0067  lr: 0.006  max_mem: 10555M\n",
            "\u001b[32m[04/15 11:12:48 d2.utils.events]: \u001b[0m eta: 1 day, 11:06:53  iter: 5059  total_loss: 0.4977  loss_cls: 0.2787  loss_box_reg: 0.175  loss_query: 0.01894  time: 2.2757  data_time: 0.0080  lr: 0.006  max_mem: 10555M\n",
            "\u001b[32m[04/15 11:13:35 d2.utils.events]: \u001b[0m eta: 1 day, 11:06:32  iter: 5079  total_loss: 0.5641  loss_cls: 0.3233  loss_box_reg: 0.2314  loss_query: 0.021  time: 2.2758  data_time: 0.0077  lr: 0.006  max_mem: 10555M\n",
            "\u001b[32m[04/15 11:14:21 d2.utils.events]: \u001b[0m eta: 1 day, 11:05:21  iter: 5099  total_loss: 0.6013  loss_cls: 0.3496  loss_box_reg: 0.2322  loss_query: 0.02491  time: 2.2757  data_time: 0.0078  lr: 0.006  max_mem: 10555M\n",
            "\u001b[32m[04/15 11:15:09 d2.utils.events]: \u001b[0m eta: 1 day, 11:03:28  iter: 5119  total_loss: 0.5963  loss_cls: 0.3317  loss_box_reg: 0.2371  loss_query: 0.02799  time: 2.2758  data_time: 0.0070  lr: 0.006  max_mem: 10555M\n",
            "\u001b[32m[04/15 11:15:53 d2.utils.events]: \u001b[0m eta: 1 day, 11:01:31  iter: 5139  total_loss: 0.5608  loss_cls: 0.3475  loss_box_reg: 0.1854  loss_query: 0.034  time: 2.2753  data_time: 0.0067  lr: 0.006  max_mem: 10555M\n",
            "\u001b[32m[04/15 11:16:40 d2.utils.events]: \u001b[0m eta: 1 day, 11:00:08  iter: 5159  total_loss: 0.8041  loss_cls: 0.4249  loss_box_reg: 0.282  loss_query: 0.04348  time: 2.2753  data_time: 0.0068  lr: 0.006  max_mem: 10555M\n",
            "\u001b[32m[04/15 11:17:25 d2.utils.events]: \u001b[0m eta: 1 day, 10:59:08  iter: 5179  total_loss: 0.3968  loss_cls: 0.262  loss_box_reg: 0.1359  loss_query: 0.01794  time: 2.2751  data_time: 0.0066  lr: 0.006  max_mem: 10555M\n",
            "\u001b[32m[04/15 11:18:12 d2.utils.events]: \u001b[0m eta: 1 day, 10:57:47  iter: 5199  total_loss: 0.636  loss_cls: 0.3915  loss_box_reg: 0.2079  loss_query: 0.02435  time: 2.2751  data_time: 0.0067  lr: 0.006  max_mem: 10555M\n",
            "\u001b[32m[04/15 11:18:57 d2.utils.events]: \u001b[0m eta: 1 day, 10:55:48  iter: 5219  total_loss: 0.4093  loss_cls: 0.2549  loss_box_reg: 0.1513  loss_query: 0.01639  time: 2.2748  data_time: 0.0086  lr: 0.006  max_mem: 10555M\n",
            "\u001b[32m[04/15 11:19:44 d2.utils.events]: \u001b[0m eta: 1 day, 10:54:36  iter: 5239  total_loss: 0.7951  loss_cls: 0.4768  loss_box_reg: 0.2565  loss_query: 0.02659  time: 2.2747  data_time: 0.0076  lr: 0.006  max_mem: 10555M\n",
            "\u001b[32m[04/15 11:20:30 d2.utils.events]: \u001b[0m eta: 1 day, 10:52:34  iter: 5259  total_loss: 0.6199  loss_cls: 0.3613  loss_box_reg: 0.2212  loss_query: 0.0205  time: 2.2746  data_time: 0.0068  lr: 0.006  max_mem: 10555M\n",
            "\u001b[32m[04/15 11:21:17 d2.utils.events]: \u001b[0m eta: 1 day, 10:51:27  iter: 5279  total_loss: 0.6987  loss_cls: 0.4143  loss_box_reg: 0.2523  loss_query: 0.0333  time: 2.2747  data_time: 0.0074  lr: 0.006  max_mem: 10555M\n",
            "\u001b[32m[04/15 11:22:04 d2.utils.events]: \u001b[0m eta: 1 day, 10:50:21  iter: 5299  total_loss: 0.5309  loss_cls: 0.2969  loss_box_reg: 0.1747  loss_query: 0.03365  time: 2.2748  data_time: 0.0073  lr: 0.006  max_mem: 10555M\n",
            "\u001b[32m[04/15 11:22:50 d2.utils.events]: \u001b[0m eta: 1 day, 10:49:19  iter: 5319  total_loss: 0.5351  loss_cls: 0.3131  loss_box_reg: 0.1961  loss_query: 0.01776  time: 2.2746  data_time: 0.0110  lr: 0.006  max_mem: 10555M\n",
            "\u001b[32m[04/15 11:23:36 d2.utils.events]: \u001b[0m eta: 1 day, 10:48:49  iter: 5339  total_loss: 0.8121  loss_cls: 0.5046  loss_box_reg: 0.3288  loss_query: 0.03242  time: 2.2746  data_time: 0.0065  lr: 0.006  max_mem: 10555M\n",
            "\u001b[32m[04/15 11:24:23 d2.utils.events]: \u001b[0m eta: 1 day, 10:47:47  iter: 5359  total_loss: 0.4571  loss_cls: 0.2687  loss_box_reg: 0.1653  loss_query: 0.02192  time: 2.2746  data_time: 0.0062  lr: 0.006  max_mem: 10555M\n",
            "\u001b[32m[04/15 11:25:09 d2.utils.events]: \u001b[0m eta: 1 day, 10:48:12  iter: 5379  total_loss: 0.6655  loss_cls: 0.4201  loss_box_reg: 0.2544  loss_query: 0.02724  time: 2.2746  data_time: 0.0068  lr: 0.006  max_mem: 10555M\n",
            "\u001b[32m[04/15 11:25:56 d2.utils.events]: \u001b[0m eta: 1 day, 10:46:51  iter: 5399  total_loss: 0.4414  loss_cls: 0.2735  loss_box_reg: 0.1419  loss_query: 0.02392  time: 2.2745  data_time: 0.0065  lr: 0.006  max_mem: 10555M\n",
            "\u001b[32m[04/15 11:26:43 d2.utils.events]: \u001b[0m eta: 1 day, 10:47:52  iter: 5419  total_loss: 0.6549  loss_cls: 0.3765  loss_box_reg: 0.2262  loss_query: 0.03746  time: 2.2746  data_time: 0.0090  lr: 0.006  max_mem: 10555M\n",
            "\u001b[32m[04/15 11:27:30 d2.utils.events]: \u001b[0m eta: 1 day, 10:45:05  iter: 5439  total_loss: 0.4601  loss_cls: 0.2887  loss_box_reg: 0.1526  loss_query: 0.02521  time: 2.2746  data_time: 0.0088  lr: 0.006  max_mem: 10555M\n",
            "\u001b[32m[04/15 11:28:18 d2.utils.events]: \u001b[0m eta: 1 day, 10:45:09  iter: 5459  total_loss: 0.6579  loss_cls: 0.3535  loss_box_reg: 0.2524  loss_query: 0.03925  time: 2.2749  data_time: 0.0073  lr: 0.006  max_mem: 10555M\n",
            "\u001b[32m[04/15 11:29:04 d2.utils.events]: \u001b[0m eta: 1 day, 10:44:23  iter: 5479  total_loss: 0.5618  loss_cls: 0.3408  loss_box_reg: 0.1633  loss_query: 0.03002  time: 2.2748  data_time: 0.0066  lr: 0.006  max_mem: 10555M\n",
            "\u001b[32m[04/15 11:29:51 d2.utils.events]: \u001b[0m eta: 1 day, 10:43:56  iter: 5499  total_loss: 0.6485  loss_cls: 0.3967  loss_box_reg: 0.2161  loss_query: 0.02985  time: 2.2749  data_time: 0.0070  lr: 0.006  max_mem: 10555M\n",
            "\u001b[32m[04/15 11:30:37 d2.utils.events]: \u001b[0m eta: 1 day, 10:44:42  iter: 5519  total_loss: 0.4679  loss_cls: 0.2729  loss_box_reg: 0.1754  loss_query: 0.02356  time: 2.2748  data_time: 0.0068  lr: 0.006  max_mem: 10555M\n",
            "\u001b[32m[04/15 11:31:24 d2.utils.events]: \u001b[0m eta: 1 day, 10:42:05  iter: 5539  total_loss: 0.5329  loss_cls: 0.3901  loss_box_reg: 0.1976  loss_query: 0.02043  time: 2.2747  data_time: 0.0069  lr: 0.006  max_mem: 10555M\n",
            "\u001b[32m[04/15 11:32:11 d2.utils.events]: \u001b[0m eta: 1 day, 10:41:14  iter: 5559  total_loss: 0.4773  loss_cls: 0.33  loss_box_reg: 0.1494  loss_query: 0.02012  time: 2.2748  data_time: 0.0072  lr: 0.006  max_mem: 10555M\n",
            "\u001b[32m[04/15 11:32:55 d2.utils.events]: \u001b[0m eta: 1 day, 10:39:37  iter: 5579  total_loss: 0.5875  loss_cls: 0.3609  loss_box_reg: 0.1978  loss_query: 0.02819  time: 2.2744  data_time: 0.0068  lr: 0.006  max_mem: 10555M\n",
            "\u001b[32m[04/15 11:33:42 d2.utils.events]: \u001b[0m eta: 1 day, 10:39:34  iter: 5599  total_loss: 0.5333  loss_cls: 0.3282  loss_box_reg: 0.1892  loss_query: 0.03068  time: 2.2745  data_time: 0.0064  lr: 0.006  max_mem: 10555M\n",
            "\u001b[32m[04/15 11:34:30 d2.utils.events]: \u001b[0m eta: 1 day, 10:38:56  iter: 5619  total_loss: 0.417  loss_cls: 0.2632  loss_box_reg: 0.1383  loss_query: 0.02165  time: 2.2748  data_time: 0.0072  lr: 0.006  max_mem: 10555M\n",
            "\u001b[32m[04/15 11:35:17 d2.utils.events]: \u001b[0m eta: 1 day, 10:38:10  iter: 5639  total_loss: 0.5159  loss_cls: 0.3031  loss_box_reg: 0.1955  loss_query: 0.03101  time: 2.2747  data_time: 0.0075  lr: 0.006  max_mem: 10555M\n",
            "\u001b[32m[04/15 11:36:01 d2.utils.events]: \u001b[0m eta: 1 day, 10:36:58  iter: 5659  total_loss: 0.7056  loss_cls: 0.4037  loss_box_reg: 0.2548  loss_query: 0.03985  time: 2.2743  data_time: 0.0073  lr: 0.006  max_mem: 10555M\n",
            "\u001b[32m[04/15 11:36:48 d2.utils.events]: \u001b[0m eta: 1 day, 10:35:55  iter: 5679  total_loss: 0.5479  loss_cls: 0.3158  loss_box_reg: 0.2066  loss_query: 0.02946  time: 2.2744  data_time: 0.0067  lr: 0.006  max_mem: 10555M\n",
            "\u001b[32m[04/15 11:37:35 d2.utils.events]: \u001b[0m eta: 1 day, 10:36:44  iter: 5699  total_loss: 0.6269  loss_cls: 0.3535  loss_box_reg: 0.2165  loss_query: 0.0313  time: 2.2745  data_time: 0.0081  lr: 0.006  max_mem: 10555M\n",
            "\u001b[32m[04/15 11:38:20 d2.utils.events]: \u001b[0m eta: 1 day, 10:34:15  iter: 5719  total_loss: 0.481  loss_cls: 0.2789  loss_box_reg: 0.189  loss_query: 0.0239  time: 2.2742  data_time: 0.0076  lr: 0.006  max_mem: 10555M\n",
            "\u001b[32m[04/15 11:39:08 d2.utils.events]: \u001b[0m eta: 1 day, 10:34:13  iter: 5739  total_loss: 0.8876  loss_cls: 0.5625  loss_box_reg: 0.2882  loss_query: 0.02698  time: 2.2744  data_time: 0.0063  lr: 0.006  max_mem: 10555M\n",
            "\u001b[32m[04/15 11:39:53 d2.utils.events]: \u001b[0m eta: 1 day, 10:32:44  iter: 5759  total_loss: 0.6076  loss_cls: 0.3885  loss_box_reg: 0.2077  loss_query: 0.02552  time: 2.2741  data_time: 0.0076  lr: 0.006  max_mem: 10555M\n",
            "\u001b[32m[04/15 11:40:40 d2.utils.events]: \u001b[0m eta: 1 day, 10:31:57  iter: 5779  total_loss: 0.568  loss_cls: 0.3807  loss_box_reg: 0.1877  loss_query: 0.02594  time: 2.2741  data_time: 0.0064  lr: 0.006  max_mem: 10555M\n",
            "\u001b[32m[04/15 11:41:25 d2.utils.events]: \u001b[0m eta: 1 day, 10:31:11  iter: 5799  total_loss: 0.6327  loss_cls: 0.371  loss_box_reg: 0.258  loss_query: 0.0214  time: 2.2739  data_time: 0.0102  lr: 0.006  max_mem: 10555M\n",
            "\u001b[32m[04/15 11:42:13 d2.utils.events]: \u001b[0m eta: 1 day, 10:29:54  iter: 5819  total_loss: 0.6264  loss_cls: 0.3864  loss_box_reg: 0.2192  loss_query: 0.03578  time: 2.2740  data_time: 0.0058  lr: 0.006  max_mem: 10555M\n",
            "\u001b[32m[04/15 11:43:00 d2.utils.events]: \u001b[0m eta: 1 day, 10:29:41  iter: 5839  total_loss: 0.7395  loss_cls: 0.4943  loss_box_reg: 0.228  loss_query: 0.04091  time: 2.2741  data_time: 0.0066  lr: 0.006  max_mem: 10555M\n",
            "\u001b[32m[04/15 11:43:47 d2.utils.events]: \u001b[0m eta: 1 day, 10:29:20  iter: 5859  total_loss: 0.4697  loss_cls: 0.2937  loss_box_reg: 0.1567  loss_query: 0.017  time: 2.2742  data_time: 0.0068  lr: 0.006  max_mem: 10555M\n",
            "\u001b[32m[04/15 11:44:33 d2.utils.events]: \u001b[0m eta: 1 day, 10:28:54  iter: 5879  total_loss: 0.7721  loss_cls: 0.4616  loss_box_reg: 0.2576  loss_query: 0.03094  time: 2.2742  data_time: 0.0066  lr: 0.006  max_mem: 10555M\n",
            "\u001b[32m[04/15 11:45:20 d2.utils.events]: \u001b[0m eta: 1 day, 10:27:30  iter: 5899  total_loss: 0.5085  loss_cls: 0.3139  loss_box_reg: 0.1896  loss_query: 0.02176  time: 2.2741  data_time: 0.0059  lr: 0.006  max_mem: 10555M\n",
            "\u001b[32m[04/15 11:46:06 d2.utils.events]: \u001b[0m eta: 1 day, 10:27:04  iter: 5919  total_loss: 0.626  loss_cls: 0.402  loss_box_reg: 0.2003  loss_query: 0.0266  time: 2.2740  data_time: 0.0062  lr: 0.006  max_mem: 10555M\n",
            "\u001b[32m[04/15 11:46:54 d2.utils.events]: \u001b[0m eta: 1 day, 10:26:44  iter: 5939  total_loss: 0.6857  loss_cls: 0.403  loss_box_reg: 0.2572  loss_query: 0.02895  time: 2.2743  data_time: 0.0070  lr: 0.006  max_mem: 10555M\n",
            "\u001b[32m[04/15 11:47:41 d2.utils.events]: \u001b[0m eta: 1 day, 10:26:48  iter: 5959  total_loss: 0.6061  loss_cls: 0.3978  loss_box_reg: 0.1854  loss_query: 0.03371  time: 2.2743  data_time: 0.0068  lr: 0.006  max_mem: 10555M\n",
            "\u001b[32m[04/15 11:48:28 d2.utils.events]: \u001b[0m eta: 1 day, 10:25:12  iter: 5979  total_loss: 0.5992  loss_cls: 0.3962  loss_box_reg: 0.1784  loss_query: 0.03337  time: 2.2743  data_time: 0.0070  lr: 0.006  max_mem: 10555M\n",
            "\u001b[32m[04/15 11:49:16 fvcore.common.checkpoint]: \u001b[0mSaving checkpoint to work_dirs/visdrone_querydet/model_0005999.pth\n",
            "\u001b[32m[04/15 11:49:30 d2.utils.events]: \u001b[0m eta: 1 day, 10:26:22  iter: 5999  total_loss: 0.5889  loss_cls: 0.3617  loss_box_reg: 0.205  loss_query: 0.03528  time: 2.2745  data_time: 0.0070  lr: 0.006  max_mem: 10555M\n",
            "\u001b[32m[04/15 11:50:17 d2.utils.events]: \u001b[0m eta: 1 day, 10:27:52  iter: 6019  total_loss: 0.7088  loss_cls: 0.4124  loss_box_reg: 0.2303  loss_query: 0.03534  time: 2.2747  data_time: 0.0069  lr: 0.006  max_mem: 10555M\n",
            "\u001b[32m[04/15 11:51:04 d2.utils.events]: \u001b[0m eta: 1 day, 10:27:16  iter: 6039  total_loss: 0.6126  loss_cls: 0.3808  loss_box_reg: 0.2047  loss_query: 0.03287  time: 2.2747  data_time: 0.0058  lr: 0.006  max_mem: 10555M\n",
            "\u001b[32m[04/15 11:51:51 d2.utils.events]: \u001b[0m eta: 1 day, 10:27:43  iter: 6059  total_loss: 0.7471  loss_cls: 0.4535  loss_box_reg: 0.2786  loss_query: 0.04834  time: 2.2747  data_time: 0.0068  lr: 0.006  max_mem: 10555M\n",
            "\u001b[32m[04/15 11:52:37 d2.utils.events]: \u001b[0m eta: 1 day, 10:26:32  iter: 6079  total_loss: 0.5696  loss_cls: 0.3491  loss_box_reg: 0.217  loss_query: 0.03512  time: 2.2745  data_time: 0.0067  lr: 0.006  max_mem: 10555M\n",
            "\u001b[32m[04/15 11:53:23 d2.utils.events]: \u001b[0m eta: 1 day, 10:25:09  iter: 6099  total_loss: 0.6775  loss_cls: 0.3899  loss_box_reg: 0.1986  loss_query: 0.02732  time: 2.2744  data_time: 0.0066  lr: 0.006  max_mem: 10555M\n",
            "\u001b[32m[04/15 11:54:09 d2.utils.events]: \u001b[0m eta: 1 day, 10:24:12  iter: 6119  total_loss: 0.8399  loss_cls: 0.4607  loss_box_reg: 0.3228  loss_query: 0.04358  time: 2.2744  data_time: 0.0080  lr: 0.006  max_mem: 10555M\n",
            "\u001b[32m[04/15 11:54:55 d2.utils.events]: \u001b[0m eta: 1 day, 10:23:26  iter: 6139  total_loss: 0.5755  loss_cls: 0.3538  loss_box_reg: 0.2049  loss_query: 0.02589  time: 2.2743  data_time: 0.0069  lr: 0.006  max_mem: 10555M\n",
            "\u001b[32m[04/15 11:55:42 d2.utils.events]: \u001b[0m eta: 1 day, 10:23:13  iter: 6159  total_loss: 0.7328  loss_cls: 0.442  loss_box_reg: 0.2339  loss_query: 0.01997  time: 2.2744  data_time: 0.0065  lr: 0.006  max_mem: 10555M\n",
            "\u001b[32m[04/15 11:56:28 d2.utils.events]: \u001b[0m eta: 1 day, 10:21:54  iter: 6179  total_loss: 0.6219  loss_cls: 0.4077  loss_box_reg: 0.2283  loss_query: 0.02343  time: 2.2743  data_time: 0.0074  lr: 0.006  max_mem: 10555M\n",
            "\u001b[32m[04/15 11:57:14 d2.utils.events]: \u001b[0m eta: 1 day, 10:21:56  iter: 6199  total_loss: 0.777  loss_cls: 0.4754  loss_box_reg: 0.2829  loss_query: 0.03958  time: 2.2742  data_time: 0.0080  lr: 0.006  max_mem: 10555M\n",
            "\u001b[32m[04/15 11:57:59 d2.utils.events]: \u001b[0m eta: 1 day, 10:21:18  iter: 6219  total_loss: 0.5089  loss_cls: 0.293  loss_box_reg: 0.1782  loss_query: 0.02341  time: 2.2738  data_time: 0.0082  lr: 0.006  max_mem: 10555M\n",
            "\u001b[32m[04/15 11:58:45 d2.utils.events]: \u001b[0m eta: 1 day, 10:21:01  iter: 6239  total_loss: 0.9337  loss_cls: 0.4731  loss_box_reg: 0.3171  loss_query: 0.03567  time: 2.2737  data_time: 0.0064  lr: 0.006  max_mem: 10555M\n",
            "\u001b[32m[04/15 11:59:31 d2.utils.events]: \u001b[0m eta: 1 day, 10:21:19  iter: 6259  total_loss: 0.4962  loss_cls: 0.2788  loss_box_reg: 0.1665  loss_query: 0.02864  time: 2.2737  data_time: 0.0064  lr: 0.006  max_mem: 10555M\n",
            "\u001b[32m[04/15 12:00:19 d2.utils.events]: \u001b[0m eta: 1 day, 10:21:17  iter: 6279  total_loss: 0.4627  loss_cls: 0.3006  loss_box_reg: 0.1601  loss_query: 0.0255  time: 2.2739  data_time: 0.0063  lr: 0.006  max_mem: 10555M\n",
            "\u001b[32m[04/15 12:01:07 d2.utils.events]: \u001b[0m eta: 1 day, 10:20:44  iter: 6299  total_loss: 0.7543  loss_cls: 0.4251  loss_box_reg: 0.2879  loss_query: 0.03647  time: 2.2741  data_time: 0.0064  lr: 0.006  max_mem: 10555M\n",
            "\u001b[32m[04/15 12:01:53 d2.utils.events]: \u001b[0m eta: 1 day, 10:20:54  iter: 6319  total_loss: 0.6023  loss_cls: 0.3367  loss_box_reg: 0.2229  loss_query: 0.02663  time: 2.2741  data_time: 0.0091  lr: 0.006  max_mem: 10555M\n",
            "\u001b[32m[04/15 12:02:39 d2.utils.events]: \u001b[0m eta: 1 day, 10:19:56  iter: 6339  total_loss: 0.5448  loss_cls: 0.3234  loss_box_reg: 0.1874  loss_query: 0.02488  time: 2.2740  data_time: 0.0069  lr: 0.006  max_mem: 10555M\n",
            "\u001b[32m[04/15 12:03:26 d2.utils.events]: \u001b[0m eta: 1 day, 10:19:22  iter: 6359  total_loss: 0.6074  loss_cls: 0.3668  loss_box_reg: 0.2189  loss_query: 0.02999  time: 2.2739  data_time: 0.0080  lr: 0.006  max_mem: 10555M\n",
            "\u001b[32m[04/15 12:04:12 d2.utils.events]: \u001b[0m eta: 1 day, 10:17:30  iter: 6379  total_loss: 0.5495  loss_cls: 0.3223  loss_box_reg: 0.2121  loss_query: 0.01284  time: 2.2739  data_time: 0.0059  lr: 0.006  max_mem: 10555M\n",
            "\u001b[32m[04/15 12:04:59 d2.utils.events]: \u001b[0m eta: 1 day, 10:16:44  iter: 6399  total_loss: 0.7682  loss_cls: 0.4221  loss_box_reg: 0.289  loss_query: 0.04013  time: 2.2739  data_time: 0.0072  lr: 0.006  max_mem: 10555M\n",
            "\u001b[32m[04/15 12:05:43 d2.utils.events]: \u001b[0m eta: 1 day, 10:14:08  iter: 6419  total_loss: 0.4681  loss_cls: 0.2746  loss_box_reg: 0.1657  loss_query: 0.02992  time: 2.2736  data_time: 0.0076  lr: 0.006  max_mem: 10555M\n",
            "\u001b[32m[04/15 12:06:29 d2.utils.events]: \u001b[0m eta: 1 day, 10:13:22  iter: 6439  total_loss: 0.6093  loss_cls: 0.3851  loss_box_reg: 0.2115  loss_query: 0.0298  time: 2.2734  data_time: 0.0067  lr: 0.006  max_mem: 10555M\n",
            "\u001b[32m[04/15 12:07:16 d2.utils.events]: \u001b[0m eta: 1 day, 10:12:36  iter: 6459  total_loss: 0.7032  loss_cls: 0.4058  loss_box_reg: 0.2584  loss_query: 0.04054  time: 2.2734  data_time: 0.0068  lr: 0.006  max_mem: 10555M\n",
            "\u001b[32m[04/15 12:08:01 d2.utils.events]: \u001b[0m eta: 1 day, 10:11:31  iter: 6479  total_loss: 0.5942  loss_cls: 0.3492  loss_box_reg: 0.1954  loss_query: 0.03348  time: 2.2732  data_time: 0.0073  lr: 0.006  max_mem: 10555M\n",
            "\u001b[32m[04/15 12:08:48 d2.utils.events]: \u001b[0m eta: 1 day, 10:10:25  iter: 6499  total_loss: 0.474  loss_cls: 0.2788  loss_box_reg: 0.167  loss_query: 0.0345  time: 2.2733  data_time: 0.0065  lr: 0.006  max_mem: 10555M\n",
            "\u001b[32m[04/15 12:09:36 d2.utils.events]: \u001b[0m eta: 1 day, 10:09:56  iter: 6519  total_loss: 0.6349  loss_cls: 0.3495  loss_box_reg: 0.2378  loss_query: 0.03757  time: 2.2734  data_time: 0.0074  lr: 0.006  max_mem: 10555M\n",
            "\u001b[32m[04/15 12:10:21 d2.utils.events]: \u001b[0m eta: 1 day, 10:09:30  iter: 6539  total_loss: 0.4997  loss_cls: 0.289  loss_box_reg: 0.1945  loss_query: 0.02255  time: 2.2732  data_time: 0.0071  lr: 0.006  max_mem: 10555M\n",
            "\u001b[32m[04/15 12:11:07 d2.utils.events]: \u001b[0m eta: 1 day, 10:08:46  iter: 6559  total_loss: 0.613  loss_cls: 0.3419  loss_box_reg: 0.2033  loss_query: 0.02595  time: 2.2731  data_time: 0.0072  lr: 0.006  max_mem: 10555M\n",
            "\u001b[32m[04/15 12:11:54 d2.utils.events]: \u001b[0m eta: 1 day, 10:10:24  iter: 6579  total_loss: 0.7195  loss_cls: 0.4137  loss_box_reg: 0.2826  loss_query: 0.02637  time: 2.2731  data_time: 0.0067  lr: 0.006  max_mem: 10555M\n",
            "\u001b[32m[04/15 12:12:41 d2.utils.events]: \u001b[0m eta: 1 day, 10:09:03  iter: 6599  total_loss: 0.577  loss_cls: 0.3452  loss_box_reg: 0.1901  loss_query: 0.02028  time: 2.2732  data_time: 0.0060  lr: 0.006  max_mem: 10555M\n",
            "\u001b[32m[04/15 12:13:28 d2.utils.events]: \u001b[0m eta: 1 day, 10:07:10  iter: 6619  total_loss: 0.6971  loss_cls: 0.4199  loss_box_reg: 0.2115  loss_query: 0.02705  time: 2.2733  data_time: 0.0070  lr: 0.006  max_mem: 10555M\n",
            "\u001b[32m[04/15 12:14:15 d2.utils.events]: \u001b[0m eta: 1 day, 10:07:55  iter: 6639  total_loss: 0.8986  loss_cls: 0.5376  loss_box_reg: 0.2936  loss_query: 0.03543  time: 2.2733  data_time: 0.0062  lr: 0.006  max_mem: 10555M\n",
            "\u001b[32m[04/15 12:15:01 d2.utils.events]: \u001b[0m eta: 1 day, 10:07:45  iter: 6659  total_loss: 0.5914  loss_cls: 0.3502  loss_box_reg: 0.22  loss_query: 0.04028  time: 2.2732  data_time: 0.0091  lr: 0.006  max_mem: 10555M\n",
            "\u001b[32m[04/15 12:15:49 d2.utils.events]: \u001b[0m eta: 1 day, 10:07:25  iter: 6679  total_loss: 0.5342  loss_cls: 0.3353  loss_box_reg: 0.1795  loss_query: 0.03075  time: 2.2734  data_time: 0.0077  lr: 0.006  max_mem: 10555M\n",
            "\u001b[32m[04/15 12:16:36 d2.utils.events]: \u001b[0m eta: 1 day, 10:06:32  iter: 6699  total_loss: 0.6979  loss_cls: 0.4043  loss_box_reg: 0.218  loss_query: 0.02944  time: 2.2735  data_time: 0.0067  lr: 0.006  max_mem: 10555M\n",
            "\u001b[32m[04/15 12:17:23 d2.utils.events]: \u001b[0m eta: 1 day, 10:06:32  iter: 6719  total_loss: 0.5491  loss_cls: 0.3048  loss_box_reg: 0.2075  loss_query: 0.03324  time: 2.2735  data_time: 0.0076  lr: 0.006  max_mem: 10555M\n",
            "\u001b[32m[04/15 12:18:10 d2.utils.events]: \u001b[0m eta: 1 day, 10:05:47  iter: 6739  total_loss: 0.634  loss_cls: 0.372  loss_box_reg: 0.2274  loss_query: 0.03102  time: 2.2735  data_time: 0.0092  lr: 0.006  max_mem: 10555M\n",
            "\u001b[32m[04/15 12:18:55 d2.utils.events]: \u001b[0m eta: 1 day, 10:05:26  iter: 6759  total_loss: 0.5465  loss_cls: 0.3234  loss_box_reg: 0.1914  loss_query: 0.03175  time: 2.2733  data_time: 0.0061  lr: 0.006  max_mem: 10555M\n",
            "\u001b[32m[04/15 12:19:42 d2.utils.events]: \u001b[0m eta: 1 day, 10:04:54  iter: 6779  total_loss: 0.6697  loss_cls: 0.4005  loss_box_reg: 0.2497  loss_query: 0.03674  time: 2.2734  data_time: 0.0107  lr: 0.006  max_mem: 10555M\n",
            "--- Logging error ---\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.7/logging/__init__.py\", line 1029, in emit\n",
            "    self.flush()\n",
            "  File \"/usr/local/lib/python3.7/logging/__init__.py\", line 1009, in flush\n",
            "    self.stream.flush()\n",
            "OSError: [Errno 107] Transport endpoint is not connected\n",
            "Call stack:\n",
            "  File \"train_visdrone.py\", line 18, in <module>\n",
            "  File \"/usr/local/lib/python3.7/site-packages/detectron2/engine/launch.py\", line 62, in launch\n",
            "    main_func(*args)\n",
            "  File \"/content/gdrive/MyDrive/.shared/QueryDet-PyTorch/train_tools/visdrone_train.py\", line 250, in start_train\n",
            "  File \"/usr/local/lib/python3.7/site-packages/detectron2/engine/defaults.py\", line 431, in train\n",
            "    super().train(self.start_iter, self.max_iter)\n",
            "  File \"/usr/local/lib/python3.7/site-packages/detectron2/engine/train_loop.py\", line 139, in train\n",
            "    self.after_step()\n",
            "  File \"/usr/local/lib/python3.7/site-packages/detectron2/engine/train_loop.py\", line 169, in after_step\n",
            "    h.after_step()\n",
            "  File \"/usr/local/lib/python3.7/site-packages/detectron2/engine/hooks.py\", line 173, in after_step\n",
            "    writer.write()\n",
            "  File \"/usr/local/lib/python3.7/site-packages/detectron2/utils/events.py\", line 269, in write\n",
            "    memory=\"max_mem: {:.0f}M\".format(max_mem_mb) if max_mem_mb is not None else \"\",\n",
            "Message: ' eta: 1 day, 10:04:54  iter: 6779  total_loss: 0.6697  loss_cls: 0.4005  loss_box_reg: 0.2497  loss_query: 0.03674  time: 2.2734  data_time: 0.0107  lr: 0.006  max_mem: 10555M'\n",
            "Arguments: ()\n",
            "\u001b[4m\u001b[5m\u001b[31mERROR\u001b[0m \u001b[32m[04/15 12:20:05 d2.engine.train_loop]: \u001b[0mException during training:\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.7/site-packages/detectron2/engine/train_loop.py\", line 139, in train\n",
            "    self.after_step()\n",
            "  File \"/usr/local/lib/python3.7/site-packages/detectron2/engine/train_loop.py\", line 169, in after_step\n",
            "    h.after_step()\n",
            "  File \"/usr/local/lib/python3.7/site-packages/detectron2/engine/hooks.py\", line 173, in after_step\n",
            "    writer.write()\n",
            "  File \"/usr/local/lib/python3.7/site-packages/detectron2/utils/events.py\", line 121, in write\n",
            "    self._file_handle.flush()\n",
            "OSError: [Errno 107] Transport endpoint is not connected\n",
            "--- Logging error ---\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.7/site-packages/detectron2/engine/train_loop.py\", line 139, in train\n",
            "    self.after_step()\n",
            "  File \"/usr/local/lib/python3.7/site-packages/detectron2/engine/train_loop.py\", line 169, in after_step\n",
            "    h.after_step()\n",
            "  File \"/usr/local/lib/python3.7/site-packages/detectron2/engine/hooks.py\", line 173, in after_step\n",
            "    writer.write()\n",
            "  File \"/usr/local/lib/python3.7/site-packages/detectron2/utils/events.py\", line 121, in write\n",
            "    self._file_handle.flush()\n",
            "OSError: [Errno 107] Transport endpoint is not connected\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.7/logging/__init__.py\", line 1029, in emit\n",
            "    self.flush()\n",
            "  File \"/usr/local/lib/python3.7/logging/__init__.py\", line 1009, in flush\n",
            "    self.stream.flush()\n",
            "OSError: [Errno 107] Transport endpoint is not connected\n",
            "Call stack:\n",
            "  File \"train_visdrone.py\", line 18, in <module>\n",
            "  File \"/usr/local/lib/python3.7/site-packages/detectron2/engine/launch.py\", line 62, in launch\n",
            "    main_func(*args)\n",
            "  File \"/content/gdrive/MyDrive/.shared/QueryDet-PyTorch/train_tools/visdrone_train.py\", line 250, in start_train\n",
            "  File \"/usr/local/lib/python3.7/site-packages/detectron2/engine/defaults.py\", line 431, in train\n",
            "    super().train(self.start_iter, self.max_iter)\n",
            "  File \"/usr/local/lib/python3.7/site-packages/detectron2/engine/train_loop.py\", line 145, in train\n",
            "    logger.exception(\"Exception during training:\")\n",
            "Message: 'Exception during training:'\n",
            "Arguments: ()\n",
            "\u001b[32m[04/15 12:20:05 d2.engine.hooks]: \u001b[0mOverall training speed: 6777 iterations in 4:16:49 (2.2737 s / it)\n",
            "--- Logging error ---\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.7/site-packages/detectron2/engine/train_loop.py\", line 139, in train\n",
            "    self.after_step()\n",
            "  File \"/usr/local/lib/python3.7/site-packages/detectron2/engine/train_loop.py\", line 169, in after_step\n",
            "    h.after_step()\n",
            "  File \"/usr/local/lib/python3.7/site-packages/detectron2/engine/hooks.py\", line 173, in after_step\n",
            "    writer.write()\n",
            "  File \"/usr/local/lib/python3.7/site-packages/detectron2/utils/events.py\", line 121, in write\n",
            "    self._file_handle.flush()\n",
            "OSError: [Errno 107] Transport endpoint is not connected\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.7/logging/__init__.py\", line 1029, in emit\n",
            "    self.flush()\n",
            "  File \"/usr/local/lib/python3.7/logging/__init__.py\", line 1009, in flush\n",
            "    self.stream.flush()\n",
            "OSError: [Errno 107] Transport endpoint is not connected\n",
            "Call stack:\n",
            "  File \"train_visdrone.py\", line 18, in <module>\n",
            "  File \"/usr/local/lib/python3.7/site-packages/detectron2/engine/launch.py\", line 62, in launch\n",
            "    main_func(*args)\n",
            "  File \"/content/gdrive/MyDrive/.shared/QueryDet-PyTorch/train_tools/visdrone_train.py\", line 250, in start_train\n",
            "  File \"/usr/local/lib/python3.7/site-packages/detectron2/engine/defaults.py\", line 431, in train\n",
            "    super().train(self.start_iter, self.max_iter)\n",
            "  File \"/usr/local/lib/python3.7/site-packages/detectron2/engine/train_loop.py\", line 148, in train\n",
            "    self.after_train()\n",
            "  File \"/usr/local/lib/python3.7/site-packages/detectron2/engine/train_loop.py\", line 157, in after_train\n",
            "    h.after_train()\n",
            "  File \"/usr/local/lib/python3.7/site-packages/detectron2/engine/hooks.py\", line 120, in after_train\n",
            "    total_time_minus_hooks / num_iter,\n",
            "Message: 'Overall training speed: 6777 iterations in 4:16:49 (2.2737 s / it)'\n",
            "Arguments: ()\n",
            "\u001b[32m[04/15 12:20:05 d2.engine.hooks]: \u001b[0mTotal training time: 4:24:32 (0:07:43 on hooks)\n",
            "--- Logging error ---\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.7/site-packages/detectron2/engine/train_loop.py\", line 139, in train\n",
            "    self.after_step()\n",
            "  File \"/usr/local/lib/python3.7/site-packages/detectron2/engine/train_loop.py\", line 169, in after_step\n",
            "    h.after_step()\n",
            "  File \"/usr/local/lib/python3.7/site-packages/detectron2/engine/hooks.py\", line 173, in after_step\n",
            "    writer.write()\n",
            "  File \"/usr/local/lib/python3.7/site-packages/detectron2/utils/events.py\", line 121, in write\n",
            "    self._file_handle.flush()\n",
            "OSError: [Errno 107] Transport endpoint is not connected\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.7/logging/__init__.py\", line 1029, in emit\n",
            "    self.flush()\n",
            "  File \"/usr/local/lib/python3.7/logging/__init__.py\", line 1009, in flush\n",
            "    self.stream.flush()\n",
            "OSError: [Errno 107] Transport endpoint is not connected\n",
            "Call stack:\n",
            "  File \"train_visdrone.py\", line 18, in <module>\n",
            "  File \"/usr/local/lib/python3.7/site-packages/detectron2/engine/launch.py\", line 62, in launch\n",
            "    main_func(*args)\n",
            "  File \"/content/gdrive/MyDrive/.shared/QueryDet-PyTorch/train_tools/visdrone_train.py\", line 250, in start_train\n",
            "  File \"/usr/local/lib/python3.7/site-packages/detectron2/engine/defaults.py\", line 431, in train\n",
            "    super().train(self.start_iter, self.max_iter)\n",
            "  File \"/usr/local/lib/python3.7/site-packages/detectron2/engine/train_loop.py\", line 148, in train\n",
            "    self.after_train()\n",
            "  File \"/usr/local/lib/python3.7/site-packages/detectron2/engine/train_loop.py\", line 157, in after_train\n",
            "    h.after_train()\n",
            "  File \"/usr/local/lib/python3.7/site-packages/detectron2/engine/hooks.py\", line 127, in after_train\n",
            "    str(datetime.timedelta(seconds=int(hook_time))),\n",
            "Message: 'Total training time: 4:24:32 (0:07:43 on hooks)'\n",
            "Arguments: ()\n",
            "\u001b[32m[04/15 12:20:06 d2.utils.events]: \u001b[0m eta: 1 day, 10:04:54  iter: 6779  total_loss: 0.6697  loss_cls: 0.4005  loss_box_reg: 0.2497  loss_query: 0.03674  time: 2.2734  data_time: 0.0107  lr: 0.006  max_mem: 10555M\n",
            "--- Logging error ---\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.7/site-packages/detectron2/engine/train_loop.py\", line 139, in train\n",
            "    self.after_step()\n",
            "  File \"/usr/local/lib/python3.7/site-packages/detectron2/engine/train_loop.py\", line 169, in after_step\n",
            "    h.after_step()\n",
            "  File \"/usr/local/lib/python3.7/site-packages/detectron2/engine/hooks.py\", line 173, in after_step\n",
            "    writer.write()\n",
            "  File \"/usr/local/lib/python3.7/site-packages/detectron2/utils/events.py\", line 121, in write\n",
            "    self._file_handle.flush()\n",
            "OSError: [Errno 107] Transport endpoint is not connected\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.7/logging/__init__.py\", line 1029, in emit\n",
            "    self.flush()\n",
            "  File \"/usr/local/lib/python3.7/logging/__init__.py\", line 1009, in flush\n",
            "    self.stream.flush()\n",
            "OSError: [Errno 107] Transport endpoint is not connected\n",
            "Call stack:\n",
            "  File \"train_visdrone.py\", line 18, in <module>\n",
            "  File \"/usr/local/lib/python3.7/site-packages/detectron2/engine/launch.py\", line 62, in launch\n",
            "    main_func(*args)\n",
            "  File \"/content/gdrive/MyDrive/.shared/QueryDet-PyTorch/train_tools/visdrone_train.py\", line 250, in start_train\n",
            "  File \"/usr/local/lib/python3.7/site-packages/detectron2/engine/defaults.py\", line 431, in train\n",
            "    super().train(self.start_iter, self.max_iter)\n",
            "  File \"/usr/local/lib/python3.7/site-packages/detectron2/engine/train_loop.py\", line 148, in train\n",
            "    self.after_train()\n",
            "  File \"/usr/local/lib/python3.7/site-packages/detectron2/engine/train_loop.py\", line 157, in after_train\n",
            "    h.after_train()\n",
            "  File \"/usr/local/lib/python3.7/site-packages/detectron2/engine/hooks.py\", line 179, in after_train\n",
            "    writer.write()\n",
            "  File \"/usr/local/lib/python3.7/site-packages/detectron2/utils/events.py\", line 269, in write\n",
            "    memory=\"max_mem: {:.0f}M\".format(max_mem_mb) if max_mem_mb is not None else \"\",\n",
            "Message: ' eta: 1 day, 10:04:54  iter: 6779  total_loss: 0.6697  loss_cls: 0.4005  loss_box_reg: 0.2497  loss_query: 0.03674  time: 2.2734  data_time: 0.0107  lr: 0.006  max_mem: 10555M'\n",
            "Arguments: ()\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.7/site-packages/detectron2/engine/train_loop.py\", line 139, in train\n",
            "    self.after_step()\n",
            "  File \"/usr/local/lib/python3.7/site-packages/detectron2/engine/train_loop.py\", line 169, in after_step\n",
            "    h.after_step()\n",
            "  File \"/usr/local/lib/python3.7/site-packages/detectron2/engine/hooks.py\", line 173, in after_step\n",
            "    writer.write()\n",
            "  File \"/usr/local/lib/python3.7/site-packages/detectron2/utils/events.py\", line 121, in write\n",
            "    self._file_handle.flush()\n",
            "OSError: [Errno 107] Transport endpoint is not connected\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"train_visdrone.py\", line 18, in <module>\n",
            "  File \"/usr/local/lib/python3.7/site-packages/detectron2/engine/launch.py\", line 62, in launch\n",
            "    main_func(*args)\n",
            "  File \"/content/gdrive/MyDrive/.shared/QueryDet-PyTorch/train_tools/visdrone_train.py\", line 250, in start_train\n",
            "  File \"/usr/local/lib/python3.7/site-packages/detectron2/engine/defaults.py\", line 431, in train\n",
            "    super().train(self.start_iter, self.max_iter)\n",
            "  File \"/usr/local/lib/python3.7/site-packages/detectron2/engine/train_loop.py\", line 148, in train\n",
            "    self.after_train()\n",
            "  File \"/usr/local/lib/python3.7/site-packages/detectron2/engine/train_loop.py\", line 157, in after_train\n",
            "    h.after_train()\n",
            "  File \"/usr/local/lib/python3.7/site-packages/detectron2/engine/hooks.py\", line 179, in after_train\n",
            "    writer.write()\n",
            "  File \"/usr/local/lib/python3.7/site-packages/detectron2/utils/events.py\", line 121, in write\n",
            "    self._file_handle.flush()\n",
            "OSError: [Errno 107] Transport endpoint is not connected\n",
            "Error in atexit._run_exitfuncs:\n",
            "OSError: [Errno 107] Transport endpoint is not connected\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "OSError: [Errno 107] Transport endpoint is not connected\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "qjBhf06r4Qoh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "qixiFyG_JMhs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!python train_visdrone.py --resume --config /content/gdrive/MyDrive/QueryDet-PyTorch/work_dirs/visdrone_querydet/config.yaml --num-gpu 1 OUTPUT_DIR work_dirs/visdrone_querydet"
      ],
      "metadata": {
        "id": "rlHLfP5BFq34",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "cb9aa451-ef40-425c-f942-883c6345a51c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "** fvcore version of PathManager will be deprecated soon. **\n",
            "** Please migrate to the version in iopath repo. **\n",
            "https://github.com/facebookresearch/iopath \n",
            "\n",
            "Command Line Args: Namespace(config_file='/content/gdrive/MyDrive/QueryDet-PyTorch/work_dirs/visdrone_querydet/config.yaml', dist_url='tcp://127.0.0.1:49152', eval_only=False, machine_rank=0, no_pretrain=False, num_gpus=1, num_machines=1, opts=['OUTPUT_DIR', 'work_dirs/visdrone_querydet'], resume=True)\n",
            "\u001b[32m[04/15 02:41:05 detectron2]: \u001b[0mRank of current process: 0. World size: 1\n",
            "\u001b[32m[04/15 02:41:07 detectron2]: \u001b[0mEnvironment info:\n",
            "----------------------  ---------------------------------------------------------------\n",
            "sys.platform            linux\n",
            "Python                  3.7.6 (default, Jan  8 2020, 19:59:22) [GCC 7.3.0]\n",
            "numpy                   1.21.6\n",
            "detectron2              0.4 @/usr/local/lib/python3.7/site-packages/detectron2\n",
            "Compiler                GCC 7.3\n",
            "CUDA compiler           CUDA 10.1\n",
            "detectron2 arch flags   3.7, 5.0, 5.2, 6.0, 6.1, 7.0, 7.5\n",
            "DETECTRON2_ENV_MODULE   <not set>\n",
            "PyTorch                 1.8.1+cu101 @/usr/local/lib/python3.7/site-packages/torch\n",
            "PyTorch debug build     False\n",
            "GPU available           True\n",
            "GPU 0                   Tesla T4 (arch=7.5)\n",
            "CUDA_HOME               /usr/local/cuda\n",
            "Pillow                  9.5.0\n",
            "torchvision             0.9.1+cu101 @/usr/local/lib/python3.7/site-packages/torchvision\n",
            "torchvision arch flags  3.5, 5.0, 6.0, 7.0, 7.5\n",
            "fvcore                  0.1.3.post20210317\n",
            "cv2                     4.7.0\n",
            "----------------------  ---------------------------------------------------------------\n",
            "PyTorch built with:\n",
            "  - GCC 7.3\n",
            "  - C++ Version: 201402\n",
            "  - Intel(R) Math Kernel Library Version 2020.0.0 Product Build 20191122 for Intel(R) 64 architecture applications\n",
            "  - Intel(R) MKL-DNN v1.7.0 (Git Hash 7aed236906b1f7a05c0917e5257a1af05e9ff683)\n",
            "  - OpenMP 201511 (a.k.a. OpenMP 4.5)\n",
            "  - NNPACK is enabled\n",
            "  - CPU capability usage: AVX2\n",
            "  - CUDA Runtime 10.1\n",
            "  - NVCC architecture flags: -gencode;arch=compute_37,code=sm_37;-gencode;arch=compute_50,code=sm_50;-gencode;arch=compute_60,code=sm_60;-gencode;arch=compute_70,code=sm_70\n",
            "  - CuDNN 7.6.3\n",
            "  - Magma 2.5.2\n",
            "  - Build settings: BLAS_INFO=mkl, BUILD_TYPE=Release, CUDA_VERSION=10.1, CUDNN_VERSION=7.6.3, CXX_COMPILER=/opt/rh/devtoolset-7/root/usr/bin/c++, CXX_FLAGS= -Wno-deprecated -fvisibility-inlines-hidden -DUSE_PTHREADPOOL -fopenmp -DNDEBUG -DUSE_KINETO -DUSE_FBGEMM -DUSE_QNNPACK -DUSE_PYTORCH_QNNPACK -DUSE_XNNPACK -O2 -fPIC -Wno-narrowing -Wall -Wextra -Werror=return-type -Wno-missing-field-initializers -Wno-type-limits -Wno-array-bounds -Wno-unknown-pragmas -Wno-sign-compare -Wno-unused-parameter -Wno-unused-variable -Wno-unused-function -Wno-unused-result -Wno-unused-local-typedefs -Wno-strict-overflow -Wno-strict-aliasing -Wno-error=deprecated-declarations -Wno-stringop-overflow -Wno-psabi -Wno-error=pedantic -Wno-error=redundant-decls -Wno-error=old-style-cast -fdiagnostics-color=always -faligned-new -Wno-unused-but-set-variable -Wno-maybe-uninitialized -fno-math-errno -fno-trapping-math -Werror=format -Wno-stringop-overflow, LAPACK_INFO=mkl, PERF_WITH_AVX=1, PERF_WITH_AVX2=1, PERF_WITH_AVX512=1, TORCH_VERSION=1.8.1, USE_CUDA=ON, USE_CUDNN=ON, USE_EXCEPTION_PTR=1, USE_GFLAGS=OFF, USE_GLOG=OFF, USE_MKL=ON, USE_MKLDNN=ON, USE_MPI=OFF, USE_NCCL=ON, USE_NNPACK=ON, USE_OPENMP=ON, \n",
            "\n",
            "\u001b[32m[04/15 02:41:07 detectron2]: \u001b[0mCommand line arguments: Namespace(config_file='/content/gdrive/MyDrive/QueryDet-PyTorch/work_dirs/visdrone_querydet/config.yaml', dist_url='tcp://127.0.0.1:49152', eval_only=False, machine_rank=0, no_pretrain=False, num_gpus=1, num_machines=1, opts=['OUTPUT_DIR', 'work_dirs/visdrone_querydet'], resume=True)\n",
            "\u001b[32m[04/15 02:41:08 detectron2]: \u001b[0mContents of args.config_file=/content/gdrive/MyDrive/QueryDet-PyTorch/work_dirs/visdrone_querydet/config.yaml:\n",
            "CUDNN_BENCHMARK: false\n",
            "DATALOADER:\n",
            "  ASPECT_RATIO_GROUPING: true\n",
            "  FILTER_EMPTY_ANNOTATIONS: true\n",
            "  NUM_WORKERS: 4\n",
            "  REPEAT_THRESHOLD: 0.0\n",
            "  SAMPLER_TRAIN: TrainingSampler\n",
            "DATASETS:\n",
            "  PRECOMPUTED_PROPOSAL_TOPK_TEST: 1000\n",
            "  PRECOMPUTED_PROPOSAL_TOPK_TRAIN: 2000\n",
            "  PROPOSAL_FILES_TEST: []\n",
            "  PROPOSAL_FILES_TRAIN: []\n",
            "  TEST:\n",
            "  - coco_2017_val\n",
            "  TRAIN:\n",
            "  - coco_2017_train\n",
            "GLOBAL:\n",
            "  HACK: 1.0\n",
            "INPUT:\n",
            "  CROP:\n",
            "    ENABLED: false\n",
            "    SIZE:\n",
            "    - 0.9\n",
            "    - 0.9\n",
            "    TYPE: relative_range\n",
            "  FORMAT: BGR\n",
            "  MASK_FORMAT: polygon\n",
            "  MAX_SIZE_TEST: 1333\n",
            "  MAX_SIZE_TRAIN: 1333\n",
            "  MIN_SIZE_TEST: 800\n",
            "  MIN_SIZE_TRAIN:\n",
            "  - 640\n",
            "  - 672\n",
            "  - 704\n",
            "  - 736\n",
            "  - 768\n",
            "  - 800\n",
            "  MIN_SIZE_TRAIN_SAMPLING: choice\n",
            "  RANDOM_FLIP: horizontal\n",
            "META_INFO:\n",
            "  EVAL_AP: true\n",
            "  EVAL_GPU_TIME: true\n",
            "  VIS_ROOT: ''\n",
            "MODEL:\n",
            "  ANCHOR_GENERATOR:\n",
            "    ANGLES:\n",
            "    - - -90\n",
            "      - 0\n",
            "      - 90\n",
            "    ASPECT_RATIOS:\n",
            "    - - 0.5\n",
            "      - 1.0\n",
            "      - 2.0\n",
            "    NAME: AnchorGeneratorWithCenter\n",
            "    OFFSET: 0.0\n",
            "    SIZES:\n",
            "    - - 16\n",
            "      - 20.15873679831797\n",
            "      - 25.39841683149119\n",
            "    - - 32\n",
            "      - 40.31747359663594\n",
            "      - 50.79683366298238\n",
            "    - - 64\n",
            "      - 80.63494719327188\n",
            "      - 101.59366732596476\n",
            "    - - 128\n",
            "      - 161.26989438654377\n",
            "      - 203.18733465192952\n",
            "    - - 256\n",
            "      - 322.53978877308754\n",
            "      - 406.37466930385904\n",
            "    - - 512\n",
            "      - 645.0795775461751\n",
            "      - 812.7493386077181\n",
            "  BACKBONE:\n",
            "    FREEZE_AT: 2\n",
            "    NAME: build_retinanet_resnet_fpn_backbone\n",
            "  CUSTOM:\n",
            "    CLEAR_CUDA_CACHE: false\n",
            "    CLS_WEIGHTS:\n",
            "    - 1.0\n",
            "    - 1.4\n",
            "    - 1.8\n",
            "    - 2.2\n",
            "    - 2.6\n",
            "    - 2.6\n",
            "    FOCAL_LOSS_ALPHAS:\n",
            "    - 0.25\n",
            "    - 0.25\n",
            "    - 0.25\n",
            "    - 0.25\n",
            "    - 0.25\n",
            "    - 0.25\n",
            "    FOCAL_LOSS_GAMMAS:\n",
            "    - 2.0\n",
            "    - 2.0\n",
            "    - 2.0\n",
            "    - 2.0\n",
            "    - 2.0\n",
            "    - 2.0\n",
            "    GIOU_LOSS: false\n",
            "    GRADIENT_CHECKPOINT: false\n",
            "    HEAD_BN: false\n",
            "    REG_WEIGHTS:\n",
            "    - 1.0\n",
            "    - 1.4\n",
            "    - 1.8\n",
            "    - 2.2\n",
            "    - 2.6\n",
            "    - 2.6\n",
            "    SOFT_NMS_METHOD: linear\n",
            "    SOFT_NMS_PRUND: 0.001\n",
            "    SOFT_NMS_SIGMA: 0.5\n",
            "    SOFT_NMS_THRESHOLD: 0.5\n",
            "    USE_LOOP_MATCHER: true\n",
            "    USE_SOFT_NMS: false\n",
            "  DEVICE: cuda\n",
            "  FPN:\n",
            "    FUSE_TYPE: sum\n",
            "    IN_FEATURES:\n",
            "    - res2\n",
            "    - res3\n",
            "    - res4\n",
            "    - res5\n",
            "    NORM: ''\n",
            "    OUT_CHANNELS: 256\n",
            "    TOP_LEVELS: 2\n",
            "  KEYPOINT_ON: false\n",
            "  LOAD_PROPOSALS: false\n",
            "  MASK_ON: false\n",
            "  META_ARCHITECTURE: RetinaNetQueryDet\n",
            "  PANOPTIC_FPN:\n",
            "    COMBINE:\n",
            "      ENABLED: true\n",
            "      INSTANCES_CONFIDENCE_THRESH: 0.5\n",
            "      OVERLAP_THRESH: 0.5\n",
            "      STUFF_AREA_LIMIT: 4096\n",
            "    INSTANCE_LOSS_WEIGHT: 1.0\n",
            "  PIXEL_MEAN:\n",
            "  - 103.53\n",
            "  - 116.28\n",
            "  - 123.675\n",
            "  PIXEL_STD:\n",
            "  - 1.0\n",
            "  - 1.0\n",
            "  - 1.0\n",
            "  PROPOSAL_GENERATOR:\n",
            "    MIN_SIZE: 0\n",
            "    NAME: RPN\n",
            "  QUERY:\n",
            "    CONTEXT: 2\n",
            "    ENCODE_CENTER_DIS_COEFF:\n",
            "    - 1.0\n",
            "    - 1.0\n",
            "    ENCODE_SMALL_OBJ_SCALE:\n",
            "    - - 0\n",
            "      - 32\n",
            "    - - 0\n",
            "      - 64\n",
            "    FEATURES_VALUE_TEST:\n",
            "    - 0\n",
            "    - 1\n",
            "    FEATURES_VALUE_TRAIN:\n",
            "    - 0\n",
            "    - 1\n",
            "    FEATURES_WHOLE_TEST:\n",
            "    - 2\n",
            "    - 3\n",
            "    - 4\n",
            "    - 5\n",
            "    FEATURES_WHOLE_TRAIN:\n",
            "    - 2\n",
            "    - 3\n",
            "    - 4\n",
            "    - 5\n",
            "    QUERY_INFER: false\n",
            "    QUERY_LOSS_GAMMA:\n",
            "    - 1.3\n",
            "    - 1.3\n",
            "    QUERY_LOSS_WEIGHT:\n",
            "    - 10.0\n",
            "    - 10.0\n",
            "    Q_FEATURE_TEST:\n",
            "    - 1\n",
            "    - 2\n",
            "    Q_FEATURE_TRAIN:\n",
            "    - 1\n",
            "    - 2\n",
            "    THRESHOLD: 0.12\n",
            "  RESNETS:\n",
            "    DEFORM_MODULATED: false\n",
            "    DEFORM_NUM_GROUPS: 1\n",
            "    DEFORM_ON_PER_STAGE:\n",
            "    - false\n",
            "    - false\n",
            "    - false\n",
            "    - false\n",
            "    DEPTH: 50\n",
            "    NORM: FrozenBN\n",
            "    NUM_GROUPS: 1\n",
            "    OUT_FEATURES:\n",
            "    - res2\n",
            "    - res3\n",
            "    - res4\n",
            "    - res5\n",
            "    RES2_OUT_CHANNELS: 256\n",
            "    RES5_DILATION: 1\n",
            "    STEM_OUT_CHANNELS: 64\n",
            "    STRIDE_IN_1X1: true\n",
            "    WIDTH_PER_GROUP: 64\n",
            "  RETINANET:\n",
            "    BBOX_REG_LOSS_TYPE: smooth_l1\n",
            "    BBOX_REG_WEIGHTS:\n",
            "    - 1.0\n",
            "    - 1.0\n",
            "    - 1.0\n",
            "    - 1.0\n",
            "    FOCAL_LOSS_ALPHA: 0.25\n",
            "    FOCAL_LOSS_GAMMA: 2.0\n",
            "    IN_FEATURES:\n",
            "    - p2\n",
            "    - p3\n",
            "    - p4\n",
            "    - p5\n",
            "    - p6\n",
            "    - p7\n",
            "    IOU_LABELS:\n",
            "    - 0\n",
            "    - -1\n",
            "    - 1\n",
            "    IOU_THRESHOLDS:\n",
            "    - 0.4\n",
            "    - 0.5\n",
            "    NMS_THRESH_TEST: 0.5\n",
            "    NORM: ''\n",
            "    NUM_CLASSES: 10\n",
            "    NUM_CONVS: 4\n",
            "    PRIOR_PROB: 0.01\n",
            "    SCORE_THRESH_TEST: 0.05\n",
            "    SMOOTH_L1_LOSS_BETA: 0.1\n",
            "    TOPK_CANDIDATES_TEST: 1000\n",
            "  ROI_BOX_CASCADE_HEAD:\n",
            "    BBOX_REG_WEIGHTS:\n",
            "    - - 10.0\n",
            "      - 10.0\n",
            "      - 5.0\n",
            "      - 5.0\n",
            "    - - 20.0\n",
            "      - 20.0\n",
            "      - 10.0\n",
            "      - 10.0\n",
            "    - - 30.0\n",
            "      - 30.0\n",
            "      - 15.0\n",
            "      - 15.0\n",
            "    IOUS:\n",
            "    - 0.5\n",
            "    - 0.6\n",
            "    - 0.7\n",
            "  ROI_BOX_HEAD:\n",
            "    BBOX_REG_LOSS_TYPE: smooth_l1\n",
            "    BBOX_REG_LOSS_WEIGHT: 1.0\n",
            "    BBOX_REG_WEIGHTS:\n",
            "    - 10.0\n",
            "    - 10.0\n",
            "    - 5.0\n",
            "    - 5.0\n",
            "    CLS_AGNOSTIC_BBOX_REG: false\n",
            "    CONV_DIM: 256\n",
            "    FC_DIM: 1024\n",
            "    NAME: ''\n",
            "    NORM: ''\n",
            "    NUM_CONV: 0\n",
            "    NUM_FC: 0\n",
            "    POOLER_RESOLUTION: 14\n",
            "    POOLER_SAMPLING_RATIO: 0\n",
            "    POOLER_TYPE: ROIAlignV2\n",
            "    SMOOTH_L1_BETA: 0.0\n",
            "    TRAIN_ON_PRED_BOXES: false\n",
            "  ROI_HEADS:\n",
            "    BATCH_SIZE_PER_IMAGE: 512\n",
            "    IN_FEATURES:\n",
            "    - res4\n",
            "    IOU_LABELS:\n",
            "    - 0\n",
            "    - 1\n",
            "    IOU_THRESHOLDS:\n",
            "    - 0.5\n",
            "    NAME: Res5ROIHeads\n",
            "    NMS_THRESH_TEST: 0.5\n",
            "    NUM_CLASSES: 80\n",
            "    POSITIVE_FRACTION: 0.25\n",
            "    PROPOSAL_APPEND_GT: true\n",
            "    SCORE_THRESH_TEST: 0.05\n",
            "  ROI_KEYPOINT_HEAD:\n",
            "    CONV_DIMS:\n",
            "    - 512\n",
            "    - 512\n",
            "    - 512\n",
            "    - 512\n",
            "    - 512\n",
            "    - 512\n",
            "    - 512\n",
            "    - 512\n",
            "    LOSS_WEIGHT: 1.0\n",
            "    MIN_KEYPOINTS_PER_IMAGE: 1\n",
            "    NAME: KRCNNConvDeconvUpsampleHead\n",
            "    NORMALIZE_LOSS_BY_VISIBLE_KEYPOINTS: true\n",
            "    NUM_KEYPOINTS: 17\n",
            "    POOLER_RESOLUTION: 14\n",
            "    POOLER_SAMPLING_RATIO: 0\n",
            "    POOLER_TYPE: ROIAlignV2\n",
            "  ROI_MASK_HEAD:\n",
            "    CLS_AGNOSTIC_MASK: false\n",
            "    CONV_DIM: 256\n",
            "    NAME: MaskRCNNConvUpsampleHead\n",
            "    NORM: ''\n",
            "    NUM_CONV: 0\n",
            "    POOLER_RESOLUTION: 14\n",
            "    POOLER_SAMPLING_RATIO: 0\n",
            "    POOLER_TYPE: ROIAlignV2\n",
            "  RPN:\n",
            "    BATCH_SIZE_PER_IMAGE: 256\n",
            "    BBOX_REG_LOSS_TYPE: smooth_l1\n",
            "    BBOX_REG_LOSS_WEIGHT: 1.0\n",
            "    BBOX_REG_WEIGHTS:\n",
            "    - 1.0\n",
            "    - 1.0\n",
            "    - 1.0\n",
            "    - 1.0\n",
            "    BOUNDARY_THRESH: -1\n",
            "    HEAD_NAME: StandardRPNHead\n",
            "    IN_FEATURES:\n",
            "    - res4\n",
            "    IOU_LABELS:\n",
            "    - 0\n",
            "    - -1\n",
            "    - 1\n",
            "    IOU_THRESHOLDS:\n",
            "    - 0.3\n",
            "    - 0.7\n",
            "    LOSS_WEIGHT: 1.0\n",
            "    NMS_THRESH: 0.7\n",
            "    POSITIVE_FRACTION: 0.5\n",
            "    POST_NMS_TOPK_TEST: 1000\n",
            "    POST_NMS_TOPK_TRAIN: 2000\n",
            "    PRE_NMS_TOPK_TEST: 6000\n",
            "    PRE_NMS_TOPK_TRAIN: 12000\n",
            "    SMOOTH_L1_BETA: 0.0\n",
            "  SEM_SEG_HEAD:\n",
            "    COMMON_STRIDE: 4\n",
            "    CONVS_DIM: 128\n",
            "    IGNORE_VALUE: 255\n",
            "    IN_FEATURES:\n",
            "    - p2\n",
            "    - p3\n",
            "    - p4\n",
            "    - p5\n",
            "    LOSS_WEIGHT: 1.0\n",
            "    NAME: SemSegFPNHead\n",
            "    NORM: GN\n",
            "    NUM_CLASSES: 54\n",
            "  WEIGHTS: /content/gdrive/MyDrive/QueryDet-PyTorch/work_dirs/visdrone_querydet/model_0007999.pth\n",
            "OUTPUT_DIR: work_dirs/visdrone_querydet\n",
            "SEED: -1\n",
            "SOLVER:\n",
            "  AMP:\n",
            "    ENABLED: true\n",
            "  BASE_LR: 0.006\n",
            "  BIAS_LR_FACTOR: 1.0\n",
            "  CHECKPOINT_PERIOD: 2000\n",
            "  CLIP_GRADIENTS:\n",
            "    CLIP_TYPE: value\n",
            "    CLIP_VALUE: 35.0\n",
            "    ENABLED: true\n",
            "    NORM_TYPE: 2.0\n",
            "  GAMMA: 0.1\n",
            "  IMS_PER_BATCH: 3\n",
            "  LR_SCHEDULER_NAME: WarmupMultiStepLR\n",
            "  MAX_ITER: 60000\n",
            "  MOMENTUM: 0.9\n",
            "  NESTEROV: false\n",
            "  REFERENCE_WORLD_SIZE: 0\n",
            "  STEPS:\n",
            "  - 15000\n",
            "  - 20000\n",
            "  WARMUP_FACTOR: 0.001\n",
            "  WARMUP_ITERS: 1000\n",
            "  WARMUP_METHOD: linear\n",
            "  WEIGHT_DECAY: 0.0001\n",
            "  WEIGHT_DECAY_BIAS: 0.0001\n",
            "  WEIGHT_DECAY_NORM: 0.0\n",
            "TEST:\n",
            "  AUG:\n",
            "    ENABLED: false\n",
            "    FLIP: true\n",
            "    MAX_SIZE: 4000\n",
            "    MIN_SIZES:\n",
            "    - 400\n",
            "    - 500\n",
            "    - 600\n",
            "    - 700\n",
            "    - 800\n",
            "    - 900\n",
            "    - 1000\n",
            "    - 1100\n",
            "    - 1200\n",
            "  DETECTIONS_PER_IMAGE: 500\n",
            "  EVAL_PERIOD: 0\n",
            "  EXPECTED_RESULTS: []\n",
            "  KEYPOINT_OKS_SIGMAS: []\n",
            "  PRECISE_BN:\n",
            "    ENABLED: false\n",
            "    NUM_ITER: 200\n",
            "VERSION: 2\n",
            "VISDRONE:\n",
            "  MAX_LENGTH: 1999\n",
            "  SHORT_LENGTH:\n",
            "  - 1200\n",
            "  TEST_IMG_ROOT: data/visdrone/coco_format/val_images\n",
            "  TEST_JSON: data/visdrone/coco_format/annotations/val_label.json\n",
            "  TEST_LENGTH: 3999\n",
            "  TRAIN_JSON: data/visdrone/coco_format/annotations/train_label.json\n",
            "  TRING_IMG_ROOT: data//visdrone/coco_format/train_images\n",
            "VIS_PERIOD: 0\n",
            "\n",
            "\u001b[32m[04/15 02:41:08 detectron2]: \u001b[0mRunning with full config:\n",
            "CUDNN_BENCHMARK: False\n",
            "DATALOADER:\n",
            "  ASPECT_RATIO_GROUPING: True\n",
            "  FILTER_EMPTY_ANNOTATIONS: True\n",
            "  NUM_WORKERS: 4\n",
            "  REPEAT_THRESHOLD: 0.0\n",
            "  SAMPLER_TRAIN: TrainingSampler\n",
            "DATASETS:\n",
            "  PRECOMPUTED_PROPOSAL_TOPK_TEST: 1000\n",
            "  PRECOMPUTED_PROPOSAL_TOPK_TRAIN: 2000\n",
            "  PROPOSAL_FILES_TEST: ()\n",
            "  PROPOSAL_FILES_TRAIN: ()\n",
            "  TEST: ('coco_2017_val',)\n",
            "  TRAIN: ('coco_2017_train',)\n",
            "GLOBAL:\n",
            "  HACK: 1.0\n",
            "INPUT:\n",
            "  CROP:\n",
            "    ENABLED: False\n",
            "    SIZE: [0.9, 0.9]\n",
            "    TYPE: relative_range\n",
            "  FORMAT: BGR\n",
            "  MASK_FORMAT: polygon\n",
            "  MAX_SIZE_TEST: 1333\n",
            "  MAX_SIZE_TRAIN: 1333\n",
            "  MIN_SIZE_TEST: 800\n",
            "  MIN_SIZE_TRAIN: (640, 672, 704, 736, 768, 800)\n",
            "  MIN_SIZE_TRAIN_SAMPLING: choice\n",
            "  RANDOM_FLIP: horizontal\n",
            "META_INFO:\n",
            "  EVAL_AP: True\n",
            "  EVAL_GPU_TIME: True\n",
            "  VIS_ROOT: \n",
            "MODEL:\n",
            "  ANCHOR_GENERATOR:\n",
            "    ANGLES: [[-90, 0, 90]]\n",
            "    ASPECT_RATIOS: [[0.5, 1.0, 2.0]]\n",
            "    NAME: AnchorGeneratorWithCenter\n",
            "    OFFSET: 0.0\n",
            "    SIZES: [[16, 20.15873679831797, 25.39841683149119], [32, 40.31747359663594, 50.79683366298238], [64, 80.63494719327188, 101.59366732596476], [128, 161.26989438654377, 203.18733465192952], [256, 322.53978877308754, 406.37466930385904], [512, 645.0795775461751, 812.7493386077181]]\n",
            "  BACKBONE:\n",
            "    FREEZE_AT: 2\n",
            "    NAME: build_retinanet_resnet_fpn_backbone\n",
            "  CUSTOM:\n",
            "    CLEAR_CUDA_CACHE: False\n",
            "    CLS_WEIGHTS: [1.0, 1.4, 1.8, 2.2, 2.6, 2.6]\n",
            "    FOCAL_LOSS_ALPHAS: [0.25, 0.25, 0.25, 0.25, 0.25, 0.25]\n",
            "    FOCAL_LOSS_GAMMAS: [2.0, 2.0, 2.0, 2.0, 2.0, 2.0]\n",
            "    GIOU_LOSS: False\n",
            "    GRADIENT_CHECKPOINT: False\n",
            "    HEAD_BN: False\n",
            "    REG_WEIGHTS: [1.0, 1.4, 1.8, 2.2, 2.6, 2.6]\n",
            "    SOFT_NMS_METHOD: linear\n",
            "    SOFT_NMS_PRUND: 0.001\n",
            "    SOFT_NMS_SIGMA: 0.5\n",
            "    SOFT_NMS_THRESHOLD: 0.5\n",
            "    USE_LOOP_MATCHER: True\n",
            "    USE_SOFT_NMS: False\n",
            "  DEVICE: cuda\n",
            "  FPN:\n",
            "    FUSE_TYPE: sum\n",
            "    IN_FEATURES: ['res2', 'res3', 'res4', 'res5']\n",
            "    NORM: \n",
            "    OUT_CHANNELS: 256\n",
            "    TOP_LEVELS: 2\n",
            "  KEYPOINT_ON: False\n",
            "  LOAD_PROPOSALS: False\n",
            "  MASK_ON: False\n",
            "  META_ARCHITECTURE: RetinaNetQueryDet\n",
            "  PANOPTIC_FPN:\n",
            "    COMBINE:\n",
            "      ENABLED: True\n",
            "      INSTANCES_CONFIDENCE_THRESH: 0.5\n",
            "      OVERLAP_THRESH: 0.5\n",
            "      STUFF_AREA_LIMIT: 4096\n",
            "    INSTANCE_LOSS_WEIGHT: 1.0\n",
            "  PIXEL_MEAN: [103.53, 116.28, 123.675]\n",
            "  PIXEL_STD: [1.0, 1.0, 1.0]\n",
            "  PROPOSAL_GENERATOR:\n",
            "    MIN_SIZE: 0\n",
            "    NAME: RPN\n",
            "  QUERY:\n",
            "    CONTEXT: 2\n",
            "    ENCODE_CENTER_DIS_COEFF: [1.0, 1.0]\n",
            "    ENCODE_SMALL_OBJ_SCALE: [[0, 32], [0, 64]]\n",
            "    FEATURES_VALUE_TEST: [0, 1]\n",
            "    FEATURES_VALUE_TRAIN: [0, 1]\n",
            "    FEATURES_WHOLE_TEST: [2, 3, 4, 5]\n",
            "    FEATURES_WHOLE_TRAIN: [2, 3, 4, 5]\n",
            "    QUERY_INFER: False\n",
            "    QUERY_LOSS_GAMMA: [1.3, 1.3]\n",
            "    QUERY_LOSS_WEIGHT: [10.0, 10.0]\n",
            "    Q_FEATURE_TEST: [1, 2]\n",
            "    Q_FEATURE_TRAIN: [1, 2]\n",
            "    THRESHOLD: 0.12\n",
            "  RESNETS:\n",
            "    DEFORM_MODULATED: False\n",
            "    DEFORM_NUM_GROUPS: 1\n",
            "    DEFORM_ON_PER_STAGE: [False, False, False, False]\n",
            "    DEPTH: 50\n",
            "    NORM: FrozenBN\n",
            "    NUM_GROUPS: 1\n",
            "    OUT_FEATURES: ['res2', 'res3', 'res4', 'res5']\n",
            "    RES2_OUT_CHANNELS: 256\n",
            "    RES5_DILATION: 1\n",
            "    STEM_OUT_CHANNELS: 64\n",
            "    STRIDE_IN_1X1: True\n",
            "    WIDTH_PER_GROUP: 64\n",
            "  RETINANET:\n",
            "    BBOX_REG_LOSS_TYPE: smooth_l1\n",
            "    BBOX_REG_WEIGHTS: (1.0, 1.0, 1.0, 1.0)\n",
            "    FOCAL_LOSS_ALPHA: 0.25\n",
            "    FOCAL_LOSS_GAMMA: 2.0\n",
            "    IN_FEATURES: ['p2', 'p3', 'p4', 'p5', 'p6', 'p7']\n",
            "    IOU_LABELS: [0, -1, 1]\n",
            "    IOU_THRESHOLDS: [0.4, 0.5]\n",
            "    NMS_THRESH_TEST: 0.5\n",
            "    NORM: \n",
            "    NUM_CLASSES: 10\n",
            "    NUM_CONVS: 4\n",
            "    PRIOR_PROB: 0.01\n",
            "    SCORE_THRESH_TEST: 0.05\n",
            "    SMOOTH_L1_LOSS_BETA: 0.1\n",
            "    TOPK_CANDIDATES_TEST: 1000\n",
            "  ROI_BOX_CASCADE_HEAD:\n",
            "    BBOX_REG_WEIGHTS: ([10.0, 10.0, 5.0, 5.0], [20.0, 20.0, 10.0, 10.0], [30.0, 30.0, 15.0, 15.0])\n",
            "    IOUS: (0.5, 0.6, 0.7)\n",
            "  ROI_BOX_HEAD:\n",
            "    BBOX_REG_LOSS_TYPE: smooth_l1\n",
            "    BBOX_REG_LOSS_WEIGHT: 1.0\n",
            "    BBOX_REG_WEIGHTS: (10.0, 10.0, 5.0, 5.0)\n",
            "    CLS_AGNOSTIC_BBOX_REG: False\n",
            "    CONV_DIM: 256\n",
            "    FC_DIM: 1024\n",
            "    NAME: \n",
            "    NORM: \n",
            "    NUM_CONV: 0\n",
            "    NUM_FC: 0\n",
            "    POOLER_RESOLUTION: 14\n",
            "    POOLER_SAMPLING_RATIO: 0\n",
            "    POOLER_TYPE: ROIAlignV2\n",
            "    SMOOTH_L1_BETA: 0.0\n",
            "    TRAIN_ON_PRED_BOXES: False\n",
            "  ROI_HEADS:\n",
            "    BATCH_SIZE_PER_IMAGE: 512\n",
            "    IN_FEATURES: ['res4']\n",
            "    IOU_LABELS: [0, 1]\n",
            "    IOU_THRESHOLDS: [0.5]\n",
            "    NAME: Res5ROIHeads\n",
            "    NMS_THRESH_TEST: 0.5\n",
            "    NUM_CLASSES: 80\n",
            "    POSITIVE_FRACTION: 0.25\n",
            "    PROPOSAL_APPEND_GT: True\n",
            "    SCORE_THRESH_TEST: 0.05\n",
            "  ROI_KEYPOINT_HEAD:\n",
            "    CONV_DIMS: (512, 512, 512, 512, 512, 512, 512, 512)\n",
            "    LOSS_WEIGHT: 1.0\n",
            "    MIN_KEYPOINTS_PER_IMAGE: 1\n",
            "    NAME: KRCNNConvDeconvUpsampleHead\n",
            "    NORMALIZE_LOSS_BY_VISIBLE_KEYPOINTS: True\n",
            "    NUM_KEYPOINTS: 17\n",
            "    POOLER_RESOLUTION: 14\n",
            "    POOLER_SAMPLING_RATIO: 0\n",
            "    POOLER_TYPE: ROIAlignV2\n",
            "  ROI_MASK_HEAD:\n",
            "    CLS_AGNOSTIC_MASK: False\n",
            "    CONV_DIM: 256\n",
            "    NAME: MaskRCNNConvUpsampleHead\n",
            "    NORM: \n",
            "    NUM_CONV: 0\n",
            "    POOLER_RESOLUTION: 14\n",
            "    POOLER_SAMPLING_RATIO: 0\n",
            "    POOLER_TYPE: ROIAlignV2\n",
            "  RPN:\n",
            "    BATCH_SIZE_PER_IMAGE: 256\n",
            "    BBOX_REG_LOSS_TYPE: smooth_l1\n",
            "    BBOX_REG_LOSS_WEIGHT: 1.0\n",
            "    BBOX_REG_WEIGHTS: (1.0, 1.0, 1.0, 1.0)\n",
            "    BOUNDARY_THRESH: -1\n",
            "    HEAD_NAME: StandardRPNHead\n",
            "    IN_FEATURES: ['res4']\n",
            "    IOU_LABELS: [0, -1, 1]\n",
            "    IOU_THRESHOLDS: [0.3, 0.7]\n",
            "    LOSS_WEIGHT: 1.0\n",
            "    NMS_THRESH: 0.7\n",
            "    POSITIVE_FRACTION: 0.5\n",
            "    POST_NMS_TOPK_TEST: 1000\n",
            "    POST_NMS_TOPK_TRAIN: 2000\n",
            "    PRE_NMS_TOPK_TEST: 6000\n",
            "    PRE_NMS_TOPK_TRAIN: 12000\n",
            "    SMOOTH_L1_BETA: 0.0\n",
            "  SEM_SEG_HEAD:\n",
            "    COMMON_STRIDE: 4\n",
            "    CONVS_DIM: 128\n",
            "    IGNORE_VALUE: 255\n",
            "    IN_FEATURES: ['p2', 'p3', 'p4', 'p5']\n",
            "    LOSS_WEIGHT: 1.0\n",
            "    NAME: SemSegFPNHead\n",
            "    NORM: GN\n",
            "    NUM_CLASSES: 54\n",
            "  WEIGHTS: /content/gdrive/MyDrive/QueryDet-PyTorch/work_dirs/visdrone_querydet/model_0007999.pth\n",
            "OUTPUT_DIR: work_dirs/visdrone_querydet\n",
            "SEED: -1\n",
            "SOLVER:\n",
            "  AMP:\n",
            "    ENABLED: True\n",
            "  BASE_LR: 0.006\n",
            "  BIAS_LR_FACTOR: 1.0\n",
            "  CHECKPOINT_PERIOD: 2000\n",
            "  CLIP_GRADIENTS:\n",
            "    CLIP_TYPE: value\n",
            "    CLIP_VALUE: 35.0\n",
            "    ENABLED: True\n",
            "    NORM_TYPE: 2.0\n",
            "  GAMMA: 0.1\n",
            "  IMS_PER_BATCH: 3\n",
            "  LR_SCHEDULER_NAME: WarmupMultiStepLR\n",
            "  MAX_ITER: 60000\n",
            "  MOMENTUM: 0.9\n",
            "  NESTEROV: False\n",
            "  REFERENCE_WORLD_SIZE: 0\n",
            "  STEPS: (15000, 20000)\n",
            "  WARMUP_FACTOR: 0.001\n",
            "  WARMUP_ITERS: 1000\n",
            "  WARMUP_METHOD: linear\n",
            "  WEIGHT_DECAY: 0.0001\n",
            "  WEIGHT_DECAY_BIAS: 0.0001\n",
            "  WEIGHT_DECAY_NORM: 0.0\n",
            "TEST:\n",
            "  AUG:\n",
            "    ENABLED: False\n",
            "    FLIP: True\n",
            "    MAX_SIZE: 4000\n",
            "    MIN_SIZES: (400, 500, 600, 700, 800, 900, 1000, 1100, 1200)\n",
            "  DETECTIONS_PER_IMAGE: 500\n",
            "  EVAL_PERIOD: 0\n",
            "  EXPECTED_RESULTS: []\n",
            "  KEYPOINT_OKS_SIGMAS: []\n",
            "  PRECISE_BN:\n",
            "    ENABLED: False\n",
            "    NUM_ITER: 200\n",
            "VERSION: 2\n",
            "VISDRONE:\n",
            "  MAX_LENGTH: 1999\n",
            "  SHORT_LENGTH: [1200]\n",
            "  TEST_IMG_ROOT: data/visdrone/coco_format/val_images\n",
            "  TEST_JSON: data/visdrone/coco_format/annotations/val_label.json\n",
            "  TEST_LENGTH: 3999\n",
            "  TRAIN_JSON: data/visdrone/coco_format/annotations/train_label.json\n",
            "  TRING_IMG_ROOT: data//visdrone/coco_format/train_images\n",
            "VIS_PERIOD: 0\n",
            "\u001b[32m[04/15 02:41:09 detectron2]: \u001b[0mFull config saved to work_dirs/visdrone_querydet/config.yaml\n",
            "\u001b[32m[04/15 02:41:09 d2.utils.env]: \u001b[0mUsing a generated random seed 9464501\n",
            "\u001b[32m[04/15 02:41:14 d2.engine.defaults]: \u001b[0mModel:\n",
            "RetinaNetQueryDet(\n",
            "  (backbone): FPN(\n",
            "    (fpn_lateral2): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1))\n",
            "    (fpn_output2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (fpn_lateral3): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1))\n",
            "    (fpn_output3): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (fpn_lateral4): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1))\n",
            "    (fpn_output4): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (fpn_lateral5): Conv2d(2048, 256, kernel_size=(1, 1), stride=(1, 1))\n",
            "    (fpn_output5): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (top_block): LastLevelP6P7(\n",
            "      (p6): Conv2d(2048, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
            "      (p7): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
            "    )\n",
            "    (bottom_up): ResNet(\n",
            "      (stem): BasicStem(\n",
            "        (conv1): Conv2d(\n",
            "          3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)\n",
            "        )\n",
            "      )\n",
            "      (res2): Sequential(\n",
            "        (0): BottleneckBlock(\n",
            "          (shortcut): Conv2d(\n",
            "            64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
            "          )\n",
            "          (conv1): Conv2d(\n",
            "            64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)\n",
            "          )\n",
            "          (conv2): Conv2d(\n",
            "            64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)\n",
            "          )\n",
            "          (conv3): Conv2d(\n",
            "            64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
            "          )\n",
            "        )\n",
            "        (1): BottleneckBlock(\n",
            "          (conv1): Conv2d(\n",
            "            256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)\n",
            "          )\n",
            "          (conv2): Conv2d(\n",
            "            64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)\n",
            "          )\n",
            "          (conv3): Conv2d(\n",
            "            64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
            "          )\n",
            "        )\n",
            "        (2): BottleneckBlock(\n",
            "          (conv1): Conv2d(\n",
            "            256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)\n",
            "          )\n",
            "          (conv2): Conv2d(\n",
            "            64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)\n",
            "          )\n",
            "          (conv3): Conv2d(\n",
            "            64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
            "          )\n",
            "        )\n",
            "      )\n",
            "      (res3): Sequential(\n",
            "        (0): BottleneckBlock(\n",
            "          (shortcut): Conv2d(\n",
            "            256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
            "          )\n",
            "          (conv1): Conv2d(\n",
            "            256, 128, kernel_size=(1, 1), stride=(2, 2), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)\n",
            "          )\n",
            "          (conv2): Conv2d(\n",
            "            128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)\n",
            "          )\n",
            "          (conv3): Conv2d(\n",
            "            128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
            "          )\n",
            "        )\n",
            "        (1): BottleneckBlock(\n",
            "          (conv1): Conv2d(\n",
            "            512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)\n",
            "          )\n",
            "          (conv2): Conv2d(\n",
            "            128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)\n",
            "          )\n",
            "          (conv3): Conv2d(\n",
            "            128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
            "          )\n",
            "        )\n",
            "        (2): BottleneckBlock(\n",
            "          (conv1): Conv2d(\n",
            "            512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)\n",
            "          )\n",
            "          (conv2): Conv2d(\n",
            "            128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)\n",
            "          )\n",
            "          (conv3): Conv2d(\n",
            "            128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
            "          )\n",
            "        )\n",
            "        (3): BottleneckBlock(\n",
            "          (conv1): Conv2d(\n",
            "            512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)\n",
            "          )\n",
            "          (conv2): Conv2d(\n",
            "            128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)\n",
            "          )\n",
            "          (conv3): Conv2d(\n",
            "            128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
            "          )\n",
            "        )\n",
            "      )\n",
            "      (res4): Sequential(\n",
            "        (0): BottleneckBlock(\n",
            "          (shortcut): Conv2d(\n",
            "            512, 1024, kernel_size=(1, 1), stride=(2, 2), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n",
            "          )\n",
            "          (conv1): Conv2d(\n",
            "            512, 256, kernel_size=(1, 1), stride=(2, 2), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
            "          )\n",
            "          (conv2): Conv2d(\n",
            "            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
            "          )\n",
            "          (conv3): Conv2d(\n",
            "            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n",
            "          )\n",
            "        )\n",
            "        (1): BottleneckBlock(\n",
            "          (conv1): Conv2d(\n",
            "            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
            "          )\n",
            "          (conv2): Conv2d(\n",
            "            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
            "          )\n",
            "          (conv3): Conv2d(\n",
            "            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n",
            "          )\n",
            "        )\n",
            "        (2): BottleneckBlock(\n",
            "          (conv1): Conv2d(\n",
            "            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
            "          )\n",
            "          (conv2): Conv2d(\n",
            "            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
            "          )\n",
            "          (conv3): Conv2d(\n",
            "            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n",
            "          )\n",
            "        )\n",
            "        (3): BottleneckBlock(\n",
            "          (conv1): Conv2d(\n",
            "            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
            "          )\n",
            "          (conv2): Conv2d(\n",
            "            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
            "          )\n",
            "          (conv3): Conv2d(\n",
            "            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n",
            "          )\n",
            "        )\n",
            "        (4): BottleneckBlock(\n",
            "          (conv1): Conv2d(\n",
            "            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
            "          )\n",
            "          (conv2): Conv2d(\n",
            "            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
            "          )\n",
            "          (conv3): Conv2d(\n",
            "            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n",
            "          )\n",
            "        )\n",
            "        (5): BottleneckBlock(\n",
            "          (conv1): Conv2d(\n",
            "            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
            "          )\n",
            "          (conv2): Conv2d(\n",
            "            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
            "          )\n",
            "          (conv3): Conv2d(\n",
            "            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n",
            "          )\n",
            "        )\n",
            "      )\n",
            "      (res5): Sequential(\n",
            "        (0): BottleneckBlock(\n",
            "          (shortcut): Conv2d(\n",
            "            1024, 2048, kernel_size=(1, 1), stride=(2, 2), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=2048, eps=1e-05)\n",
            "          )\n",
            "          (conv1): Conv2d(\n",
            "            1024, 512, kernel_size=(1, 1), stride=(2, 2), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
            "          )\n",
            "          (conv2): Conv2d(\n",
            "            512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
            "          )\n",
            "          (conv3): Conv2d(\n",
            "            512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=2048, eps=1e-05)\n",
            "          )\n",
            "        )\n",
            "        (1): BottleneckBlock(\n",
            "          (conv1): Conv2d(\n",
            "            2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
            "          )\n",
            "          (conv2): Conv2d(\n",
            "            512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
            "          )\n",
            "          (conv3): Conv2d(\n",
            "            512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=2048, eps=1e-05)\n",
            "          )\n",
            "        )\n",
            "        (2): BottleneckBlock(\n",
            "          (conv1): Conv2d(\n",
            "            2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
            "          )\n",
            "          (conv2): Conv2d(\n",
            "            512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
            "          )\n",
            "          (conv3): Conv2d(\n",
            "            512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=2048, eps=1e-05)\n",
            "          )\n",
            "        )\n",
            "      )\n",
            "    )\n",
            "  )\n",
            "  (det_head): RetinaNetHead_3x3(\n",
            "    (cls_layer_0): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (bbox_layer_0): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (cls_layer_1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (bbox_layer_1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (cls_layer_2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (bbox_layer_2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (cls_layer_3): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (bbox_layer_3): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (cls_score): Conv2d(256, 90, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (bbox_pred): Conv2d(256, 36, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "  )\n",
            "  (query_head): Head_3x3(\n",
            "    (layer_0): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (layer_1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (layer_2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (layer_3): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (pred_net): Conv2d(256, 1, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "  )\n",
            "  (anchor_generator): AnchorGeneratorWithCenter(\n",
            "    (cell_anchors): BufferList()\n",
            "  )\n",
            "  (query_anchor_generator): AnchorGeneratorWithCenter(\n",
            "    (cell_anchors): BufferList()\n",
            "  )\n",
            ")\n",
            "\u001b[32m[04/15 02:41:14 fvcore.common.checkpoint]: \u001b[0mLoading checkpoint from /content/gdrive/MyDrive/QueryDet-PyTorch/work_dirs/visdrone_querydet/model_0007999.pth\n",
            "\u001b[32m[04/15 02:41:45 d2.data.common]: \u001b[0mSerializing 25884 elements to byte tensors and concatenating them all ...\n",
            "\u001b[32m[04/15 02:41:46 d2.data.common]: \u001b[0mSerialized dataset takes 15.78 MiB\n",
            "/usr/local/lib/python3.7/site-packages/torch/utils/data/dataloader.py:477: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  cpuset_checked))\n",
            "\u001b[32m[04/15 02:41:47 d2.engine.train_loop]: \u001b[0mStarting training from iteration 0\n",
            "\u001b[32m[04/15 02:42:46 d2.utils.events]: \u001b[0m eta: 1 day, 13:59:09  iter: 19  total_loss: 1.24  loss_cls: 0.8018  loss_box_reg: 0.3581  loss_query: 0.027  time: 2.4575  data_time: 0.1029  lr: 0.00011989  max_mem: 10548M\n",
            "\u001b[32m[04/15 02:43:35 d2.utils.events]: \u001b[0m eta: 1 day, 14:25:47  iter: 39  total_loss: 0.6601  loss_cls: 0.3759  loss_box_reg: 0.2442  loss_query: 0.02694  time: 2.3909  data_time: 0.0079  lr: 0.00023977  max_mem: 10548M\n",
            "\u001b[32m[04/15 02:44:21 d2.utils.events]: \u001b[0m eta: 1 day, 14:28:26  iter: 59  total_loss: 0.5347  loss_cls: 0.3377  loss_box_reg: 0.1694  loss_query: 0.03098  time: 2.3420  data_time: 0.0070  lr: 0.00035965  max_mem: 10548M\n",
            "\u001b[32m[04/15 02:45:14 d2.utils.events]: \u001b[0m eta: 1 day, 14:44:50  iter: 79  total_loss: 0.7858  loss_cls: 0.477  loss_box_reg: 0.2817  loss_query: 0.03057  time: 2.4023  data_time: 0.2449  lr: 0.00047953  max_mem: 10548M\n",
            "\u001b[32m[04/15 02:46:00 d2.utils.events]: \u001b[0m eta: 1 day, 14:29:34  iter: 99  total_loss: 0.3472  loss_cls: 0.1962  loss_box_reg: 0.1462  loss_query: 0.02088  time: 2.3740  data_time: 0.0062  lr: 0.00059941  max_mem: 10548M\n",
            "\u001b[32m[04/15 02:46:47 d2.utils.events]: \u001b[0m eta: 1 day, 14:28:47  iter: 119  total_loss: 0.5142  loss_cls: 0.3025  loss_box_reg: 0.1599  loss_query: 0.02194  time: 2.3594  data_time: 0.0068  lr: 0.00071929  max_mem: 10548M\n",
            "\u001b[32m[04/15 02:47:35 d2.utils.events]: \u001b[0m eta: 1 day, 14:28:07  iter: 139  total_loss: 0.5422  loss_cls: 0.3344  loss_box_reg: 0.1795  loss_query: 0.03182  time: 2.3559  data_time: 0.0078  lr: 0.00083917  max_mem: 10548M\n",
            "\u001b[32m[04/15 02:48:23 d2.utils.events]: \u001b[0m eta: 1 day, 14:40:32  iter: 159  total_loss: 0.5841  loss_cls: 0.3515  loss_box_reg: 0.2124  loss_query: 0.04236  time: 2.3551  data_time: 0.0088  lr: 0.00095905  max_mem: 10548M\n",
            "\u001b[32m[04/15 02:49:10 d2.utils.events]: \u001b[0m eta: 1 day, 14:45:03  iter: 179  total_loss: 0.6908  loss_cls: 0.4207  loss_box_reg: 0.2412  loss_query: 0.03656  time: 2.3491  data_time: 0.0076  lr: 0.0010789  max_mem: 10548M\n",
            "\u001b[32m[04/15 02:49:55 d2.utils.events]: \u001b[0m eta: 1 day, 14:42:03  iter: 199  total_loss: 0.4925  loss_cls: 0.3049  loss_box_reg: 0.1484  loss_query: 0.03214  time: 2.3325  data_time: 0.0081  lr: 0.0011988  max_mem: 10548M\n",
            "\u001b[32m[04/15 02:50:41 d2.utils.events]: \u001b[0m eta: 1 day, 14:39:30  iter: 219  total_loss: 0.5061  loss_cls: 0.2948  loss_box_reg: 0.2155  loss_query: 0.02114  time: 2.3242  data_time: 0.0076  lr: 0.0013187  max_mem: 10548M\n",
            "\u001b[32m[04/15 02:51:26 d2.utils.events]: \u001b[0m eta: 1 day, 14:34:35  iter: 239  total_loss: 0.5181  loss_cls: 0.3273  loss_box_reg: 0.2048  loss_query: 0.02908  time: 2.3143  data_time: 0.0076  lr: 0.0014386  max_mem: 10548M\n",
            "\u001b[32m[04/15 02:52:13 d2.utils.events]: \u001b[0m eta: 1 day, 14:35:52  iter: 259  total_loss: 0.7342  loss_cls: 0.418  loss_box_reg: 0.2883  loss_query: 0.03095  time: 2.3140  data_time: 0.0073  lr: 0.0015584  max_mem: 10548M\n",
            "\u001b[32m[04/15 02:53:00 d2.utils.events]: \u001b[0m eta: 1 day, 14:37:41  iter: 279  total_loss: 0.4959  loss_cls: 0.2893  loss_box_reg: 0.1719  loss_query: 0.02634  time: 2.3126  data_time: 0.0082  lr: 0.0016783  max_mem: 10548M\n",
            "\u001b[32m[04/15 02:53:47 d2.utils.events]: \u001b[0m eta: 1 day, 14:36:54  iter: 299  total_loss: 0.5432  loss_cls: 0.3368  loss_box_reg: 0.1829  loss_query: 0.02975  time: 2.3119  data_time: 0.0073  lr: 0.0017982  max_mem: 10548M\n",
            "\u001b[32m[04/15 02:54:34 d2.utils.events]: \u001b[0m eta: 1 day, 14:36:40  iter: 319  total_loss: 0.7949  loss_cls: 0.3931  loss_box_reg: 0.3123  loss_query: 0.03327  time: 2.3102  data_time: 0.0068  lr: 0.0019181  max_mem: 10548M\n",
            "\u001b[32m[04/15 02:55:21 d2.utils.events]: \u001b[0m eta: 1 day, 14:36:07  iter: 339  total_loss: 0.5259  loss_cls: 0.315  loss_box_reg: 0.172  loss_query: 0.03616  time: 2.3098  data_time: 0.0067  lr: 0.002038  max_mem: 10548M\n",
            "\u001b[32m[04/15 02:56:09 d2.utils.events]: \u001b[0m eta: 1 day, 14:35:20  iter: 359  total_loss: 0.4672  loss_cls: 0.2612  loss_box_reg: 0.1818  loss_query: 0.02407  time: 2.3128  data_time: 0.0075  lr: 0.0021578  max_mem: 10549M\n",
            "\u001b[32m[04/15 02:56:56 d2.utils.events]: \u001b[0m eta: 1 day, 14:34:20  iter: 379  total_loss: 0.5194  loss_cls: 0.3185  loss_box_reg: 0.1663  loss_query: 0.01134  time: 2.3122  data_time: 0.0059  lr: 0.0022777  max_mem: 10549M\n",
            "\u001b[32m[04/15 02:57:44 d2.utils.events]: \u001b[0m eta: 1 day, 14:34:04  iter: 399  total_loss: 0.6204  loss_cls: 0.3594  loss_box_reg: 0.2255  loss_query: 0.03537  time: 2.3123  data_time: 0.0074  lr: 0.0023976  max_mem: 10549M\n",
            "\u001b[32m[04/15 02:58:29 d2.utils.events]: \u001b[0m eta: 1 day, 14:32:32  iter: 419  total_loss: 0.4646  loss_cls: 0.2829  loss_box_reg: 0.1623  loss_query: 0.02437  time: 2.3068  data_time: 0.0077  lr: 0.0025175  max_mem: 10549M\n",
            "\u001b[32m[04/15 02:59:14 d2.utils.events]: \u001b[0m eta: 1 day, 14:30:31  iter: 439  total_loss: 0.5896  loss_cls: 0.334  loss_box_reg: 0.2158  loss_query: 0.02916  time: 2.3037  data_time: 0.0067  lr: 0.0026374  max_mem: 10549M\n",
            "\u001b[32m[04/15 03:00:02 d2.utils.events]: \u001b[0m eta: 1 day, 14:30:49  iter: 459  total_loss: 0.6778  loss_cls: 0.3908  loss_box_reg: 0.2298  loss_query: 0.02803  time: 2.3060  data_time: 0.0092  lr: 0.0027572  max_mem: 10549M\n",
            "\u001b[32m[04/15 03:00:50 d2.utils.events]: \u001b[0m eta: 1 day, 14:30:58  iter: 479  total_loss: 0.489  loss_cls: 0.2846  loss_box_reg: 0.1801  loss_query: 0.02356  time: 2.3074  data_time: 0.0080  lr: 0.0028771  max_mem: 10549M\n",
            "\u001b[32m[04/15 03:01:37 d2.utils.events]: \u001b[0m eta: 1 day, 14:31:05  iter: 499  total_loss: 0.4572  loss_cls: 0.2768  loss_box_reg: 0.1526  loss_query: 0.01997  time: 2.3067  data_time: 0.0078  lr: 0.002997  max_mem: 10549M\n",
            "\u001b[32m[04/15 03:02:25 d2.utils.events]: \u001b[0m eta: 1 day, 14:30:38  iter: 519  total_loss: 0.8769  loss_cls: 0.5343  loss_box_reg: 0.3016  loss_query: 0.04611  time: 2.3087  data_time: 0.0066  lr: 0.0031169  max_mem: 10549M\n",
            "\u001b[32m[04/15 03:03:12 d2.utils.events]: \u001b[0m eta: 1 day, 14:30:26  iter: 539  total_loss: 0.4987  loss_cls: 0.2771  loss_box_reg: 0.186  loss_query: 0.03227  time: 2.3081  data_time: 0.0106  lr: 0.0032368  max_mem: 10549M\n",
            "\u001b[32m[04/15 03:04:01 d2.utils.events]: \u001b[0m eta: 1 day, 14:31:19  iter: 559  total_loss: 0.6722  loss_cls: 0.3925  loss_box_reg: 0.235  loss_query: 0.04439  time: 2.3105  data_time: 0.0107  lr: 0.0033566  max_mem: 10549M\n",
            "\u001b[32m[04/15 03:04:48 d2.utils.events]: \u001b[0m eta: 1 day, 14:30:20  iter: 579  total_loss: 0.4532  loss_cls: 0.2844  loss_box_reg: 0.1494  loss_query: 0.01964  time: 2.3104  data_time: 0.0071  lr: 0.0034765  max_mem: 10549M\n",
            "\u001b[32m[04/15 03:05:36 d2.utils.events]: \u001b[0m eta: 1 day, 14:28:57  iter: 599  total_loss: 0.4794  loss_cls: 0.3032  loss_box_reg: 0.1589  loss_query: 0.01875  time: 2.3104  data_time: 0.0080  lr: 0.0035964  max_mem: 10549M\n",
            "\u001b[32m[04/15 03:06:23 d2.utils.events]: \u001b[0m eta: 1 day, 14:27:38  iter: 619  total_loss: 0.7636  loss_cls: 0.4296  loss_box_reg: 0.2732  loss_query: 0.0419  time: 2.3101  data_time: 0.0090  lr: 0.0037163  max_mem: 10549M\n",
            "\u001b[32m[04/15 03:07:09 d2.utils.events]: \u001b[0m eta: 1 day, 14:25:39  iter: 639  total_loss: 0.5308  loss_cls: 0.3271  loss_box_reg: 0.15  loss_query: 0.0241  time: 2.3079  data_time: 0.0073  lr: 0.0038362  max_mem: 10549M\n",
            "\u001b[32m[04/15 03:07:55 d2.utils.events]: \u001b[0m eta: 1 day, 14:23:13  iter: 659  total_loss: 0.5629  loss_cls: 0.3536  loss_box_reg: 0.1898  loss_query: 0.01783  time: 2.3063  data_time: 0.0072  lr: 0.003956  max_mem: 10549M\n",
            "\u001b[32m[04/15 03:08:42 d2.utils.events]: \u001b[0m eta: 1 day, 14:22:55  iter: 679  total_loss: 0.6977  loss_cls: 0.3848  loss_box_reg: 0.2382  loss_query: 0.03911  time: 2.3053  data_time: 0.0086  lr: 0.0040759  max_mem: 10549M\n",
            "\u001b[32m[04/15 03:09:28 d2.utils.events]: \u001b[0m eta: 1 day, 14:22:25  iter: 699  total_loss: 0.5385  loss_cls: 0.3551  loss_box_reg: 0.1692  loss_query: 0.01852  time: 2.3047  data_time: 0.0090  lr: 0.0041958  max_mem: 10549M\n",
            "\u001b[32m[04/15 03:10:16 d2.utils.events]: \u001b[0m eta: 1 day, 14:22:09  iter: 719  total_loss: 0.6233  loss_cls: 0.3703  loss_box_reg: 0.2019  loss_query: 0.029  time: 2.3047  data_time: 0.0066  lr: 0.0043157  max_mem: 10549M\n",
            "\u001b[32m[04/15 03:11:02 d2.utils.events]: \u001b[0m eta: 1 day, 14:22:05  iter: 739  total_loss: 0.5872  loss_cls: 0.3474  loss_box_reg: 0.2047  loss_query: 0.04152  time: 2.3043  data_time: 0.0076  lr: 0.0044356  max_mem: 10549M\n",
            "\u001b[32m[04/15 03:11:50 d2.utils.events]: \u001b[0m eta: 1 day, 14:20:05  iter: 759  total_loss: 0.5584  loss_cls: 0.3299  loss_box_reg: 0.1746  loss_query: 0.0328  time: 2.3043  data_time: 0.0078  lr: 0.0045554  max_mem: 10549M\n",
            "\u001b[32m[04/15 03:12:36 d2.utils.events]: \u001b[0m eta: 1 day, 14:18:43  iter: 779  total_loss: 0.6435  loss_cls: 0.3959  loss_box_reg: 0.2307  loss_query: 0.02122  time: 2.3040  data_time: 0.0059  lr: 0.0046753  max_mem: 10549M\n",
            "\u001b[32m[04/15 03:13:23 d2.utils.events]: \u001b[0m eta: 1 day, 14:17:57  iter: 799  total_loss: 0.5443  loss_cls: 0.3541  loss_box_reg: 0.1729  loss_query: 0.03314  time: 2.3033  data_time: 0.0079  lr: 0.0047952  max_mem: 10549M\n",
            "\u001b[32m[04/15 03:14:09 d2.utils.events]: \u001b[0m eta: 1 day, 14:16:50  iter: 819  total_loss: 0.6704  loss_cls: 0.3594  loss_box_reg: 0.225  loss_query: 0.03013  time: 2.3023  data_time: 0.0071  lr: 0.0049151  max_mem: 10549M\n",
            "\u001b[32m[04/15 03:15:08 d2.utils.events]: \u001b[0m eta: 1 day, 14:16:59  iter: 839  total_loss: 0.7594  loss_cls: 0.4363  loss_box_reg: 0.2812  loss_query: 0.04077  time: 2.3158  data_time: 0.5080  lr: 0.005035  max_mem: 10549M\n",
            "\u001b[32m[04/15 03:15:56 d2.utils.events]: \u001b[0m eta: 1 day, 14:17:25  iter: 859  total_loss: 0.5797  loss_cls: 0.335  loss_box_reg: 0.1935  loss_query: 0.05003  time: 2.3173  data_time: 0.0078  lr: 0.0051548  max_mem: 10549M\n",
            "\u001b[32m[04/15 03:16:44 d2.utils.events]: \u001b[0m eta: 1 day, 14:17:32  iter: 879  total_loss: 0.6672  loss_cls: 0.3943  loss_box_reg: 0.2453  loss_query: 0.02768  time: 2.3176  data_time: 0.0075  lr: 0.0052747  max_mem: 10549M\n",
            "\u001b[32m[04/15 03:17:32 d2.utils.events]: \u001b[0m eta: 1 day, 14:17:54  iter: 899  total_loss: 0.5879  loss_cls: 0.3352  loss_box_reg: 0.2224  loss_query: 0.0341  time: 2.3181  data_time: 0.0073  lr: 0.0053946  max_mem: 10549M\n",
            "\u001b[32m[04/15 03:18:18 d2.utils.events]: \u001b[0m eta: 1 day, 14:16:30  iter: 919  total_loss: 0.5245  loss_cls: 0.3151  loss_box_reg: 0.205  loss_query: 0.0307  time: 2.3168  data_time: 0.0067  lr: 0.0055145  max_mem: 10549M\n",
            "\u001b[32m[04/15 03:19:06 d2.utils.events]: \u001b[0m eta: 1 day, 14:16:20  iter: 939  total_loss: 0.8611  loss_cls: 0.4768  loss_box_reg: 0.311  loss_query: 0.04168  time: 2.3165  data_time: 0.0078  lr: 0.0056344  max_mem: 10549M\n",
            "\u001b[32m[04/15 03:19:53 d2.utils.events]: \u001b[0m eta: 1 day, 14:15:34  iter: 959  total_loss: 0.6046  loss_cls: 0.3638  loss_box_reg: 0.2104  loss_query: 0.02753  time: 2.3168  data_time: 0.0073  lr: 0.0057542  max_mem: 10549M\n",
            "\u001b[32m[04/15 03:20:40 d2.utils.events]: \u001b[0m eta: 1 day, 14:14:59  iter: 979  total_loss: 0.7337  loss_cls: 0.4016  loss_box_reg: 0.2608  loss_query: 0.03883  time: 2.3158  data_time: 0.0092  lr: 0.0058741  max_mem: 10549M\n",
            "\u001b[32m[04/15 03:21:28 d2.utils.events]: \u001b[0m eta: 1 day, 14:13:24  iter: 999  total_loss: 0.4631  loss_cls: 0.2792  loss_box_reg: 0.1668  loss_query: 0.02598  time: 2.3158  data_time: 0.0076  lr: 0.005994  max_mem: 10549M\n",
            "\u001b[32m[04/15 03:22:15 d2.utils.events]: \u001b[0m eta: 1 day, 14:13:38  iter: 1019  total_loss: 0.6516  loss_cls: 0.3756  loss_box_reg: 0.2088  loss_query: 0.03171  time: 2.3161  data_time: 0.0077  lr: 0.006  max_mem: 10549M\n",
            "\u001b[32m[04/15 03:23:04 d2.utils.events]: \u001b[0m eta: 1 day, 14:12:30  iter: 1039  total_loss: 0.5309  loss_cls: 0.302  loss_box_reg: 0.1734  loss_query: 0.02644  time: 2.3164  data_time: 0.0082  lr: 0.006  max_mem: 10549M\n",
            "\u001b[32m[04/15 03:23:51 d2.utils.events]: \u001b[0m eta: 1 day, 14:12:04  iter: 1059  total_loss: 0.7537  loss_cls: 0.434  loss_box_reg: 0.2723  loss_query: 0.02901  time: 2.3160  data_time: 0.0072  lr: 0.006  max_mem: 10549M\n",
            "\u001b[32m[04/15 03:24:38 d2.utils.events]: \u001b[0m eta: 1 day, 14:11:05  iter: 1079  total_loss: 0.6419  loss_cls: 0.3729  loss_box_reg: 0.2041  loss_query: 0.02378  time: 2.3161  data_time: 0.0073  lr: 0.006  max_mem: 10549M\n",
            "\u001b[32m[04/15 03:25:26 d2.utils.events]: \u001b[0m eta: 1 day, 14:11:08  iter: 1099  total_loss: 0.5566  loss_cls: 0.3639  loss_box_reg: 0.2062  loss_query: 0.03351  time: 2.3156  data_time: 0.0091  lr: 0.006  max_mem: 10549M\n",
            "\u001b[32m[04/15 03:26:12 d2.utils.events]: \u001b[0m eta: 1 day, 14:10:22  iter: 1119  total_loss: 0.6057  loss_cls: 0.3763  loss_box_reg: 0.1919  loss_query: 0.02824  time: 2.3148  data_time: 0.0066  lr: 0.006  max_mem: 10549M\n",
            "\u001b[32m[04/15 03:26:59 d2.utils.events]: \u001b[0m eta: 1 day, 14:08:58  iter: 1139  total_loss: 0.522  loss_cls: 0.3223  loss_box_reg: 0.1695  loss_query: 0.01867  time: 2.3146  data_time: 0.0072  lr: 0.006  max_mem: 10549M\n",
            "\u001b[32m[04/15 03:27:45 d2.utils.events]: \u001b[0m eta: 1 day, 14:06:39  iter: 1159  total_loss: 0.6238  loss_cls: 0.3528  loss_box_reg: 0.2431  loss_query: 0.03011  time: 2.3133  data_time: 0.0089  lr: 0.006  max_mem: 10549M\n",
            "\u001b[32m[04/15 03:28:33 d2.utils.events]: \u001b[0m eta: 1 day, 14:05:26  iter: 1179  total_loss: 0.7775  loss_cls: 0.4546  loss_box_reg: 0.2719  loss_query: 0.03446  time: 2.3135  data_time: 0.0074  lr: 0.006  max_mem: 10549M\n",
            "\u001b[32m[04/15 03:29:21 d2.utils.events]: \u001b[0m eta: 1 day, 14:06:25  iter: 1199  total_loss: 0.6988  loss_cls: 0.4305  loss_box_reg: 0.2622  loss_query: 0.04308  time: 2.3143  data_time: 0.0085  lr: 0.006  max_mem: 10549M\n",
            "\u001b[32m[04/15 03:30:06 d2.utils.events]: \u001b[0m eta: 1 day, 14:05:39  iter: 1219  total_loss: 0.5347  loss_cls: 0.306  loss_box_reg: 0.1915  loss_query: 0.03782  time: 2.3123  data_time: 0.0074  lr: 0.006  max_mem: 10549M\n",
            "\u001b[32m[04/15 03:30:52 d2.utils.events]: \u001b[0m eta: 1 day, 14:05:22  iter: 1239  total_loss: 0.4979  loss_cls: 0.2891  loss_box_reg: 0.1746  loss_query: 0.02676  time: 2.3114  data_time: 0.0058  lr: 0.006  max_mem: 10549M\n",
            "\u001b[32m[04/15 03:31:38 d2.utils.events]: \u001b[0m eta: 1 day, 14:03:36  iter: 1259  total_loss: 0.6218  loss_cls: 0.4034  loss_box_reg: 0.2149  loss_query: 0.03389  time: 2.3104  data_time: 0.0073  lr: 0.006  max_mem: 10549M\n",
            "\u001b[32m[04/15 03:32:24 d2.utils.events]: \u001b[0m eta: 1 day, 14:01:33  iter: 1279  total_loss: 0.6029  loss_cls: 0.3578  loss_box_reg: 0.2062  loss_query: 0.026  time: 2.3093  data_time: 0.0084  lr: 0.006  max_mem: 10549M\n",
            "\u001b[32m[04/15 03:33:11 d2.utils.events]: \u001b[0m eta: 1 day, 14:00:24  iter: 1299  total_loss: 0.5629  loss_cls: 0.3463  loss_box_reg: 0.1986  loss_query: 0.02006  time: 2.3088  data_time: 0.0075  lr: 0.006  max_mem: 10549M\n",
            "\u001b[32m[04/15 03:33:57 d2.utils.events]: \u001b[0m eta: 1 day, 13:59:12  iter: 1319  total_loss: 0.9147  loss_cls: 0.5307  loss_box_reg: 0.3058  loss_query: 0.04073  time: 2.3079  data_time: 0.0067  lr: 0.006  max_mem: 10549M\n",
            "\u001b[32m[04/15 03:34:56 d2.utils.events]: \u001b[0m eta: 1 day, 13:57:54  iter: 1339  total_loss: 0.5013  loss_cls: 0.3064  loss_box_reg: 0.1916  loss_query: 0.01975  time: 2.3164  data_time: 0.6071  lr: 0.006  max_mem: 10549M\n",
            "\u001b[32m[04/15 03:35:45 d2.utils.events]: \u001b[0m eta: 1 day, 13:58:04  iter: 1359  total_loss: 0.7639  loss_cls: 0.4026  loss_box_reg: 0.2722  loss_query: 0.0329  time: 2.3174  data_time: 0.0090  lr: 0.006  max_mem: 10549M\n",
            "\u001b[32m[04/15 03:36:32 d2.utils.events]: \u001b[0m eta: 1 day, 13:56:56  iter: 1379  total_loss: 0.6593  loss_cls: 0.4465  loss_box_reg: 0.1806  loss_query: 0.03548  time: 2.3172  data_time: 0.0071  lr: 0.006  max_mem: 10549M\n",
            "\u001b[32m[04/15 03:37:19 d2.utils.events]: \u001b[0m eta: 1 day, 13:55:18  iter: 1399  total_loss: 0.5014  loss_cls: 0.2831  loss_box_reg: 0.1706  loss_query: 0.01811  time: 2.3169  data_time: 0.0071  lr: 0.006  max_mem: 10549M\n",
            "\u001b[32m[04/15 03:38:07 d2.utils.events]: \u001b[0m eta: 1 day, 13:55:01  iter: 1419  total_loss: 0.7813  loss_cls: 0.4675  loss_box_reg: 0.2694  loss_query: 0.03124  time: 2.3172  data_time: 0.0073  lr: 0.006  max_mem: 10549M\n",
            "\u001b[32m[04/15 03:38:55 d2.utils.events]: \u001b[0m eta: 1 day, 13:55:20  iter: 1439  total_loss: 0.7896  loss_cls: 0.4452  loss_box_reg: 0.2603  loss_query: 0.03045  time: 2.3174  data_time: 0.0072  lr: 0.006  max_mem: 10549M\n",
            "\u001b[32m[04/15 03:39:43 d2.utils.events]: \u001b[0m eta: 1 day, 13:54:26  iter: 1459  total_loss: 0.6272  loss_cls: 0.3389  loss_box_reg: 0.2378  loss_query: 0.03476  time: 2.3174  data_time: 0.0070  lr: 0.006  max_mem: 10549M\n",
            "\u001b[32m[04/15 03:40:32 d2.utils.events]: \u001b[0m eta: 1 day, 13:53:39  iter: 1479  total_loss: 0.5005  loss_cls: 0.2961  loss_box_reg: 0.1794  loss_query: 0.02711  time: 2.3181  data_time: 0.0084  lr: 0.006  max_mem: 10549M\n",
            "\u001b[32m[04/15 03:41:19 d2.utils.events]: \u001b[0m eta: 1 day, 13:52:47  iter: 1499  total_loss: 0.5357  loss_cls: 0.3354  loss_box_reg: 0.2013  loss_query: 0.02727  time: 2.3178  data_time: 0.0083  lr: 0.006  max_mem: 10549M\n",
            "\u001b[32m[04/15 03:42:06 d2.utils.events]: \u001b[0m eta: 1 day, 13:51:30  iter: 1519  total_loss: 0.5863  loss_cls: 0.3802  loss_box_reg: 0.1932  loss_query: 0.03049  time: 2.3178  data_time: 0.0069  lr: 0.006  max_mem: 10549M\n",
            "\u001b[32m[04/15 03:42:53 d2.utils.events]: \u001b[0m eta: 1 day, 13:50:09  iter: 1539  total_loss: 0.6612  loss_cls: 0.3706  loss_box_reg: 0.2329  loss_query: 0.03489  time: 2.3178  data_time: 0.0081  lr: 0.006  max_mem: 10549M\n",
            "\u001b[32m[04/15 03:43:42 d2.utils.events]: \u001b[0m eta: 1 day, 13:49:49  iter: 1559  total_loss: 0.7683  loss_cls: 0.4617  loss_box_reg: 0.2682  loss_query: 0.03573  time: 2.3182  data_time: 0.0076  lr: 0.006  max_mem: 10549M\n",
            "\u001b[32m[04/15 03:44:32 d2.utils.events]: \u001b[0m eta: 1 day, 13:50:02  iter: 1579  total_loss: 0.7566  loss_cls: 0.4559  loss_box_reg: 0.3127  loss_query: 0.03631  time: 2.3193  data_time: 0.0089  lr: 0.006  max_mem: 10549M\n",
            "\u001b[32m[04/15 03:45:20 d2.utils.events]: \u001b[0m eta: 1 day, 13:49:16  iter: 1599  total_loss: 0.4694  loss_cls: 0.2734  loss_box_reg: 0.175  loss_query: 0.02479  time: 2.3197  data_time: 0.0059  lr: 0.006  max_mem: 10549M\n",
            "\u001b[32m[04/15 03:46:07 d2.utils.events]: \u001b[0m eta: 1 day, 13:48:55  iter: 1619  total_loss: 0.7605  loss_cls: 0.4299  loss_box_reg: 0.2741  loss_query: 0.0469  time: 2.3198  data_time: 0.0078  lr: 0.006  max_mem: 10549M\n",
            "\u001b[32m[04/15 03:46:54 d2.utils.events]: \u001b[0m eta: 1 day, 13:48:31  iter: 1639  total_loss: 0.5742  loss_cls: 0.3466  loss_box_reg: 0.1779  loss_query: 0.02712  time: 2.3192  data_time: 0.0088  lr: 0.006  max_mem: 10549M\n",
            "\u001b[32m[04/15 03:47:41 d2.utils.events]: \u001b[0m eta: 1 day, 13:49:20  iter: 1659  total_loss: 0.6532  loss_cls: 0.3815  loss_box_reg: 0.2386  loss_query: 0.0381  time: 2.3189  data_time: 0.0081  lr: 0.006  max_mem: 10549M\n",
            "\u001b[32m[04/15 03:48:29 d2.utils.events]: \u001b[0m eta: 1 day, 13:49:30  iter: 1679  total_loss: 0.5476  loss_cls: 0.3011  loss_box_reg: 0.2115  loss_query: 0.03936  time: 2.3193  data_time: 0.0079  lr: 0.006  max_mem: 10549M\n",
            "\u001b[32m[04/15 03:49:17 d2.utils.events]: \u001b[0m eta: 1 day, 13:48:26  iter: 1699  total_loss: 0.3979  loss_cls: 0.2278  loss_box_reg: 0.157  loss_query: 0.02011  time: 2.3194  data_time: 0.0072  lr: 0.006  max_mem: 10549M\n",
            "\u001b[32m[04/15 03:50:04 d2.utils.events]: \u001b[0m eta: 1 day, 13:47:17  iter: 1719  total_loss: 0.7224  loss_cls: 0.455  loss_box_reg: 0.2219  loss_query: 0.03112  time: 2.3189  data_time: 0.0062  lr: 0.006  max_mem: 10549M\n",
            "\u001b[32m[04/15 03:50:50 d2.utils.events]: \u001b[0m eta: 1 day, 13:45:51  iter: 1739  total_loss: 0.6145  loss_cls: 0.37  loss_box_reg: 0.2263  loss_query: 0.03452  time: 2.3183  data_time: 0.0083  lr: 0.006  max_mem: 10549M\n",
            "\u001b[32m[04/15 03:51:39 d2.utils.events]: \u001b[0m eta: 1 day, 13:46:37  iter: 1759  total_loss: 0.7298  loss_cls: 0.4243  loss_box_reg: 0.2466  loss_query: 0.02936  time: 2.3188  data_time: 0.0073  lr: 0.006  max_mem: 10549M\n",
            "\u001b[32m[04/15 03:52:29 d2.utils.events]: \u001b[0m eta: 1 day, 13:46:54  iter: 1779  total_loss: 0.7394  loss_cls: 0.4183  loss_box_reg: 0.2814  loss_query: 0.039  time: 2.3195  data_time: 0.0068  lr: 0.006  max_mem: 10549M\n",
            "\u001b[32m[04/15 03:53:15 d2.utils.events]: \u001b[0m eta: 1 day, 13:45:40  iter: 1799  total_loss: 0.4602  loss_cls: 0.264  loss_box_reg: 0.1631  loss_query: 0.02369  time: 2.3190  data_time: 0.0067  lr: 0.006  max_mem: 10549M\n",
            "\u001b[32m[04/15 03:54:04 d2.utils.events]: \u001b[0m eta: 1 day, 13:46:47  iter: 1819  total_loss: 0.5534  loss_cls: 0.3299  loss_box_reg: 0.1918  loss_query: 0.03372  time: 2.3196  data_time: 0.0062  lr: 0.006  max_mem: 10549M\n",
            "\u001b[32m[04/15 03:54:52 d2.utils.events]: \u001b[0m eta: 1 day, 13:44:11  iter: 1839  total_loss: 0.6688  loss_cls: 0.3947  loss_box_reg: 0.2337  loss_query: 0.02855  time: 2.3198  data_time: 0.0073  lr: 0.006  max_mem: 10549M\n",
            "\u001b[32m[04/15 03:55:39 d2.utils.events]: \u001b[0m eta: 1 day, 13:42:43  iter: 1859  total_loss: 0.4688  loss_cls: 0.2942  loss_box_reg: 0.1647  loss_query: 0.02227  time: 2.3199  data_time: 0.0061  lr: 0.006  max_mem: 10549M\n",
            "\u001b[32m[04/15 03:56:26 d2.utils.events]: \u001b[0m eta: 1 day, 13:41:41  iter: 1879  total_loss: 0.4787  loss_cls: 0.2633  loss_box_reg: 0.1926  loss_query: 0.02794  time: 2.3197  data_time: 0.0082  lr: 0.006  max_mem: 10549M\n",
            "\u001b[32m[04/15 03:57:14 d2.utils.events]: \u001b[0m eta: 1 day, 13:40:13  iter: 1899  total_loss: 0.7201  loss_cls: 0.4215  loss_box_reg: 0.2849  loss_query: 0.03264  time: 2.3196  data_time: 0.0085  lr: 0.006  max_mem: 10549M\n",
            "\u001b[32m[04/15 03:58:02 d2.utils.events]: \u001b[0m eta: 1 day, 13:40:39  iter: 1919  total_loss: 0.7379  loss_cls: 0.4215  loss_box_reg: 0.2665  loss_query: 0.0306  time: 2.3201  data_time: 0.0088  lr: 0.006  max_mem: 10549M\n",
            "\u001b[32m[04/15 03:58:50 d2.utils.events]: \u001b[0m eta: 1 day, 13:38:40  iter: 1939  total_loss: 0.4398  loss_cls: 0.3062  loss_box_reg: 0.1343  loss_query: 0.0174  time: 2.3201  data_time: 0.0062  lr: 0.006  max_mem: 10549M\n",
            "\u001b[32m[04/15 03:59:38 d2.utils.events]: \u001b[0m eta: 1 day, 13:38:50  iter: 1959  total_loss: 0.7301  loss_cls: 0.4841  loss_box_reg: 0.2377  loss_query: 0.03575  time: 2.3204  data_time: 0.0070  lr: 0.006  max_mem: 10549M\n",
            "\u001b[32m[04/15 04:00:24 d2.utils.events]: \u001b[0m eta: 1 day, 13:37:07  iter: 1979  total_loss: 0.5039  loss_cls: 0.3041  loss_box_reg: 0.1785  loss_query: 0.02333  time: 2.3199  data_time: 0.0076  lr: 0.006  max_mem: 10549M\n",
            "\u001b[32m[04/15 04:01:11 fvcore.common.checkpoint]: \u001b[0mSaving checkpoint to work_dirs/visdrone_querydet/model_0001999.pth\n",
            "\u001b[32m[04/15 04:01:22 d2.utils.events]: \u001b[0m eta: 1 day, 13:37:01  iter: 1999  total_loss: 0.8244  loss_cls: 0.4919  loss_box_reg: 0.2689  loss_query: 0.03747  time: 2.3198  data_time: 0.0065  lr: 0.006  max_mem: 10549M\n",
            "\u001b[32m[04/15 04:02:09 d2.utils.events]: \u001b[0m eta: 1 day, 13:36:46  iter: 2019  total_loss: 0.7968  loss_cls: 0.4761  loss_box_reg: 0.263  loss_query: 0.04439  time: 2.3191  data_time: 0.0086  lr: 0.006  max_mem: 10549M\n",
            "\u001b[32m[04/15 04:02:55 d2.utils.events]: \u001b[0m eta: 1 day, 13:34:47  iter: 2039  total_loss: 0.5908  loss_cls: 0.3604  loss_box_reg: 0.2067  loss_query: 0.01971  time: 2.3184  data_time: 0.0068  lr: 0.006  max_mem: 10549M\n",
            "\u001b[32m[04/15 04:03:43 d2.utils.events]: \u001b[0m eta: 1 day, 13:34:00  iter: 2059  total_loss: 0.7318  loss_cls: 0.4435  loss_box_reg: 0.2516  loss_query: 0.03437  time: 2.3187  data_time: 0.0086  lr: 0.006  max_mem: 10549M\n",
            "\u001b[32m[04/15 04:04:32 d2.utils.events]: \u001b[0m eta: 1 day, 13:33:13  iter: 2079  total_loss: 0.7568  loss_cls: 0.4037  loss_box_reg: 0.2675  loss_query: 0.047  time: 2.3191  data_time: 0.0069  lr: 0.006  max_mem: 10549M\n",
            "\u001b[32m[04/15 04:05:18 d2.utils.events]: \u001b[0m eta: 1 day, 13:32:24  iter: 2099  total_loss: 0.6447  loss_cls: 0.3784  loss_box_reg: 0.2214  loss_query: 0.03361  time: 2.3183  data_time: 0.0101  lr: 0.006  max_mem: 10549M\n",
            "\u001b[32m[04/15 04:06:04 d2.utils.events]: \u001b[0m eta: 1 day, 13:31:52  iter: 2119  total_loss: 0.5365  loss_cls: 0.3343  loss_box_reg: 0.1876  loss_query: 0.03435  time: 2.3176  data_time: 0.0071  lr: 0.006  max_mem: 10549M\n",
            "\u001b[32m[04/15 04:06:51 d2.utils.events]: \u001b[0m eta: 1 day, 13:31:19  iter: 2139  total_loss: 0.5843  loss_cls: 0.3519  loss_box_reg: 0.2111  loss_query: 0.01806  time: 2.3174  data_time: 0.0069  lr: 0.006  max_mem: 10549M\n",
            "\u001b[32m[04/15 04:07:37 d2.utils.events]: \u001b[0m eta: 1 day, 13:30:19  iter: 2159  total_loss: 0.4934  loss_cls: 0.3125  loss_box_reg: 0.182  loss_query: 0.0323  time: 2.3167  data_time: 0.0070  lr: 0.006  max_mem: 10549M\n",
            "\u001b[32m[04/15 04:08:23 d2.utils.events]: \u001b[0m eta: 1 day, 13:29:17  iter: 2179  total_loss: 0.4199  loss_cls: 0.2716  loss_box_reg: 0.1686  loss_query: 0.01391  time: 2.3158  data_time: 0.0073  lr: 0.006  max_mem: 10549M\n",
            "\u001b[32m[04/15 04:09:09 d2.utils.events]: \u001b[0m eta: 1 day, 13:26:16  iter: 2199  total_loss: 0.4641  loss_cls: 0.3221  loss_box_reg: 0.144  loss_query: 0.02407  time: 2.3153  data_time: 0.0072  lr: 0.006  max_mem: 10549M\n",
            "\u001b[32m[04/15 04:09:57 d2.utils.events]: \u001b[0m eta: 1 day, 13:26:01  iter: 2219  total_loss: 0.7377  loss_cls: 0.4224  loss_box_reg: 0.3042  loss_query: 0.05133  time: 2.3154  data_time: 0.0071  lr: 0.006  max_mem: 10549M\n",
            "\u001b[32m[04/15 04:10:43 d2.utils.events]: \u001b[0m eta: 1 day, 13:25:20  iter: 2239  total_loss: 0.6722  loss_cls: 0.4048  loss_box_reg: 0.238  loss_query: 0.04525  time: 2.3148  data_time: 0.0072  lr: 0.006  max_mem: 10549M\n",
            "\u001b[32m[04/15 04:11:29 d2.utils.events]: \u001b[0m eta: 1 day, 13:25:23  iter: 2259  total_loss: 0.595  loss_cls: 0.3739  loss_box_reg: 0.1998  loss_query: 0.03808  time: 2.3143  data_time: 0.0071  lr: 0.006  max_mem: 10549M\n",
            "\u001b[32m[04/15 04:12:18 d2.utils.events]: \u001b[0m eta: 1 day, 13:26:07  iter: 2279  total_loss: 0.6264  loss_cls: 0.3731  loss_box_reg: 0.253  loss_query: 0.02462  time: 2.3146  data_time: 0.0086  lr: 0.006  max_mem: 10549M\n",
            "\u001b[32m[04/15 04:13:07 d2.utils.events]: \u001b[0m eta: 1 day, 13:26:25  iter: 2299  total_loss: 0.6199  loss_cls: 0.3375  loss_box_reg: 0.2374  loss_query: 0.04194  time: 2.3154  data_time: 0.0078  lr: 0.006  max_mem: 10549M\n",
            "\u001b[32m[04/15 04:13:55 d2.utils.events]: \u001b[0m eta: 1 day, 13:26:54  iter: 2319  total_loss: 0.6288  loss_cls: 0.371  loss_box_reg: 0.2228  loss_query: 0.02401  time: 2.3156  data_time: 0.0075  lr: 0.006  max_mem: 10549M\n",
            "\u001b[32m[04/15 04:14:49 d2.utils.events]: \u001b[0m eta: 1 day, 13:26:41  iter: 2339  total_loss: 0.6302  loss_cls: 0.4033  loss_box_reg: 0.2021  loss_query: 0.03266  time: 2.3186  data_time: 0.3373  lr: 0.006  max_mem: 10549M\n",
            "\u001b[32m[04/15 04:15:38 d2.utils.events]: \u001b[0m eta: 1 day, 13:25:54  iter: 2359  total_loss: 0.8835  loss_cls: 0.5238  loss_box_reg: 0.2984  loss_query: 0.03753  time: 2.3190  data_time: 0.0073  lr: 0.006  max_mem: 10549M\n",
            "\u001b[32m[04/15 04:16:25 d2.utils.events]: \u001b[0m eta: 1 day, 13:25:30  iter: 2379  total_loss: 0.7705  loss_cls: 0.4134  loss_box_reg: 0.2651  loss_query: 0.02826  time: 2.3188  data_time: 0.0079  lr: 0.006  max_mem: 10549M\n",
            "\u001b[32m[04/15 04:17:11 d2.utils.events]: \u001b[0m eta: 1 day, 13:25:24  iter: 2399  total_loss: 0.7364  loss_cls: 0.4241  loss_box_reg: 0.2444  loss_query: 0.03287  time: 2.3184  data_time: 0.0076  lr: 0.006  max_mem: 10549M\n",
            "\u001b[32m[04/15 04:17:59 d2.utils.events]: \u001b[0m eta: 1 day, 13:24:55  iter: 2419  total_loss: 0.6003  loss_cls: 0.3684  loss_box_reg: 0.2108  loss_query: 0.03102  time: 2.3183  data_time: 0.0075  lr: 0.006  max_mem: 10549M\n",
            "\u001b[32m[04/15 04:18:46 d2.utils.events]: \u001b[0m eta: 1 day, 13:24:08  iter: 2439  total_loss: 0.6251  loss_cls: 0.371  loss_box_reg: 0.2364  loss_query: 0.01801  time: 2.3184  data_time: 0.0094  lr: 0.006  max_mem: 10549M\n",
            "\u001b[32m[04/15 04:19:32 d2.utils.events]: \u001b[0m eta: 1 day, 13:22:00  iter: 2459  total_loss: 0.5535  loss_cls: 0.3451  loss_box_reg: 0.1807  loss_query: 0.03031  time: 2.3175  data_time: 0.0075  lr: 0.006  max_mem: 10549M\n",
            "\u001b[32m[04/15 04:20:20 d2.utils.events]: \u001b[0m eta: 1 day, 13:19:39  iter: 2479  total_loss: 0.4667  loss_cls: 0.2819  loss_box_reg: 0.1314  loss_query: 0.02759  time: 2.3175  data_time: 0.0078  lr: 0.006  max_mem: 10549M\n",
            "\u001b[32m[04/15 04:21:07 d2.utils.events]: \u001b[0m eta: 1 day, 13:18:31  iter: 2499  total_loss: 0.4347  loss_cls: 0.2716  loss_box_reg: 0.1698  loss_query: 0.01398  time: 2.3175  data_time: 0.0075  lr: 0.006  max_mem: 10549M\n",
            "\u001b[32m[04/15 04:21:54 d2.utils.events]: \u001b[0m eta: 1 day, 13:17:55  iter: 2519  total_loss: 0.5679  loss_cls: 0.336  loss_box_reg: 0.2057  loss_query: 0.02777  time: 2.3172  data_time: 0.0065  lr: 0.006  max_mem: 10549M\n",
            "\u001b[32m[04/15 04:22:43 d2.utils.events]: \u001b[0m eta: 1 day, 13:17:10  iter: 2539  total_loss: 0.7143  loss_cls: 0.4108  loss_box_reg: 0.2368  loss_query: 0.03739  time: 2.3176  data_time: 0.0075  lr: 0.006  max_mem: 10549M\n",
            "\u001b[32m[04/15 04:23:30 d2.utils.events]: \u001b[0m eta: 1 day, 13:16:04  iter: 2559  total_loss: 0.7308  loss_cls: 0.4513  loss_box_reg: 0.2439  loss_query: 0.03271  time: 2.3176  data_time: 0.0088  lr: 0.006  max_mem: 10549M\n",
            "\u001b[32m[04/15 04:24:17 d2.utils.events]: \u001b[0m eta: 1 day, 13:14:32  iter: 2579  total_loss: 0.688  loss_cls: 0.3653  loss_box_reg: 0.2682  loss_query: 0.03915  time: 2.3175  data_time: 0.0084  lr: 0.006  max_mem: 10549M\n",
            "\u001b[32m[04/15 04:25:03 d2.utils.events]: \u001b[0m eta: 1 day, 13:13:25  iter: 2599  total_loss: 0.5275  loss_cls: 0.3093  loss_box_reg: 0.1884  loss_query: 0.01884  time: 2.3167  data_time: 0.0087  lr: 0.006  max_mem: 10549M\n",
            "\u001b[32m[04/15 04:25:51 d2.utils.events]: \u001b[0m eta: 1 day, 13:13:36  iter: 2619  total_loss: 0.7745  loss_cls: 0.4717  loss_box_reg: 0.253  loss_query: 0.04164  time: 2.3170  data_time: 0.0080  lr: 0.006  max_mem: 10549M\n",
            "\u001b[32m[04/15 04:26:39 d2.utils.events]: \u001b[0m eta: 1 day, 13:13:00  iter: 2639  total_loss: 0.5278  loss_cls: 0.3325  loss_box_reg: 0.1722  loss_query: 0.0256  time: 2.3170  data_time: 0.0064  lr: 0.006  max_mem: 10549M\n",
            "\u001b[32m[04/15 04:27:25 d2.utils.events]: \u001b[0m eta: 1 day, 13:10:52  iter: 2659  total_loss: 0.5089  loss_cls: 0.3032  loss_box_reg: 0.1729  loss_query: 0.02245  time: 2.3167  data_time: 0.0067  lr: 0.006  max_mem: 10549M\n",
            "\u001b[32m[04/15 04:28:14 d2.utils.events]: \u001b[0m eta: 1 day, 13:09:53  iter: 2679  total_loss: 0.7897  loss_cls: 0.4819  loss_box_reg: 0.2608  loss_query: 0.03075  time: 2.3170  data_time: 0.0088  lr: 0.006  max_mem: 10549M\n",
            "\u001b[32m[04/15 04:29:00 d2.utils.events]: \u001b[0m eta: 1 day, 13:09:11  iter: 2699  total_loss: 0.5277  loss_cls: 0.3379  loss_box_reg: 0.1751  loss_query: 0.02432  time: 2.3164  data_time: 0.0073  lr: 0.006  max_mem: 10549M\n",
            "\u001b[32m[04/15 04:29:48 d2.utils.events]: \u001b[0m eta: 1 day, 13:08:32  iter: 2719  total_loss: 0.6849  loss_cls: 0.3916  loss_box_reg: 0.2802  loss_query: 0.03669  time: 2.3165  data_time: 0.0069  lr: 0.006  max_mem: 10549M\n",
            "\u001b[32m[04/15 04:30:34 d2.utils.events]: \u001b[0m eta: 1 day, 13:07:33  iter: 2739  total_loss: 0.6875  loss_cls: 0.4098  loss_box_reg: 0.2071  loss_query: 0.03653  time: 2.3160  data_time: 0.0079  lr: 0.006  max_mem: 10549M\n",
            "\u001b[32m[04/15 04:31:22 d2.utils.events]: \u001b[0m eta: 1 day, 13:06:45  iter: 2759  total_loss: 0.4676  loss_cls: 0.3144  loss_box_reg: 0.1452  loss_query: 0.01715  time: 2.3162  data_time: 0.0082  lr: 0.006  max_mem: 10549M\n",
            "\u001b[32m[04/15 04:32:09 d2.utils.events]: \u001b[0m eta: 1 day, 13:04:56  iter: 2779  total_loss: 0.6239  loss_cls: 0.4113  loss_box_reg: 0.1956  loss_query: 0.0335  time: 2.3160  data_time: 0.0081  lr: 0.006  max_mem: 10549M\n",
            "\u001b[32m[04/15 04:32:55 d2.utils.events]: \u001b[0m eta: 1 day, 13:04:51  iter: 2799  total_loss: 0.6072  loss_cls: 0.3512  loss_box_reg: 0.2375  loss_query: 0.01265  time: 2.3156  data_time: 0.0074  lr: 0.006  max_mem: 10549M\n",
            "\u001b[32m[04/15 04:33:44 d2.utils.events]: \u001b[0m eta: 1 day, 13:03:37  iter: 2819  total_loss: 0.7466  loss_cls: 0.4558  loss_box_reg: 0.2598  loss_query: 0.03351  time: 2.3161  data_time: 0.0092  lr: 0.006  max_mem: 10549M\n",
            "\u001b[32m[04/15 04:34:30 d2.utils.events]: \u001b[0m eta: 1 day, 13:03:19  iter: 2839  total_loss: 0.5783  loss_cls: 0.3451  loss_box_reg: 0.2217  loss_query: 0.03786  time: 2.3159  data_time: 0.0079  lr: 0.006  max_mem: 10549M\n",
            "\u001b[32m[04/15 04:35:16 d2.utils.events]: \u001b[0m eta: 1 day, 13:01:30  iter: 2859  total_loss: 0.6108  loss_cls: 0.3695  loss_box_reg: 0.215  loss_query: 0.02122  time: 2.3155  data_time: 0.0069  lr: 0.006  max_mem: 10549M\n",
            "\u001b[32m[04/15 04:36:04 d2.utils.events]: \u001b[0m eta: 1 day, 13:00:22  iter: 2879  total_loss: 0.8164  loss_cls: 0.4807  loss_box_reg: 0.2732  loss_query: 0.01827  time: 2.3154  data_time: 0.0068  lr: 0.006  max_mem: 10549M\n",
            "\u001b[32m[04/15 04:36:51 d2.utils.events]: \u001b[0m eta: 1 day, 12:59:44  iter: 2899  total_loss: 0.5328  loss_cls: 0.3625  loss_box_reg: 0.1684  loss_query: 0.02965  time: 2.3156  data_time: 0.0070  lr: 0.006  max_mem: 10549M\n",
            "\u001b[32m[04/15 04:37:38 d2.utils.events]: \u001b[0m eta: 1 day, 12:58:04  iter: 2919  total_loss: 0.5116  loss_cls: 0.3461  loss_box_reg: 0.1848  loss_query: 0.01718  time: 2.3154  data_time: 0.0064  lr: 0.006  max_mem: 10549M\n",
            "\u001b[32m[04/15 04:38:25 d2.utils.events]: \u001b[0m eta: 1 day, 12:57:29  iter: 2939  total_loss: 0.7046  loss_cls: 0.4388  loss_box_reg: 0.2433  loss_query: 0.01777  time: 2.3151  data_time: 0.0085  lr: 0.006  max_mem: 10549M\n",
            "\u001b[32m[04/15 04:39:12 d2.utils.events]: \u001b[0m eta: 1 day, 12:56:00  iter: 2959  total_loss: 0.5685  loss_cls: 0.3593  loss_box_reg: 0.1669  loss_query: 0.0247  time: 2.3150  data_time: 0.0077  lr: 0.006  max_mem: 10549M\n",
            "\u001b[32m[04/15 04:39:59 d2.utils.events]: \u001b[0m eta: 1 day, 12:55:44  iter: 2979  total_loss: 0.5867  loss_cls: 0.3602  loss_box_reg: 0.2344  loss_query: 0.03276  time: 2.3149  data_time: 0.0080  lr: 0.006  max_mem: 10549M\n",
            "\u001b[32m[04/15 04:40:46 d2.utils.events]: \u001b[0m eta: 1 day, 12:55:01  iter: 2999  total_loss: 0.5726  loss_cls: 0.315  loss_box_reg: 0.2156  loss_query: 0.03793  time: 2.3149  data_time: 0.0058  lr: 0.006  max_mem: 10549M\n",
            "\u001b[32m[04/15 04:41:33 d2.utils.events]: \u001b[0m eta: 1 day, 12:54:10  iter: 3019  total_loss: 0.5457  loss_cls: 0.3395  loss_box_reg: 0.1904  loss_query: 0.02636  time: 2.3147  data_time: 0.0068  lr: 0.006  max_mem: 10549M\n",
            "\u001b[32m[04/15 04:42:20 d2.utils.events]: \u001b[0m eta: 1 day, 12:53:32  iter: 3039  total_loss: 0.7799  loss_cls: 0.4224  loss_box_reg: 0.2978  loss_query: 0.03648  time: 2.3145  data_time: 0.0073  lr: 0.006  max_mem: 10549M\n",
            "\u001b[32m[04/15 04:43:07 d2.utils.events]: \u001b[0m eta: 1 day, 12:52:45  iter: 3059  total_loss: 0.6223  loss_cls: 0.3696  loss_box_reg: 0.2115  loss_query: 0.02915  time: 2.3144  data_time: 0.0072  lr: 0.006  max_mem: 10549M\n",
            "\u001b[32m[04/15 04:43:54 d2.utils.events]: \u001b[0m eta: 1 day, 12:51:55  iter: 3079  total_loss: 0.7555  loss_cls: 0.4652  loss_box_reg: 0.2426  loss_query: 0.04112  time: 2.3144  data_time: 0.0090  lr: 0.006  max_mem: 10549M\n",
            "\u001b[32m[04/15 04:44:42 d2.utils.events]: \u001b[0m eta: 1 day, 12:51:00  iter: 3099  total_loss: 0.7632  loss_cls: 0.4678  loss_box_reg: 0.2752  loss_query: 0.03862  time: 2.3146  data_time: 0.0069  lr: 0.006  max_mem: 10549M\n",
            "\u001b[32m[04/15 04:45:28 d2.utils.events]: \u001b[0m eta: 1 day, 12:50:13  iter: 3119  total_loss: 0.8325  loss_cls: 0.4652  loss_box_reg: 0.305  loss_query: 0.05096  time: 2.3141  data_time: 0.0086  lr: 0.006  max_mem: 10549M\n",
            "\u001b[32m[04/15 04:46:16 d2.utils.events]: \u001b[0m eta: 1 day, 12:49:58  iter: 3139  total_loss: 0.6427  loss_cls: 0.3746  loss_box_reg: 0.2186  loss_query: 0.03512  time: 2.3142  data_time: 0.0101  lr: 0.006  max_mem: 10549M\n",
            "\u001b[32m[04/15 04:47:03 d2.utils.events]: \u001b[0m eta: 1 day, 12:49:38  iter: 3159  total_loss: 0.6317  loss_cls: 0.3838  loss_box_reg: 0.2406  loss_query: 0.03355  time: 2.3142  data_time: 0.0066  lr: 0.006  max_mem: 10549M\n",
            "\u001b[32m[04/15 04:47:50 d2.utils.events]: \u001b[0m eta: 1 day, 12:49:23  iter: 3179  total_loss: 0.5698  loss_cls: 0.3484  loss_box_reg: 0.2059  loss_query: 0.02726  time: 2.3140  data_time: 0.0086  lr: 0.006  max_mem: 10549M\n",
            "\u001b[32m[04/15 04:48:38 d2.utils.events]: \u001b[0m eta: 1 day, 12:49:42  iter: 3199  total_loss: 0.5508  loss_cls: 0.3477  loss_box_reg: 0.1987  loss_query: 0.03717  time: 2.3143  data_time: 0.0076  lr: 0.006  max_mem: 10549M\n",
            "\u001b[32m[04/15 04:49:31 d2.utils.events]: \u001b[0m eta: 1 day, 12:48:08  iter: 3219  total_loss: 0.5094  loss_cls: 0.322  loss_box_reg: 0.1585  loss_query: 0.02908  time: 2.3161  data_time: 0.4323  lr: 0.006  max_mem: 10549M\n",
            "\u001b[32m[04/15 04:50:19 d2.utils.events]: \u001b[0m eta: 1 day, 12:47:21  iter: 3239  total_loss: 0.5109  loss_cls: 0.3196  loss_box_reg: 0.1711  loss_query: 0.02434  time: 2.3161  data_time: 0.0060  lr: 0.006  max_mem: 10549M\n",
            "\u001b[32m[04/15 04:51:05 d2.utils.events]: \u001b[0m eta: 1 day, 12:46:34  iter: 3259  total_loss: 0.6079  loss_cls: 0.3533  loss_box_reg: 0.219  loss_query: 0.04264  time: 2.3158  data_time: 0.0097  lr: 0.006  max_mem: 10549M\n",
            "\u001b[32m[04/15 04:51:51 d2.utils.events]: \u001b[0m eta: 1 day, 12:44:49  iter: 3279  total_loss: 0.6434  loss_cls: 0.4029  loss_box_reg: 0.2047  loss_query: 0.04276  time: 2.3152  data_time: 0.0073  lr: 0.006  max_mem: 10549M\n",
            "\u001b[32m[04/15 04:52:36 d2.utils.events]: \u001b[0m eta: 1 day, 12:43:26  iter: 3299  total_loss: 0.8304  loss_cls: 0.4967  loss_box_reg: 0.293  loss_query: 0.04706  time: 2.3147  data_time: 0.0083  lr: 0.006  max_mem: 10549M\n",
            "\u001b[32m[04/15 04:53:24 d2.utils.events]: \u001b[0m eta: 1 day, 12:42:27  iter: 3319  total_loss: 0.56  loss_cls: 0.3179  loss_box_reg: 0.1883  loss_query: 0.03311  time: 2.3148  data_time: 0.0084  lr: 0.006  max_mem: 10549M\n",
            "\u001b[32m[04/15 04:54:11 d2.utils.events]: \u001b[0m eta: 1 day, 12:40:52  iter: 3339  total_loss: 0.4144  loss_cls: 0.2459  loss_box_reg: 0.1502  loss_query: 0.01956  time: 2.3147  data_time: 0.0068  lr: 0.006  max_mem: 10549M\n",
            "\u001b[32m[04/15 04:54:58 d2.utils.events]: \u001b[0m eta: 1 day, 12:39:11  iter: 3359  total_loss: 0.8569  loss_cls: 0.5292  loss_box_reg: 0.29  loss_query: 0.03825  time: 2.3147  data_time: 0.0098  lr: 0.006  max_mem: 10549M\n",
            "\u001b[32m[04/15 04:55:47 d2.utils.events]: \u001b[0m eta: 1 day, 12:38:24  iter: 3379  total_loss: 0.5726  loss_cls: 0.3386  loss_box_reg: 0.1982  loss_query: 0.03543  time: 2.3148  data_time: 0.0079  lr: 0.006  max_mem: 10549M\n",
            "\u001b[32m[04/15 04:56:35 d2.utils.events]: \u001b[0m eta: 1 day, 12:38:24  iter: 3399  total_loss: 0.6721  loss_cls: 0.3799  loss_box_reg: 0.2492  loss_query: 0.03906  time: 2.3148  data_time: 0.0071  lr: 0.006  max_mem: 10549M\n",
            "\u001b[32m[04/15 04:57:23 d2.utils.events]: \u001b[0m eta: 1 day, 12:37:45  iter: 3419  total_loss: 0.5327  loss_cls: 0.3423  loss_box_reg: 0.1893  loss_query: 0.03305  time: 2.3151  data_time: 0.0114  lr: 0.006  max_mem: 10549M\n",
            "\u001b[32m[04/15 04:58:10 d2.utils.events]: \u001b[0m eta: 1 day, 12:36:58  iter: 3439  total_loss: 0.526  loss_cls: 0.3308  loss_box_reg: 0.1517  loss_query: 0.03901  time: 2.3148  data_time: 0.0073  lr: 0.006  max_mem: 10549M\n",
            "\u001b[32m[04/15 04:58:57 d2.utils.events]: \u001b[0m eta: 1 day, 12:36:12  iter: 3459  total_loss: 0.5241  loss_cls: 0.3015  loss_box_reg: 0.1549  loss_query: 0.02944  time: 2.3148  data_time: 0.0070  lr: 0.006  max_mem: 10549M\n",
            "\u001b[32m[04/15 04:59:45 d2.utils.events]: \u001b[0m eta: 1 day, 12:35:18  iter: 3479  total_loss: 0.5783  loss_cls: 0.3345  loss_box_reg: 0.1954  loss_query: 0.04169  time: 2.3149  data_time: 0.0073  lr: 0.006  max_mem: 10549M\n",
            "\u001b[32m[04/15 05:00:33 d2.utils.events]: \u001b[0m eta: 1 day, 12:34:54  iter: 3499  total_loss: 0.7944  loss_cls: 0.4577  loss_box_reg: 0.2804  loss_query: 0.04424  time: 2.3150  data_time: 0.0072  lr: 0.006  max_mem: 10549M\n",
            "\u001b[32m[04/15 05:01:20 d2.utils.events]: \u001b[0m eta: 1 day, 12:34:07  iter: 3519  total_loss: 0.7389  loss_cls: 0.442  loss_box_reg: 0.2623  loss_query: 0.03408  time: 2.3149  data_time: 0.0072  lr: 0.006  max_mem: 10549M\n",
            "\u001b[32m[04/15 05:02:07 d2.utils.events]: \u001b[0m eta: 1 day, 12:33:12  iter: 3539  total_loss: 0.5951  loss_cls: 0.339  loss_box_reg: 0.2398  loss_query: 0.0277  time: 2.3149  data_time: 0.0062  lr: 0.006  max_mem: 10549M\n",
            "\u001b[32m[04/15 05:02:55 d2.utils.events]: \u001b[0m eta: 1 day, 12:32:39  iter: 3559  total_loss: 0.6388  loss_cls: 0.3757  loss_box_reg: 0.212  loss_query: 0.02598  time: 2.3150  data_time: 0.0081  lr: 0.006  max_mem: 10549M\n",
            "\u001b[32m[04/15 05:03:43 d2.utils.events]: \u001b[0m eta: 1 day, 12:32:17  iter: 3579  total_loss: 0.6942  loss_cls: 0.414  loss_box_reg: 0.2442  loss_query: 0.02509  time: 2.3152  data_time: 0.0081  lr: 0.006  max_mem: 10549M\n",
            "\u001b[32m[04/15 05:04:30 d2.utils.events]: \u001b[0m eta: 1 day, 12:32:04  iter: 3599  total_loss: 0.4897  loss_cls: 0.3027  loss_box_reg: 0.1781  loss_query: 0.02365  time: 2.3151  data_time: 0.0072  lr: 0.006  max_mem: 10549M\n",
            "\u001b[32m[04/15 05:05:18 d2.utils.events]: \u001b[0m eta: 1 day, 12:30:45  iter: 3619  total_loss: 0.8066  loss_cls: 0.5043  loss_box_reg: 0.2663  loss_query: 0.04504  time: 2.3152  data_time: 0.0078  lr: 0.006  max_mem: 10549M\n",
            "\u001b[32m[04/15 05:06:06 d2.utils.events]: \u001b[0m eta: 1 day, 12:29:33  iter: 3639  total_loss: 0.5291  loss_cls: 0.3225  loss_box_reg: 0.1621  loss_query: 0.02482  time: 2.3154  data_time: 0.0072  lr: 0.006  max_mem: 10549M\n",
            "\u001b[32m[04/15 05:06:52 d2.utils.events]: \u001b[0m eta: 1 day, 12:29:29  iter: 3659  total_loss: 0.8471  loss_cls: 0.4948  loss_box_reg: 0.3127  loss_query: 0.03674  time: 2.3151  data_time: 0.0078  lr: 0.006  max_mem: 10549M\n",
            "\u001b[32m[04/15 05:07:39 d2.utils.events]: \u001b[0m eta: 1 day, 12:27:48  iter: 3679  total_loss: 0.5917  loss_cls: 0.3583  loss_box_reg: 0.199  loss_query: 0.02245  time: 2.3149  data_time: 0.0066  lr: 0.006  max_mem: 10549M\n",
            "\u001b[32m[04/15 05:08:27 d2.utils.events]: \u001b[0m eta: 1 day, 12:27:13  iter: 3699  total_loss: 0.7055  loss_cls: 0.4533  loss_box_reg: 0.2533  loss_query: 0.04399  time: 2.3152  data_time: 0.0080  lr: 0.006  max_mem: 10549M\n",
            "\u001b[32m[04/15 05:09:13 d2.utils.events]: \u001b[0m eta: 1 day, 12:25:59  iter: 3719  total_loss: 0.7276  loss_cls: 0.4203  loss_box_reg: 0.2752  loss_query: 0.03434  time: 2.3148  data_time: 0.0076  lr: 0.006  max_mem: 10549M\n",
            "\u001b[32m[04/15 05:10:03 d2.utils.events]: \u001b[0m eta: 1 day, 12:26:23  iter: 3739  total_loss: 0.6801  loss_cls: 0.4035  loss_box_reg: 0.2335  loss_query: 0.0462  time: 2.3153  data_time: 0.0066  lr: 0.006  max_mem: 10549M\n",
            "\u001b[32m[04/15 05:10:48 d2.utils.events]: \u001b[0m eta: 1 day, 12:24:42  iter: 3759  total_loss: 0.5172  loss_cls: 0.3025  loss_box_reg: 0.1791  loss_query: 0.02846  time: 2.3147  data_time: 0.0069  lr: 0.006  max_mem: 10549M\n",
            "\u001b[32m[04/15 05:11:37 d2.utils.events]: \u001b[0m eta: 1 day, 12:24:13  iter: 3779  total_loss: 0.6944  loss_cls: 0.4321  loss_box_reg: 0.207  loss_query: 0.02432  time: 2.3149  data_time: 0.0082  lr: 0.006  max_mem: 10549M\n",
            "\u001b[32m[04/15 05:12:23 d2.utils.events]: \u001b[0m eta: 1 day, 12:23:11  iter: 3799  total_loss: 0.706  loss_cls: 0.3839  loss_box_reg: 0.2112  loss_query: 0.04322  time: 2.3145  data_time: 0.0085  lr: 0.006  max_mem: 10549M\n",
            "\u001b[32m[04/15 05:13:08 d2.utils.events]: \u001b[0m eta: 1 day, 12:21:58  iter: 3819  total_loss: 0.6262  loss_cls: 0.3458  loss_box_reg: 0.2292  loss_query: 0.02621  time: 2.3140  data_time: 0.0063  lr: 0.006  max_mem: 10549M\n",
            "\u001b[32m[04/15 05:13:55 d2.utils.events]: \u001b[0m eta: 1 day, 12:20:46  iter: 3839  total_loss: 0.5632  loss_cls: 0.3533  loss_box_reg: 0.1729  loss_query: 0.02203  time: 2.3137  data_time: 0.0070  lr: 0.006  max_mem: 10549M\n",
            "\u001b[32m[04/15 05:14:42 d2.utils.events]: \u001b[0m eta: 1 day, 12:20:20  iter: 3859  total_loss: 0.6884  loss_cls: 0.3951  loss_box_reg: 0.2364  loss_query: 0.03042  time: 2.3137  data_time: 0.0079  lr: 0.006  max_mem: 10549M\n",
            "\u001b[32m[04/15 05:15:29 d2.utils.events]: \u001b[0m eta: 1 day, 12:19:34  iter: 3879  total_loss: 0.5764  loss_cls: 0.3434  loss_box_reg: 0.2063  loss_query: 0.02957  time: 2.3136  data_time: 0.0073  lr: 0.006  max_mem: 10549M\n",
            "\u001b[32m[04/15 05:16:18 d2.utils.events]: \u001b[0m eta: 1 day, 12:19:13  iter: 3899  total_loss: 0.6492  loss_cls: 0.3865  loss_box_reg: 0.2444  loss_query: 0.04918  time: 2.3138  data_time: 0.0077  lr: 0.006  max_mem: 10549M\n",
            "\u001b[32m[04/15 05:17:06 d2.utils.events]: \u001b[0m eta: 1 day, 12:18:46  iter: 3919  total_loss: 0.5221  loss_cls: 0.3085  loss_box_reg: 0.1802  loss_query: 0.02068  time: 2.3139  data_time: 0.0091  lr: 0.006  max_mem: 10549M\n",
            "\u001b[32m[04/15 05:17:53 d2.utils.events]: \u001b[0m eta: 1 day, 12:18:19  iter: 3939  total_loss: 0.5392  loss_cls: 0.3079  loss_box_reg: 0.2041  loss_query: 0.02352  time: 2.3138  data_time: 0.0068  lr: 0.006  max_mem: 10549M\n",
            "\u001b[32m[04/15 05:18:40 d2.utils.events]: \u001b[0m eta: 1 day, 12:17:52  iter: 3959  total_loss: 0.4632  loss_cls: 0.2805  loss_box_reg: 0.1592  loss_query: 0.03051  time: 2.3138  data_time: 0.0074  lr: 0.006  max_mem: 10549M\n",
            "\u001b[32m[04/15 05:19:26 d2.utils.events]: \u001b[0m eta: 1 day, 12:16:27  iter: 3979  total_loss: 0.6955  loss_cls: 0.3828  loss_box_reg: 0.2831  loss_query: 0.02829  time: 2.3136  data_time: 0.0093  lr: 0.006  max_mem: 10549M\n",
            "\u001b[32m[04/15 05:20:13 fvcore.common.checkpoint]: \u001b[0mSaving checkpoint to work_dirs/visdrone_querydet/model_0003999.pth\n",
            "\u001b[32m[04/15 05:20:30 d2.utils.events]: \u001b[0m eta: 1 day, 12:16:19  iter: 3999  total_loss: 0.7578  loss_cls: 0.4522  loss_box_reg: 0.2827  loss_query: 0.0333  time: 2.3135  data_time: 0.0077  lr: 0.006  max_mem: 10549M\n",
            "\u001b[32m[04/15 05:21:18 d2.utils.events]: \u001b[0m eta: 1 day, 12:15:49  iter: 4019  total_loss: 0.5836  loss_cls: 0.3342  loss_box_reg: 0.193  loss_query: 0.03846  time: 2.3136  data_time: 0.0062  lr: 0.006  max_mem: 10549M\n",
            "\u001b[32m[04/15 05:22:03 d2.utils.events]: \u001b[0m eta: 1 day, 12:15:32  iter: 4039  total_loss: 0.5035  loss_cls: 0.3425  loss_box_reg: 0.1644  loss_query: 0.03192  time: 2.3132  data_time: 0.0074  lr: 0.006  max_mem: 10549M\n",
            "\u001b[32m[04/15 05:22:50 d2.utils.events]: \u001b[0m eta: 1 day, 12:14:40  iter: 4059  total_loss: 0.6635  loss_cls: 0.374  loss_box_reg: 0.2799  loss_query: 0.02852  time: 2.3130  data_time: 0.0094  lr: 0.006  max_mem: 10549M\n",
            "\u001b[32m[04/15 05:23:38 d2.utils.events]: \u001b[0m eta: 1 day, 12:13:29  iter: 4079  total_loss: 0.6507  loss_cls: 0.3655  loss_box_reg: 0.2579  loss_query: 0.03156  time: 2.3130  data_time: 0.0084  lr: 0.006  max_mem: 10549M\n",
            "\u001b[32m[04/15 05:24:27 d2.utils.events]: \u001b[0m eta: 1 day, 12:14:05  iter: 4099  total_loss: 0.7515  loss_cls: 0.4129  loss_box_reg: 0.2953  loss_query: 0.03727  time: 2.3134  data_time: 0.0066  lr: 0.006  max_mem: 10549M\n",
            "\u001b[32m[04/15 05:25:14 d2.utils.events]: \u001b[0m eta: 1 day, 12:12:26  iter: 4119  total_loss: 0.4792  loss_cls: 0.2949  loss_box_reg: 0.1637  loss_query: 0.02806  time: 2.3134  data_time: 0.0058  lr: 0.006  max_mem: 10549M\n",
            "\u001b[32m[04/15 05:26:01 d2.utils.events]: \u001b[0m eta: 1 day, 12:11:09  iter: 4139  total_loss: 0.7205  loss_cls: 0.4096  loss_box_reg: 0.2833  loss_query: 0.02441  time: 2.3132  data_time: 0.0074  lr: 0.006  max_mem: 10549M\n",
            "\u001b[32m[04/15 05:26:48 d2.utils.events]: \u001b[0m eta: 1 day, 12:10:53  iter: 4159  total_loss: 0.7303  loss_cls: 0.4642  loss_box_reg: 0.2527  loss_query: 0.03291  time: 2.3131  data_time: 0.0076  lr: 0.006  max_mem: 10549M\n",
            "\u001b[32m[04/15 05:27:35 d2.utils.events]: \u001b[0m eta: 1 day, 12:09:55  iter: 4179  total_loss: 0.4825  loss_cls: 0.2971  loss_box_reg: 0.1693  loss_query: 0.01864  time: 2.3130  data_time: 0.0075  lr: 0.006  max_mem: 10549M\n",
            "\u001b[32m[04/15 05:28:21 d2.utils.events]: \u001b[0m eta: 1 day, 12:08:49  iter: 4199  total_loss: 0.7112  loss_cls: 0.4162  loss_box_reg: 0.2663  loss_query: 0.02864  time: 2.3128  data_time: 0.0076  lr: 0.006  max_mem: 10549M\n",
            "\u001b[32m[04/15 05:29:09 d2.utils.events]: \u001b[0m eta: 1 day, 12:08:45  iter: 4219  total_loss: 0.6528  loss_cls: 0.4035  loss_box_reg: 0.2182  loss_query: 0.02496  time: 2.3129  data_time: 0.0068  lr: 0.006  max_mem: 10549M\n",
            "\u001b[32m[04/15 05:29:57 d2.utils.events]: \u001b[0m eta: 1 day, 12:08:20  iter: 4239  total_loss: 0.731  loss_cls: 0.4439  loss_box_reg: 0.2387  loss_query: 0.02697  time: 2.3130  data_time: 0.0066  lr: 0.006  max_mem: 10549M\n",
            "\u001b[32m[04/15 05:30:44 d2.utils.events]: \u001b[0m eta: 1 day, 12:07:25  iter: 4259  total_loss: 0.6196  loss_cls: 0.3409  loss_box_reg: 0.2289  loss_query: 0.02842  time: 2.3131  data_time: 0.0115  lr: 0.006  max_mem: 10549M\n",
            "\u001b[32m[04/15 05:31:31 d2.utils.events]: \u001b[0m eta: 1 day, 12:08:07  iter: 4279  total_loss: 0.6327  loss_cls: 0.3824  loss_box_reg: 0.202  loss_query: 0.03766  time: 2.3128  data_time: 0.0082  lr: 0.006  max_mem: 10549M\n",
            "\u001b[32m[04/15 05:32:18 d2.utils.events]: \u001b[0m eta: 1 day, 12:05:52  iter: 4299  total_loss: 0.5061  loss_cls: 0.3149  loss_box_reg: 0.1765  loss_query: 0.01989  time: 2.3127  data_time: 0.0071  lr: 0.006  max_mem: 10549M\n",
            "\u001b[32m[04/15 05:33:06 d2.utils.events]: \u001b[0m eta: 1 day, 12:04:52  iter: 4319  total_loss: 0.7164  loss_cls: 0.3986  loss_box_reg: 0.2789  loss_query: 0.03301  time: 2.3129  data_time: 0.0084  lr: 0.006  max_mem: 10549M\n",
            "\u001b[32m[04/15 05:33:54 d2.utils.events]: \u001b[0m eta: 1 day, 12:06:02  iter: 4339  total_loss: 0.4619  loss_cls: 0.2735  loss_box_reg: 0.1683  loss_query: 0.0271  time: 2.3130  data_time: 0.0081  lr: 0.006  max_mem: 10549M\n",
            "\u001b[32m[04/15 05:34:40 d2.utils.events]: \u001b[0m eta: 1 day, 12:06:00  iter: 4359  total_loss: 0.5869  loss_cls: 0.3408  loss_box_reg: 0.2071  loss_query: 0.04391  time: 2.3128  data_time: 0.0070  lr: 0.006  max_mem: 10549M\n",
            "\u001b[32m[04/15 05:35:28 d2.utils.events]: \u001b[0m eta: 1 day, 12:04:14  iter: 4379  total_loss: 0.5874  loss_cls: 0.3593  loss_box_reg: 0.1999  loss_query: 0.02997  time: 2.3129  data_time: 0.0078  lr: 0.006  max_mem: 10549M\n",
            "\u001b[32m[04/15 05:36:15 d2.utils.events]: \u001b[0m eta: 1 day, 12:01:59  iter: 4399  total_loss: 0.7132  loss_cls: 0.4128  loss_box_reg: 0.2479  loss_query: 0.0224  time: 2.3127  data_time: 0.0084  lr: 0.006  max_mem: 10549M\n",
            "\u001b[32m[04/15 05:37:02 d2.utils.events]: \u001b[0m eta: 1 day, 11:59:42  iter: 4419  total_loss: 0.5974  loss_cls: 0.3932  loss_box_reg: 0.1905  loss_query: 0.02845  time: 2.3128  data_time: 0.0070  lr: 0.006  max_mem: 10549M\n",
            "\u001b[32m[04/15 05:37:48 d2.utils.events]: \u001b[0m eta: 1 day, 11:58:56  iter: 4439  total_loss: 0.6893  loss_cls: 0.4159  loss_box_reg: 0.2165  loss_query: 0.03053  time: 2.3124  data_time: 0.0080  lr: 0.006  max_mem: 10549M\n",
            "\u001b[32m[04/15 05:38:38 d2.utils.events]: \u001b[0m eta: 1 day, 12:00:41  iter: 4459  total_loss: 0.821  loss_cls: 0.496  loss_box_reg: 0.3183  loss_query: 0.04491  time: 2.3129  data_time: 0.0072  lr: 0.006  max_mem: 10549M\n",
            "\u001b[32m[04/15 05:39:25 d2.utils.events]: \u001b[0m eta: 1 day, 12:00:28  iter: 4479  total_loss: 0.6426  loss_cls: 0.3975  loss_box_reg: 0.1965  loss_query: 0.0389  time: 2.3129  data_time: 0.0072  lr: 0.006  max_mem: 10549M\n",
            "\u001b[32m[04/15 05:40:14 d2.utils.events]: \u001b[0m eta: 1 day, 11:59:33  iter: 4499  total_loss: 0.5809  loss_cls: 0.3185  loss_box_reg: 0.2221  loss_query: 0.03806  time: 2.3132  data_time: 0.0079  lr: 0.006  max_mem: 10549M\n",
            "\u001b[32m[04/15 05:41:02 d2.utils.events]: \u001b[0m eta: 1 day, 12:00:35  iter: 4519  total_loss: 0.6614  loss_cls: 0.3757  loss_box_reg: 0.2401  loss_query: 0.03319  time: 2.3133  data_time: 0.0075  lr: 0.006  max_mem: 10549M\n",
            "\u001b[32m[04/15 05:41:49 d2.utils.events]: \u001b[0m eta: 1 day, 12:00:03  iter: 4539  total_loss: 0.7169  loss_cls: 0.3994  loss_box_reg: 0.2431  loss_query: 0.02781  time: 2.3132  data_time: 0.0060  lr: 0.006  max_mem: 10549M\n",
            "\u001b[32m[04/15 05:42:36 d2.utils.events]: \u001b[0m eta: 1 day, 11:57:54  iter: 4559  total_loss: 0.4935  loss_cls: 0.3135  loss_box_reg: 0.1823  loss_query: 0.03085  time: 2.3132  data_time: 0.0069  lr: 0.006  max_mem: 10549M\n",
            "\u001b[32m[04/15 05:43:23 d2.utils.events]: \u001b[0m eta: 1 day, 11:56:41  iter: 4579  total_loss: 0.6714  loss_cls: 0.3963  loss_box_reg: 0.2326  loss_query: 0.04183  time: 2.3129  data_time: 0.0068  lr: 0.006  max_mem: 10549M\n",
            "\u001b[32m[04/15 05:44:10 d2.utils.events]: \u001b[0m eta: 1 day, 11:54:39  iter: 4599  total_loss: 0.7241  loss_cls: 0.4457  loss_box_reg: 0.2348  loss_query: 0.02886  time: 2.3129  data_time: 0.0081  lr: 0.006  max_mem: 10549M\n",
            "\u001b[32m[04/15 05:44:57 d2.utils.events]: \u001b[0m eta: 1 day, 11:55:34  iter: 4619  total_loss: 0.6532  loss_cls: 0.3748  loss_box_reg: 0.2203  loss_query: 0.03784  time: 2.3130  data_time: 0.0078  lr: 0.006  max_mem: 10549M\n",
            "\u001b[32m[04/15 05:45:44 d2.utils.events]: \u001b[0m eta: 1 day, 11:55:31  iter: 4639  total_loss: 0.6126  loss_cls: 0.3866  loss_box_reg: 0.2403  loss_query: 0.04064  time: 2.3129  data_time: 0.0075  lr: 0.006  max_mem: 10549M\n",
            "\u001b[32m[04/15 05:46:31 d2.utils.events]: \u001b[0m eta: 1 day, 11:55:18  iter: 4659  total_loss: 0.6396  loss_cls: 0.3857  loss_box_reg: 0.205  loss_query: 0.02739  time: 2.3128  data_time: 0.0087  lr: 0.006  max_mem: 10549M\n",
            "\u001b[32m[04/15 05:47:18 d2.utils.events]: \u001b[0m eta: 1 day, 11:55:15  iter: 4679  total_loss: 0.5199  loss_cls: 0.341  loss_box_reg: 0.1848  loss_query: 0.02533  time: 2.3126  data_time: 0.0072  lr: 0.006  max_mem: 10549M\n",
            "\u001b[32m[04/15 05:48:04 d2.utils.events]: \u001b[0m eta: 1 day, 11:52:52  iter: 4699  total_loss: 0.5149  loss_cls: 0.3534  loss_box_reg: 0.175  loss_query: 0.02879  time: 2.3125  data_time: 0.0060  lr: 0.006  max_mem: 10549M\n",
            "\u001b[32m[04/15 05:48:51 d2.utils.events]: \u001b[0m eta: 1 day, 11:52:48  iter: 4719  total_loss: 0.703  loss_cls: 0.4278  loss_box_reg: 0.2381  loss_query: 0.02755  time: 2.3122  data_time: 0.0078  lr: 0.006  max_mem: 10549M\n",
            "\u001b[32m[04/15 05:49:38 d2.utils.events]: \u001b[0m eta: 1 day, 11:49:46  iter: 4739  total_loss: 0.5862  loss_cls: 0.3649  loss_box_reg: 0.208  loss_query: 0.0305  time: 2.3121  data_time: 0.0077  lr: 0.006  max_mem: 10549M\n",
            "\u001b[32m[04/15 05:50:25 d2.utils.events]: \u001b[0m eta: 1 day, 11:50:50  iter: 4759  total_loss: 0.6499  loss_cls: 0.3836  loss_box_reg: 0.2412  loss_query: 0.03396  time: 2.3120  data_time: 0.0082  lr: 0.006  max_mem: 10549M\n",
            "\u001b[32m[04/15 05:51:12 d2.utils.events]: \u001b[0m eta: 1 day, 11:49:20  iter: 4779  total_loss: 0.4993  loss_cls: 0.2991  loss_box_reg: 0.1841  loss_query: 0.02227  time: 2.3119  data_time: 0.0089  lr: 0.006  max_mem: 10549M\n",
            "\u001b[32m[04/15 05:52:00 d2.utils.events]: \u001b[0m eta: 1 day, 11:49:41  iter: 4799  total_loss: 0.6778  loss_cls: 0.3547  loss_box_reg: 0.289  loss_query: 0.0292  time: 2.3119  data_time: 0.0067  lr: 0.006  max_mem: 10549M\n",
            "\u001b[32m[04/15 05:52:49 d2.utils.events]: \u001b[0m eta: 1 day, 11:50:59  iter: 4819  total_loss: 0.8659  loss_cls: 0.4999  loss_box_reg: 0.3094  loss_query: 0.04103  time: 2.3122  data_time: 0.0093  lr: 0.006  max_mem: 10549M\n",
            "\u001b[32m[04/15 05:53:37 d2.utils.events]: \u001b[0m eta: 1 day, 11:49:40  iter: 4839  total_loss: 0.4971  loss_cls: 0.3118  loss_box_reg: 0.1732  loss_query: 0.01992  time: 2.3123  data_time: 0.0079  lr: 0.006  max_mem: 10549M\n",
            "\u001b[32m[04/15 05:54:23 d2.utils.events]: \u001b[0m eta: 1 day, 11:48:14  iter: 4859  total_loss: 0.8651  loss_cls: 0.5123  loss_box_reg: 0.2817  loss_query: 0.03956  time: 2.3121  data_time: 0.0068  lr: 0.006  max_mem: 10549M\n",
            "\u001b[32m[04/15 05:55:10 d2.utils.events]: \u001b[0m eta: 1 day, 11:50:20  iter: 4879  total_loss: 0.6233  loss_cls: 0.3856  loss_box_reg: 0.2062  loss_query: 0.03553  time: 2.3121  data_time: 0.0102  lr: 0.006  max_mem: 10549M\n",
            "\u001b[32m[04/15 05:55:59 d2.utils.events]: \u001b[0m eta: 1 day, 11:49:00  iter: 4899  total_loss: 0.8074  loss_cls: 0.4903  loss_box_reg: 0.2657  loss_query: 0.03652  time: 2.3123  data_time: 0.0094  lr: 0.006  max_mem: 10549M\n",
            "\u001b[32m[04/15 05:56:45 d2.utils.events]: \u001b[0m eta: 1 day, 11:46:59  iter: 4919  total_loss: 0.4914  loss_cls: 0.3058  loss_box_reg: 0.166  loss_query: 0.02315  time: 2.3121  data_time: 0.0073  lr: 0.006  max_mem: 10549M\n",
            "\u001b[32m[04/15 05:57:32 d2.utils.events]: \u001b[0m eta: 1 day, 11:46:41  iter: 4939  total_loss: 0.6116  loss_cls: 0.387  loss_box_reg: 0.2119  loss_query: 0.03956  time: 2.3119  data_time: 0.0078  lr: 0.006  max_mem: 10549M\n",
            "\u001b[32m[04/15 05:58:19 d2.utils.events]: \u001b[0m eta: 1 day, 11:44:51  iter: 4959  total_loss: 0.6393  loss_cls: 0.3665  loss_box_reg: 0.1878  loss_query: 0.02265  time: 2.3118  data_time: 0.0079  lr: 0.006  max_mem: 10549M\n",
            "\u001b[32m[04/15 05:59:07 d2.utils.events]: \u001b[0m eta: 1 day, 11:44:04  iter: 4979  total_loss: 0.5675  loss_cls: 0.3338  loss_box_reg: 0.1989  loss_query: 0.02748  time: 2.3118  data_time: 0.0073  lr: 0.006  max_mem: 10549M\n",
            "\u001b[32m[04/15 05:59:56 d2.utils.events]: \u001b[0m eta: 1 day, 11:44:20  iter: 4999  total_loss: 0.7154  loss_cls: 0.4358  loss_box_reg: 0.2231  loss_query: 0.0642  time: 2.3121  data_time: 0.0083  lr: 0.006  max_mem: 10549M\n",
            "\u001b[32m[04/15 06:00:43 d2.utils.events]: \u001b[0m eta: 1 day, 11:41:30  iter: 5019  total_loss: 0.8473  loss_cls: 0.5241  loss_box_reg: 0.2617  loss_query: 0.02928  time: 2.3120  data_time: 0.0079  lr: 0.006  max_mem: 10549M\n",
            "\u001b[32m[04/15 06:01:30 d2.utils.events]: \u001b[0m eta: 1 day, 11:41:13  iter: 5039  total_loss: 0.645  loss_cls: 0.364  loss_box_reg: 0.2304  loss_query: 0.04046  time: 2.3120  data_time: 0.0073  lr: 0.006  max_mem: 10549M\n",
            "\u001b[32m[04/15 06:02:16 d2.utils.events]: \u001b[0m eta: 1 day, 11:39:36  iter: 5059  total_loss: 0.5821  loss_cls: 0.3834  loss_box_reg: 0.1696  loss_query: 0.02422  time: 2.3118  data_time: 0.0069  lr: 0.006  max_mem: 10549M\n",
            "\u001b[32m[04/15 06:03:03 d2.utils.events]: \u001b[0m eta: 1 day, 11:38:39  iter: 5079  total_loss: 0.7873  loss_cls: 0.4469  loss_box_reg: 0.26  loss_query: 0.02638  time: 2.3117  data_time: 0.0078  lr: 0.006  max_mem: 10549M\n",
            "\u001b[32m[04/15 06:03:53 d2.utils.events]: \u001b[0m eta: 1 day, 11:37:17  iter: 5099  total_loss: 0.7113  loss_cls: 0.392  loss_box_reg: 0.2505  loss_query: 0.03587  time: 2.3120  data_time: 0.0093  lr: 0.006  max_mem: 10549M\n",
            "\u001b[32m[04/15 06:04:40 d2.utils.events]: \u001b[0m eta: 1 day, 11:36:49  iter: 5119  total_loss: 0.7621  loss_cls: 0.4427  loss_box_reg: 0.2822  loss_query: 0.03801  time: 2.3119  data_time: 0.0089  lr: 0.006  max_mem: 10549M\n",
            "\u001b[32m[04/15 06:05:27 d2.utils.events]: \u001b[0m eta: 1 day, 11:35:44  iter: 5139  total_loss: 0.6105  loss_cls: 0.3625  loss_box_reg: 0.2235  loss_query: 0.02421  time: 2.3118  data_time: 0.0062  lr: 0.006  max_mem: 10549M\n",
            "\u001b[32m[04/15 06:06:13 d2.utils.events]: \u001b[0m eta: 1 day, 11:33:46  iter: 5159  total_loss: 0.6506  loss_cls: 0.3661  loss_box_reg: 0.2326  loss_query: 0.02762  time: 2.3115  data_time: 0.0087  lr: 0.006  max_mem: 10549M\n",
            "\u001b[32m[04/15 06:07:00 d2.utils.events]: \u001b[0m eta: 1 day, 11:34:10  iter: 5179  total_loss: 0.7189  loss_cls: 0.4392  loss_box_reg: 0.2187  loss_query: 0.03584  time: 2.3114  data_time: 0.0082  lr: 0.006  max_mem: 10549M\n",
            "\u001b[32m[04/15 06:07:48 d2.utils.events]: \u001b[0m eta: 1 day, 11:35:11  iter: 5199  total_loss: 0.7037  loss_cls: 0.3945  loss_box_reg: 0.2415  loss_query: 0.04595  time: 2.3115  data_time: 0.0083  lr: 0.006  max_mem: 10549M\n",
            "\u001b[32m[04/15 06:08:35 d2.utils.events]: \u001b[0m eta: 1 day, 11:33:32  iter: 5219  total_loss: 0.5096  loss_cls: 0.2981  loss_box_reg: 0.1695  loss_query: 0.03025  time: 2.3115  data_time: 0.0070  lr: 0.006  max_mem: 10549M\n",
            "\u001b[32m[04/15 06:09:24 d2.utils.events]: \u001b[0m eta: 1 day, 11:30:50  iter: 5239  total_loss: 0.4958  loss_cls: 0.3019  loss_box_reg: 0.1763  loss_query: 0.01871  time: 2.3116  data_time: 0.0074  lr: 0.006  max_mem: 10549M\n",
            "\u001b[32m[04/15 06:10:12 d2.utils.events]: \u001b[0m eta: 1 day, 11:31:22  iter: 5259  total_loss: 0.8129  loss_cls: 0.4842  loss_box_reg: 0.2639  loss_query: 0.03057  time: 2.3118  data_time: 0.0087  lr: 0.006  max_mem: 10549M\n",
            "\u001b[32m[04/15 06:11:01 d2.utils.events]: \u001b[0m eta: 1 day, 11:30:17  iter: 5279  total_loss: 0.691  loss_cls: 0.4062  loss_box_reg: 0.2371  loss_query: 0.03469  time: 2.3118  data_time: 0.0078  lr: 0.006  max_mem: 10549M\n",
            "\u001b[32m[04/15 06:11:49 d2.utils.events]: \u001b[0m eta: 1 day, 11:30:47  iter: 5299  total_loss: 0.69  loss_cls: 0.4264  loss_box_reg: 0.24  loss_query: 0.02656  time: 2.3119  data_time: 0.0079  lr: 0.006  max_mem: 10549M\n",
            "\u001b[32m[04/15 06:12:37 d2.utils.events]: \u001b[0m eta: 1 day, 11:30:42  iter: 5319  total_loss: 0.4631  loss_cls: 0.2744  loss_box_reg: 0.159  loss_query: 0.02891  time: 2.3119  data_time: 0.0066  lr: 0.006  max_mem: 10549M\n",
            "\u001b[32m[04/15 06:13:26 d2.utils.events]: \u001b[0m eta: 1 day, 11:30:37  iter: 5339  total_loss: 0.6051  loss_cls: 0.3661  loss_box_reg: 0.2075  loss_query: 0.02988  time: 2.3121  data_time: 0.0098  lr: 0.006  max_mem: 10549M\n",
            "\u001b[32m[04/15 06:14:14 d2.utils.events]: \u001b[0m eta: 1 day, 11:29:50  iter: 5359  total_loss: 0.5943  loss_cls: 0.3679  loss_box_reg: 0.2383  loss_query: 0.02036  time: 2.3121  data_time: 0.0061  lr: 0.006  max_mem: 10549M\n",
            "\u001b[32m[04/15 06:15:03 d2.utils.events]: \u001b[0m eta: 1 day, 11:30:16  iter: 5379  total_loss: 0.5384  loss_cls: 0.3227  loss_box_reg: 0.2059  loss_query: 0.02989  time: 2.3124  data_time: 0.0075  lr: 0.006  max_mem: 10549M\n",
            "\u001b[32m[04/15 06:15:51 d2.utils.events]: \u001b[0m eta: 1 day, 11:29:59  iter: 5399  total_loss: 0.6521  loss_cls: 0.368  loss_box_reg: 0.2405  loss_query: 0.03781  time: 2.3123  data_time: 0.0087  lr: 0.006  max_mem: 10549M\n",
            "\u001b[32m[04/15 06:16:37 d2.utils.events]: \u001b[0m eta: 1 day, 11:28:13  iter: 5419  total_loss: 0.4003  loss_cls: 0.2558  loss_box_reg: 0.1232  loss_query: 0.0238  time: 2.3119  data_time: 0.0086  lr: 0.006  max_mem: 10549M\n",
            "\u001b[32m[04/15 06:17:24 d2.utils.events]: \u001b[0m eta: 1 day, 11:27:03  iter: 5439  total_loss: 0.8344  loss_cls: 0.4765  loss_box_reg: 0.2593  loss_query: 0.02346  time: 2.3118  data_time: 0.0070  lr: 0.006  max_mem: 10549M\n",
            "\u001b[32m[04/15 06:18:12 d2.utils.events]: \u001b[0m eta: 1 day, 11:23:33  iter: 5459  total_loss: 0.6007  loss_cls: 0.3511  loss_box_reg: 0.2294  loss_query: 0.03405  time: 2.3117  data_time: 0.0085  lr: 0.006  max_mem: 10549M\n",
            "\u001b[32m[04/15 06:19:00 d2.utils.events]: \u001b[0m eta: 1 day, 11:22:30  iter: 5479  total_loss: 0.8921  loss_cls: 0.5171  loss_box_reg: 0.2763  loss_query: 0.05021  time: 2.3117  data_time: 0.0070  lr: 0.006  max_mem: 10549M\n",
            "\u001b[32m[04/15 06:19:49 d2.utils.events]: \u001b[0m eta: 1 day, 11:21:30  iter: 5499  total_loss: 0.628  loss_cls: 0.3663  loss_box_reg: 0.2237  loss_query: 0.03647  time: 2.3119  data_time: 0.0073  lr: 0.006  max_mem: 10549M\n",
            "\u001b[32m[04/15 06:20:37 d2.utils.events]: \u001b[0m eta: 1 day, 11:20:18  iter: 5519  total_loss: 0.6016  loss_cls: 0.3396  loss_box_reg: 0.1986  loss_query: 0.02641  time: 2.3119  data_time: 0.0074  lr: 0.006  max_mem: 10549M\n",
            "\u001b[32m[04/15 06:21:25 d2.utils.events]: \u001b[0m eta: 1 day, 11:19:57  iter: 5539  total_loss: 0.7582  loss_cls: 0.4744  loss_box_reg: 0.2506  loss_query: 0.03096  time: 2.3120  data_time: 0.0073  lr: 0.006  max_mem: 10549M\n",
            "\u001b[32m[04/15 06:22:15 d2.utils.events]: \u001b[0m eta: 1 day, 11:19:56  iter: 5559  total_loss: 0.5558  loss_cls: 0.2928  loss_box_reg: 0.2173  loss_query: 0.04552  time: 2.3123  data_time: 0.0075  lr: 0.006  max_mem: 10549M\n",
            "\u001b[32m[04/15 06:23:03 d2.utils.events]: \u001b[0m eta: 1 day, 11:18:53  iter: 5579  total_loss: 0.6455  loss_cls: 0.3877  loss_box_reg: 0.2352  loss_query: 0.03482  time: 2.3122  data_time: 0.0070  lr: 0.006  max_mem: 10549M\n",
            "\u001b[32m[04/15 06:23:51 d2.utils.events]: \u001b[0m eta: 1 day, 11:19:06  iter: 5599  total_loss: 0.7281  loss_cls: 0.4447  loss_box_reg: 0.2344  loss_query: 0.03073  time: 2.3122  data_time: 0.0083  lr: 0.006  max_mem: 10549M\n",
            "\u001b[32m[04/15 06:24:39 d2.utils.events]: \u001b[0m eta: 1 day, 11:16:59  iter: 5619  total_loss: 0.5336  loss_cls: 0.3151  loss_box_reg: 0.2063  loss_query: 0.02799  time: 2.3121  data_time: 0.0085  lr: 0.006  max_mem: 10549M\n",
            "\u001b[32m[04/15 06:25:26 d2.utils.events]: \u001b[0m eta: 1 day, 11:15:53  iter: 5639  total_loss: 0.5957  loss_cls: 0.3757  loss_box_reg: 0.2024  loss_query: 0.01934  time: 2.3121  data_time: 0.0063  lr: 0.006  max_mem: 10549M\n",
            "\u001b[32m[04/15 06:26:14 d2.utils.events]: \u001b[0m eta: 1 day, 11:15:16  iter: 5659  total_loss: 0.8064  loss_cls: 0.476  loss_box_reg: 0.2882  loss_query: 0.05328  time: 2.3122  data_time: 0.0080  lr: 0.006  max_mem: 10549M\n",
            "\u001b[32m[04/15 06:27:03 d2.utils.events]: \u001b[0m eta: 1 day, 11:14:20  iter: 5679  total_loss: 0.6193  loss_cls: 0.3204  loss_box_reg: 0.245  loss_query: 0.03439  time: 2.3123  data_time: 0.0087  lr: 0.006  max_mem: 10549M\n",
            "\u001b[32m[04/15 06:27:52 d2.utils.events]: \u001b[0m eta: 1 day, 11:14:10  iter: 5699  total_loss: 0.7052  loss_cls: 0.4214  loss_box_reg: 0.2575  loss_query: 0.04193  time: 2.3123  data_time: 0.0065  lr: 0.006  max_mem: 10549M\n",
            "\u001b[32m[04/15 06:28:39 d2.utils.events]: \u001b[0m eta: 1 day, 11:13:42  iter: 5719  total_loss: 0.617  loss_cls: 0.3753  loss_box_reg: 0.2313  loss_query: 0.03646  time: 2.3122  data_time: 0.0078  lr: 0.006  max_mem: 10549M\n",
            "\u001b[32m[04/15 06:29:27 d2.utils.events]: \u001b[0m eta: 1 day, 11:13:36  iter: 5739  total_loss: 0.5144  loss_cls: 0.2976  loss_box_reg: 0.2158  loss_query: 0.02976  time: 2.3121  data_time: 0.0081  lr: 0.006  max_mem: 10549M\n",
            "\u001b[32m[04/15 06:30:15 d2.utils.events]: \u001b[0m eta: 1 day, 11:12:08  iter: 5759  total_loss: 0.604  loss_cls: 0.3237  loss_box_reg: 0.2  loss_query: 0.03071  time: 2.3121  data_time: 0.0069  lr: 0.006  max_mem: 10549M\n",
            "\u001b[32m[04/15 06:31:03 d2.utils.events]: \u001b[0m eta: 1 day, 11:11:22  iter: 5779  total_loss: 0.6217  loss_cls: 0.3708  loss_box_reg: 0.2113  loss_query: 0.03017  time: 2.3122  data_time: 0.0077  lr: 0.006  max_mem: 10549M\n",
            "\u001b[32m[04/15 06:31:52 d2.utils.events]: \u001b[0m eta: 1 day, 11:10:38  iter: 5799  total_loss: 0.5511  loss_cls: 0.3025  loss_box_reg: 0.212  loss_query: 0.03254  time: 2.3121  data_time: 0.0073  lr: 0.006  max_mem: 10549M\n",
            "\u001b[32m[04/15 06:32:39 d2.utils.events]: \u001b[0m eta: 1 day, 11:09:12  iter: 5819  total_loss: 0.5878  loss_cls: 0.3335  loss_box_reg: 0.2147  loss_query: 0.02469  time: 2.3120  data_time: 0.0076  lr: 0.006  max_mem: 10549M\n",
            "\u001b[32m[04/15 06:33:28 d2.utils.events]: \u001b[0m eta: 1 day, 11:09:05  iter: 5839  total_loss: 0.612  loss_cls: 0.4074  loss_box_reg: 0.1704  loss_query: 0.02298  time: 2.3122  data_time: 0.0082  lr: 0.006  max_mem: 10549M\n",
            "\u001b[32m[04/15 06:34:16 d2.utils.events]: \u001b[0m eta: 1 day, 11:08:12  iter: 5859  total_loss: 0.8409  loss_cls: 0.5188  loss_box_reg: 0.3035  loss_query: 0.02376  time: 2.3122  data_time: 0.0086  lr: 0.006  max_mem: 10549M\n",
            "\u001b[32m[04/15 06:35:04 d2.utils.events]: \u001b[0m eta: 1 day, 11:05:55  iter: 5879  total_loss: 0.4861  loss_cls: 0.3225  loss_box_reg: 0.1401  loss_query: 0.03617  time: 2.3122  data_time: 0.0072  lr: 0.006  max_mem: 10549M\n",
            "\u001b[32m[04/15 06:35:51 d2.utils.events]: \u001b[0m eta: 1 day, 11:03:52  iter: 5899  total_loss: 0.6459  loss_cls: 0.3991  loss_box_reg: 0.2075  loss_query: 0.02912  time: 2.3121  data_time: 0.0079  lr: 0.006  max_mem: 10549M\n",
            "\u001b[32m[04/15 06:36:37 d2.utils.events]: \u001b[0m eta: 1 day, 11:02:42  iter: 5919  total_loss: 0.5588  loss_cls: 0.3415  loss_box_reg: 0.1977  loss_query: 0.03228  time: 2.3117  data_time: 0.0062  lr: 0.006  max_mem: 10549M\n",
            "\u001b[32m[04/15 06:37:26 d2.utils.events]: \u001b[0m eta: 1 day, 11:01:41  iter: 5939  total_loss: 0.5961  loss_cls: 0.3769  loss_box_reg: 0.1835  loss_query: 0.02085  time: 2.3118  data_time: 0.0080  lr: 0.006  max_mem: 10549M\n",
            "\u001b[32m[04/15 06:38:14 d2.utils.events]: \u001b[0m eta: 1 day, 11:01:08  iter: 5959  total_loss: 0.6417  loss_cls: 0.3801  loss_box_reg: 0.2493  loss_query: 0.02399  time: 2.3117  data_time: 0.0068  lr: 0.006  max_mem: 10549M\n",
            "\u001b[32m[04/15 06:39:03 d2.utils.events]: \u001b[0m eta: 1 day, 11:00:53  iter: 5979  total_loss: 0.7597  loss_cls: 0.4486  loss_box_reg: 0.2324  loss_query: 0.02981  time: 2.3118  data_time: 0.0072  lr: 0.006  max_mem: 10549M\n",
            "\u001b[32m[04/15 06:39:51 fvcore.common.checkpoint]: \u001b[0mSaving checkpoint to work_dirs/visdrone_querydet/model_0005999.pth\n",
            "\u001b[32m[04/15 06:40:14 d2.utils.events]: \u001b[0m eta: 1 day, 10:59:10  iter: 5999  total_loss: 0.5423  loss_cls: 0.3312  loss_box_reg: 0.1939  loss_query: 0.02382  time: 2.3119  data_time: 0.0075  lr: 0.006  max_mem: 10549M\n",
            "\u001b[32m[04/15 06:41:01 d2.utils.events]: \u001b[0m eta: 1 day, 10:58:58  iter: 6019  total_loss: 0.5716  loss_cls: 0.3388  loss_box_reg: 0.1929  loss_query: 0.02727  time: 2.3118  data_time: 0.0084  lr: 0.006  max_mem: 10549M\n",
            "\u001b[32m[04/15 06:41:49 d2.utils.events]: \u001b[0m eta: 1 day, 10:58:33  iter: 6039  total_loss: 0.6321  loss_cls: 0.352  loss_box_reg: 0.2183  loss_query: 0.03793  time: 2.3118  data_time: 0.0071  lr: 0.006  max_mem: 10549M\n",
            "\u001b[32m[04/15 06:42:38 d2.utils.events]: \u001b[0m eta: 1 day, 10:59:52  iter: 6059  total_loss: 0.6362  loss_cls: 0.3716  loss_box_reg: 0.2161  loss_query: 0.04282  time: 2.3121  data_time: 0.0077  lr: 0.006  max_mem: 10549M\n",
            "\u001b[32m[04/15 06:43:27 d2.utils.events]: \u001b[0m eta: 1 day, 10:59:51  iter: 6079  total_loss: 0.5316  loss_cls: 0.2917  loss_box_reg: 0.1846  loss_query: 0.03074  time: 2.3122  data_time: 0.0082  lr: 0.006  max_mem: 10549M\n",
            "\u001b[32m[04/15 06:44:15 d2.utils.events]: \u001b[0m eta: 1 day, 10:58:55  iter: 6099  total_loss: 0.7493  loss_cls: 0.4459  loss_box_reg: 0.2557  loss_query: 0.0382  time: 2.3121  data_time: 0.0068  lr: 0.006  max_mem: 10549M\n",
            "\u001b[32m[04/15 06:45:03 d2.utils.events]: \u001b[0m eta: 1 day, 10:58:17  iter: 6119  total_loss: 0.6585  loss_cls: 0.4048  loss_box_reg: 0.2099  loss_query: 0.03233  time: 2.3123  data_time: 0.0077  lr: 0.006  max_mem: 10549M\n",
            "\u001b[32m[04/15 06:45:50 d2.utils.events]: \u001b[0m eta: 1 day, 10:56:59  iter: 6139  total_loss: 0.5678  loss_cls: 0.3695  loss_box_reg: 0.1742  loss_query: 0.02648  time: 2.3121  data_time: 0.0080  lr: 0.006  max_mem: 10549M\n",
            "\u001b[32m[04/15 06:46:38 d2.utils.events]: \u001b[0m eta: 1 day, 10:56:27  iter: 6159  total_loss: 0.8077  loss_cls: 0.4975  loss_box_reg: 0.2289  loss_query: 0.03703  time: 2.3120  data_time: 0.0079  lr: 0.006  max_mem: 10549M\n",
            "\u001b[32m[04/15 06:47:26 d2.utils.events]: \u001b[0m eta: 1 day, 10:55:44  iter: 6179  total_loss: 0.5838  loss_cls: 0.3273  loss_box_reg: 0.2025  loss_query: 0.0237  time: 2.3121  data_time: 0.0073  lr: 0.006  max_mem: 10549M\n",
            "\u001b[4m\u001b[5m\u001b[31mERROR\u001b[0m \u001b[32m[04/15 06:47:48 d2.engine.train_loop]: \u001b[0mException during training:\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.7/site-packages/detectron2/engine/train_loop.py\", line 138, in train\n",
            "    self.run_step()\n",
            "  File \"/usr/local/lib/python3.7/site-packages/detectron2/engine/defaults.py\", line 441, in run_step\n",
            "    self._trainer.run_step()\n",
            "  File \"/usr/local/lib/python3.7/site-packages/detectron2/engine/train_loop.py\", line 328, in run_step\n",
            "    data = next(self._data_loader_iter)\n",
            "  File \"/usr/local/lib/python3.7/site-packages/torch/utils/data/dataloader.py\", line 517, in __next__\n",
            "    data = self._next_data()\n",
            "  File \"/usr/local/lib/python3.7/site-packages/torch/utils/data/dataloader.py\", line 1199, in _next_data\n",
            "    return self._process_data(data)\n",
            "  File \"/usr/local/lib/python3.7/site-packages/torch/utils/data/dataloader.py\", line 1225, in _process_data\n",
            "    data.reraise()\n",
            "  File \"/usr/local/lib/python3.7/site-packages/torch/_utils.py\", line 429, in reraise\n",
            "    raise self.exc_type(msg)\n",
            "OSError: Caught OSError in DataLoader worker process 1.\n",
            "Original Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.7/site-packages/torch/utils/data/_utils/worker.py\", line 202, in _worker_loop\n",
            "    data = fetcher.fetch(index)\n",
            "  File \"/usr/local/lib/python3.7/site-packages/torch/utils/data/_utils/fetch.py\", line 44, in fetch\n",
            "    data = [self.dataset[idx] for idx in possibly_batched_index]\n",
            "  File \"/usr/local/lib/python3.7/site-packages/torch/utils/data/_utils/fetch.py\", line 44, in <listcomp>\n",
            "    data = [self.dataset[idx] for idx in possibly_batched_index]\n",
            "  File \"/usr/local/lib/python3.7/site-packages/detectron2/data/common.py\", line 43, in __getitem__\n",
            "    data = self._map_func(self._dataset[cur_idx])\n",
            "  File \"/usr/local/lib/python3.7/site-packages/detectron2/utils/serialize.py\", line 23, in __call__\n",
            "    return self._obj(*args, **kwargs)\n",
            "  File \"/content/gdrive/MyDrive/.shared/QueryDet-PyTorch/visdrone/mapper.py\", line 53, in __call__\n",
            "    image = utils.read_image(dataset_dict[\"file_name\"], format=self.img_format)\n",
            "  File \"/usr/local/lib/python3.7/site-packages/detectron2/data/detection_utils.py\", line 183, in read_image\n",
            "    return convert_PIL_to_numpy(image, format)\n",
            "  File \"/usr/local/lib/python3.7/site-packages/detectron2/data/detection_utils.py\", line 75, in convert_PIL_to_numpy\n",
            "    image = image.convert(conversion_format)\n",
            "  File \"/usr/local/lib/python3.7/site-packages/PIL/Image.py\", line 933, in convert\n",
            "    self.load()\n",
            "  File \"/usr/local/lib/python3.7/site-packages/PIL/ImageFile.py\", line 266, in load\n",
            "    raise OSError(msg)\n",
            "OSError: image file is truncated (5 bytes not processed)\n",
            "\n",
            "\u001b[32m[04/15 06:47:48 d2.engine.hooks]: \u001b[0mOverall training speed: 6187 iterations in 3:58:24 (2.3120 s / it)\n",
            "\u001b[32m[04/15 06:47:48 d2.engine.hooks]: \u001b[0mTotal training time: 4:05:45 (0:07:21 on hooks)\n",
            "\u001b[32m[04/15 06:47:48 d2.utils.events]: \u001b[0m eta: 1 day, 10:54:28  iter: 6189  total_loss: 0.4413  loss_cls: 0.2708  loss_box_reg: 0.1471  loss_query: 0.02169  time: 2.3120  data_time: 0.0083  lr: 0.006  max_mem: 10549M\n",
            "Traceback (most recent call last):\n",
            "  File \"train_visdrone.py\", line 18, in <module>\n",
            "    args=(args,),\n",
            "  File \"/usr/local/lib/python3.7/site-packages/detectron2/engine/launch.py\", line 62, in launch\n",
            "    main_func(*args)\n",
            "  File \"/content/gdrive/MyDrive/.shared/QueryDet-PyTorch/train_tools/visdrone_train.py\", line 250, in start_train\n",
            "    return trainer.train()\n",
            "  File \"/usr/local/lib/python3.7/site-packages/detectron2/engine/defaults.py\", line 431, in train\n",
            "    super().train(self.start_iter, self.max_iter)\n",
            "  File \"/usr/local/lib/python3.7/site-packages/detectron2/engine/train_loop.py\", line 138, in train\n",
            "    self.run_step()\n",
            "  File \"/usr/local/lib/python3.7/site-packages/detectron2/engine/defaults.py\", line 441, in run_step\n",
            "    self._trainer.run_step()\n",
            "  File \"/usr/local/lib/python3.7/site-packages/detectron2/engine/train_loop.py\", line 328, in run_step\n",
            "    data = next(self._data_loader_iter)\n",
            "  File \"/usr/local/lib/python3.7/site-packages/torch/utils/data/dataloader.py\", line 517, in __next__\n",
            "    data = self._next_data()\n",
            "  File \"/usr/local/lib/python3.7/site-packages/torch/utils/data/dataloader.py\", line 1199, in _next_data\n",
            "    return self._process_data(data)\n",
            "  File \"/usr/local/lib/python3.7/site-packages/torch/utils/data/dataloader.py\", line 1225, in _process_data\n",
            "    data.reraise()\n",
            "  File \"/usr/local/lib/python3.7/site-packages/torch/_utils.py\", line 429, in reraise\n",
            "    raise self.exc_type(msg)\n",
            "OSError: Caught OSError in DataLoader worker process 1.\n",
            "Original Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.7/site-packages/torch/utils/data/_utils/worker.py\", line 202, in _worker_loop\n",
            "    data = fetcher.fetch(index)\n",
            "  File \"/usr/local/lib/python3.7/site-packages/torch/utils/data/_utils/fetch.py\", line 44, in fetch\n",
            "    data = [self.dataset[idx] for idx in possibly_batched_index]\n",
            "  File \"/usr/local/lib/python3.7/site-packages/torch/utils/data/_utils/fetch.py\", line 44, in <listcomp>\n",
            "    data = [self.dataset[idx] for idx in possibly_batched_index]\n",
            "  File \"/usr/local/lib/python3.7/site-packages/detectron2/data/common.py\", line 43, in __getitem__\n",
            "    data = self._map_func(self._dataset[cur_idx])\n",
            "  File \"/usr/local/lib/python3.7/site-packages/detectron2/utils/serialize.py\", line 23, in __call__\n",
            "    return self._obj(*args, **kwargs)\n",
            "  File \"/content/gdrive/MyDrive/.shared/QueryDet-PyTorch/visdrone/mapper.py\", line 53, in __call__\n",
            "    image = utils.read_image(dataset_dict[\"file_name\"], format=self.img_format)\n",
            "  File \"/usr/local/lib/python3.7/site-packages/detectron2/data/detection_utils.py\", line 183, in read_image\n",
            "    return convert_PIL_to_numpy(image, format)\n",
            "  File \"/usr/local/lib/python3.7/site-packages/detectron2/data/detection_utils.py\", line 75, in convert_PIL_to_numpy\n",
            "    image = image.convert(conversion_format)\n",
            "  File \"/usr/local/lib/python3.7/site-packages/PIL/Image.py\", line 933, in convert\n",
            "    self.load()\n",
            "  File \"/usr/local/lib/python3.7/site-packages/PIL/ImageFile.py\", line 266, in load\n",
            "    raise OSError(msg)\n",
            "OSError: image file is truncated (5 bytes not processed)\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "qfBo4pwL-Moo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!python train_visdrone.py --resume --config /content/gdrive/MyDrive/QueryDet-PyTorch/work_dirs/visdrone_querydet/config.yaml --num-gpu 1 OUTPUT_DIR work_dirs/visdrone_querydet"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3VWY3a2b5wG2",
        "outputId": "92b70de4-2684-4ce1-dff6-76c476af60da",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "** fvcore version of PathManager will be deprecated soon. **\n",
            "** Please migrate to the version in iopath repo. **\n",
            "https://github.com/facebookresearch/iopath \n",
            "\n",
            "Command Line Args: Namespace(config_file='/content/gdrive/MyDrive/QueryDet-PyTorch/work_dirs/visdrone_querydet/config.yaml', dist_url='tcp://127.0.0.1:49152', eval_only=False, machine_rank=0, no_pretrain=False, num_gpus=1, num_machines=1, opts=['OUTPUT_DIR', 'work_dirs/visdrone_querydet'], resume=True)\n",
            "\u001b[32m[04/13 13:57:54 detectron2]: \u001b[0mRank of current process: 0. World size: 1\n",
            "\u001b[32m[04/13 13:58:00 detectron2]: \u001b[0mEnvironment info:\n",
            "----------------------  ---------------------------------------------------------------\n",
            "sys.platform            linux\n",
            "Python                  3.7.6 (default, Jan  8 2020, 19:59:22) [GCC 7.3.0]\n",
            "numpy                   1.21.6\n",
            "detectron2              0.4 @/usr/local/lib/python3.7/site-packages/detectron2\n",
            "Compiler                GCC 7.3\n",
            "CUDA compiler           CUDA 10.1\n",
            "detectron2 arch flags   3.7, 5.0, 5.2, 6.0, 6.1, 7.0, 7.5\n",
            "DETECTRON2_ENV_MODULE   <not set>\n",
            "PyTorch                 1.8.1+cu101 @/usr/local/lib/python3.7/site-packages/torch\n",
            "PyTorch debug build     False\n",
            "GPU available           True\n",
            "GPU 0                   Tesla T4 (arch=7.5)\n",
            "CUDA_HOME               /usr/local/cuda\n",
            "Pillow                  9.5.0\n",
            "torchvision             0.9.1+cu101 @/usr/local/lib/python3.7/site-packages/torchvision\n",
            "torchvision arch flags  3.5, 5.0, 6.0, 7.0, 7.5\n",
            "fvcore                  0.1.3.post20210317\n",
            "cv2                     4.7.0\n",
            "----------------------  ---------------------------------------------------------------\n",
            "PyTorch built with:\n",
            "  - GCC 7.3\n",
            "  - C++ Version: 201402\n",
            "  - Intel(R) Math Kernel Library Version 2020.0.0 Product Build 20191122 for Intel(R) 64 architecture applications\n",
            "  - Intel(R) MKL-DNN v1.7.0 (Git Hash 7aed236906b1f7a05c0917e5257a1af05e9ff683)\n",
            "  - OpenMP 201511 (a.k.a. OpenMP 4.5)\n",
            "  - NNPACK is enabled\n",
            "  - CPU capability usage: AVX2\n",
            "  - CUDA Runtime 10.1\n",
            "  - NVCC architecture flags: -gencode;arch=compute_37,code=sm_37;-gencode;arch=compute_50,code=sm_50;-gencode;arch=compute_60,code=sm_60;-gencode;arch=compute_70,code=sm_70\n",
            "  - CuDNN 7.6.3\n",
            "  - Magma 2.5.2\n",
            "  - Build settings: BLAS_INFO=mkl, BUILD_TYPE=Release, CUDA_VERSION=10.1, CUDNN_VERSION=7.6.3, CXX_COMPILER=/opt/rh/devtoolset-7/root/usr/bin/c++, CXX_FLAGS= -Wno-deprecated -fvisibility-inlines-hidden -DUSE_PTHREADPOOL -fopenmp -DNDEBUG -DUSE_KINETO -DUSE_FBGEMM -DUSE_QNNPACK -DUSE_PYTORCH_QNNPACK -DUSE_XNNPACK -O2 -fPIC -Wno-narrowing -Wall -Wextra -Werror=return-type -Wno-missing-field-initializers -Wno-type-limits -Wno-array-bounds -Wno-unknown-pragmas -Wno-sign-compare -Wno-unused-parameter -Wno-unused-variable -Wno-unused-function -Wno-unused-result -Wno-unused-local-typedefs -Wno-strict-overflow -Wno-strict-aliasing -Wno-error=deprecated-declarations -Wno-stringop-overflow -Wno-psabi -Wno-error=pedantic -Wno-error=redundant-decls -Wno-error=old-style-cast -fdiagnostics-color=always -faligned-new -Wno-unused-but-set-variable -Wno-maybe-uninitialized -fno-math-errno -fno-trapping-math -Werror=format -Wno-stringop-overflow, LAPACK_INFO=mkl, PERF_WITH_AVX=1, PERF_WITH_AVX2=1, PERF_WITH_AVX512=1, TORCH_VERSION=1.8.1, USE_CUDA=ON, USE_CUDNN=ON, USE_EXCEPTION_PTR=1, USE_GFLAGS=OFF, USE_GLOG=OFF, USE_MKL=ON, USE_MKLDNN=ON, USE_MPI=OFF, USE_NCCL=ON, USE_NNPACK=ON, USE_OPENMP=ON, \n",
            "\n",
            "\u001b[32m[04/13 13:58:00 detectron2]: \u001b[0mCommand line arguments: Namespace(config_file='/content/gdrive/MyDrive/QueryDet-PyTorch/work_dirs/visdrone_querydet/config.yaml', dist_url='tcp://127.0.0.1:49152', eval_only=False, machine_rank=0, no_pretrain=False, num_gpus=1, num_machines=1, opts=['OUTPUT_DIR', 'work_dirs/visdrone_querydet'], resume=True)\n",
            "\u001b[32m[04/13 13:58:00 detectron2]: \u001b[0mContents of args.config_file=/content/gdrive/MyDrive/QueryDet-PyTorch/work_dirs/visdrone_querydet/config.yaml:\n",
            "CUDNN_BENCHMARK: false\n",
            "DATALOADER:\n",
            "  ASPECT_RATIO_GROUPING: true\n",
            "  FILTER_EMPTY_ANNOTATIONS: true\n",
            "  NUM_WORKERS: 4\n",
            "  REPEAT_THRESHOLD: 0.0\n",
            "  SAMPLER_TRAIN: TrainingSampler\n",
            "DATASETS:\n",
            "  PRECOMPUTED_PROPOSAL_TOPK_TEST: 1000\n",
            "  PRECOMPUTED_PROPOSAL_TOPK_TRAIN: 2000\n",
            "  PROPOSAL_FILES_TEST: []\n",
            "  PROPOSAL_FILES_TRAIN: []\n",
            "  TEST:\n",
            "  - coco_2017_val\n",
            "  TRAIN:\n",
            "  - coco_2017_train\n",
            "GLOBAL:\n",
            "  HACK: 1.0\n",
            "INPUT:\n",
            "  CROP:\n",
            "    ENABLED: false\n",
            "    SIZE:\n",
            "    - 0.9\n",
            "    - 0.9\n",
            "    TYPE: relative_range\n",
            "  FORMAT: BGR\n",
            "  MASK_FORMAT: polygon\n",
            "  MAX_SIZE_TEST: 1333\n",
            "  MAX_SIZE_TRAIN: 1333\n",
            "  MIN_SIZE_TEST: 800\n",
            "  MIN_SIZE_TRAIN:\n",
            "  - 640\n",
            "  - 672\n",
            "  - 704\n",
            "  - 736\n",
            "  - 768\n",
            "  - 800\n",
            "  MIN_SIZE_TRAIN_SAMPLING: choice\n",
            "  RANDOM_FLIP: horizontal\n",
            "META_INFO:\n",
            "  EVAL_AP: true\n",
            "  EVAL_GPU_TIME: true\n",
            "  VIS_ROOT: ''\n",
            "MODEL:\n",
            "  ANCHOR_GENERATOR:\n",
            "    ANGLES:\n",
            "    - - -90\n",
            "      - 0\n",
            "      - 90\n",
            "    ASPECT_RATIOS:\n",
            "    - - 0.5\n",
            "      - 1.0\n",
            "      - 2.0\n",
            "    NAME: AnchorGeneratorWithCenter\n",
            "    OFFSET: 0.0\n",
            "    SIZES:\n",
            "    - - 16\n",
            "      - 20.15873679831797\n",
            "      - 25.39841683149119\n",
            "    - - 32\n",
            "      - 40.31747359663594\n",
            "      - 50.79683366298238\n",
            "    - - 64\n",
            "      - 80.63494719327188\n",
            "      - 101.59366732596476\n",
            "    - - 128\n",
            "      - 161.26989438654377\n",
            "      - 203.18733465192952\n",
            "    - - 256\n",
            "      - 322.53978877308754\n",
            "      - 406.37466930385904\n",
            "    - - 512\n",
            "      - 645.0795775461751\n",
            "      - 812.7493386077181\n",
            "  BACKBONE:\n",
            "    FREEZE_AT: 2\n",
            "    NAME: build_retinanet_resnet_fpn_backbone\n",
            "  CUSTOM:\n",
            "    CLEAR_CUDA_CACHE: false\n",
            "    CLS_WEIGHTS:\n",
            "    - 1.0\n",
            "    - 1.4\n",
            "    - 1.8\n",
            "    - 2.2\n",
            "    - 2.6\n",
            "    - 2.6\n",
            "    FOCAL_LOSS_ALPHAS:\n",
            "    - 0.25\n",
            "    - 0.25\n",
            "    - 0.25\n",
            "    - 0.25\n",
            "    - 0.25\n",
            "    - 0.25\n",
            "    FOCAL_LOSS_GAMMAS:\n",
            "    - 2.0\n",
            "    - 2.0\n",
            "    - 2.0\n",
            "    - 2.0\n",
            "    - 2.0\n",
            "    - 2.0\n",
            "    GIOU_LOSS: false\n",
            "    GRADIENT_CHECKPOINT: false\n",
            "    HEAD_BN: false\n",
            "    REG_WEIGHTS:\n",
            "    - 1.0\n",
            "    - 1.4\n",
            "    - 1.8\n",
            "    - 2.2\n",
            "    - 2.6\n",
            "    - 2.6\n",
            "    SOFT_NMS_METHOD: linear\n",
            "    SOFT_NMS_PRUND: 0.001\n",
            "    SOFT_NMS_SIGMA: 0.5\n",
            "    SOFT_NMS_THRESHOLD: 0.5\n",
            "    USE_LOOP_MATCHER: true\n",
            "    USE_SOFT_NMS: false\n",
            "  DEVICE: cuda\n",
            "  FPN:\n",
            "    FUSE_TYPE: sum\n",
            "    IN_FEATURES:\n",
            "    - res2\n",
            "    - res3\n",
            "    - res4\n",
            "    - res5\n",
            "    NORM: ''\n",
            "    OUT_CHANNELS: 256\n",
            "    TOP_LEVELS: 2\n",
            "  KEYPOINT_ON: false\n",
            "  LOAD_PROPOSALS: false\n",
            "  MASK_ON: false\n",
            "  META_ARCHITECTURE: RetinaNetQueryDet\n",
            "  PANOPTIC_FPN:\n",
            "    COMBINE:\n",
            "      ENABLED: true\n",
            "      INSTANCES_CONFIDENCE_THRESH: 0.5\n",
            "      OVERLAP_THRESH: 0.5\n",
            "      STUFF_AREA_LIMIT: 4096\n",
            "    INSTANCE_LOSS_WEIGHT: 1.0\n",
            "  PIXEL_MEAN:\n",
            "  - 103.53\n",
            "  - 116.28\n",
            "  - 123.675\n",
            "  PIXEL_STD:\n",
            "  - 1.0\n",
            "  - 1.0\n",
            "  - 1.0\n",
            "  PROPOSAL_GENERATOR:\n",
            "    MIN_SIZE: 0\n",
            "    NAME: RPN\n",
            "  QUERY:\n",
            "    CONTEXT: 2\n",
            "    ENCODE_CENTER_DIS_COEFF:\n",
            "    - 1.0\n",
            "    - 1.0\n",
            "    ENCODE_SMALL_OBJ_SCALE:\n",
            "    - - 0\n",
            "      - 32\n",
            "    - - 0\n",
            "      - 64\n",
            "    FEATURES_VALUE_TEST:\n",
            "    - 0\n",
            "    - 1\n",
            "    FEATURES_VALUE_TRAIN:\n",
            "    - 0\n",
            "    - 1\n",
            "    FEATURES_WHOLE_TEST:\n",
            "    - 2\n",
            "    - 3\n",
            "    - 4\n",
            "    - 5\n",
            "    FEATURES_WHOLE_TRAIN:\n",
            "    - 2\n",
            "    - 3\n",
            "    - 4\n",
            "    - 5\n",
            "    QUERY_INFER: false\n",
            "    QUERY_LOSS_GAMMA:\n",
            "    - 1.3\n",
            "    - 1.3\n",
            "    QUERY_LOSS_WEIGHT:\n",
            "    - 10.0\n",
            "    - 10.0\n",
            "    Q_FEATURE_TEST:\n",
            "    - 1\n",
            "    - 2\n",
            "    Q_FEATURE_TRAIN:\n",
            "    - 1\n",
            "    - 2\n",
            "    THRESHOLD: 0.12\n",
            "  RESNETS:\n",
            "    DEFORM_MODULATED: false\n",
            "    DEFORM_NUM_GROUPS: 1\n",
            "    DEFORM_ON_PER_STAGE:\n",
            "    - false\n",
            "    - false\n",
            "    - false\n",
            "    - false\n",
            "    DEPTH: 50\n",
            "    NORM: FrozenBN\n",
            "    NUM_GROUPS: 1\n",
            "    OUT_FEATURES:\n",
            "    - res2\n",
            "    - res3\n",
            "    - res4\n",
            "    - res5\n",
            "    RES2_OUT_CHANNELS: 256\n",
            "    RES5_DILATION: 1\n",
            "    STEM_OUT_CHANNELS: 64\n",
            "    STRIDE_IN_1X1: true\n",
            "    WIDTH_PER_GROUP: 64\n",
            "  RETINANET:\n",
            "    BBOX_REG_LOSS_TYPE: smooth_l1\n",
            "    BBOX_REG_WEIGHTS:\n",
            "    - 1.0\n",
            "    - 1.0\n",
            "    - 1.0\n",
            "    - 1.0\n",
            "    FOCAL_LOSS_ALPHA: 0.25\n",
            "    FOCAL_LOSS_GAMMA: 2.0\n",
            "    IN_FEATURES:\n",
            "    - p2\n",
            "    - p3\n",
            "    - p4\n",
            "    - p5\n",
            "    - p6\n",
            "    - p7\n",
            "    IOU_LABELS:\n",
            "    - 0\n",
            "    - -1\n",
            "    - 1\n",
            "    IOU_THRESHOLDS:\n",
            "    - 0.4\n",
            "    - 0.5\n",
            "    NMS_THRESH_TEST: 0.5\n",
            "    NORM: ''\n",
            "    NUM_CLASSES: 10\n",
            "    NUM_CONVS: 4\n",
            "    PRIOR_PROB: 0.01\n",
            "    SCORE_THRESH_TEST: 0.05\n",
            "    SMOOTH_L1_LOSS_BETA: 0.1\n",
            "    TOPK_CANDIDATES_TEST: 1000\n",
            "  ROI_BOX_CASCADE_HEAD:\n",
            "    BBOX_REG_WEIGHTS:\n",
            "    - - 10.0\n",
            "      - 10.0\n",
            "      - 5.0\n",
            "      - 5.0\n",
            "    - - 20.0\n",
            "      - 20.0\n",
            "      - 10.0\n",
            "      - 10.0\n",
            "    - - 30.0\n",
            "      - 30.0\n",
            "      - 15.0\n",
            "      - 15.0\n",
            "    IOUS:\n",
            "    - 0.5\n",
            "    - 0.6\n",
            "    - 0.7\n",
            "  ROI_BOX_HEAD:\n",
            "    BBOX_REG_LOSS_TYPE: smooth_l1\n",
            "    BBOX_REG_LOSS_WEIGHT: 1.0\n",
            "    BBOX_REG_WEIGHTS:\n",
            "    - 10.0\n",
            "    - 10.0\n",
            "    - 5.0\n",
            "    - 5.0\n",
            "    CLS_AGNOSTIC_BBOX_REG: false\n",
            "    CONV_DIM: 256\n",
            "    FC_DIM: 1024\n",
            "    NAME: ''\n",
            "    NORM: ''\n",
            "    NUM_CONV: 0\n",
            "    NUM_FC: 0\n",
            "    POOLER_RESOLUTION: 14\n",
            "    POOLER_SAMPLING_RATIO: 0\n",
            "    POOLER_TYPE: ROIAlignV2\n",
            "    SMOOTH_L1_BETA: 0.0\n",
            "    TRAIN_ON_PRED_BOXES: false\n",
            "  ROI_HEADS:\n",
            "    BATCH_SIZE_PER_IMAGE: 512\n",
            "    IN_FEATURES:\n",
            "    - res4\n",
            "    IOU_LABELS:\n",
            "    - 0\n",
            "    - 1\n",
            "    IOU_THRESHOLDS:\n",
            "    - 0.5\n",
            "    NAME: Res5ROIHeads\n",
            "    NMS_THRESH_TEST: 0.5\n",
            "    NUM_CLASSES: 80\n",
            "    POSITIVE_FRACTION: 0.25\n",
            "    PROPOSAL_APPEND_GT: true\n",
            "    SCORE_THRESH_TEST: 0.05\n",
            "  ROI_KEYPOINT_HEAD:\n",
            "    CONV_DIMS:\n",
            "    - 512\n",
            "    - 512\n",
            "    - 512\n",
            "    - 512\n",
            "    - 512\n",
            "    - 512\n",
            "    - 512\n",
            "    - 512\n",
            "    LOSS_WEIGHT: 1.0\n",
            "    MIN_KEYPOINTS_PER_IMAGE: 1\n",
            "    NAME: KRCNNConvDeconvUpsampleHead\n",
            "    NORMALIZE_LOSS_BY_VISIBLE_KEYPOINTS: true\n",
            "    NUM_KEYPOINTS: 17\n",
            "    POOLER_RESOLUTION: 14\n",
            "    POOLER_SAMPLING_RATIO: 0\n",
            "    POOLER_TYPE: ROIAlignV2\n",
            "  ROI_MASK_HEAD:\n",
            "    CLS_AGNOSTIC_MASK: false\n",
            "    CONV_DIM: 256\n",
            "    NAME: MaskRCNNConvUpsampleHead\n",
            "    NORM: ''\n",
            "    NUM_CONV: 0\n",
            "    POOLER_RESOLUTION: 14\n",
            "    POOLER_SAMPLING_RATIO: 0\n",
            "    POOLER_TYPE: ROIAlignV2\n",
            "  RPN:\n",
            "    BATCH_SIZE_PER_IMAGE: 256\n",
            "    BBOX_REG_LOSS_TYPE: smooth_l1\n",
            "    BBOX_REG_LOSS_WEIGHT: 1.0\n",
            "    BBOX_REG_WEIGHTS:\n",
            "    - 1.0\n",
            "    - 1.0\n",
            "    - 1.0\n",
            "    - 1.0\n",
            "    BOUNDARY_THRESH: -1\n",
            "    HEAD_NAME: StandardRPNHead\n",
            "    IN_FEATURES:\n",
            "    - res4\n",
            "    IOU_LABELS:\n",
            "    - 0\n",
            "    - -1\n",
            "    - 1\n",
            "    IOU_THRESHOLDS:\n",
            "    - 0.3\n",
            "    - 0.7\n",
            "    LOSS_WEIGHT: 1.0\n",
            "    NMS_THRESH: 0.7\n",
            "    POSITIVE_FRACTION: 0.5\n",
            "    POST_NMS_TOPK_TEST: 1000\n",
            "    POST_NMS_TOPK_TRAIN: 2000\n",
            "    PRE_NMS_TOPK_TEST: 6000\n",
            "    PRE_NMS_TOPK_TRAIN: 12000\n",
            "    SMOOTH_L1_BETA: 0.0\n",
            "  SEM_SEG_HEAD:\n",
            "    COMMON_STRIDE: 4\n",
            "    CONVS_DIM: 128\n",
            "    IGNORE_VALUE: 255\n",
            "    IN_FEATURES:\n",
            "    - p2\n",
            "    - p3\n",
            "    - p4\n",
            "    - p5\n",
            "    LOSS_WEIGHT: 1.0\n",
            "    NAME: SemSegFPNHead\n",
            "    NORM: GN\n",
            "    NUM_CLASSES: 54\n",
            "  WEIGHTS: /content/gdrive/MyDrive/QueryDet-PyTorch/work_dirs/visdrone_querydet/model_final.pth\n",
            "OUTPUT_DIR: work_dirs/visdrone_querydet\n",
            "SEED: -1\n",
            "SOLVER:\n",
            "  AMP:\n",
            "    ENABLED: true\n",
            "  BASE_LR: 0.006\n",
            "  BIAS_LR_FACTOR: 1.0\n",
            "  CHECKPOINT_PERIOD: 2000\n",
            "  CLIP_GRADIENTS:\n",
            "    CLIP_TYPE: value\n",
            "    CLIP_VALUE: 35.0\n",
            "    ENABLED: true\n",
            "    NORM_TYPE: 2.0\n",
            "  GAMMA: 0.1\n",
            "  IMS_PER_BATCH: 3\n",
            "  LR_SCHEDULER_NAME: WarmupMultiStepLR\n",
            "  MAX_ITER: 60000\n",
            "  MOMENTUM: 0.9\n",
            "  NESTEROV: false\n",
            "  REFERENCE_WORLD_SIZE: 0\n",
            "  STEPS:\n",
            "  - 15000\n",
            "  - 20000\n",
            "  WARMUP_FACTOR: 0.001\n",
            "  WARMUP_ITERS: 1000\n",
            "  WARMUP_METHOD: linear\n",
            "  WEIGHT_DECAY: 0.0001\n",
            "  WEIGHT_DECAY_BIAS: 0.0001\n",
            "  WEIGHT_DECAY_NORM: 0.0\n",
            "TEST:\n",
            "  AUG:\n",
            "    ENABLED: false\n",
            "    FLIP: true\n",
            "    MAX_SIZE: 4000\n",
            "    MIN_SIZES:\n",
            "    - 400\n",
            "    - 500\n",
            "    - 600\n",
            "    - 700\n",
            "    - 800\n",
            "    - 900\n",
            "    - 1000\n",
            "    - 1100\n",
            "    - 1200\n",
            "  DETECTIONS_PER_IMAGE: 500\n",
            "  EVAL_PERIOD: 0\n",
            "  EXPECTED_RESULTS: []\n",
            "  KEYPOINT_OKS_SIGMAS: []\n",
            "  PRECISE_BN:\n",
            "    ENABLED: false\n",
            "    NUM_ITER: 200\n",
            "VERSION: 2\n",
            "VISDRONE:\n",
            "  MAX_LENGTH: 1999\n",
            "  SHORT_LENGTH:\n",
            "  - 1200\n",
            "  TEST_IMG_ROOT: data/visdrone/coco_format/val_images\n",
            "  TEST_JSON: data/visdrone/coco_format/annotations/val_label.json\n",
            "  TEST_LENGTH: 3999\n",
            "  TRAIN_JSON: data/visdrone/coco_format/annotations/train_label.json\n",
            "  TRING_IMG_ROOT: data//visdrone/coco_format/train_images\n",
            "VIS_PERIOD: 0\n",
            "\n",
            "\u001b[32m[04/13 13:58:00 detectron2]: \u001b[0mRunning with full config:\n",
            "CUDNN_BENCHMARK: False\n",
            "DATALOADER:\n",
            "  ASPECT_RATIO_GROUPING: True\n",
            "  FILTER_EMPTY_ANNOTATIONS: True\n",
            "  NUM_WORKERS: 4\n",
            "  REPEAT_THRESHOLD: 0.0\n",
            "  SAMPLER_TRAIN: TrainingSampler\n",
            "DATASETS:\n",
            "  PRECOMPUTED_PROPOSAL_TOPK_TEST: 1000\n",
            "  PRECOMPUTED_PROPOSAL_TOPK_TRAIN: 2000\n",
            "  PROPOSAL_FILES_TEST: ()\n",
            "  PROPOSAL_FILES_TRAIN: ()\n",
            "  TEST: ('coco_2017_val',)\n",
            "  TRAIN: ('coco_2017_train',)\n",
            "GLOBAL:\n",
            "  HACK: 1.0\n",
            "INPUT:\n",
            "  CROP:\n",
            "    ENABLED: False\n",
            "    SIZE: [0.9, 0.9]\n",
            "    TYPE: relative_range\n",
            "  FORMAT: BGR\n",
            "  MASK_FORMAT: polygon\n",
            "  MAX_SIZE_TEST: 1333\n",
            "  MAX_SIZE_TRAIN: 1333\n",
            "  MIN_SIZE_TEST: 800\n",
            "  MIN_SIZE_TRAIN: (640, 672, 704, 736, 768, 800)\n",
            "  MIN_SIZE_TRAIN_SAMPLING: choice\n",
            "  RANDOM_FLIP: horizontal\n",
            "META_INFO:\n",
            "  EVAL_AP: True\n",
            "  EVAL_GPU_TIME: True\n",
            "  VIS_ROOT: \n",
            "MODEL:\n",
            "  ANCHOR_GENERATOR:\n",
            "    ANGLES: [[-90, 0, 90]]\n",
            "    ASPECT_RATIOS: [[0.5, 1.0, 2.0]]\n",
            "    NAME: AnchorGeneratorWithCenter\n",
            "    OFFSET: 0.0\n",
            "    SIZES: [[16, 20.15873679831797, 25.39841683149119], [32, 40.31747359663594, 50.79683366298238], [64, 80.63494719327188, 101.59366732596476], [128, 161.26989438654377, 203.18733465192952], [256, 322.53978877308754, 406.37466930385904], [512, 645.0795775461751, 812.7493386077181]]\n",
            "  BACKBONE:\n",
            "    FREEZE_AT: 2\n",
            "    NAME: build_retinanet_resnet_fpn_backbone\n",
            "  CUSTOM:\n",
            "    CLEAR_CUDA_CACHE: False\n",
            "    CLS_WEIGHTS: [1.0, 1.4, 1.8, 2.2, 2.6, 2.6]\n",
            "    FOCAL_LOSS_ALPHAS: [0.25, 0.25, 0.25, 0.25, 0.25, 0.25]\n",
            "    FOCAL_LOSS_GAMMAS: [2.0, 2.0, 2.0, 2.0, 2.0, 2.0]\n",
            "    GIOU_LOSS: False\n",
            "    GRADIENT_CHECKPOINT: False\n",
            "    HEAD_BN: False\n",
            "    REG_WEIGHTS: [1.0, 1.4, 1.8, 2.2, 2.6, 2.6]\n",
            "    SOFT_NMS_METHOD: linear\n",
            "    SOFT_NMS_PRUND: 0.001\n",
            "    SOFT_NMS_SIGMA: 0.5\n",
            "    SOFT_NMS_THRESHOLD: 0.5\n",
            "    USE_LOOP_MATCHER: True\n",
            "    USE_SOFT_NMS: False\n",
            "  DEVICE: cuda\n",
            "  FPN:\n",
            "    FUSE_TYPE: sum\n",
            "    IN_FEATURES: ['res2', 'res3', 'res4', 'res5']\n",
            "    NORM: \n",
            "    OUT_CHANNELS: 256\n",
            "    TOP_LEVELS: 2\n",
            "  KEYPOINT_ON: False\n",
            "  LOAD_PROPOSALS: False\n",
            "  MASK_ON: False\n",
            "  META_ARCHITECTURE: RetinaNetQueryDet\n",
            "  PANOPTIC_FPN:\n",
            "    COMBINE:\n",
            "      ENABLED: True\n",
            "      INSTANCES_CONFIDENCE_THRESH: 0.5\n",
            "      OVERLAP_THRESH: 0.5\n",
            "      STUFF_AREA_LIMIT: 4096\n",
            "    INSTANCE_LOSS_WEIGHT: 1.0\n",
            "  PIXEL_MEAN: [103.53, 116.28, 123.675]\n",
            "  PIXEL_STD: [1.0, 1.0, 1.0]\n",
            "  PROPOSAL_GENERATOR:\n",
            "    MIN_SIZE: 0\n",
            "    NAME: RPN\n",
            "  QUERY:\n",
            "    CONTEXT: 2\n",
            "    ENCODE_CENTER_DIS_COEFF: [1.0, 1.0]\n",
            "    ENCODE_SMALL_OBJ_SCALE: [[0, 32], [0, 64]]\n",
            "    FEATURES_VALUE_TEST: [0, 1]\n",
            "    FEATURES_VALUE_TRAIN: [0, 1]\n",
            "    FEATURES_WHOLE_TEST: [2, 3, 4, 5]\n",
            "    FEATURES_WHOLE_TRAIN: [2, 3, 4, 5]\n",
            "    QUERY_INFER: False\n",
            "    QUERY_LOSS_GAMMA: [1.3, 1.3]\n",
            "    QUERY_LOSS_WEIGHT: [10.0, 10.0]\n",
            "    Q_FEATURE_TEST: [1, 2]\n",
            "    Q_FEATURE_TRAIN: [1, 2]\n",
            "    THRESHOLD: 0.12\n",
            "  RESNETS:\n",
            "    DEFORM_MODULATED: False\n",
            "    DEFORM_NUM_GROUPS: 1\n",
            "    DEFORM_ON_PER_STAGE: [False, False, False, False]\n",
            "    DEPTH: 50\n",
            "    NORM: FrozenBN\n",
            "    NUM_GROUPS: 1\n",
            "    OUT_FEATURES: ['res2', 'res3', 'res4', 'res5']\n",
            "    RES2_OUT_CHANNELS: 256\n",
            "    RES5_DILATION: 1\n",
            "    STEM_OUT_CHANNELS: 64\n",
            "    STRIDE_IN_1X1: True\n",
            "    WIDTH_PER_GROUP: 64\n",
            "  RETINANET:\n",
            "    BBOX_REG_LOSS_TYPE: smooth_l1\n",
            "    BBOX_REG_WEIGHTS: (1.0, 1.0, 1.0, 1.0)\n",
            "    FOCAL_LOSS_ALPHA: 0.25\n",
            "    FOCAL_LOSS_GAMMA: 2.0\n",
            "    IN_FEATURES: ['p2', 'p3', 'p4', 'p5', 'p6', 'p7']\n",
            "    IOU_LABELS: [0, -1, 1]\n",
            "    IOU_THRESHOLDS: [0.4, 0.5]\n",
            "    NMS_THRESH_TEST: 0.5\n",
            "    NORM: \n",
            "    NUM_CLASSES: 10\n",
            "    NUM_CONVS: 4\n",
            "    PRIOR_PROB: 0.01\n",
            "    SCORE_THRESH_TEST: 0.05\n",
            "    SMOOTH_L1_LOSS_BETA: 0.1\n",
            "    TOPK_CANDIDATES_TEST: 1000\n",
            "  ROI_BOX_CASCADE_HEAD:\n",
            "    BBOX_REG_WEIGHTS: ([10.0, 10.0, 5.0, 5.0], [20.0, 20.0, 10.0, 10.0], [30.0, 30.0, 15.0, 15.0])\n",
            "    IOUS: (0.5, 0.6, 0.7)\n",
            "  ROI_BOX_HEAD:\n",
            "    BBOX_REG_LOSS_TYPE: smooth_l1\n",
            "    BBOX_REG_LOSS_WEIGHT: 1.0\n",
            "    BBOX_REG_WEIGHTS: (10.0, 10.0, 5.0, 5.0)\n",
            "    CLS_AGNOSTIC_BBOX_REG: False\n",
            "    CONV_DIM: 256\n",
            "    FC_DIM: 1024\n",
            "    NAME: \n",
            "    NORM: \n",
            "    NUM_CONV: 0\n",
            "    NUM_FC: 0\n",
            "    POOLER_RESOLUTION: 14\n",
            "    POOLER_SAMPLING_RATIO: 0\n",
            "    POOLER_TYPE: ROIAlignV2\n",
            "    SMOOTH_L1_BETA: 0.0\n",
            "    TRAIN_ON_PRED_BOXES: False\n",
            "  ROI_HEADS:\n",
            "    BATCH_SIZE_PER_IMAGE: 512\n",
            "    IN_FEATURES: ['res4']\n",
            "    IOU_LABELS: [0, 1]\n",
            "    IOU_THRESHOLDS: [0.5]\n",
            "    NAME: Res5ROIHeads\n",
            "    NMS_THRESH_TEST: 0.5\n",
            "    NUM_CLASSES: 80\n",
            "    POSITIVE_FRACTION: 0.25\n",
            "    PROPOSAL_APPEND_GT: True\n",
            "    SCORE_THRESH_TEST: 0.05\n",
            "  ROI_KEYPOINT_HEAD:\n",
            "    CONV_DIMS: (512, 512, 512, 512, 512, 512, 512, 512)\n",
            "    LOSS_WEIGHT: 1.0\n",
            "    MIN_KEYPOINTS_PER_IMAGE: 1\n",
            "    NAME: KRCNNConvDeconvUpsampleHead\n",
            "    NORMALIZE_LOSS_BY_VISIBLE_KEYPOINTS: True\n",
            "    NUM_KEYPOINTS: 17\n",
            "    POOLER_RESOLUTION: 14\n",
            "    POOLER_SAMPLING_RATIO: 0\n",
            "    POOLER_TYPE: ROIAlignV2\n",
            "  ROI_MASK_HEAD:\n",
            "    CLS_AGNOSTIC_MASK: False\n",
            "    CONV_DIM: 256\n",
            "    NAME: MaskRCNNConvUpsampleHead\n",
            "    NORM: \n",
            "    NUM_CONV: 0\n",
            "    POOLER_RESOLUTION: 14\n",
            "    POOLER_SAMPLING_RATIO: 0\n",
            "    POOLER_TYPE: ROIAlignV2\n",
            "  RPN:\n",
            "    BATCH_SIZE_PER_IMAGE: 256\n",
            "    BBOX_REG_LOSS_TYPE: smooth_l1\n",
            "    BBOX_REG_LOSS_WEIGHT: 1.0\n",
            "    BBOX_REG_WEIGHTS: (1.0, 1.0, 1.0, 1.0)\n",
            "    BOUNDARY_THRESH: -1\n",
            "    HEAD_NAME: StandardRPNHead\n",
            "    IN_FEATURES: ['res4']\n",
            "    IOU_LABELS: [0, -1, 1]\n",
            "    IOU_THRESHOLDS: [0.3, 0.7]\n",
            "    LOSS_WEIGHT: 1.0\n",
            "    NMS_THRESH: 0.7\n",
            "    POSITIVE_FRACTION: 0.5\n",
            "    POST_NMS_TOPK_TEST: 1000\n",
            "    POST_NMS_TOPK_TRAIN: 2000\n",
            "    PRE_NMS_TOPK_TEST: 6000\n",
            "    PRE_NMS_TOPK_TRAIN: 12000\n",
            "    SMOOTH_L1_BETA: 0.0\n",
            "  SEM_SEG_HEAD:\n",
            "    COMMON_STRIDE: 4\n",
            "    CONVS_DIM: 128\n",
            "    IGNORE_VALUE: 255\n",
            "    IN_FEATURES: ['p2', 'p3', 'p4', 'p5']\n",
            "    LOSS_WEIGHT: 1.0\n",
            "    NAME: SemSegFPNHead\n",
            "    NORM: GN\n",
            "    NUM_CLASSES: 54\n",
            "  WEIGHTS: /content/gdrive/MyDrive/QueryDet-PyTorch/work_dirs/visdrone_querydet/model_final.pth\n",
            "OUTPUT_DIR: work_dirs/visdrone_querydet\n",
            "SEED: -1\n",
            "SOLVER:\n",
            "  AMP:\n",
            "    ENABLED: True\n",
            "  BASE_LR: 0.006\n",
            "  BIAS_LR_FACTOR: 1.0\n",
            "  CHECKPOINT_PERIOD: 2000\n",
            "  CLIP_GRADIENTS:\n",
            "    CLIP_TYPE: value\n",
            "    CLIP_VALUE: 35.0\n",
            "    ENABLED: True\n",
            "    NORM_TYPE: 2.0\n",
            "  GAMMA: 0.1\n",
            "  IMS_PER_BATCH: 3\n",
            "  LR_SCHEDULER_NAME: WarmupMultiStepLR\n",
            "  MAX_ITER: 60000\n",
            "  MOMENTUM: 0.9\n",
            "  NESTEROV: False\n",
            "  REFERENCE_WORLD_SIZE: 0\n",
            "  STEPS: (15000, 20000)\n",
            "  WARMUP_FACTOR: 0.001\n",
            "  WARMUP_ITERS: 1000\n",
            "  WARMUP_METHOD: linear\n",
            "  WEIGHT_DECAY: 0.0001\n",
            "  WEIGHT_DECAY_BIAS: 0.0001\n",
            "  WEIGHT_DECAY_NORM: 0.0\n",
            "TEST:\n",
            "  AUG:\n",
            "    ENABLED: False\n",
            "    FLIP: True\n",
            "    MAX_SIZE: 4000\n",
            "    MIN_SIZES: (400, 500, 600, 700, 800, 900, 1000, 1100, 1200)\n",
            "  DETECTIONS_PER_IMAGE: 500\n",
            "  EVAL_PERIOD: 0\n",
            "  EXPECTED_RESULTS: []\n",
            "  KEYPOINT_OKS_SIGMAS: []\n",
            "  PRECISE_BN:\n",
            "    ENABLED: False\n",
            "    NUM_ITER: 200\n",
            "VERSION: 2\n",
            "VISDRONE:\n",
            "  MAX_LENGTH: 1999\n",
            "  SHORT_LENGTH: [1200]\n",
            "  TEST_IMG_ROOT: data/visdrone/coco_format/val_images\n",
            "  TEST_JSON: data/visdrone/coco_format/annotations/val_label.json\n",
            "  TEST_LENGTH: 3999\n",
            "  TRAIN_JSON: data/visdrone/coco_format/annotations/train_label.json\n",
            "  TRING_IMG_ROOT: data//visdrone/coco_format/train_images\n",
            "VIS_PERIOD: 0\n",
            "\u001b[32m[04/13 13:58:02 detectron2]: \u001b[0mFull config saved to work_dirs/visdrone_querydet/config.yaml\n",
            "\u001b[32m[04/13 13:58:02 d2.utils.env]: \u001b[0mUsing a generated random seed 2339458\n",
            "\u001b[32m[04/13 13:58:06 d2.engine.defaults]: \u001b[0mModel:\n",
            "RetinaNetQueryDet(\n",
            "  (backbone): FPN(\n",
            "    (fpn_lateral2): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1))\n",
            "    (fpn_output2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (fpn_lateral3): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1))\n",
            "    (fpn_output3): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (fpn_lateral4): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1))\n",
            "    (fpn_output4): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (fpn_lateral5): Conv2d(2048, 256, kernel_size=(1, 1), stride=(1, 1))\n",
            "    (fpn_output5): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (top_block): LastLevelP6P7(\n",
            "      (p6): Conv2d(2048, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
            "      (p7): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
            "    )\n",
            "    (bottom_up): ResNet(\n",
            "      (stem): BasicStem(\n",
            "        (conv1): Conv2d(\n",
            "          3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)\n",
            "        )\n",
            "      )\n",
            "      (res2): Sequential(\n",
            "        (0): BottleneckBlock(\n",
            "          (shortcut): Conv2d(\n",
            "            64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
            "          )\n",
            "          (conv1): Conv2d(\n",
            "            64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)\n",
            "          )\n",
            "          (conv2): Conv2d(\n",
            "            64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)\n",
            "          )\n",
            "          (conv3): Conv2d(\n",
            "            64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
            "          )\n",
            "        )\n",
            "        (1): BottleneckBlock(\n",
            "          (conv1): Conv2d(\n",
            "            256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)\n",
            "          )\n",
            "          (conv2): Conv2d(\n",
            "            64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)\n",
            "          )\n",
            "          (conv3): Conv2d(\n",
            "            64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
            "          )\n",
            "        )\n",
            "        (2): BottleneckBlock(\n",
            "          (conv1): Conv2d(\n",
            "            256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)\n",
            "          )\n",
            "          (conv2): Conv2d(\n",
            "            64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)\n",
            "          )\n",
            "          (conv3): Conv2d(\n",
            "            64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
            "          )\n",
            "        )\n",
            "      )\n",
            "      (res3): Sequential(\n",
            "        (0): BottleneckBlock(\n",
            "          (shortcut): Conv2d(\n",
            "            256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
            "          )\n",
            "          (conv1): Conv2d(\n",
            "            256, 128, kernel_size=(1, 1), stride=(2, 2), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)\n",
            "          )\n",
            "          (conv2): Conv2d(\n",
            "            128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)\n",
            "          )\n",
            "          (conv3): Conv2d(\n",
            "            128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
            "          )\n",
            "        )\n",
            "        (1): BottleneckBlock(\n",
            "          (conv1): Conv2d(\n",
            "            512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)\n",
            "          )\n",
            "          (conv2): Conv2d(\n",
            "            128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)\n",
            "          )\n",
            "          (conv3): Conv2d(\n",
            "            128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
            "          )\n",
            "        )\n",
            "        (2): BottleneckBlock(\n",
            "          (conv1): Conv2d(\n",
            "            512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)\n",
            "          )\n",
            "          (conv2): Conv2d(\n",
            "            128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)\n",
            "          )\n",
            "          (conv3): Conv2d(\n",
            "            128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
            "          )\n",
            "        )\n",
            "        (3): BottleneckBlock(\n",
            "          (conv1): Conv2d(\n",
            "            512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)\n",
            "          )\n",
            "          (conv2): Conv2d(\n",
            "            128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)\n",
            "          )\n",
            "          (conv3): Conv2d(\n",
            "            128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
            "          )\n",
            "        )\n",
            "      )\n",
            "      (res4): Sequential(\n",
            "        (0): BottleneckBlock(\n",
            "          (shortcut): Conv2d(\n",
            "            512, 1024, kernel_size=(1, 1), stride=(2, 2), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n",
            "          )\n",
            "          (conv1): Conv2d(\n",
            "            512, 256, kernel_size=(1, 1), stride=(2, 2), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
            "          )\n",
            "          (conv2): Conv2d(\n",
            "            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
            "          )\n",
            "          (conv3): Conv2d(\n",
            "            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n",
            "          )\n",
            "        )\n",
            "        (1): BottleneckBlock(\n",
            "          (conv1): Conv2d(\n",
            "            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
            "          )\n",
            "          (conv2): Conv2d(\n",
            "            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
            "          )\n",
            "          (conv3): Conv2d(\n",
            "            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n",
            "          )\n",
            "        )\n",
            "        (2): BottleneckBlock(\n",
            "          (conv1): Conv2d(\n",
            "            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
            "          )\n",
            "          (conv2): Conv2d(\n",
            "            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
            "          )\n",
            "          (conv3): Conv2d(\n",
            "            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n",
            "          )\n",
            "        )\n",
            "        (3): BottleneckBlock(\n",
            "          (conv1): Conv2d(\n",
            "            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
            "          )\n",
            "          (conv2): Conv2d(\n",
            "            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
            "          )\n",
            "          (conv3): Conv2d(\n",
            "            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n",
            "          )\n",
            "        )\n",
            "        (4): BottleneckBlock(\n",
            "          (conv1): Conv2d(\n",
            "            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
            "          )\n",
            "          (conv2): Conv2d(\n",
            "            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
            "          )\n",
            "          (conv3): Conv2d(\n",
            "            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n",
            "          )\n",
            "        )\n",
            "        (5): BottleneckBlock(\n",
            "          (conv1): Conv2d(\n",
            "            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
            "          )\n",
            "          (conv2): Conv2d(\n",
            "            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
            "          )\n",
            "          (conv3): Conv2d(\n",
            "            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n",
            "          )\n",
            "        )\n",
            "      )\n",
            "      (res5): Sequential(\n",
            "        (0): BottleneckBlock(\n",
            "          (shortcut): Conv2d(\n",
            "            1024, 2048, kernel_size=(1, 1), stride=(2, 2), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=2048, eps=1e-05)\n",
            "          )\n",
            "          (conv1): Conv2d(\n",
            "            1024, 512, kernel_size=(1, 1), stride=(2, 2), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
            "          )\n",
            "          (conv2): Conv2d(\n",
            "            512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
            "          )\n",
            "          (conv3): Conv2d(\n",
            "            512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=2048, eps=1e-05)\n",
            "          )\n",
            "        )\n",
            "        (1): BottleneckBlock(\n",
            "          (conv1): Conv2d(\n",
            "            2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
            "          )\n",
            "          (conv2): Conv2d(\n",
            "            512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
            "          )\n",
            "          (conv3): Conv2d(\n",
            "            512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=2048, eps=1e-05)\n",
            "          )\n",
            "        )\n",
            "        (2): BottleneckBlock(\n",
            "          (conv1): Conv2d(\n",
            "            2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
            "          )\n",
            "          (conv2): Conv2d(\n",
            "            512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
            "          )\n",
            "          (conv3): Conv2d(\n",
            "            512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=2048, eps=1e-05)\n",
            "          )\n",
            "        )\n",
            "      )\n",
            "    )\n",
            "  )\n",
            "  (det_head): RetinaNetHead_3x3(\n",
            "    (cls_layer_0): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (bbox_layer_0): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (cls_layer_1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (bbox_layer_1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (cls_layer_2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (bbox_layer_2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (cls_layer_3): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (bbox_layer_3): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (cls_score): Conv2d(256, 90, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (bbox_pred): Conv2d(256, 36, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "  )\n",
            "  (query_head): Head_3x3(\n",
            "    (layer_0): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (layer_1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (layer_2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (layer_3): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (pred_net): Conv2d(256, 1, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "  )\n",
            "  (anchor_generator): AnchorGeneratorWithCenter(\n",
            "    (cell_anchors): BufferList()\n",
            "  )\n",
            "  (query_anchor_generator): AnchorGeneratorWithCenter(\n",
            "    (cell_anchors): BufferList()\n",
            "  )\n",
            ")\n",
            "\u001b[32m[04/13 13:58:06 fvcore.common.checkpoint]: \u001b[0mLoading checkpoint from /content/gdrive/MyDrive/QueryDet-PyTorch/work_dirs/visdrone_querydet/model_final.pth\n",
            "\u001b[32m[04/13 13:58:24 d2.data.common]: \u001b[0mSerializing 25884 elements to byte tensors and concatenating them all ...\n",
            "\u001b[32m[04/13 13:58:24 d2.data.common]: \u001b[0mSerialized dataset takes 15.78 MiB\n",
            "/usr/local/lib/python3.7/site-packages/torch/utils/data/dataloader.py:477: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  cpuset_checked))\n",
            "\u001b[32m[04/13 13:58:28 d2.engine.train_loop]: \u001b[0mStarting training from iteration 0\n",
            "\u001b[32m[04/13 13:59:36 d2.utils.events]: \u001b[0m eta: 1 day, 13:31:54  iter: 19  total_loss: 3.167  loss_cls: 2.153  loss_box_reg: 0.9008  loss_query: 0.05441  time: 2.4999  data_time: 0.4443  lr: 0.00011989  max_mem: 10549M\n",
            "\u001b[32m[04/13 14:00:26 d2.utils.events]: \u001b[0m eta: 1 day, 13:35:04  iter: 39  total_loss: 2.346  loss_cls: 1.508  loss_box_reg: 0.7302  loss_query: 0.06703  time: 2.3744  data_time: 0.0076  lr: 0.00023977  max_mem: 10549M\n",
            "\u001b[32m[04/13 14:01:14 d2.utils.events]: \u001b[0m eta: 1 day, 14:19:04  iter: 59  total_loss: 1.895  loss_cls: 1.28  loss_box_reg: 0.6963  loss_query: 0.05843  time: 2.3513  data_time: 0.0075  lr: 0.00035965  max_mem: 10549M\n",
            "\u001b[32m[04/13 14:02:03 d2.utils.events]: \u001b[0m eta: 1 day, 14:25:22  iter: 79  total_loss: 1.606  loss_cls: 1.029  loss_box_reg: 0.5667  loss_query: 0.04669  time: 2.3494  data_time: 0.0624  lr: 0.00047953  max_mem: 10549M\n",
            "\u001b[32m[04/13 14:02:49 d2.utils.events]: \u001b[0m eta: 1 day, 14:25:29  iter: 99  total_loss: 1.741  loss_cls: 1.184  loss_box_reg: 0.5231  loss_query: 0.04992  time: 2.3239  data_time: 0.0056  lr: 0.00059941  max_mem: 10549M\n",
            "\u001b[32m[04/13 14:03:37 d2.utils.events]: \u001b[0m eta: 1 day, 14:32:26  iter: 119  total_loss: 1.989  loss_cls: 1.219  loss_box_reg: 0.7293  loss_query: 0.02876  time: 2.3150  data_time: 0.0059  lr: 0.00071929  max_mem: 10549M\n",
            "\u001b[32m[04/13 14:04:26 d2.utils.events]: \u001b[0m eta: 1 day, 14:44:06  iter: 139  total_loss: 1.851  loss_cls: 1.127  loss_box_reg: 0.6641  loss_query: 0.04584  time: 2.3205  data_time: 0.0064  lr: 0.00083917  max_mem: 10549M\n",
            "\u001b[32m[04/13 14:05:15 d2.utils.events]: \u001b[0m eta: 1 day, 14:58:11  iter: 159  total_loss: 1.212  loss_cls: 0.7921  loss_box_reg: 0.442  loss_query: 0.05236  time: 2.3297  data_time: 0.0066  lr: 0.00095905  max_mem: 10549M\n",
            "\u001b[32m[04/13 14:06:05 d2.utils.events]: \u001b[0m eta: 1 day, 15:00:55  iter: 179  total_loss: 1.626  loss_cls: 0.9036  loss_box_reg: 0.6165  loss_query: 0.0466  time: 2.3332  data_time: 0.0068  lr: 0.0010789  max_mem: 10549M\n",
            "\u001b[32m[04/13 14:06:55 d2.utils.events]: \u001b[0m eta: 1 day, 15:11:15  iter: 199  total_loss: 1.911  loss_cls: 1.161  loss_box_reg: 0.6843  loss_query: 0.07045  time: 2.3421  data_time: 0.0069  lr: 0.0011988  max_mem: 10549M\n",
            "\u001b[32m[04/13 14:07:45 d2.utils.events]: \u001b[0m eta: 1 day, 15:19:34  iter: 219  total_loss: 1.743  loss_cls: 1.083  loss_box_reg: 0.5128  loss_query: 0.03974  time: 2.3488  data_time: 0.0064  lr: 0.0013187  max_mem: 10549M\n",
            "\u001b[32m[04/13 14:08:36 d2.utils.events]: \u001b[0m eta: 1 day, 15:26:02  iter: 239  total_loss: 1.352  loss_cls: 0.844  loss_box_reg: 0.4131  loss_query: 0.04716  time: 2.3530  data_time: 0.0066  lr: 0.0014386  max_mem: 10549M\n",
            "\u001b[32m[04/13 14:09:23 d2.utils.events]: \u001b[0m eta: 1 day, 15:25:14  iter: 259  total_loss: 1.573  loss_cls: 0.9736  loss_box_reg: 0.5142  loss_query: 0.06042  time: 2.3466  data_time: 0.0088  lr: 0.0015584  max_mem: 10549M\n",
            "\u001b[32m[04/13 14:10:12 d2.utils.events]: \u001b[0m eta: 1 day, 15:29:48  iter: 279  total_loss: 1.705  loss_cls: 1.155  loss_box_reg: 0.5056  loss_query: 0.05276  time: 2.3475  data_time: 0.0072  lr: 0.0016783  max_mem: 10549M\n",
            "\u001b[32m[04/13 14:11:02 d2.utils.events]: \u001b[0m eta: 1 day, 15:33:32  iter: 299  total_loss: 2.285  loss_cls: 1.514  loss_box_reg: 0.6558  loss_query: 0.06551  time: 2.3515  data_time: 0.0069  lr: 0.0017982  max_mem: 10549M\n",
            "\u001b[32m[04/13 14:11:52 d2.utils.events]: \u001b[0m eta: 1 day, 15:35:48  iter: 319  total_loss: 1.903  loss_cls: 1.226  loss_box_reg: 0.5817  loss_query: 0.06796  time: 2.3556  data_time: 0.0069  lr: 0.0019181  max_mem: 10549M\n",
            "\u001b[32m[04/13 14:12:44 d2.utils.events]: \u001b[0m eta: 1 day, 15:36:56  iter: 339  total_loss: 1.541  loss_cls: 1.024  loss_box_reg: 0.4586  loss_query: 0.05585  time: 2.3621  data_time: 0.0076  lr: 0.002038  max_mem: 10549M\n",
            "\u001b[32m[04/13 14:13:34 d2.utils.events]: \u001b[0m eta: 1 day, 15:38:30  iter: 359  total_loss: 1.562  loss_cls: 1.019  loss_box_reg: 0.4903  loss_query: 0.04178  time: 2.3640  data_time: 0.0068  lr: 0.0021578  max_mem: 10549M\n",
            "\u001b[32m[04/13 14:14:24 d2.utils.events]: \u001b[0m eta: 1 day, 15:42:03  iter: 379  total_loss: 1.458  loss_cls: 0.9472  loss_box_reg: 0.4399  loss_query: 0.0692  time: 2.3680  data_time: 0.0075  lr: 0.0022777  max_mem: 10549M\n",
            "\u001b[32m[04/13 14:15:14 d2.utils.events]: \u001b[0m eta: 1 day, 15:41:15  iter: 399  total_loss: 1.036  loss_cls: 0.704  loss_box_reg: 0.3035  loss_query: 0.05069  time: 2.3679  data_time: 0.0067  lr: 0.0023976  max_mem: 10549M\n",
            "\u001b[32m[04/13 14:16:02 d2.utils.events]: \u001b[0m eta: 1 day, 15:40:27  iter: 419  total_loss: 1.736  loss_cls: 1.098  loss_box_reg: 0.5836  loss_query: 0.05133  time: 2.3664  data_time: 0.0069  lr: 0.0025175  max_mem: 10549M\n",
            "\u001b[32m[04/13 14:16:52 d2.utils.events]: \u001b[0m eta: 1 day, 15:42:13  iter: 439  total_loss: 1.774  loss_cls: 1.081  loss_box_reg: 0.5715  loss_query: 0.06657  time: 2.3666  data_time: 0.0064  lr: 0.0026374  max_mem: 10549M\n",
            "\u001b[32m[04/13 14:17:42 d2.utils.events]: \u001b[0m eta: 1 day, 15:43:54  iter: 459  total_loss: 1.689  loss_cls: 1.147  loss_box_reg: 0.4686  loss_query: 0.05698  time: 2.3689  data_time: 0.0061  lr: 0.0027572  max_mem: 10549M\n",
            "\u001b[32m[04/13 14:18:32 d2.utils.events]: \u001b[0m eta: 1 day, 15:45:44  iter: 479  total_loss: 1.644  loss_cls: 1.15  loss_box_reg: 0.4512  loss_query: 0.04496  time: 2.3710  data_time: 0.0065  lr: 0.0028771  max_mem: 10549M\n",
            "\u001b[32m[04/13 14:19:22 d2.utils.events]: \u001b[0m eta: 1 day, 15:46:05  iter: 499  total_loss: 1.787  loss_cls: 1.169  loss_box_reg: 0.5662  loss_query: 0.04905  time: 2.3723  data_time: 0.0063  lr: 0.002997  max_mem: 10549M\n",
            "\u001b[32m[04/13 14:20:13 d2.utils.events]: \u001b[0m eta: 1 day, 15:46:35  iter: 519  total_loss: 2.129  loss_cls: 1.461  loss_box_reg: 0.5675  loss_query: 0.05955  time: 2.3756  data_time: 0.0070  lr: 0.0031169  max_mem: 10549M\n",
            "\u001b[32m[04/13 14:21:03 d2.utils.events]: \u001b[0m eta: 1 day, 15:46:14  iter: 539  total_loss: 1.074  loss_cls: 0.6775  loss_box_reg: 0.3513  loss_query: 0.06154  time: 2.3765  data_time: 0.0077  lr: 0.0032368  max_mem: 10549M\n",
            "\u001b[32m[04/13 14:21:52 d2.utils.events]: \u001b[0m eta: 1 day, 15:46:07  iter: 559  total_loss: 1.692  loss_cls: 1.08  loss_box_reg: 0.4937  loss_query: 0.06452  time: 2.3758  data_time: 0.0074  lr: 0.0033566  max_mem: 10549M\n",
            "\u001b[32m[04/13 14:22:42 d2.utils.events]: \u001b[0m eta: 1 day, 15:44:59  iter: 579  total_loss: 1.711  loss_cls: 1.129  loss_box_reg: 0.5268  loss_query: 0.04489  time: 2.3732  data_time: 0.0074  lr: 0.0034765  max_mem: 10549M\n",
            "\u001b[32m[04/13 14:23:32 d2.utils.events]: \u001b[0m eta: 1 day, 15:44:42  iter: 599  total_loss: 1.461  loss_cls: 0.9125  loss_box_reg: 0.4203  loss_query: 0.0461  time: 2.3748  data_time: 0.0076  lr: 0.0035964  max_mem: 10549M\n",
            "\u001b[32m[04/13 14:24:20 d2.utils.events]: \u001b[0m eta: 1 day, 15:44:23  iter: 619  total_loss: 1.902  loss_cls: 1.208  loss_box_reg: 0.5558  loss_query: 0.03969  time: 2.3729  data_time: 0.0072  lr: 0.0037163  max_mem: 10549M\n",
            "\u001b[32m[04/13 14:25:11 d2.utils.events]: \u001b[0m eta: 1 day, 15:44:07  iter: 639  total_loss: 1.781  loss_cls: 1.051  loss_box_reg: 0.6076  loss_query: 0.04111  time: 2.3744  data_time: 0.0071  lr: 0.0038362  max_mem: 10549M\n",
            "\u001b[32m[04/13 14:26:01 d2.utils.events]: \u001b[0m eta: 1 day, 15:43:46  iter: 659  total_loss: 1.795  loss_cls: 1.12  loss_box_reg: 0.5801  loss_query: 0.05408  time: 2.3757  data_time: 0.0068  lr: 0.003956  max_mem: 10549M\n",
            "\u001b[32m[04/13 14:26:50 d2.utils.events]: \u001b[0m eta: 1 day, 15:43:02  iter: 679  total_loss: 1.392  loss_cls: 0.9085  loss_box_reg: 0.4287  loss_query: 0.04745  time: 2.3748  data_time: 0.0074  lr: 0.0040759  max_mem: 10549M\n",
            "\u001b[32m[04/13 14:27:39 d2.utils.events]: \u001b[0m eta: 1 day, 15:42:37  iter: 699  total_loss: 1.532  loss_cls: 0.9292  loss_box_reg: 0.5295  loss_query: 0.0523  time: 2.3747  data_time: 0.0073  lr: 0.0041958  max_mem: 10549M\n",
            "\u001b[32m[04/13 14:28:27 d2.utils.events]: \u001b[0m eta: 1 day, 15:40:57  iter: 719  total_loss: 1.858  loss_cls: 1.151  loss_box_reg: 0.6255  loss_query: 0.04141  time: 2.3727  data_time: 0.0100  lr: 0.0043157  max_mem: 10549M\n",
            "\u001b[32m[04/13 14:29:16 d2.utils.events]: \u001b[0m eta: 1 day, 15:39:46  iter: 739  total_loss: 1.185  loss_cls: 0.7787  loss_box_reg: 0.4741  loss_query: 0.04155  time: 2.3719  data_time: 0.0083  lr: 0.0044356  max_mem: 10549M\n",
            "\u001b[32m[04/13 14:30:05 d2.utils.events]: \u001b[0m eta: 1 day, 15:39:06  iter: 759  total_loss: 1.533  loss_cls: 1.02  loss_box_reg: 0.4717  loss_query: 0.0366  time: 2.3717  data_time: 0.0078  lr: 0.0045554  max_mem: 10549M\n",
            "\u001b[32m[04/13 14:30:53 d2.utils.events]: \u001b[0m eta: 1 day, 15:37:57  iter: 779  total_loss: 1.873  loss_cls: 1.37  loss_box_reg: 0.462  loss_query: 0.07134  time: 2.3707  data_time: 0.0071  lr: 0.0046753  max_mem: 10549M\n",
            "\u001b[32m[04/13 14:31:42 d2.utils.events]: \u001b[0m eta: 1 day, 15:36:40  iter: 799  total_loss: 1.61  loss_cls: 0.9839  loss_box_reg: 0.4169  loss_query: 0.05194  time: 2.3706  data_time: 0.0071  lr: 0.0047952  max_mem: 10549M\n",
            "\u001b[32m[04/13 14:32:32 d2.utils.events]: \u001b[0m eta: 1 day, 15:35:22  iter: 819  total_loss: 1.327  loss_cls: 0.8679  loss_box_reg: 0.4288  loss_query: 0.05409  time: 2.3708  data_time: 0.0066  lr: 0.0049151  max_mem: 10549M\n",
            "\u001b[32m[04/13 14:33:51 d2.utils.events]: \u001b[0m eta: 1 day, 15:35:33  iter: 839  total_loss: 0.7514  loss_cls: 0.4839  loss_box_reg: 0.2649  loss_query: 0.04139  time: 2.4060  data_time: 1.3502  lr: 0.005035  max_mem: 10549M\n",
            "\u001b[32m[04/13 14:34:40 d2.utils.events]: \u001b[0m eta: 1 day, 15:34:34  iter: 859  total_loss: 1.234  loss_cls: 0.8249  loss_box_reg: 0.3923  loss_query: 0.0442  time: 2.4054  data_time: 0.0066  lr: 0.0051548  max_mem: 10549M\n",
            "\u001b[32m[04/13 14:35:30 d2.utils.events]: \u001b[0m eta: 1 day, 15:34:17  iter: 879  total_loss: 1.445  loss_cls: 0.8881  loss_box_reg: 0.4648  loss_query: 0.0601  time: 2.4051  data_time: 0.0095  lr: 0.0052747  max_mem: 10549M\n",
            "\u001b[32m[04/13 14:36:21 d2.utils.events]: \u001b[0m eta: 1 day, 15:33:43  iter: 899  total_loss: 1.435  loss_cls: 0.935  loss_box_reg: 0.4372  loss_query: 0.08017  time: 2.4058  data_time: 0.0076  lr: 0.0053946  max_mem: 10549M\n",
            "\u001b[32m[04/13 14:37:12 d2.utils.events]: \u001b[0m eta: 1 day, 15:32:55  iter: 919  total_loss: 1.761  loss_cls: 1.204  loss_box_reg: 0.4761  loss_query: 0.07217  time: 2.4060  data_time: 0.0065  lr: 0.0055145  max_mem: 10549M\n",
            "\u001b[32m[04/13 14:38:01 d2.utils.events]: \u001b[0m eta: 1 day, 15:32:04  iter: 939  total_loss: 1.457  loss_cls: 0.9669  loss_box_reg: 0.3947  loss_query: 0.05575  time: 2.4052  data_time: 0.0080  lr: 0.0056344  max_mem: 10549M\n",
            "\u001b[32m[04/13 14:38:50 d2.utils.events]: \u001b[0m eta: 1 day, 15:31:16  iter: 959  total_loss: 1.372  loss_cls: 0.9402  loss_box_reg: 0.4027  loss_query: 0.04942  time: 2.4047  data_time: 0.0079  lr: 0.0057542  max_mem: 10549M\n",
            "\u001b[32m[04/13 14:39:42 d2.utils.events]: \u001b[0m eta: 1 day, 15:30:30  iter: 979  total_loss: 1.184  loss_cls: 0.7695  loss_box_reg: 0.3112  loss_query: 0.05389  time: 2.4046  data_time: 0.0068  lr: 0.0058741  max_mem: 10549M\n",
            "\u001b[32m[04/13 14:40:33 d2.utils.events]: \u001b[0m eta: 1 day, 15:30:34  iter: 999  total_loss: 1.682  loss_cls: 1.149  loss_box_reg: 0.5079  loss_query: 0.06441  time: 2.4051  data_time: 0.0069  lr: 0.005994  max_mem: 10549M\n",
            "\u001b[32m[04/13 14:41:23 d2.utils.events]: \u001b[0m eta: 1 day, 15:32:31  iter: 1019  total_loss: 1.445  loss_cls: 0.9076  loss_box_reg: 0.464  loss_query: 0.05268  time: 2.4039  data_time: 0.0091  lr: 0.006  max_mem: 10549M\n",
            "\u001b[32m[04/13 14:42:14 d2.utils.events]: \u001b[0m eta: 1 day, 15:32:37  iter: 1039  total_loss: 1.647  loss_cls: 1.077  loss_box_reg: 0.4981  loss_query: 0.07251  time: 2.4047  data_time: 0.0078  lr: 0.006  max_mem: 10549M\n",
            "\u001b[32m[04/13 14:43:03 d2.utils.events]: \u001b[0m eta: 1 day, 15:32:35  iter: 1059  total_loss: 1.789  loss_cls: 1.251  loss_box_reg: 0.4682  loss_query: 0.06783  time: 2.4037  data_time: 0.0064  lr: 0.006  max_mem: 10549M\n",
            "\u001b[32m[04/13 14:43:53 d2.utils.events]: \u001b[0m eta: 1 day, 15:32:26  iter: 1079  total_loss: 1.06  loss_cls: 0.7132  loss_box_reg: 0.3474  loss_query: 0.04689  time: 2.4037  data_time: 0.0080  lr: 0.006  max_mem: 10549M\n",
            "\u001b[32m[04/13 14:44:43 d2.utils.events]: \u001b[0m eta: 1 day, 15:33:28  iter: 1099  total_loss: 1.28  loss_cls: 0.83  loss_box_reg: 0.4381  loss_query: 0.05933  time: 2.4030  data_time: 0.0066  lr: 0.006  max_mem: 10549M\n",
            "\u001b[32m[04/13 14:45:35 d2.utils.events]: \u001b[0m eta: 1 day, 15:34:50  iter: 1119  total_loss: 1.197  loss_cls: 0.7842  loss_box_reg: 0.3521  loss_query: 0.04338  time: 2.4032  data_time: 0.0068  lr: 0.006  max_mem: 10549M\n",
            "\u001b[32m[04/13 14:46:27 d2.utils.events]: \u001b[0m eta: 1 day, 15:35:37  iter: 1139  total_loss: 1.414  loss_cls: 0.9287  loss_box_reg: 0.3665  loss_query: 0.07034  time: 2.4036  data_time: 0.0082  lr: 0.006  max_mem: 10549M\n",
            "\u001b[32m[04/13 14:47:17 d2.utils.events]: \u001b[0m eta: 1 day, 15:35:11  iter: 1159  total_loss: 1.286  loss_cls: 0.8268  loss_box_reg: 0.3986  loss_query: 0.09836  time: 2.4039  data_time: 0.0083  lr: 0.006  max_mem: 10549M\n",
            "\u001b[32m[04/13 14:48:07 d2.utils.events]: \u001b[0m eta: 1 day, 15:34:40  iter: 1179  total_loss: 1.045  loss_cls: 0.6851  loss_box_reg: 0.3021  loss_query: 0.07361  time: 2.4039  data_time: 0.0063  lr: 0.006  max_mem: 10549M\n",
            "\u001b[32m[04/13 14:48:56 d2.utils.events]: \u001b[0m eta: 1 day, 15:32:37  iter: 1199  total_loss: 0.8929  loss_cls: 0.5816  loss_box_reg: 0.2678  loss_query: 0.04201  time: 2.4031  data_time: 0.0103  lr: 0.006  max_mem: 10549M\n",
            "\u001b[32m[04/13 14:49:46 d2.utils.events]: \u001b[0m eta: 1 day, 15:30:48  iter: 1219  total_loss: 1.358  loss_cls: 0.8502  loss_box_reg: 0.4111  loss_query: 0.04623  time: 2.4034  data_time: 0.0078  lr: 0.006  max_mem: 10549M\n",
            "\u001b[32m[04/13 14:50:36 d2.utils.events]: \u001b[0m eta: 1 day, 15:30:54  iter: 1239  total_loss: 1.375  loss_cls: 0.8659  loss_box_reg: 0.4393  loss_query: 0.04855  time: 2.4032  data_time: 0.0076  lr: 0.006  max_mem: 10549M\n",
            "\u001b[32m[04/13 14:51:26 d2.utils.events]: \u001b[0m eta: 1 day, 15:30:06  iter: 1259  total_loss: 1.064  loss_cls: 0.7055  loss_box_reg: 0.3441  loss_query: 0.0419  time: 2.4033  data_time: 0.0084  lr: 0.006  max_mem: 10549M\n",
            "\u001b[32m[04/13 14:52:15 d2.utils.events]: \u001b[0m eta: 1 day, 15:29:40  iter: 1279  total_loss: 1.323  loss_cls: 0.819  loss_box_reg: 0.4374  loss_query: 0.0431  time: 2.4033  data_time: 0.0061  lr: 0.006  max_mem: 10549M\n",
            "\u001b[32m[04/13 14:53:05 d2.utils.events]: \u001b[0m eta: 1 day, 15:28:35  iter: 1299  total_loss: 1.057  loss_cls: 0.6911  loss_box_reg: 0.3128  loss_query: 0.05533  time: 2.4027  data_time: 0.0068  lr: 0.006  max_mem: 10549M\n",
            "\u001b[32m[04/13 14:53:52 d2.utils.events]: \u001b[0m eta: 1 day, 15:26:43  iter: 1319  total_loss: 1.452  loss_cls: 0.8998  loss_box_reg: 0.4693  loss_query: 0.0522  time: 2.4007  data_time: 0.0063  lr: 0.006  max_mem: 10549M\n",
            "\u001b[32m[04/13 14:54:41 d2.utils.events]: \u001b[0m eta: 1 day, 15:26:52  iter: 1339  total_loss: 1.034  loss_cls: 0.6668  loss_box_reg: 0.3473  loss_query: 0.05595  time: 2.3999  data_time: 0.0070  lr: 0.006  max_mem: 10549M\n",
            "\u001b[32m[04/13 14:55:28 d2.utils.events]: \u001b[0m eta: 1 day, 15:21:59  iter: 1359  total_loss: 1.011  loss_cls: 0.6431  loss_box_reg: 0.3559  loss_query: 0.04641  time: 2.3981  data_time: 0.0082  lr: 0.006  max_mem: 10549M\n",
            "\u001b[32m[04/13 14:56:19 d2.utils.events]: \u001b[0m eta: 1 day, 15:20:48  iter: 1379  total_loss: 0.9426  loss_cls: 0.5958  loss_box_reg: 0.3083  loss_query: 0.03933  time: 2.3987  data_time: 0.0091  lr: 0.006  max_mem: 10549M\n",
            "\u001b[32m[04/13 14:57:09 d2.utils.events]: \u001b[0m eta: 1 day, 15:20:00  iter: 1399  total_loss: 1.48  loss_cls: 0.914  loss_box_reg: 0.4852  loss_query: 0.06342  time: 2.3984  data_time: 0.0057  lr: 0.006  max_mem: 10549M\n",
            "\u001b[32m[04/13 14:57:58 d2.utils.events]: \u001b[0m eta: 1 day, 15:19:54  iter: 1419  total_loss: 1.12  loss_cls: 0.6773  loss_box_reg: 0.3646  loss_query: 0.07081  time: 2.3982  data_time: 0.0075  lr: 0.006  max_mem: 10549M\n",
            "\u001b[32m[04/13 14:58:47 d2.utils.events]: \u001b[0m eta: 1 day, 15:17:41  iter: 1439  total_loss: 1.341  loss_cls: 0.8749  loss_box_reg: 0.443  loss_query: 0.05039  time: 2.3972  data_time: 0.0067  lr: 0.006  max_mem: 10549M\n",
            "\u001b[32m[04/13 14:59:36 d2.utils.events]: \u001b[0m eta: 1 day, 15:17:01  iter: 1459  total_loss: 1.467  loss_cls: 0.9543  loss_box_reg: 0.4676  loss_query: 0.05118  time: 2.3969  data_time: 0.0065  lr: 0.006  max_mem: 10549M\n",
            "\u001b[32m[04/13 15:00:24 d2.utils.events]: \u001b[0m eta: 1 day, 15:15:41  iter: 1479  total_loss: 1.195  loss_cls: 0.7531  loss_box_reg: 0.3816  loss_query: 0.04732  time: 2.3962  data_time: 0.0057  lr: 0.006  max_mem: 10549M\n",
            "\u001b[32m[04/13 15:01:25 d2.utils.events]: \u001b[0m eta: 1 day, 15:15:16  iter: 1499  total_loss: 1.551  loss_cls: 0.9439  loss_box_reg: 0.5016  loss_query: 0.0554  time: 2.4036  data_time: 0.5695  lr: 0.006  max_mem: 10549M\n",
            "\u001b[32m[04/13 15:02:15 d2.utils.events]: \u001b[0m eta: 1 day, 15:13:36  iter: 1519  total_loss: 1.238  loss_cls: 0.7995  loss_box_reg: 0.358  loss_query: 0.04024  time: 2.4033  data_time: 0.0071  lr: 0.006  max_mem: 10549M\n",
            "\u001b[32m[04/13 15:03:06 d2.utils.events]: \u001b[0m eta: 1 day, 15:12:56  iter: 1539  total_loss: 1.344  loss_cls: 0.8879  loss_box_reg: 0.4256  loss_query: 0.05251  time: 2.4027  data_time: 0.0064  lr: 0.006  max_mem: 10549M\n",
            "\u001b[32m[04/13 15:03:57 d2.utils.events]: \u001b[0m eta: 1 day, 15:11:27  iter: 1559  total_loss: 1.122  loss_cls: 0.7344  loss_box_reg: 0.3255  loss_query: 0.04598  time: 2.4028  data_time: 0.0069  lr: 0.006  max_mem: 10549M\n",
            "\u001b[32m[04/13 15:04:46 d2.utils.events]: \u001b[0m eta: 1 day, 15:12:05  iter: 1579  total_loss: 1.199  loss_cls: 0.815  loss_box_reg: 0.365  loss_query: 0.06401  time: 2.4023  data_time: 0.0072  lr: 0.006  max_mem: 10549M\n",
            "\u001b[32m[04/13 15:05:35 d2.utils.events]: \u001b[0m eta: 1 day, 15:09:46  iter: 1599  total_loss: 1.036  loss_cls: 0.6742  loss_box_reg: 0.3198  loss_query: 0.05142  time: 2.4016  data_time: 0.0073  lr: 0.006  max_mem: 10549M\n",
            "\u001b[32m[04/13 15:06:26 d2.utils.events]: \u001b[0m eta: 1 day, 15:09:13  iter: 1619  total_loss: 1.009  loss_cls: 0.6407  loss_box_reg: 0.3396  loss_query: 0.04149  time: 2.4018  data_time: 0.0071  lr: 0.006  max_mem: 10549M\n",
            "\u001b[32m[04/13 15:07:15 d2.utils.events]: \u001b[0m eta: 1 day, 15:07:29  iter: 1639  total_loss: 1.345  loss_cls: 0.9103  loss_box_reg: 0.414  loss_query: 0.04447  time: 2.4012  data_time: 0.0078  lr: 0.006  max_mem: 10549M\n",
            "\u001b[32m[04/13 15:08:04 d2.utils.events]: \u001b[0m eta: 1 day, 15:06:23  iter: 1659  total_loss: 0.9777  loss_cls: 0.6409  loss_box_reg: 0.2817  loss_query: 0.0523  time: 2.4005  data_time: 0.0065  lr: 0.006  max_mem: 10549M\n",
            "\u001b[32m[04/13 15:08:54 d2.utils.events]: \u001b[0m eta: 1 day, 15:05:35  iter: 1679  total_loss: 0.9504  loss_cls: 0.614  loss_box_reg: 0.2919  loss_query: 0.04145  time: 2.4010  data_time: 0.0078  lr: 0.006  max_mem: 10549M\n",
            "\u001b[32m[04/13 15:09:44 d2.utils.events]: \u001b[0m eta: 1 day, 15:04:00  iter: 1699  total_loss: 1.147  loss_cls: 0.7611  loss_box_reg: 0.3524  loss_query: 0.03575  time: 2.4008  data_time: 0.0065  lr: 0.006  max_mem: 10549M\n",
            "\u001b[32m[04/13 15:10:33 d2.utils.events]: \u001b[0m eta: 1 day, 15:03:58  iter: 1719  total_loss: 1.095  loss_cls: 0.7182  loss_box_reg: 0.378  loss_query: 0.03371  time: 2.4000  data_time: 0.0069  lr: 0.006  max_mem: 10549M\n",
            "\u001b[32m[04/13 15:11:21 d2.utils.events]: \u001b[0m eta: 1 day, 15:02:37  iter: 1739  total_loss: 1.218  loss_cls: 0.7877  loss_box_reg: 0.3609  loss_query: 0.02275  time: 2.3994  data_time: 0.0071  lr: 0.006  max_mem: 10549M\n",
            "\u001b[32m[04/13 15:12:11 d2.utils.events]: \u001b[0m eta: 1 day, 15:01:06  iter: 1759  total_loss: 1.449  loss_cls: 0.9464  loss_box_reg: 0.4424  loss_query: 0.04426  time: 2.3994  data_time: 0.0072  lr: 0.006  max_mem: 10549M\n",
            "\u001b[32m[04/13 15:13:03 d2.utils.events]: \u001b[0m eta: 1 day, 15:01:33  iter: 1779  total_loss: 1.332  loss_cls: 0.8361  loss_box_reg: 0.4572  loss_query: 0.05996  time: 2.4000  data_time: 0.0068  lr: 0.006  max_mem: 10549M\n",
            "\u001b[32m[04/13 15:13:51 d2.utils.events]: \u001b[0m eta: 1 day, 15:01:32  iter: 1799  total_loss: 0.8806  loss_cls: 0.5646  loss_box_reg: 0.2715  loss_query: 0.06179  time: 2.3993  data_time: 0.0068  lr: 0.006  max_mem: 10549M\n",
            "\u001b[32m[04/13 15:14:41 d2.utils.events]: \u001b[0m eta: 1 day, 15:01:20  iter: 1819  total_loss: 0.9763  loss_cls: 0.5819  loss_box_reg: 0.3628  loss_query: 0.04759  time: 2.3991  data_time: 0.0068  lr: 0.006  max_mem: 10549M\n",
            "\u001b[32m[04/13 15:15:30 d2.utils.events]: \u001b[0m eta: 1 day, 14:58:35  iter: 1839  total_loss: 0.9257  loss_cls: 0.5784  loss_box_reg: 0.272  loss_query: 0.0368  time: 2.3987  data_time: 0.0083  lr: 0.006  max_mem: 10549M\n",
            "\u001b[32m[04/13 15:16:18 d2.utils.events]: \u001b[0m eta: 1 day, 14:56:44  iter: 1859  total_loss: 1.062  loss_cls: 0.6462  loss_box_reg: 0.3811  loss_query: 0.04055  time: 2.3979  data_time: 0.0078  lr: 0.006  max_mem: 10549M\n",
            "\u001b[32m[04/13 15:17:07 d2.utils.events]: \u001b[0m eta: 1 day, 14:54:46  iter: 1879  total_loss: 1.194  loss_cls: 0.7546  loss_box_reg: 0.3975  loss_query: 0.05095  time: 2.3974  data_time: 0.0061  lr: 0.006  max_mem: 10549M\n",
            "\u001b[32m[04/13 15:17:56 d2.utils.events]: \u001b[0m eta: 1 day, 14:52:28  iter: 1899  total_loss: 1.22  loss_cls: 0.742  loss_box_reg: 0.3113  loss_query: 0.03741  time: 2.3969  data_time: 0.0080  lr: 0.006  max_mem: 10549M\n",
            "\u001b[32m[04/13 15:18:45 d2.utils.events]: \u001b[0m eta: 1 day, 14:51:35  iter: 1919  total_loss: 1.204  loss_cls: 0.7985  loss_box_reg: 0.3813  loss_query: 0.04068  time: 2.3965  data_time: 0.0079  lr: 0.006  max_mem: 10549M\n",
            "\u001b[32m[04/13 15:19:36 d2.utils.events]: \u001b[0m eta: 1 day, 14:52:11  iter: 1939  total_loss: 1.17  loss_cls: 0.7087  loss_box_reg: 0.3676  loss_query: 0.08501  time: 2.3969  data_time: 0.0083  lr: 0.006  max_mem: 10549M\n",
            "\u001b[32m[04/13 15:20:24 d2.utils.events]: \u001b[0m eta: 1 day, 14:50:37  iter: 1959  total_loss: 0.9464  loss_cls: 0.5988  loss_box_reg: 0.3105  loss_query: 0.04215  time: 2.3962  data_time: 0.0071  lr: 0.006  max_mem: 10549M\n",
            "\u001b[32m[04/13 15:21:16 d2.utils.events]: \u001b[0m eta: 1 day, 14:50:21  iter: 1979  total_loss: 1.021  loss_cls: 0.6458  loss_box_reg: 0.3356  loss_query: 0.04483  time: 2.3965  data_time: 0.0071  lr: 0.006  max_mem: 10549M\n",
            "\u001b[32m[04/13 15:22:04 fvcore.common.checkpoint]: \u001b[0mSaving checkpoint to work_dirs/visdrone_querydet/model_0001999.pth\n",
            "\u001b[32m[04/13 15:22:22 d2.utils.events]: \u001b[0m eta: 1 day, 14:48:14  iter: 1999  total_loss: 1.003  loss_cls: 0.6287  loss_box_reg: 0.3516  loss_query: 0.03839  time: 2.3956  data_time: 0.0064  lr: 0.006  max_mem: 10549M\n",
            "\u001b[32m[04/13 15:23:12 d2.utils.events]: \u001b[0m eta: 1 day, 14:47:26  iter: 2019  total_loss: 1.132  loss_cls: 0.6976  loss_box_reg: 0.3669  loss_query: 0.04744  time: 2.3958  data_time: 0.0069  lr: 0.006  max_mem: 10549M\n",
            "\u001b[32m[04/13 15:24:04 d2.utils.events]: \u001b[0m eta: 1 day, 14:46:35  iter: 2039  total_loss: 1.03  loss_cls: 0.6739  loss_box_reg: 0.3302  loss_query: 0.04465  time: 2.3959  data_time: 0.0064  lr: 0.006  max_mem: 10549M\n",
            "\u001b[32m[04/13 15:24:54 d2.utils.events]: \u001b[0m eta: 1 day, 14:45:44  iter: 2059  total_loss: 0.8707  loss_cls: 0.5711  loss_box_reg: 0.2508  loss_query: 0.04083  time: 2.3959  data_time: 0.0079  lr: 0.006  max_mem: 10549M\n",
            "\u001b[32m[04/13 15:25:42 d2.utils.events]: \u001b[0m eta: 1 day, 14:44:55  iter: 2079  total_loss: 0.9764  loss_cls: 0.6351  loss_box_reg: 0.3112  loss_query: 0.0443  time: 2.3952  data_time: 0.0061  lr: 0.006  max_mem: 10549M\n",
            "\u001b[32m[04/13 15:26:30 d2.utils.events]: \u001b[0m eta: 1 day, 14:43:20  iter: 2099  total_loss: 1.104  loss_cls: 0.693  loss_box_reg: 0.3522  loss_query: 0.04568  time: 2.3945  data_time: 0.0079  lr: 0.006  max_mem: 10549M\n",
            "\u001b[32m[04/13 15:27:20 d2.utils.events]: \u001b[0m eta: 1 day, 14:42:28  iter: 2119  total_loss: 1.39  loss_cls: 0.8177  loss_box_reg: 0.4657  loss_query: 0.07233  time: 2.3941  data_time: 0.0065  lr: 0.006  max_mem: 10549M\n",
            "\u001b[32m[04/13 15:28:10 d2.utils.events]: \u001b[0m eta: 1 day, 14:41:19  iter: 2139  total_loss: 0.8912  loss_cls: 0.5647  loss_box_reg: 0.2901  loss_query: 0.04054  time: 2.3939  data_time: 0.0078  lr: 0.006  max_mem: 10549M\n",
            "\u001b[32m[04/13 15:28:58 d2.utils.events]: \u001b[0m eta: 1 day, 14:40:03  iter: 2159  total_loss: 1.365  loss_cls: 0.8199  loss_box_reg: 0.4651  loss_query: 0.04519  time: 2.3932  data_time: 0.0061  lr: 0.006  max_mem: 10549M\n",
            "\u001b[32m[04/13 15:29:47 d2.utils.events]: \u001b[0m eta: 1 day, 14:39:27  iter: 2179  total_loss: 1.265  loss_cls: 0.7814  loss_box_reg: 0.3993  loss_query: 0.05891  time: 2.3929  data_time: 0.0071  lr: 0.006  max_mem: 10549M\n",
            "\u001b[32m[04/13 15:30:36 d2.utils.events]: \u001b[0m eta: 1 day, 14:38:39  iter: 2199  total_loss: 1.096  loss_cls: 0.7261  loss_box_reg: 0.3215  loss_query: 0.038  time: 2.3924  data_time: 0.0066  lr: 0.006  max_mem: 10549M\n",
            "\u001b[32m[04/13 15:31:25 d2.utils.events]: \u001b[0m eta: 1 day, 14:37:51  iter: 2219  total_loss: 1.246  loss_cls: 0.8095  loss_box_reg: 0.4278  loss_query: 0.06328  time: 2.3923  data_time: 0.0072  lr: 0.006  max_mem: 10549M\n",
            "\u001b[32m[04/13 15:32:14 d2.utils.events]: \u001b[0m eta: 1 day, 14:36:20  iter: 2239  total_loss: 0.8342  loss_cls: 0.5475  loss_box_reg: 0.2204  loss_query: 0.03144  time: 2.3920  data_time: 0.0064  lr: 0.006  max_mem: 10549M\n",
            "\u001b[32m[04/13 15:33:03 d2.utils.events]: \u001b[0m eta: 1 day, 14:35:05  iter: 2259  total_loss: 1.04  loss_cls: 0.6598  loss_box_reg: 0.3801  loss_query: 0.03455  time: 2.3914  data_time: 0.0062  lr: 0.006  max_mem: 10549M\n",
            "\u001b[32m[04/13 15:33:53 d2.utils.events]: \u001b[0m eta: 1 day, 14:33:33  iter: 2279  total_loss: 1.074  loss_cls: 0.6186  loss_box_reg: 0.3684  loss_query: 0.04887  time: 2.3911  data_time: 0.0073  lr: 0.006  max_mem: 10549M\n",
            "\u001b[32m[04/13 15:34:41 d2.utils.events]: \u001b[0m eta: 1 day, 14:32:45  iter: 2299  total_loss: 1.074  loss_cls: 0.6555  loss_box_reg: 0.348  loss_query: 0.03899  time: 2.3904  data_time: 0.0072  lr: 0.006  max_mem: 10549M\n",
            "\u001b[32m[04/13 15:35:30 d2.utils.events]: \u001b[0m eta: 1 day, 14:31:28  iter: 2319  total_loss: 1.06  loss_cls: 0.653  loss_box_reg: 0.3709  loss_query: 0.04481  time: 2.3901  data_time: 0.0086  lr: 0.006  max_mem: 10549M\n",
            "\u001b[32m[04/13 15:36:20 d2.utils.events]: \u001b[0m eta: 1 day, 14:30:40  iter: 2339  total_loss: 1.028  loss_cls: 0.6369  loss_box_reg: 0.3465  loss_query: 0.03321  time: 2.3901  data_time: 0.0064  lr: 0.006  max_mem: 10549M\n",
            "\u001b[32m[04/13 15:37:11 d2.utils.events]: \u001b[0m eta: 1 day, 14:30:47  iter: 2359  total_loss: 1.23  loss_cls: 0.7222  loss_box_reg: 0.4152  loss_query: 0.03613  time: 2.3900  data_time: 0.0084  lr: 0.006  max_mem: 10549M\n",
            "\u001b[32m[04/13 15:38:00 d2.utils.events]: \u001b[0m eta: 1 day, 14:29:04  iter: 2379  total_loss: 1.175  loss_cls: 0.7088  loss_box_reg: 0.3833  loss_query: 0.04689  time: 2.3893  data_time: 0.0078  lr: 0.006  max_mem: 10549M\n",
            "\u001b[32m[04/13 15:38:50 d2.utils.events]: \u001b[0m eta: 1 day, 14:28:45  iter: 2399  total_loss: 0.8936  loss_cls: 0.5866  loss_box_reg: 0.2972  loss_query: 0.03634  time: 2.3895  data_time: 0.0083  lr: 0.006  max_mem: 10549M\n",
            "\u001b[32m[04/13 15:39:38 d2.utils.events]: \u001b[0m eta: 1 day, 14:27:57  iter: 2419  total_loss: 0.8749  loss_cls: 0.5495  loss_box_reg: 0.3031  loss_query: 0.04138  time: 2.3889  data_time: 0.0081  lr: 0.006  max_mem: 10549M\n",
            "\u001b[32m[04/13 15:40:27 d2.utils.events]: \u001b[0m eta: 1 day, 14:27:35  iter: 2439  total_loss: 0.8888  loss_cls: 0.5238  loss_box_reg: 0.2969  loss_query: 0.03604  time: 2.3886  data_time: 0.0083  lr: 0.006  max_mem: 10549M\n",
            "\u001b[32m[04/13 15:41:17 d2.utils.events]: \u001b[0m eta: 1 day, 14:26:21  iter: 2459  total_loss: 1.057  loss_cls: 0.6958  loss_box_reg: 0.3153  loss_query: 0.04278  time: 2.3887  data_time: 0.0061  lr: 0.006  max_mem: 10549M\n",
            "\u001b[32m[04/13 15:42:05 d2.utils.events]: \u001b[0m eta: 1 day, 14:25:33  iter: 2479  total_loss: 1.011  loss_cls: 0.6337  loss_box_reg: 0.3284  loss_query: 0.03834  time: 2.3881  data_time: 0.0068  lr: 0.006  max_mem: 10549M\n",
            "\u001b[32m[04/13 15:42:54 d2.utils.events]: \u001b[0m eta: 1 day, 14:23:45  iter: 2499  total_loss: 1.053  loss_cls: 0.6445  loss_box_reg: 0.3449  loss_query: 0.04749  time: 2.3876  data_time: 0.0065  lr: 0.006  max_mem: 10549M\n",
            "\u001b[32m[04/13 15:43:42 d2.utils.events]: \u001b[0m eta: 1 day, 14:21:13  iter: 2519  total_loss: 0.9813  loss_cls: 0.6283  loss_box_reg: 0.2915  loss_query: 0.05806  time: 2.3870  data_time: 0.0077  lr: 0.006  max_mem: 10549M\n",
            "\u001b[32m[04/13 15:44:31 d2.utils.events]: \u001b[0m eta: 1 day, 14:20:09  iter: 2539  total_loss: 1.081  loss_cls: 0.6482  loss_box_reg: 0.3259  loss_query: 0.06432  time: 2.3869  data_time: 0.0079  lr: 0.006  max_mem: 10549M\n",
            "\u001b[32m[04/13 15:45:22 d2.utils.events]: \u001b[0m eta: 1 day, 14:19:31  iter: 2559  total_loss: 1.37  loss_cls: 0.8304  loss_box_reg: 0.4378  loss_query: 0.04526  time: 2.3872  data_time: 0.0072  lr: 0.006  max_mem: 10549M\n",
            "\u001b[32m[04/13 15:46:13 d2.utils.events]: \u001b[0m eta: 1 day, 14:17:53  iter: 2579  total_loss: 1.082  loss_cls: 0.6889  loss_box_reg: 0.3331  loss_query: 0.04523  time: 2.3873  data_time: 0.0063  lr: 0.006  max_mem: 10549M\n",
            "\u001b[32m[04/13 15:47:02 d2.utils.events]: \u001b[0m eta: 1 day, 14:18:00  iter: 2599  total_loss: 1.035  loss_cls: 0.6387  loss_box_reg: 0.3497  loss_query: 0.04202  time: 2.3870  data_time: 0.0072  lr: 0.006  max_mem: 10549M\n",
            "\u001b[32m[04/13 15:47:51 d2.utils.events]: \u001b[0m eta: 1 day, 14:17:06  iter: 2619  total_loss: 1.031  loss_cls: 0.6521  loss_box_reg: 0.3225  loss_query: 0.05068  time: 2.3866  data_time: 0.0069  lr: 0.006  max_mem: 10549M\n",
            "\u001b[32m[04/13 15:48:42 d2.utils.events]: \u001b[0m eta: 1 day, 14:16:42  iter: 2639  total_loss: 1.071  loss_cls: 0.6878  loss_box_reg: 0.3595  loss_query: 0.04721  time: 2.3871  data_time: 0.0074  lr: 0.006  max_mem: 10549M\n",
            "\u001b[32m[04/13 15:49:31 d2.utils.events]: \u001b[0m eta: 1 day, 14:15:36  iter: 2659  total_loss: 0.911  loss_cls: 0.5609  loss_box_reg: 0.2604  loss_query: 0.03251  time: 2.3864  data_time: 0.0066  lr: 0.006  max_mem: 10549M\n",
            "\u001b[32m[04/13 15:50:21 d2.utils.events]: \u001b[0m eta: 1 day, 14:14:55  iter: 2679  total_loss: 1.045  loss_cls: 0.6752  loss_box_reg: 0.3047  loss_query: 0.04237  time: 2.3863  data_time: 0.0084  lr: 0.006  max_mem: 10549M\n",
            "\u001b[32m[04/13 15:51:12 d2.utils.events]: \u001b[0m eta: 1 day, 14:14:31  iter: 2699  total_loss: 0.9294  loss_cls: 0.6178  loss_box_reg: 0.2784  loss_query: 0.03397  time: 2.3863  data_time: 0.0084  lr: 0.006  max_mem: 10549M\n",
            "\u001b[32m[04/13 15:52:03 d2.utils.events]: \u001b[0m eta: 1 day, 14:15:05  iter: 2719  total_loss: 1.458  loss_cls: 0.8566  loss_box_reg: 0.4896  loss_query: 0.06953  time: 2.3865  data_time: 0.0082  lr: 0.006  max_mem: 10549M\n",
            "\u001b[32m[04/13 15:52:54 d2.utils.events]: \u001b[0m eta: 1 day, 14:16:02  iter: 2739  total_loss: 0.7685  loss_cls: 0.4797  loss_box_reg: 0.2245  loss_query: 0.04039  time: 2.3864  data_time: 0.0063  lr: 0.006  max_mem: 10549M\n",
            "\u001b[32m[04/13 15:53:43 d2.utils.events]: \u001b[0m eta: 1 day, 14:14:33  iter: 2759  total_loss: 0.9832  loss_cls: 0.5941  loss_box_reg: 0.3254  loss_query: 0.03739  time: 2.3861  data_time: 0.0069  lr: 0.006  max_mem: 10549M\n",
            "\u001b[32m[04/13 15:54:33 d2.utils.events]: \u001b[0m eta: 1 day, 14:12:51  iter: 2779  total_loss: 0.9761  loss_cls: 0.5985  loss_box_reg: 0.3144  loss_query: 0.03031  time: 2.3860  data_time: 0.0081  lr: 0.006  max_mem: 10549M\n",
            "\u001b[32m[04/13 15:55:23 d2.utils.events]: \u001b[0m eta: 1 day, 14:12:57  iter: 2799  total_loss: 1.244  loss_cls: 0.7657  loss_box_reg: 0.4055  loss_query: 0.04187  time: 2.3862  data_time: 0.0071  lr: 0.006  max_mem: 10549M\n",
            "\u001b[32m[04/13 15:56:14 d2.utils.events]: \u001b[0m eta: 1 day, 14:11:15  iter: 2819  total_loss: 1.083  loss_cls: 0.7071  loss_box_reg: 0.3567  loss_query: 0.03283  time: 2.3861  data_time: 0.0067  lr: 0.006  max_mem: 10549M\n",
            "\u001b[32m[04/13 15:57:05 d2.utils.events]: \u001b[0m eta: 1 day, 14:11:32  iter: 2839  total_loss: 0.9806  loss_cls: 0.6031  loss_box_reg: 0.322  loss_query: 0.04102  time: 2.3863  data_time: 0.0072  lr: 0.006  max_mem: 10549M\n",
            "\u001b[32m[04/13 15:57:57 d2.utils.events]: \u001b[0m eta: 1 day, 14:11:44  iter: 2859  total_loss: 1.297  loss_cls: 0.7315  loss_box_reg: 0.4437  loss_query: 0.06004  time: 2.3866  data_time: 0.0081  lr: 0.006  max_mem: 10549M\n",
            "\u001b[32m[04/13 15:58:45 d2.utils.events]: \u001b[0m eta: 1 day, 14:11:09  iter: 2879  total_loss: 0.8564  loss_cls: 0.5347  loss_box_reg: 0.2403  loss_query: 0.05548  time: 2.3861  data_time: 0.0076  lr: 0.006  max_mem: 10549M\n",
            "\u001b[32m[04/13 15:59:37 d2.utils.events]: \u001b[0m eta: 1 day, 14:10:21  iter: 2899  total_loss: 0.9985  loss_cls: 0.6294  loss_box_reg: 0.3811  loss_query: 0.05378  time: 2.3864  data_time: 0.0077  lr: 0.006  max_mem: 10549M\n",
            "\u001b[32m[04/13 16:00:28 d2.utils.events]: \u001b[0m eta: 1 day, 14:10:16  iter: 2919  total_loss: 0.9453  loss_cls: 0.5723  loss_box_reg: 0.3192  loss_query: 0.05273  time: 2.3863  data_time: 0.0074  lr: 0.006  max_mem: 10549M\n",
            "\u001b[32m[04/13 16:01:18 d2.utils.events]: \u001b[0m eta: 1 day, 14:08:38  iter: 2939  total_loss: 0.7909  loss_cls: 0.5589  loss_box_reg: 0.2166  loss_query: 0.03114  time: 2.3863  data_time: 0.0082  lr: 0.006  max_mem: 10549M\n",
            "\u001b[32m[04/13 16:02:09 d2.utils.events]: \u001b[0m eta: 1 day, 14:08:40  iter: 2959  total_loss: 1.025  loss_cls: 0.6609  loss_box_reg: 0.3079  loss_query: 0.02763  time: 2.3863  data_time: 0.0065  lr: 0.006  max_mem: 10549M\n",
            "\u001b[32m[04/13 16:03:00 d2.utils.events]: \u001b[0m eta: 1 day, 14:08:08  iter: 2979  total_loss: 1.124  loss_cls: 0.6813  loss_box_reg: 0.395  loss_query: 0.0469  time: 2.3868  data_time: 0.0080  lr: 0.006  max_mem: 10549M\n",
            "\u001b[32m[04/13 16:03:48 d2.utils.events]: \u001b[0m eta: 1 day, 14:07:18  iter: 2999  total_loss: 0.95  loss_cls: 0.5684  loss_box_reg: 0.3369  loss_query: 0.03695  time: 2.3860  data_time: 0.0078  lr: 0.006  max_mem: 10549M\n",
            "\u001b[32m[04/13 16:04:38 d2.utils.events]: \u001b[0m eta: 1 day, 14:06:30  iter: 3019  total_loss: 0.9993  loss_cls: 0.6173  loss_box_reg: 0.3491  loss_query: 0.03959  time: 2.3860  data_time: 0.0073  lr: 0.006  max_mem: 10549M\n",
            "\u001b[32m[04/13 16:05:27 d2.utils.events]: \u001b[0m eta: 1 day, 14:05:27  iter: 3039  total_loss: 1.065  loss_cls: 0.6892  loss_box_reg: 0.3327  loss_query: 0.04138  time: 2.3858  data_time: 0.0095  lr: 0.006  max_mem: 10549M\n",
            "\u001b[32m[04/13 16:06:18 d2.utils.events]: \u001b[0m eta: 1 day, 14:04:13  iter: 3059  total_loss: 1.027  loss_cls: 0.6471  loss_box_reg: 0.3018  loss_query: 0.04757  time: 2.3855  data_time: 0.0079  lr: 0.006  max_mem: 10549M\n",
            "\u001b[32m[04/13 16:07:07 d2.utils.events]: \u001b[0m eta: 1 day, 14:03:25  iter: 3079  total_loss: 0.9619  loss_cls: 0.6213  loss_box_reg: 0.3247  loss_query: 0.04172  time: 2.3851  data_time: 0.0072  lr: 0.006  max_mem: 10549M\n",
            "\u001b[32m[04/13 16:07:58 d2.utils.events]: \u001b[0m eta: 1 day, 14:03:19  iter: 3099  total_loss: 1.136  loss_cls: 0.6683  loss_box_reg: 0.4111  loss_query: 0.04353  time: 2.3855  data_time: 0.0076  lr: 0.006  max_mem: 10549M\n",
            "\u001b[32m[04/13 16:08:48 d2.utils.events]: \u001b[0m eta: 1 day, 14:01:31  iter: 3119  total_loss: 0.7228  loss_cls: 0.487  loss_box_reg: 0.2076  loss_query: 0.02126  time: 2.3855  data_time: 0.0092  lr: 0.006  max_mem: 10549M\n",
            "\u001b[32m[04/13 16:09:37 d2.utils.events]: \u001b[0m eta: 1 day, 14:01:01  iter: 3139  total_loss: 1.03  loss_cls: 0.6349  loss_box_reg: 0.3293  loss_query: 0.0473  time: 2.3853  data_time: 0.0066  lr: 0.006  max_mem: 10549M\n",
            "\u001b[32m[04/13 16:10:27 d2.utils.events]: \u001b[0m eta: 1 day, 14:00:59  iter: 3159  total_loss: 1.17  loss_cls: 0.7413  loss_box_reg: 0.4168  loss_query: 0.05957  time: 2.3852  data_time: 0.0067  lr: 0.006  max_mem: 10549M\n",
            "\u001b[32m[04/13 16:11:18 d2.utils.events]: \u001b[0m eta: 1 day, 14:00:20  iter: 3179  total_loss: 1.111  loss_cls: 0.6489  loss_box_reg: 0.36  loss_query: 0.04788  time: 2.3852  data_time: 0.0087  lr: 0.006  max_mem: 10549M\n",
            "\u001b[32m[04/13 16:12:09 d2.utils.events]: \u001b[0m eta: 1 day, 13:59:35  iter: 3199  total_loss: 0.8625  loss_cls: 0.561  loss_box_reg: 0.2596  loss_query: 0.03991  time: 2.3851  data_time: 0.0069  lr: 0.006  max_mem: 10549M\n",
            "\u001b[32m[04/13 16:12:57 d2.utils.events]: \u001b[0m eta: 1 day, 13:58:35  iter: 3219  total_loss: 0.8851  loss_cls: 0.5697  loss_box_reg: 0.2674  loss_query: 0.03295  time: 2.3848  data_time: 0.0081  lr: 0.006  max_mem: 10549M\n",
            "\u001b[32m[04/13 16:13:48 d2.utils.events]: \u001b[0m eta: 1 day, 13:58:43  iter: 3239  total_loss: 1.1  loss_cls: 0.6585  loss_box_reg: 0.3159  loss_query: 0.05442  time: 2.3849  data_time: 0.0066  lr: 0.006  max_mem: 10549M\n",
            "\u001b[32m[04/13 16:14:37 d2.utils.events]: \u001b[0m eta: 1 day, 13:58:49  iter: 3259  total_loss: 0.8726  loss_cls: 0.5421  loss_box_reg: 0.2916  loss_query: 0.03745  time: 2.3848  data_time: 0.0083  lr: 0.006  max_mem: 10549M\n",
            "\u001b[32m[04/13 16:15:26 d2.utils.events]: \u001b[0m eta: 1 day, 13:58:03  iter: 3279  total_loss: 0.9109  loss_cls: 0.568  loss_box_reg: 0.2889  loss_query: 0.04731  time: 2.3844  data_time: 0.0065  lr: 0.006  max_mem: 10549M\n",
            "\u001b[32m[04/13 16:16:18 d2.utils.events]: \u001b[0m eta: 1 day, 13:57:31  iter: 3299  total_loss: 1.089  loss_cls: 0.6557  loss_box_reg: 0.3699  loss_query: 0.05623  time: 2.3847  data_time: 0.0075  lr: 0.006  max_mem: 10549M\n",
            "\u001b[32m[04/13 16:17:07 d2.utils.events]: \u001b[0m eta: 1 day, 13:56:58  iter: 3319  total_loss: 0.9648  loss_cls: 0.5535  loss_box_reg: 0.3472  loss_query: 0.0433  time: 2.3847  data_time: 0.0068  lr: 0.006  max_mem: 10549M\n",
            "\u001b[32m[04/13 16:17:57 d2.utils.events]: \u001b[0m eta: 1 day, 13:56:23  iter: 3339  total_loss: 1.011  loss_cls: 0.6282  loss_box_reg: 0.347  loss_query: 0.03093  time: 2.3848  data_time: 0.0072  lr: 0.006  max_mem: 10549M\n",
            "\u001b[32m[04/13 16:18:49 d2.utils.events]: \u001b[0m eta: 1 day, 13:56:02  iter: 3359  total_loss: 1.006  loss_cls: 0.6436  loss_box_reg: 0.3764  loss_query: 0.03681  time: 2.3849  data_time: 0.0078  lr: 0.006  max_mem: 10549M\n",
            "\u001b[32m[04/13 16:19:38 d2.utils.events]: \u001b[0m eta: 1 day, 13:55:06  iter: 3379  total_loss: 0.9234  loss_cls: 0.5588  loss_box_reg: 0.2753  loss_query: 0.03984  time: 2.3847  data_time: 0.0075  lr: 0.006  max_mem: 10549M\n",
            "\u001b[32m[04/13 16:20:28 d2.utils.events]: \u001b[0m eta: 1 day, 13:53:45  iter: 3399  total_loss: 0.7625  loss_cls: 0.4678  loss_box_reg: 0.2691  loss_query: 0.03125  time: 2.3845  data_time: 0.0066  lr: 0.006  max_mem: 10549M\n",
            "\u001b[32m[04/13 16:21:18 d2.utils.events]: \u001b[0m eta: 1 day, 13:53:11  iter: 3419  total_loss: 0.977  loss_cls: 0.6696  loss_box_reg: 0.296  loss_query: 0.02948  time: 2.3844  data_time: 0.0061  lr: 0.006  max_mem: 10549M\n",
            "\u001b[32m[04/13 16:22:07 d2.utils.events]: \u001b[0m eta: 1 day, 13:51:38  iter: 3439  total_loss: 1.075  loss_cls: 0.6656  loss_box_reg: 0.3335  loss_query: 0.03132  time: 2.3840  data_time: 0.0066  lr: 0.006  max_mem: 10549M\n",
            "\u001b[32m[04/13 16:22:56 d2.utils.events]: \u001b[0m eta: 1 day, 13:50:45  iter: 3459  total_loss: 1.205  loss_cls: 0.7498  loss_box_reg: 0.347  loss_query: 0.06045  time: 2.3839  data_time: 0.0065  lr: 0.006  max_mem: 10549M\n",
            "\u001b[32m[04/13 16:23:44 d2.utils.events]: \u001b[0m eta: 1 day, 13:50:05  iter: 3479  total_loss: 0.9472  loss_cls: 0.6022  loss_box_reg: 0.3036  loss_query: 0.04285  time: 2.3834  data_time: 0.0077  lr: 0.006  max_mem: 10549M\n",
            "\u001b[32m[04/13 16:24:35 d2.utils.events]: \u001b[0m eta: 1 day, 13:49:16  iter: 3499  total_loss: 0.9066  loss_cls: 0.5525  loss_box_reg: 0.2797  loss_query: 0.03707  time: 2.3835  data_time: 0.0065  lr: 0.006  max_mem: 10549M\n",
            "\u001b[32m[04/13 16:25:27 d2.utils.events]: \u001b[0m eta: 1 day, 13:49:25  iter: 3519  total_loss: 0.9062  loss_cls: 0.5925  loss_box_reg: 0.3086  loss_query: 0.05198  time: 2.3835  data_time: 0.0072  lr: 0.006  max_mem: 10549M\n",
            "\u001b[32m[04/13 16:26:19 d2.utils.events]: \u001b[0m eta: 1 day, 13:48:55  iter: 3539  total_loss: 0.9539  loss_cls: 0.5745  loss_box_reg: 0.3079  loss_query: 0.04321  time: 2.3837  data_time: 0.0077  lr: 0.006  max_mem: 10549M\n",
            "\u001b[32m[04/13 16:27:08 d2.utils.events]: \u001b[0m eta: 1 day, 13:48:03  iter: 3559  total_loss: 0.7929  loss_cls: 0.4793  loss_box_reg: 0.2624  loss_query: 0.05312  time: 2.3836  data_time: 0.0060  lr: 0.006  max_mem: 10549M\n",
            "\u001b[32m[04/13 16:27:58 d2.utils.events]: \u001b[0m eta: 1 day, 13:47:15  iter: 3579  total_loss: 0.9609  loss_cls: 0.5596  loss_box_reg: 0.2963  loss_query: 0.05698  time: 2.3837  data_time: 0.0065  lr: 0.006  max_mem: 10549M\n",
            "\u001b[32m[04/13 16:28:48 d2.utils.events]: \u001b[0m eta: 1 day, 13:46:47  iter: 3599  total_loss: 0.9705  loss_cls: 0.5905  loss_box_reg: 0.3046  loss_query: 0.04071  time: 2.3838  data_time: 0.0097  lr: 0.006  max_mem: 10549M\n",
            "\u001b[32m[04/13 16:29:37 d2.utils.events]: \u001b[0m eta: 1 day, 13:45:37  iter: 3619  total_loss: 1.098  loss_cls: 0.6698  loss_box_reg: 0.3742  loss_query: 0.04573  time: 2.3834  data_time: 0.0070  lr: 0.006  max_mem: 10549M\n",
            "\u001b[32m[04/13 16:30:27 d2.utils.events]: \u001b[0m eta: 1 day, 13:44:21  iter: 3639  total_loss: 0.8793  loss_cls: 0.5511  loss_box_reg: 0.2732  loss_query: 0.04261  time: 2.3831  data_time: 0.0093  lr: 0.006  max_mem: 10549M\n",
            "\u001b[32m[04/13 16:31:17 d2.utils.events]: \u001b[0m eta: 1 day, 13:45:18  iter: 3659  total_loss: 1.095  loss_cls: 0.646  loss_box_reg: 0.3875  loss_query: 0.05532  time: 2.3833  data_time: 0.0063  lr: 0.006  max_mem: 10549M\n",
            "\u001b[32m[04/13 16:32:08 d2.utils.events]: \u001b[0m eta: 1 day, 13:44:21  iter: 3679  total_loss: 0.8431  loss_cls: 0.5299  loss_box_reg: 0.2737  loss_query: 0.03386  time: 2.3835  data_time: 0.0076  lr: 0.006  max_mem: 10549M\n",
            "\u001b[32m[04/13 16:32:58 d2.utils.events]: \u001b[0m eta: 1 day, 13:43:46  iter: 3699  total_loss: 0.7676  loss_cls: 0.5147  loss_box_reg: 0.2463  loss_query: 0.04229  time: 2.3834  data_time: 0.0079  lr: 0.006  max_mem: 10549M\n",
            "\u001b[32m[04/13 16:33:46 d2.utils.events]: \u001b[0m eta: 1 day, 13:41:31  iter: 3719  total_loss: 0.8517  loss_cls: 0.5348  loss_box_reg: 0.2783  loss_query: 0.02914  time: 2.3830  data_time: 0.0071  lr: 0.006  max_mem: 10549M\n",
            "\u001b[32m[04/13 16:34:36 d2.utils.events]: \u001b[0m eta: 1 day, 13:40:35  iter: 3739  total_loss: 1.055  loss_cls: 0.6347  loss_box_reg: 0.3844  loss_query: 0.04046  time: 2.3828  data_time: 0.0071  lr: 0.006  max_mem: 10549M\n",
            "\u001b[32m[04/13 16:35:27 d2.utils.events]: \u001b[0m eta: 1 day, 13:39:55  iter: 3759  total_loss: 0.9452  loss_cls: 0.5797  loss_box_reg: 0.2891  loss_query: 0.0444  time: 2.3829  data_time: 0.0076  lr: 0.006  max_mem: 10549M\n",
            "\u001b[32m[04/13 16:36:17 d2.utils.events]: \u001b[0m eta: 1 day, 13:38:50  iter: 3779  total_loss: 1.045  loss_cls: 0.6753  loss_box_reg: 0.3413  loss_query: 0.02831  time: 2.3829  data_time: 0.0067  lr: 0.006  max_mem: 10549M\n",
            "\u001b[32m[04/13 16:37:06 d2.utils.events]: \u001b[0m eta: 1 day, 13:36:34  iter: 3799  total_loss: 0.8879  loss_cls: 0.5016  loss_box_reg: 0.3501  loss_query: 0.03625  time: 2.3828  data_time: 0.0068  lr: 0.006  max_mem: 10549M\n",
            "\u001b[32m[04/13 16:37:56 d2.utils.events]: \u001b[0m eta: 1 day, 13:35:39  iter: 3819  total_loss: 0.8125  loss_cls: 0.4626  loss_box_reg: 0.2915  loss_query: 0.0323  time: 2.3829  data_time: 0.0064  lr: 0.006  max_mem: 10549M\n",
            "\u001b[32m[04/13 16:38:46 d2.utils.events]: \u001b[0m eta: 1 day, 13:34:14  iter: 3839  total_loss: 0.8906  loss_cls: 0.5942  loss_box_reg: 0.3004  loss_query: 0.03458  time: 2.3826  data_time: 0.0071  lr: 0.006  max_mem: 10549M\n",
            "\u001b[32m[04/13 16:39:36 d2.utils.events]: \u001b[0m eta: 1 day, 13:33:09  iter: 3859  total_loss: 0.8825  loss_cls: 0.5011  loss_box_reg: 0.3449  loss_query: 0.04192  time: 2.3824  data_time: 0.0080  lr: 0.006  max_mem: 10549M\n",
            "\u001b[32m[04/13 16:40:26 d2.utils.events]: \u001b[0m eta: 1 day, 13:32:05  iter: 3879  total_loss: 0.9912  loss_cls: 0.5427  loss_box_reg: 0.3626  loss_query: 0.05422  time: 2.3822  data_time: 0.0063  lr: 0.006  max_mem: 10549M\n",
            "\u001b[32m[04/13 16:41:16 d2.utils.events]: \u001b[0m eta: 1 day, 13:31:25  iter: 3899  total_loss: 0.7077  loss_cls: 0.4283  loss_box_reg: 0.2342  loss_query: 0.0356  time: 2.3821  data_time: 0.0071  lr: 0.006  max_mem: 10549M\n",
            "\u001b[32m[04/13 16:42:05 d2.utils.events]: \u001b[0m eta: 1 day, 13:30:42  iter: 3919  total_loss: 0.9571  loss_cls: 0.5895  loss_box_reg: 0.307  loss_query: 0.04735  time: 2.3821  data_time: 0.0096  lr: 0.006  max_mem: 10549M\n",
            "\u001b[32m[04/13 16:42:53 d2.utils.events]: \u001b[0m eta: 1 day, 13:29:48  iter: 3939  total_loss: 0.955  loss_cls: 0.6125  loss_box_reg: 0.2942  loss_query: 0.02921  time: 2.3817  data_time: 0.0062  lr: 0.006  max_mem: 10549M\n",
            "\u001b[32m[04/13 16:43:43 d2.utils.events]: \u001b[0m eta: 1 day, 13:28:41  iter: 3959  total_loss: 0.9993  loss_cls: 0.6124  loss_box_reg: 0.3416  loss_query: 0.03888  time: 2.3816  data_time: 0.0069  lr: 0.006  max_mem: 10549M\n",
            "\u001b[32m[04/13 16:44:35 d2.utils.events]: \u001b[0m eta: 1 day, 13:27:38  iter: 3979  total_loss: 0.8755  loss_cls: 0.5506  loss_box_reg: 0.2622  loss_query: 0.0441  time: 2.3819  data_time: 0.0075  lr: 0.006  max_mem: 10549M\n",
            "\u001b[32m[04/13 16:45:24 fvcore.common.checkpoint]: \u001b[0mSaving checkpoint to work_dirs/visdrone_querydet/model_0003999.pth\n",
            "\u001b[32m[04/13 16:45:44 d2.utils.events]: \u001b[0m eta: 1 day, 13:27:26  iter: 3999  total_loss: 0.9735  loss_cls: 0.6071  loss_box_reg: 0.31  loss_query: 0.05193  time: 2.3817  data_time: 0.0072  lr: 0.006  max_mem: 10549M\n",
            "\u001b[32m[04/13 16:46:36 d2.utils.events]: \u001b[0m eta: 1 day, 13:26:36  iter: 4019  total_loss: 0.801  loss_cls: 0.5213  loss_box_reg: 0.2491  loss_query: 0.03654  time: 2.3820  data_time: 0.0067  lr: 0.006  max_mem: 10549M\n",
            "\u001b[32m[04/13 16:47:26 d2.utils.events]: \u001b[0m eta: 1 day, 13:25:29  iter: 4039  total_loss: 0.7927  loss_cls: 0.501  loss_box_reg: 0.2796  loss_query: 0.03683  time: 2.3820  data_time: 0.0073  lr: 0.006  max_mem: 10549M\n",
            "\u001b[32m[04/13 16:48:16 d2.utils.events]: \u001b[0m eta: 1 day, 13:25:02  iter: 4059  total_loss: 0.887  loss_cls: 0.5638  loss_box_reg: 0.2878  loss_query: 0.04414  time: 2.3821  data_time: 0.0080  lr: 0.006  max_mem: 10549M\n",
            "\u001b[32m[04/13 16:49:05 d2.utils.events]: \u001b[0m eta: 1 day, 13:23:52  iter: 4079  total_loss: 0.7916  loss_cls: 0.4669  loss_box_reg: 0.2498  loss_query: 0.03351  time: 2.3814  data_time: 0.0092  lr: 0.006  max_mem: 10549M\n",
            "\u001b[32m[04/13 16:49:54 d2.utils.events]: \u001b[0m eta: 1 day, 13:22:50  iter: 4099  total_loss: 0.7872  loss_cls: 0.4991  loss_box_reg: 0.2631  loss_query: 0.02708  time: 2.3813  data_time: 0.0077  lr: 0.006  max_mem: 10549M\n",
            "\u001b[32m[04/13 16:50:43 d2.utils.events]: \u001b[0m eta: 1 day, 13:22:35  iter: 4119  total_loss: 0.9555  loss_cls: 0.5708  loss_box_reg: 0.332  loss_query: 0.04527  time: 2.3813  data_time: 0.0065  lr: 0.006  max_mem: 10549M\n",
            "\u001b[32m[04/13 16:51:32 d2.utils.events]: \u001b[0m eta: 1 day, 13:21:49  iter: 4139  total_loss: 0.5643  loss_cls: 0.3749  loss_box_reg: 0.1752  loss_query: 0.01894  time: 2.3812  data_time: 0.0064  lr: 0.006  max_mem: 10549M\n",
            "\u001b[32m[04/13 16:52:21 d2.utils.events]: \u001b[0m eta: 1 day, 13:20:44  iter: 4159  total_loss: 1.184  loss_cls: 0.7163  loss_box_reg: 0.3958  loss_query: 0.02807  time: 2.3811  data_time: 0.0060  lr: 0.006  max_mem: 10549M\n",
            "\u001b[32m[04/13 16:53:08 d2.utils.events]: \u001b[0m eta: 1 day, 13:18:37  iter: 4179  total_loss: 0.7057  loss_cls: 0.4984  loss_box_reg: 0.2215  loss_query: 0.01769  time: 2.3805  data_time: 0.0066  lr: 0.006  max_mem: 10549M\n",
            "\u001b[32m[04/13 16:53:56 d2.utils.events]: \u001b[0m eta: 1 day, 13:16:42  iter: 4199  total_loss: 0.8071  loss_cls: 0.4856  loss_box_reg: 0.2848  loss_query: 0.01853  time: 2.3801  data_time: 0.0072  lr: 0.006  max_mem: 10549M\n",
            "\u001b[32m[04/13 16:54:45 d2.utils.events]: \u001b[0m eta: 1 day, 13:15:56  iter: 4219  total_loss: 0.9787  loss_cls: 0.6299  loss_box_reg: 0.3256  loss_query: 0.03338  time: 2.3798  data_time: 0.0074  lr: 0.006  max_mem: 10549M\n",
            "\u001b[32m[04/13 16:55:35 d2.utils.events]: \u001b[0m eta: 1 day, 13:13:57  iter: 4239  total_loss: 1.004  loss_cls: 0.6393  loss_box_reg: 0.3285  loss_query: 0.03987  time: 2.3801  data_time: 0.0075  lr: 0.006  max_mem: 10549M\n",
            "\u001b[32m[04/13 16:56:22 d2.utils.events]: \u001b[0m eta: 1 day, 13:12:56  iter: 4259  total_loss: 1.114  loss_cls: 0.7278  loss_box_reg: 0.3496  loss_query: 0.03616  time: 2.3794  data_time: 0.0067  lr: 0.006  max_mem: 10549M\n",
            "\u001b[32m[04/13 16:57:10 d2.utils.events]: \u001b[0m eta: 1 day, 13:11:57  iter: 4279  total_loss: 0.8736  loss_cls: 0.5353  loss_box_reg: 0.323  loss_query: 0.03499  time: 2.3790  data_time: 0.0088  lr: 0.006  max_mem: 10549M\n",
            "\u001b[32m[04/13 16:57:58 d2.utils.events]: \u001b[0m eta: 1 day, 13:11:07  iter: 4299  total_loss: 0.8925  loss_cls: 0.5552  loss_box_reg: 0.2621  loss_query: 0.03941  time: 2.3787  data_time: 0.0082  lr: 0.006  max_mem: 10549M\n",
            "\u001b[32m[04/13 16:58:47 d2.utils.events]: \u001b[0m eta: 1 day, 13:09:56  iter: 4319  total_loss: 1.004  loss_cls: 0.6211  loss_box_reg: 0.3275  loss_query: 0.0363  time: 2.3785  data_time: 0.0086  lr: 0.006  max_mem: 10549M\n",
            "\u001b[32m[04/13 16:59:37 d2.utils.events]: \u001b[0m eta: 1 day, 13:08:47  iter: 4339  total_loss: 1.053  loss_cls: 0.6407  loss_box_reg: 0.3508  loss_query: 0.0484  time: 2.3786  data_time: 0.0061  lr: 0.006  max_mem: 10549M\n",
            "\u001b[32m[04/13 17:00:26 d2.utils.events]: \u001b[0m eta: 1 day, 13:07:26  iter: 4359  total_loss: 0.7139  loss_cls: 0.4376  loss_box_reg: 0.2303  loss_query: 0.03041  time: 2.3785  data_time: 0.0069  lr: 0.006  max_mem: 10549M\n",
            "\u001b[32m[04/13 17:01:14 d2.utils.events]: \u001b[0m eta: 1 day, 13:06:12  iter: 4379  total_loss: 0.8404  loss_cls: 0.5355  loss_box_reg: 0.2523  loss_query: 0.03651  time: 2.3781  data_time: 0.0060  lr: 0.006  max_mem: 10549M\n",
            "\u001b[32m[04/13 17:02:05 d2.utils.events]: \u001b[0m eta: 1 day, 13:06:47  iter: 4399  total_loss: 1.203  loss_cls: 0.7693  loss_box_reg: 0.3985  loss_query: 0.05359  time: 2.3784  data_time: 0.0068  lr: 0.006  max_mem: 10549M\n",
            "\u001b[32m[04/13 17:02:53 d2.utils.events]: \u001b[0m eta: 1 day, 13:05:35  iter: 4419  total_loss: 0.8282  loss_cls: 0.4875  loss_box_reg: 0.2634  loss_query: 0.04327  time: 2.3781  data_time: 0.0071  lr: 0.006  max_mem: 10549M\n",
            "\u001b[32m[04/13 17:03:42 d2.utils.events]: \u001b[0m eta: 1 day, 13:05:05  iter: 4439  total_loss: 0.8459  loss_cls: 0.585  loss_box_reg: 0.2439  loss_query: 0.03094  time: 2.3782  data_time: 0.0058  lr: 0.006  max_mem: 10549M\n",
            "\u001b[32m[04/13 17:04:32 d2.utils.events]: \u001b[0m eta: 1 day, 13:04:38  iter: 4459  total_loss: 0.9688  loss_cls: 0.5596  loss_box_reg: 0.3243  loss_query: 0.04589  time: 2.3782  data_time: 0.0069  lr: 0.006  max_mem: 10549M\n",
            "\u001b[32m[04/13 17:05:21 d2.utils.events]: \u001b[0m eta: 1 day, 13:03:50  iter: 4479  total_loss: 0.9476  loss_cls: 0.5821  loss_box_reg: 0.321  loss_query: 0.03261  time: 2.3781  data_time: 0.0063  lr: 0.006  max_mem: 10549M\n",
            "\u001b[32m[04/13 17:06:10 d2.utils.events]: \u001b[0m eta: 1 day, 13:02:23  iter: 4499  total_loss: 0.9157  loss_cls: 0.5579  loss_box_reg: 0.3134  loss_query: 0.0344  time: 2.3779  data_time: 0.0077  lr: 0.006  max_mem: 10549M\n",
            "\u001b[32m[04/13 17:06:57 d2.utils.events]: \u001b[0m eta: 1 day, 13:00:31  iter: 4519  total_loss: 0.7279  loss_cls: 0.4679  loss_box_reg: 0.2474  loss_query: 0.03522  time: 2.3775  data_time: 0.0094  lr: 0.006  max_mem: 10549M\n",
            "\u001b[32m[04/13 17:07:47 d2.utils.events]: \u001b[0m eta: 1 day, 12:59:23  iter: 4539  total_loss: 0.8724  loss_cls: 0.58  loss_box_reg: 0.3138  loss_query: 0.03407  time: 2.3775  data_time: 0.0068  lr: 0.006  max_mem: 10549M\n",
            "\u001b[32m[04/13 17:08:36 d2.utils.events]: \u001b[0m eta: 1 day, 12:57:50  iter: 4559  total_loss: 0.7261  loss_cls: 0.4869  loss_box_reg: 0.2368  loss_query: 0.03543  time: 2.3776  data_time: 0.0069  lr: 0.006  max_mem: 10549M\n",
            "\u001b[32m[04/13 17:09:25 d2.utils.events]: \u001b[0m eta: 1 day, 12:56:56  iter: 4579  total_loss: 0.8768  loss_cls: 0.5414  loss_box_reg: 0.2788  loss_query: 0.05068  time: 2.3774  data_time: 0.0071  lr: 0.006  max_mem: 10549M\n",
            "\u001b[32m[04/13 17:10:14 d2.utils.events]: \u001b[0m eta: 1 day, 12:55:20  iter: 4599  total_loss: 0.8596  loss_cls: 0.5435  loss_box_reg: 0.2839  loss_query: 0.03776  time: 2.3774  data_time: 0.0070  lr: 0.006  max_mem: 10549M\n",
            "\u001b[32m[04/13 17:11:04 d2.utils.events]: \u001b[0m eta: 1 day, 12:55:20  iter: 4619  total_loss: 0.9833  loss_cls: 0.5872  loss_box_reg: 0.3508  loss_query: 0.03904  time: 2.3774  data_time: 0.0065  lr: 0.006  max_mem: 10549M\n",
            "\u001b[32m[04/13 17:11:53 d2.utils.events]: \u001b[0m eta: 1 day, 12:54:56  iter: 4639  total_loss: 1.127  loss_cls: 0.6691  loss_box_reg: 0.3469  loss_query: 0.06444  time: 2.3773  data_time: 0.0086  lr: 0.006  max_mem: 10549M\n",
            "\u001b[32m[04/13 17:12:41 d2.utils.events]: \u001b[0m eta: 1 day, 12:53:07  iter: 4659  total_loss: 0.8958  loss_cls: 0.5554  loss_box_reg: 0.2817  loss_query: 0.05507  time: 2.3771  data_time: 0.0069  lr: 0.006  max_mem: 10549M\n",
            "\u001b[32m[04/13 17:13:31 d2.utils.events]: \u001b[0m eta: 1 day, 12:53:02  iter: 4679  total_loss: 0.9777  loss_cls: 0.5847  loss_box_reg: 0.3244  loss_query: 0.05007  time: 2.3771  data_time: 0.0072  lr: 0.006  max_mem: 10549M\n",
            "\u001b[32m[04/13 17:14:22 d2.utils.events]: \u001b[0m eta: 1 day, 12:52:14  iter: 4699  total_loss: 0.7377  loss_cls: 0.4413  loss_box_reg: 0.2687  loss_query: 0.04247  time: 2.3774  data_time: 0.0081  lr: 0.006  max_mem: 10549M\n",
            "\u001b[32m[04/13 17:15:12 d2.utils.events]: \u001b[0m eta: 1 day, 12:51:20  iter: 4719  total_loss: 0.8374  loss_cls: 0.4953  loss_box_reg: 0.2747  loss_query: 0.04141  time: 2.3774  data_time: 0.0066  lr: 0.006  max_mem: 10549M\n",
            "\u001b[32m[04/13 17:16:02 d2.utils.events]: \u001b[0m eta: 1 day, 12:50:54  iter: 4739  total_loss: 1.159  loss_cls: 0.6931  loss_box_reg: 0.3799  loss_query: 0.06292  time: 2.3775  data_time: 0.0075  lr: 0.006  max_mem: 10549M\n",
            "\u001b[32m[04/13 17:16:52 d2.utils.events]: \u001b[0m eta: 1 day, 12:50:06  iter: 4759  total_loss: 0.7233  loss_cls: 0.4337  loss_box_reg: 0.2184  loss_query: 0.04471  time: 2.3775  data_time: 0.0084  lr: 0.006  max_mem: 10549M\n",
            "\u001b[32m[04/13 17:17:41 d2.utils.events]: \u001b[0m eta: 1 day, 12:49:38  iter: 4779  total_loss: 0.8369  loss_cls: 0.516  loss_box_reg: 0.2868  loss_query: 0.02917  time: 2.3773  data_time: 0.0084  lr: 0.006  max_mem: 10549M\n",
            "\u001b[32m[04/13 17:18:31 d2.utils.events]: \u001b[0m eta: 1 day, 12:49:27  iter: 4799  total_loss: 0.8019  loss_cls: 0.4795  loss_box_reg: 0.2834  loss_query: 0.04218  time: 2.3773  data_time: 0.0066  lr: 0.006  max_mem: 10549M\n",
            "\u001b[32m[04/13 17:19:20 d2.utils.events]: \u001b[0m eta: 1 day, 12:48:30  iter: 4819  total_loss: 0.8709  loss_cls: 0.5775  loss_box_reg: 0.2677  loss_query: 0.04195  time: 2.3772  data_time: 0.0084  lr: 0.006  max_mem: 10549M\n",
            "\u001b[32m[04/13 17:20:08 d2.utils.events]: \u001b[0m eta: 1 day, 12:47:14  iter: 4839  total_loss: 0.7614  loss_cls: 0.5044  loss_box_reg: 0.2565  loss_query: 0.02288  time: 2.3770  data_time: 0.0075  lr: 0.006  max_mem: 10549M\n",
            "\u001b[32m[04/13 17:20:57 d2.utils.events]: \u001b[0m eta: 1 day, 12:46:41  iter: 4859  total_loss: 0.9421  loss_cls: 0.5739  loss_box_reg: 0.3457  loss_query: 0.04914  time: 2.3769  data_time: 0.0069  lr: 0.006  max_mem: 10549M\n",
            "\u001b[32m[04/13 17:21:47 d2.utils.events]: \u001b[0m eta: 1 day, 12:46:15  iter: 4879  total_loss: 0.9946  loss_cls: 0.604  loss_box_reg: 0.3325  loss_query: 0.04242  time: 2.3769  data_time: 0.0063  lr: 0.006  max_mem: 10549M\n",
            "\u001b[32m[04/13 17:22:36 d2.utils.events]: \u001b[0m eta: 1 day, 12:45:27  iter: 4899  total_loss: 0.8401  loss_cls: 0.5186  loss_box_reg: 0.2818  loss_query: 0.03829  time: 2.3768  data_time: 0.0076  lr: 0.006  max_mem: 10549M\n",
            "\u001b[32m[04/13 17:23:26 d2.utils.events]: \u001b[0m eta: 1 day, 12:43:41  iter: 4919  total_loss: 1.246  loss_cls: 0.7494  loss_box_reg: 0.3944  loss_query: 0.05605  time: 2.3768  data_time: 0.0068  lr: 0.006  max_mem: 10549M\n",
            "\u001b[32m[04/13 17:24:17 d2.utils.events]: \u001b[0m eta: 1 day, 12:43:29  iter: 4939  total_loss: 0.7363  loss_cls: 0.4704  loss_box_reg: 0.268  loss_query: 0.05029  time: 2.3771  data_time: 0.0593  lr: 0.006  max_mem: 10549M\n",
            "\u001b[32m[04/13 17:25:07 d2.utils.events]: \u001b[0m eta: 1 day, 12:42:05  iter: 4959  total_loss: 0.5986  loss_cls: 0.3778  loss_box_reg: 0.1855  loss_query: 0.02611  time: 2.3771  data_time: 0.0068  lr: 0.006  max_mem: 10549M\n",
            "\u001b[32m[04/13 17:25:57 d2.utils.events]: \u001b[0m eta: 1 day, 12:41:17  iter: 4979  total_loss: 0.8707  loss_cls: 0.5501  loss_box_reg: 0.2745  loss_query: 0.0414  time: 2.3773  data_time: 0.0067  lr: 0.006  max_mem: 10549M\n",
            "\u001b[32m[04/13 17:26:48 d2.utils.events]: \u001b[0m eta: 1 day, 12:40:50  iter: 4999  total_loss: 0.8893  loss_cls: 0.5444  loss_box_reg: 0.2785  loss_query: 0.04621  time: 2.3776  data_time: 0.0074  lr: 0.006  max_mem: 10549M\n",
            "\u001b[32m[04/13 17:27:36 d2.utils.events]: \u001b[0m eta: 1 day, 12:38:43  iter: 5019  total_loss: 0.6684  loss_cls: 0.4484  loss_box_reg: 0.2251  loss_query: 0.03119  time: 2.3772  data_time: 0.0069  lr: 0.006  max_mem: 10549M\n",
            "\u001b[32m[04/13 17:28:27 d2.utils.events]: \u001b[0m eta: 1 day, 12:36:24  iter: 5039  total_loss: 1.037  loss_cls: 0.628  loss_box_reg: 0.3525  loss_query: 0.03413  time: 2.3770  data_time: 0.0075  lr: 0.006  max_mem: 10549M\n",
            "\u001b[32m[04/13 17:29:15 d2.utils.events]: \u001b[0m eta: 1 day, 12:34:39  iter: 5059  total_loss: 0.857  loss_cls: 0.5677  loss_box_reg: 0.2793  loss_query: 0.03263  time: 2.3768  data_time: 0.0065  lr: 0.006  max_mem: 10549M\n",
            "\u001b[32m[04/13 17:30:04 d2.utils.events]: \u001b[0m eta: 1 day, 12:33:51  iter: 5079  total_loss: 0.5869  loss_cls: 0.377  loss_box_reg: 0.1752  loss_query: 0.02531  time: 2.3767  data_time: 0.0062  lr: 0.006  max_mem: 10549M\n",
            "\u001b[32m[04/13 17:30:53 d2.utils.events]: \u001b[0m eta: 1 day, 12:33:37  iter: 5099  total_loss: 1.021  loss_cls: 0.6253  loss_box_reg: 0.3151  loss_query: 0.03076  time: 2.3767  data_time: 0.0067  lr: 0.006  max_mem: 10549M\n",
            "\u001b[32m[04/13 17:31:42 d2.utils.events]: \u001b[0m eta: 1 day, 12:32:36  iter: 5119  total_loss: 0.9684  loss_cls: 0.5378  loss_box_reg: 0.3509  loss_query: 0.03091  time: 2.3766  data_time: 0.0070  lr: 0.006  max_mem: 10549M\n",
            "\u001b[32m[04/13 17:32:32 d2.utils.events]: \u001b[0m eta: 1 day, 12:32:24  iter: 5139  total_loss: 0.9564  loss_cls: 0.5909  loss_box_reg: 0.3192  loss_query: 0.02251  time: 2.3766  data_time: 0.0069  lr: 0.006  max_mem: 10549M\n",
            "\u001b[32m[04/13 17:33:21 d2.utils.events]: \u001b[0m eta: 1 day, 12:31:22  iter: 5159  total_loss: 0.7649  loss_cls: 0.4207  loss_box_reg: 0.2656  loss_query: 0.04832  time: 2.3766  data_time: 0.0072  lr: 0.006  max_mem: 10549M\n",
            "\u001b[32m[04/13 17:34:12 d2.utils.events]: \u001b[0m eta: 1 day, 12:33:53  iter: 5179  total_loss: 0.8797  loss_cls: 0.5199  loss_box_reg: 0.2856  loss_query: 0.05259  time: 2.3769  data_time: 0.0079  lr: 0.006  max_mem: 10549M\n",
            "\u001b[32m[04/13 17:35:03 d2.utils.events]: \u001b[0m eta: 1 day, 12:34:48  iter: 5199  total_loss: 0.8525  loss_cls: 0.5482  loss_box_reg: 0.2972  loss_query: 0.03714  time: 2.3771  data_time: 0.0069  lr: 0.006  max_mem: 10549M\n",
            "\u001b[32m[04/13 17:35:53 d2.utils.events]: \u001b[0m eta: 1 day, 12:34:09  iter: 5219  total_loss: 0.937  loss_cls: 0.5456  loss_box_reg: 0.3098  loss_query: 0.0536  time: 2.3773  data_time: 0.0066  lr: 0.006  max_mem: 10549M\n",
            "\u001b[32m[04/13 17:36:43 d2.utils.events]: \u001b[0m eta: 1 day, 12:33:35  iter: 5239  total_loss: 0.74  loss_cls: 0.4722  loss_box_reg: 0.2509  loss_query: 0.04394  time: 2.3774  data_time: 0.0077  lr: 0.006  max_mem: 10549M\n",
            "\u001b[32m[04/13 17:37:32 d2.utils.events]: \u001b[0m eta: 1 day, 12:32:25  iter: 5259  total_loss: 0.98  loss_cls: 0.5597  loss_box_reg: 0.3477  loss_query: 0.04463  time: 2.3771  data_time: 0.0061  lr: 0.006  max_mem: 10549M\n",
            "\u001b[32m[04/13 17:38:21 d2.utils.events]: \u001b[0m eta: 1 day, 12:32:16  iter: 5279  total_loss: 0.8834  loss_cls: 0.5657  loss_box_reg: 0.3085  loss_query: 0.03243  time: 2.3769  data_time: 0.0072  lr: 0.006  max_mem: 10549M\n",
            "\u001b[32m[04/13 17:39:11 d2.utils.events]: \u001b[0m eta: 1 day, 12:31:58  iter: 5299  total_loss: 0.8621  loss_cls: 0.5261  loss_box_reg: 0.2839  loss_query: 0.038  time: 2.3770  data_time: 0.0067  lr: 0.006  max_mem: 10549M\n",
            "\u001b[32m[04/13 17:40:00 d2.utils.events]: \u001b[0m eta: 1 day, 12:31:43  iter: 5319  total_loss: 0.7023  loss_cls: 0.4411  loss_box_reg: 0.2193  loss_query: 0.03361  time: 2.3770  data_time: 0.0081  lr: 0.006  max_mem: 10549M\n",
            "\u001b[32m[04/13 17:40:50 d2.utils.events]: \u001b[0m eta: 1 day, 12:30:48  iter: 5339  total_loss: 0.9045  loss_cls: 0.5482  loss_box_reg: 0.2944  loss_query: 0.04221  time: 2.3770  data_time: 0.0073  lr: 0.006  max_mem: 10549M\n",
            "\u001b[32m[04/13 17:41:39 d2.utils.events]: \u001b[0m eta: 1 day, 12:30:00  iter: 5359  total_loss: 0.7195  loss_cls: 0.4343  loss_box_reg: 0.2519  loss_query: 0.03003  time: 2.3769  data_time: 0.0080  lr: 0.006  max_mem: 10549M\n",
            "\u001b[32m[04/13 17:42:27 d2.utils.events]: \u001b[0m eta: 1 day, 12:29:29  iter: 5379  total_loss: 0.6337  loss_cls: 0.4197  loss_box_reg: 0.2047  loss_query: 0.03913  time: 2.3767  data_time: 0.0064  lr: 0.006  max_mem: 10549M\n",
            "\u001b[32m[04/13 17:43:17 d2.utils.events]: \u001b[0m eta: 1 day, 12:27:58  iter: 5399  total_loss: 0.9494  loss_cls: 0.595  loss_box_reg: 0.2773  loss_query: 0.02903  time: 2.3767  data_time: 0.0071  lr: 0.006  max_mem: 10549M\n",
            "\u001b[32m[04/13 17:44:07 d2.utils.events]: \u001b[0m eta: 1 day, 12:27:53  iter: 5419  total_loss: 0.8083  loss_cls: 0.5011  loss_box_reg: 0.2714  loss_query: 0.0476  time: 2.3768  data_time: 0.0074  lr: 0.006  max_mem: 10549M\n",
            "\u001b[32m[04/13 17:44:56 d2.utils.events]: \u001b[0m eta: 1 day, 12:27:27  iter: 5439  total_loss: 0.9043  loss_cls: 0.5459  loss_box_reg: 0.2935  loss_query: 0.0415  time: 2.3768  data_time: 0.0067  lr: 0.006  max_mem: 10549M\n",
            "\u001b[32m[04/13 17:45:46 d2.utils.events]: \u001b[0m eta: 1 day, 12:26:03  iter: 5459  total_loss: 0.7962  loss_cls: 0.5084  loss_box_reg: 0.2837  loss_query: 0.02762  time: 2.3768  data_time: 0.0075  lr: 0.006  max_mem: 10549M\n",
            "\u001b[32m[04/13 17:46:36 d2.utils.events]: \u001b[0m eta: 1 day, 12:25:37  iter: 5479  total_loss: 0.8607  loss_cls: 0.4931  loss_box_reg: 0.3111  loss_query: 0.04573  time: 2.3767  data_time: 0.0063  lr: 0.006  max_mem: 10549M\n",
            "\u001b[32m[04/13 17:47:24 d2.utils.events]: \u001b[0m eta: 1 day, 12:24:50  iter: 5499  total_loss: 0.7731  loss_cls: 0.5147  loss_box_reg: 0.2413  loss_query: 0.03329  time: 2.3767  data_time: 0.0089  lr: 0.006  max_mem: 10549M\n",
            "\u001b[32m[04/13 17:48:13 d2.utils.events]: \u001b[0m eta: 1 day, 12:24:26  iter: 5519  total_loss: 1.131  loss_cls: 0.6893  loss_box_reg: 0.3708  loss_query: 0.04402  time: 2.3765  data_time: 0.0070  lr: 0.006  max_mem: 10549M\n",
            "\u001b[32m[04/13 17:49:02 d2.utils.events]: \u001b[0m eta: 1 day, 12:23:42  iter: 5539  total_loss: 0.8051  loss_cls: 0.5185  loss_box_reg: 0.2731  loss_query: 0.0412  time: 2.3765  data_time: 0.0066  lr: 0.006  max_mem: 10549M\n",
            "\u001b[32m[04/13 17:49:51 d2.utils.events]: \u001b[0m eta: 1 day, 12:22:38  iter: 5559  total_loss: 0.6587  loss_cls: 0.4337  loss_box_reg: 0.2151  loss_query: 0.02338  time: 2.3763  data_time: 0.0069  lr: 0.006  max_mem: 10549M\n",
            "\u001b[32m[04/13 17:50:41 d2.utils.events]: \u001b[0m eta: 1 day, 12:21:44  iter: 5579  total_loss: 1.006  loss_cls: 0.6677  loss_box_reg: 0.3109  loss_query: 0.03563  time: 2.3763  data_time: 0.0059  lr: 0.006  max_mem: 10549M\n",
            "\u001b[32m[04/13 17:51:28 d2.utils.events]: \u001b[0m eta: 1 day, 12:20:40  iter: 5599  total_loss: 0.8141  loss_cls: 0.4683  loss_box_reg: 0.2811  loss_query: 0.03372  time: 2.3760  data_time: 0.0073  lr: 0.006  max_mem: 10549M\n",
            "\u001b[32m[04/13 17:52:15 d2.utils.events]: \u001b[0m eta: 1 day, 12:18:42  iter: 5619  total_loss: 0.7727  loss_cls: 0.4608  loss_box_reg: 0.262  loss_query: 0.0302  time: 2.3754  data_time: 0.0067  lr: 0.006  max_mem: 10549M\n",
            "\u001b[32m[04/13 17:53:05 d2.utils.events]: \u001b[0m eta: 1 day, 12:17:19  iter: 5639  total_loss: 1.062  loss_cls: 0.6417  loss_box_reg: 0.3163  loss_query: 0.02604  time: 2.3755  data_time: 0.0077  lr: 0.006  max_mem: 10549M\n",
            "\u001b[32m[04/13 17:53:53 d2.utils.events]: \u001b[0m eta: 1 day, 12:15:17  iter: 5659  total_loss: 0.8565  loss_cls: 0.5202  loss_box_reg: 0.3144  loss_query: 0.03725  time: 2.3753  data_time: 0.0079  lr: 0.006  max_mem: 10549M\n",
            "\u001b[32m[04/13 17:54:43 d2.utils.events]: \u001b[0m eta: 1 day, 12:13:37  iter: 5679  total_loss: 0.6455  loss_cls: 0.4088  loss_box_reg: 0.2176  loss_query: 0.02901  time: 2.3753  data_time: 0.0097  lr: 0.006  max_mem: 10549M\n",
            "\u001b[32m[04/13 17:55:31 d2.utils.events]: \u001b[0m eta: 1 day, 12:11:10  iter: 5699  total_loss: 0.5554  loss_cls: 0.3434  loss_box_reg: 0.2047  loss_query: 0.01679  time: 2.3751  data_time: 0.0083  lr: 0.006  max_mem: 10549M\n",
            "\u001b[32m[04/13 17:56:21 d2.utils.events]: \u001b[0m eta: 1 day, 12:10:57  iter: 5719  total_loss: 0.9081  loss_cls: 0.5547  loss_box_reg: 0.341  loss_query: 0.03618  time: 2.3753  data_time: 0.0080  lr: 0.006  max_mem: 10549M\n",
            "\u001b[32m[04/13 17:57:10 d2.utils.events]: \u001b[0m eta: 1 day, 12:08:25  iter: 5739  total_loss: 0.89  loss_cls: 0.5206  loss_box_reg: 0.3068  loss_query: 0.04981  time: 2.3751  data_time: 0.0068  lr: 0.006  max_mem: 10549M\n",
            "\u001b[32m[04/13 17:57:59 d2.utils.events]: \u001b[0m eta: 1 day, 12:07:20  iter: 5759  total_loss: 0.6596  loss_cls: 0.4413  loss_box_reg: 0.2002  loss_query: 0.02621  time: 2.3751  data_time: 0.0084  lr: 0.006  max_mem: 10549M\n",
            "\u001b[32m[04/13 17:58:49 d2.utils.events]: \u001b[0m eta: 1 day, 12:06:15  iter: 5779  total_loss: 1.002  loss_cls: 0.6008  loss_box_reg: 0.3622  loss_query: 0.04023  time: 2.3752  data_time: 0.0072  lr: 0.006  max_mem: 10549M\n",
            "\u001b[32m[04/13 17:59:39 d2.utils.events]: \u001b[0m eta: 1 day, 12:05:48  iter: 5799  total_loss: 0.8512  loss_cls: 0.534  loss_box_reg: 0.2536  loss_query: 0.04535  time: 2.3753  data_time: 0.0082  lr: 0.006  max_mem: 10549M\n",
            "\u001b[32m[04/13 18:00:28 d2.utils.events]: \u001b[0m eta: 1 day, 12:05:00  iter: 5819  total_loss: 0.5853  loss_cls: 0.364  loss_box_reg: 0.2093  loss_query: 0.03139  time: 2.3752  data_time: 0.0070  lr: 0.006  max_mem: 10549M\n",
            "\u001b[32m[04/13 18:01:16 d2.utils.events]: \u001b[0m eta: 1 day, 12:04:07  iter: 5839  total_loss: 0.7387  loss_cls: 0.4795  loss_box_reg: 0.2301  loss_query: 0.03076  time: 2.3749  data_time: 0.0094  lr: 0.006  max_mem: 10549M\n",
            "\u001b[32m[04/13 18:02:04 d2.utils.events]: \u001b[0m eta: 1 day, 12:02:36  iter: 5859  total_loss: 0.6355  loss_cls: 0.371  loss_box_reg: 0.2372  loss_query: 0.01959  time: 2.3747  data_time: 0.0062  lr: 0.006  max_mem: 10549M\n",
            "\u001b[32m[04/13 18:02:53 d2.utils.events]: \u001b[0m eta: 1 day, 12:02:15  iter: 5879  total_loss: 1.067  loss_cls: 0.6462  loss_box_reg: 0.3407  loss_query: 0.04916  time: 2.3747  data_time: 0.0077  lr: 0.006  max_mem: 10549M\n",
            "\u001b[32m[04/13 18:03:44 d2.utils.events]: \u001b[0m eta: 1 day, 12:01:53  iter: 5899  total_loss: 0.898  loss_cls: 0.5522  loss_box_reg: 0.298  loss_query: 0.04818  time: 2.3750  data_time: 0.0061  lr: 0.006  max_mem: 10549M\n",
            "\u001b[32m[04/13 18:04:34 d2.utils.events]: \u001b[0m eta: 1 day, 12:01:37  iter: 5919  total_loss: 0.7889  loss_cls: 0.5206  loss_box_reg: 0.2399  loss_query: 0.05602  time: 2.3749  data_time: 0.0068  lr: 0.006  max_mem: 10549M\n",
            "\u001b[32m[04/13 18:05:22 d2.utils.events]: \u001b[0m eta: 1 day, 11:59:52  iter: 5939  total_loss: 0.7249  loss_cls: 0.4284  loss_box_reg: 0.2532  loss_query: 0.0337  time: 2.3746  data_time: 0.0071  lr: 0.006  max_mem: 10549M\n",
            "\u001b[32m[04/13 18:06:12 d2.utils.events]: \u001b[0m eta: 1 day, 12:00:29  iter: 5959  total_loss: 1.071  loss_cls: 0.6468  loss_box_reg: 0.3205  loss_query: 0.04318  time: 2.3747  data_time: 0.0076  lr: 0.006  max_mem: 10549M\n",
            "\u001b[32m[04/13 18:07:01 d2.utils.events]: \u001b[0m eta: 1 day, 11:59:31  iter: 5979  total_loss: 0.7304  loss_cls: 0.4874  loss_box_reg: 0.2466  loss_query: 0.0346  time: 2.3747  data_time: 0.0093  lr: 0.006  max_mem: 10549M\n",
            "\u001b[32m[04/13 18:07:52 fvcore.common.checkpoint]: \u001b[0mSaving checkpoint to work_dirs/visdrone_querydet/model_0005999.pth\n",
            "\u001b[32m[04/13 18:08:11 d2.utils.events]: \u001b[0m eta: 1 day, 11:58:43  iter: 5999  total_loss: 0.7775  loss_cls: 0.5567  loss_box_reg: 0.2197  loss_query: 0.03829  time: 2.3748  data_time: 0.0066  lr: 0.006  max_mem: 10549M\n",
            "\u001b[32m[04/13 18:09:02 d2.utils.events]: \u001b[0m eta: 1 day, 12:00:01  iter: 6019  total_loss: 0.9519  loss_cls: 0.5727  loss_box_reg: 0.3547  loss_query: 0.05521  time: 2.3751  data_time: 0.0073  lr: 0.006  max_mem: 10549M\n",
            "\u001b[32m[04/13 18:09:52 d2.utils.events]: \u001b[0m eta: 1 day, 11:59:50  iter: 6039  total_loss: 0.8159  loss_cls: 0.497  loss_box_reg: 0.27  loss_query: 0.04411  time: 2.3751  data_time: 0.0081  lr: 0.006  max_mem: 10549M\n",
            "\u001b[32m[04/13 18:10:41 d2.utils.events]: \u001b[0m eta: 1 day, 11:59:18  iter: 6059  total_loss: 0.6623  loss_cls: 0.3867  loss_box_reg: 0.2345  loss_query: 0.02904  time: 2.3751  data_time: 0.0066  lr: 0.006  max_mem: 10549M\n",
            "\u001b[32m[04/13 18:11:31 d2.utils.events]: \u001b[0m eta: 1 day, 11:59:01  iter: 6079  total_loss: 0.8844  loss_cls: 0.5511  loss_box_reg: 0.3045  loss_query: 0.03867  time: 2.3750  data_time: 0.0064  lr: 0.006  max_mem: 10549M\n",
            "\u001b[32m[04/13 18:12:19 d2.utils.events]: \u001b[0m eta: 1 day, 11:57:29  iter: 6099  total_loss: 0.777  loss_cls: 0.4568  loss_box_reg: 0.2637  loss_query: 0.02048  time: 2.3749  data_time: 0.0069  lr: 0.006  max_mem: 10549M\n",
            "\u001b[32m[04/13 18:13:09 d2.utils.events]: \u001b[0m eta: 1 day, 11:56:54  iter: 6119  total_loss: 0.8663  loss_cls: 0.5239  loss_box_reg: 0.2721  loss_query: 0.02334  time: 2.3749  data_time: 0.0059  lr: 0.006  max_mem: 10549M\n",
            "\u001b[32m[04/13 18:13:58 d2.utils.events]: \u001b[0m eta: 1 day, 11:56:06  iter: 6139  total_loss: 0.9417  loss_cls: 0.609  loss_box_reg: 0.2922  loss_query: 0.04166  time: 2.3749  data_time: 0.0078  lr: 0.006  max_mem: 10549M\n",
            "\u001b[32m[04/13 18:14:47 d2.utils.events]: \u001b[0m eta: 1 day, 11:54:56  iter: 6159  total_loss: 0.7035  loss_cls: 0.419  loss_box_reg: 0.2678  loss_query: 0.02656  time: 2.3748  data_time: 0.0070  lr: 0.006  max_mem: 10549M\n",
            "\u001b[32m[04/13 18:15:39 d2.utils.events]: \u001b[0m eta: 1 day, 11:54:08  iter: 6179  total_loss: 0.9186  loss_cls: 0.535  loss_box_reg: 0.321  loss_query: 0.05377  time: 2.3750  data_time: 0.0086  lr: 0.006  max_mem: 10549M\n",
            "\u001b[32m[04/13 18:16:29 d2.utils.events]: \u001b[0m eta: 1 day, 11:52:37  iter: 6199  total_loss: 0.7117  loss_cls: 0.4281  loss_box_reg: 0.2513  loss_query: 0.02751  time: 2.3751  data_time: 0.0089  lr: 0.006  max_mem: 10549M\n",
            "\u001b[32m[04/13 18:17:17 d2.utils.events]: \u001b[0m eta: 1 day, 11:51:21  iter: 6219  total_loss: 0.7995  loss_cls: 0.4852  loss_box_reg: 0.2814  loss_query: 0.02702  time: 2.3750  data_time: 0.0077  lr: 0.006  max_mem: 10549M\n",
            "\u001b[32m[04/13 18:18:06 d2.utils.events]: \u001b[0m eta: 1 day, 11:49:06  iter: 6239  total_loss: 0.6597  loss_cls: 0.4467  loss_box_reg: 0.2066  loss_query: 0.02488  time: 2.3749  data_time: 0.0077  lr: 0.006  max_mem: 10549M\n",
            "\u001b[32m[04/13 18:18:55 d2.utils.events]: \u001b[0m eta: 1 day, 11:48:41  iter: 6259  total_loss: 0.839  loss_cls: 0.4913  loss_box_reg: 0.2833  loss_query: 0.04619  time: 2.3746  data_time: 0.0076  lr: 0.006  max_mem: 10549M\n",
            "\u001b[32m[04/13 18:19:44 d2.utils.events]: \u001b[0m eta: 1 day, 11:48:22  iter: 6279  total_loss: 0.9078  loss_cls: 0.5965  loss_box_reg: 0.2888  loss_query: 0.03642  time: 2.3746  data_time: 0.0060  lr: 0.006  max_mem: 10549M\n",
            "\u001b[32m[04/13 18:20:33 d2.utils.events]: \u001b[0m eta: 1 day, 11:46:42  iter: 6299  total_loss: 0.9286  loss_cls: 0.582  loss_box_reg: 0.2834  loss_query: 0.04035  time: 2.3745  data_time: 0.0086  lr: 0.006  max_mem: 10549M\n",
            "\u001b[32m[04/13 18:21:21 d2.utils.events]: \u001b[0m eta: 1 day, 11:45:19  iter: 6319  total_loss: 0.9243  loss_cls: 0.5817  loss_box_reg: 0.3204  loss_query: 0.04239  time: 2.3743  data_time: 0.0058  lr: 0.006  max_mem: 10549M\n",
            "\u001b[32m[04/13 18:22:12 d2.utils.events]: \u001b[0m eta: 1 day, 11:44:36  iter: 6339  total_loss: 1.013  loss_cls: 0.5997  loss_box_reg: 0.365  loss_query: 0.03854  time: 2.3745  data_time: 0.0066  lr: 0.006  max_mem: 10549M\n",
            "\u001b[32m[04/13 18:23:01 d2.utils.events]: \u001b[0m eta: 1 day, 11:44:22  iter: 6359  total_loss: 0.7068  loss_cls: 0.4229  loss_box_reg: 0.2304  loss_query: 0.04155  time: 2.3745  data_time: 0.0095  lr: 0.006  max_mem: 10549M\n",
            "\u001b[32m[04/13 18:23:52 d2.utils.events]: \u001b[0m eta: 1 day, 11:44:59  iter: 6379  total_loss: 0.7989  loss_cls: 0.4842  loss_box_reg: 0.2813  loss_query: 0.03586  time: 2.3747  data_time: 0.0118  lr: 0.006  max_mem: 10549M\n",
            "\u001b[32m[04/13 18:24:44 d2.utils.events]: \u001b[0m eta: 1 day, 11:44:05  iter: 6399  total_loss: 0.6461  loss_cls: 0.3782  loss_box_reg: 0.2198  loss_query: 0.02718  time: 2.3749  data_time: 0.0092  lr: 0.006  max_mem: 10549M\n",
            "\u001b[32m[04/13 18:25:33 d2.utils.events]: \u001b[0m eta: 1 day, 11:41:41  iter: 6419  total_loss: 0.8617  loss_cls: 0.5449  loss_box_reg: 0.266  loss_query: 0.02572  time: 2.3748  data_time: 0.0069  lr: 0.006  max_mem: 10549M\n",
            "\u001b[32m[04/13 18:26:21 d2.utils.events]: \u001b[0m eta: 1 day, 11:40:20  iter: 6439  total_loss: 0.9291  loss_cls: 0.5618  loss_box_reg: 0.3273  loss_query: 0.03919  time: 2.3747  data_time: 0.0061  lr: 0.006  max_mem: 10549M\n",
            "\u001b[32m[04/13 18:27:12 d2.utils.events]: \u001b[0m eta: 1 day, 11:39:48  iter: 6459  total_loss: 0.8242  loss_cls: 0.5345  loss_box_reg: 0.2961  loss_query: 0.04551  time: 2.3750  data_time: 0.0090  lr: 0.006  max_mem: 10549M\n",
            "\u001b[32m[04/13 18:28:01 d2.utils.events]: \u001b[0m eta: 1 day, 11:38:55  iter: 6479  total_loss: 0.5843  loss_cls: 0.3664  loss_box_reg: 0.2101  loss_query: 0.02599  time: 2.3748  data_time: 0.0079  lr: 0.006  max_mem: 10549M\n",
            "\u001b[32m[04/13 18:28:51 d2.utils.events]: \u001b[0m eta: 1 day, 11:38:02  iter: 6499  total_loss: 0.8946  loss_cls: 0.5453  loss_box_reg: 0.2912  loss_query: 0.03468  time: 2.3749  data_time: 0.0066  lr: 0.006  max_mem: 10549M\n",
            "\u001b[32m[04/13 18:29:40 d2.utils.events]: \u001b[0m eta: 1 day, 11:37:14  iter: 6519  total_loss: 0.7974  loss_cls: 0.5358  loss_box_reg: 0.2593  loss_query: 0.03393  time: 2.3747  data_time: 0.0069  lr: 0.006  max_mem: 10549M\n",
            "\u001b[32m[04/13 18:30:33 d2.utils.events]: \u001b[0m eta: 1 day, 11:36:07  iter: 6539  total_loss: 0.8637  loss_cls: 0.4973  loss_box_reg: 0.2796  loss_query: 0.04606  time: 2.3749  data_time: 0.0071  lr: 0.006  max_mem: 10549M\n",
            "\u001b[32m[04/13 18:31:22 d2.utils.events]: \u001b[0m eta: 1 day, 11:35:38  iter: 6559  total_loss: 0.811  loss_cls: 0.4881  loss_box_reg: 0.2577  loss_query: 0.03215  time: 2.3749  data_time: 0.0091  lr: 0.006  max_mem: 10549M\n",
            "\u001b[32m[04/13 18:32:09 d2.utils.events]: \u001b[0m eta: 1 day, 11:34:23  iter: 6579  total_loss: 0.7711  loss_cls: 0.4924  loss_box_reg: 0.2743  loss_query: 0.0334  time: 2.3745  data_time: 0.0074  lr: 0.006  max_mem: 10549M\n",
            "\u001b[32m[04/13 18:32:58 d2.utils.events]: \u001b[0m eta: 1 day, 11:33:56  iter: 6599  total_loss: 0.7974  loss_cls: 0.509  loss_box_reg: 0.2739  loss_query: 0.03118  time: 2.3745  data_time: 0.0097  lr: 0.006  max_mem: 10549M\n",
            "\u001b[32m[04/13 18:33:47 d2.utils.events]: \u001b[0m eta: 1 day, 11:33:41  iter: 6619  total_loss: 1.059  loss_cls: 0.6241  loss_box_reg: 0.3221  loss_query: 0.0384  time: 2.3743  data_time: 0.0072  lr: 0.006  max_mem: 10549M\n",
            "\u001b[32m[04/13 18:34:37 d2.utils.events]: \u001b[0m eta: 1 day, 11:32:20  iter: 6639  total_loss: 0.6449  loss_cls: 0.3633  loss_box_reg: 0.2632  loss_query: 0.03254  time: 2.3741  data_time: 0.0074  lr: 0.006  max_mem: 10549M\n",
            "\u001b[32m[04/13 18:35:26 d2.utils.events]: \u001b[0m eta: 1 day, 11:31:43  iter: 6659  total_loss: 1.147  loss_cls: 0.6875  loss_box_reg: 0.3834  loss_query: 0.0433  time: 2.3741  data_time: 0.0066  lr: 0.006  max_mem: 10549M\n",
            "\u001b[32m[04/13 18:36:16 d2.utils.events]: \u001b[0m eta: 1 day, 11:31:31  iter: 6679  total_loss: 0.8541  loss_cls: 0.529  loss_box_reg: 0.2919  loss_query: 0.05748  time: 2.3743  data_time: 0.0098  lr: 0.006  max_mem: 10549M\n",
            "\u001b[32m[04/13 18:37:06 d2.utils.events]: \u001b[0m eta: 1 day, 11:31:16  iter: 6699  total_loss: 0.6417  loss_cls: 0.3877  loss_box_reg: 0.2292  loss_query: 0.03422  time: 2.3743  data_time: 0.0077  lr: 0.006  max_mem: 10549M\n",
            "\u001b[32m[04/13 18:37:54 d2.utils.events]: \u001b[0m eta: 1 day, 11:29:55  iter: 6719  total_loss: 0.7364  loss_cls: 0.4468  loss_box_reg: 0.2717  loss_query: 0.03455  time: 2.3741  data_time: 0.0064  lr: 0.006  max_mem: 10549M\n",
            "\u001b[32m[04/13 18:38:46 d2.utils.events]: \u001b[0m eta: 1 day, 11:30:31  iter: 6739  total_loss: 1.026  loss_cls: 0.6182  loss_box_reg: 0.3575  loss_query: 0.04188  time: 2.3742  data_time: 0.0066  lr: 0.006  max_mem: 10549M\n",
            "\u001b[32m[04/13 18:39:34 d2.utils.events]: \u001b[0m eta: 1 day, 11:30:59  iter: 6759  total_loss: 0.6933  loss_cls: 0.4241  loss_box_reg: 0.2317  loss_query: 0.03778  time: 2.3741  data_time: 0.0076  lr: 0.006  max_mem: 10549M\n",
            "\u001b[32m[04/13 18:40:21 d2.utils.events]: \u001b[0m eta: 1 day, 11:30:11  iter: 6779  total_loss: 0.7595  loss_cls: 0.4968  loss_box_reg: 0.2666  loss_query: 0.01925  time: 2.3738  data_time: 0.0075  lr: 0.006  max_mem: 10549M\n",
            "\u001b[32m[04/13 18:41:11 d2.utils.events]: \u001b[0m eta: 1 day, 11:27:53  iter: 6799  total_loss: 0.8419  loss_cls: 0.5127  loss_box_reg: 0.2611  loss_query: 0.04699  time: 2.3737  data_time: 0.0087  lr: 0.006  max_mem: 10549M\n",
            "\u001b[32m[04/13 18:42:00 d2.utils.events]: \u001b[0m eta: 1 day, 11:28:09  iter: 6819  total_loss: 0.7856  loss_cls: 0.4738  loss_box_reg: 0.2841  loss_query: 0.03117  time: 2.3737  data_time: 0.0075  lr: 0.006  max_mem: 10549M\n",
            "\u001b[32m[04/13 18:42:50 d2.utils.events]: \u001b[0m eta: 1 day, 11:28:07  iter: 6839  total_loss: 0.8865  loss_cls: 0.5324  loss_box_reg: 0.3001  loss_query: 0.03981  time: 2.3737  data_time: 0.0075  lr: 0.006  max_mem: 10549M\n",
            "\u001b[32m[04/13 18:43:38 d2.utils.events]: \u001b[0m eta: 1 day, 11:27:19  iter: 6859  total_loss: 0.842  loss_cls: 0.5332  loss_box_reg: 0.2732  loss_query: 0.04294  time: 2.3734  data_time: 0.0096  lr: 0.006  max_mem: 10549M\n",
            "\u001b[32m[04/13 18:44:29 d2.utils.events]: \u001b[0m eta: 1 day, 11:26:11  iter: 6879  total_loss: 0.7811  loss_cls: 0.4737  loss_box_reg: 0.2797  loss_query: 0.03334  time: 2.3736  data_time: 0.0080  lr: 0.006  max_mem: 10549M\n",
            "\u001b[32m[04/13 18:45:16 d2.utils.events]: \u001b[0m eta: 1 day, 11:23:07  iter: 6899  total_loss: 0.6728  loss_cls: 0.4365  loss_box_reg: 0.2233  loss_query: 0.03297  time: 2.3733  data_time: 0.0067  lr: 0.006  max_mem: 10549M\n",
            "\u001b[32m[04/13 18:46:05 d2.utils.events]: \u001b[0m eta: 1 day, 11:21:55  iter: 6919  total_loss: 0.8109  loss_cls: 0.4648  loss_box_reg: 0.2929  loss_query: 0.03575  time: 2.3732  data_time: 0.0067  lr: 0.006  max_mem: 10549M\n",
            "\u001b[32m[04/13 18:46:57 d2.utils.events]: \u001b[0m eta: 1 day, 11:23:21  iter: 6939  total_loss: 0.8963  loss_cls: 0.5405  loss_box_reg: 0.3191  loss_query: 0.03646  time: 2.3733  data_time: 0.0081  lr: 0.006  max_mem: 10549M\n",
            "\u001b[32m[04/13 18:47:48 d2.utils.events]: \u001b[0m eta: 1 day, 11:22:03  iter: 6959  total_loss: 0.8786  loss_cls: 0.4922  loss_box_reg: 0.3595  loss_query: 0.04073  time: 2.3735  data_time: 0.0092  lr: 0.006  max_mem: 10549M\n",
            "\u001b[32m[04/13 18:48:39 d2.utils.events]: \u001b[0m eta: 1 day, 11:21:48  iter: 6979  total_loss: 0.6845  loss_cls: 0.4264  loss_box_reg: 0.2248  loss_query: 0.03614  time: 2.3737  data_time: 0.0067  lr: 0.006  max_mem: 10549M\n",
            "\u001b[32m[04/13 18:49:28 d2.utils.events]: \u001b[0m eta: 1 day, 11:20:07  iter: 6999  total_loss: 0.7075  loss_cls: 0.4187  loss_box_reg: 0.2528  loss_query: 0.032  time: 2.3736  data_time: 0.0068  lr: 0.006  max_mem: 10549M\n",
            "\u001b[32m[04/13 18:50:17 d2.utils.events]: \u001b[0m eta: 1 day, 11:17:35  iter: 7019  total_loss: 0.7382  loss_cls: 0.4492  loss_box_reg: 0.2739  loss_query: 0.02632  time: 2.3735  data_time: 0.0072  lr: 0.006  max_mem: 10549M\n",
            "\u001b[32m[04/13 18:51:06 d2.utils.events]: \u001b[0m eta: 1 day, 11:16:47  iter: 7039  total_loss: 0.8376  loss_cls: 0.5413  loss_box_reg: 0.2923  loss_query: 0.02839  time: 2.3735  data_time: 0.0078  lr: 0.006  max_mem: 10549M\n",
            "\u001b[32m[04/13 18:51:55 d2.utils.events]: \u001b[0m eta: 1 day, 11:15:25  iter: 7059  total_loss: 0.9847  loss_cls: 0.6164  loss_box_reg: 0.3294  loss_query: 0.04193  time: 2.3733  data_time: 0.0074  lr: 0.006  max_mem: 10549M\n",
            "\u001b[32m[04/13 18:52:45 d2.utils.events]: \u001b[0m eta: 1 day, 11:15:02  iter: 7079  total_loss: 0.8006  loss_cls: 0.4409  loss_box_reg: 0.2876  loss_query: 0.06795  time: 2.3732  data_time: 0.0084  lr: 0.006  max_mem: 10549M\n",
            "\u001b[32m[04/13 18:53:34 d2.utils.events]: \u001b[0m eta: 1 day, 11:15:08  iter: 7099  total_loss: 0.5956  loss_cls: 0.3621  loss_box_reg: 0.1709  loss_query: 0.03761  time: 2.3731  data_time: 0.0068  lr: 0.006  max_mem: 10549M\n",
            "\u001b[32m[04/13 18:54:24 d2.utils.events]: \u001b[0m eta: 1 day, 11:15:19  iter: 7119  total_loss: 1.23  loss_cls: 0.7366  loss_box_reg: 0.437  loss_query: 0.0549  time: 2.3733  data_time: 0.0129  lr: 0.006  max_mem: 10549M\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "qASvAZjzFrZZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!python train_visdrone.py --resume --config /content/gdrive/MyDrive/QueryDet-PyTorch/work_dirs/visdrone_querydet/config.yaml --num-gpu 1 OUTPUT_DIR work_dirs/visdrone_querydet"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IldnI3Qli4Ww",
        "outputId": "3fc0060b-1fe4-4c1c-b85d-bbff80e2377a",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "** fvcore version of PathManager will be deprecated soon. **\n",
            "** Please migrate to the version in iopath repo. **\n",
            "https://github.com/facebookresearch/iopath \n",
            "\n",
            "Command Line Args: Namespace(config_file='/content/gdrive/MyDrive/QueryDet-PyTorch/work_dirs/visdrone_querydet/config.yaml', dist_url='tcp://127.0.0.1:49152', eval_only=False, machine_rank=0, no_pretrain=False, num_gpus=1, num_machines=1, opts=['OUTPUT_DIR', 'work_dirs/visdrone_querydet'], resume=True)\n",
            "\u001b[32m[04/14 07:59:09 detectron2]: \u001b[0mRank of current process: 0. World size: 1\n",
            "\u001b[32m[04/14 07:59:16 detectron2]: \u001b[0mEnvironment info:\n",
            "----------------------  ---------------------------------------------------------------\n",
            "sys.platform            linux\n",
            "Python                  3.7.6 (default, Jan  8 2020, 19:59:22) [GCC 7.3.0]\n",
            "numpy                   1.21.6\n",
            "detectron2              0.4 @/usr/local/lib/python3.7/site-packages/detectron2\n",
            "Compiler                GCC 7.3\n",
            "CUDA compiler           CUDA 10.1\n",
            "detectron2 arch flags   3.7, 5.0, 5.2, 6.0, 6.1, 7.0, 7.5\n",
            "DETECTRON2_ENV_MODULE   <not set>\n",
            "PyTorch                 1.8.1+cu101 @/usr/local/lib/python3.7/site-packages/torch\n",
            "PyTorch debug build     False\n",
            "GPU available           True\n",
            "GPU 0                   Tesla T4 (arch=7.5)\n",
            "CUDA_HOME               /usr/local/cuda\n",
            "Pillow                  9.5.0\n",
            "torchvision             0.9.1+cu101 @/usr/local/lib/python3.7/site-packages/torchvision\n",
            "torchvision arch flags  3.5, 5.0, 6.0, 7.0, 7.5\n",
            "fvcore                  0.1.3.post20210317\n",
            "cv2                     4.7.0\n",
            "----------------------  ---------------------------------------------------------------\n",
            "PyTorch built with:\n",
            "  - GCC 7.3\n",
            "  - C++ Version: 201402\n",
            "  - Intel(R) Math Kernel Library Version 2020.0.0 Product Build 20191122 for Intel(R) 64 architecture applications\n",
            "  - Intel(R) MKL-DNN v1.7.0 (Git Hash 7aed236906b1f7a05c0917e5257a1af05e9ff683)\n",
            "  - OpenMP 201511 (a.k.a. OpenMP 4.5)\n",
            "  - NNPACK is enabled\n",
            "  - CPU capability usage: AVX2\n",
            "  - CUDA Runtime 10.1\n",
            "  - NVCC architecture flags: -gencode;arch=compute_37,code=sm_37;-gencode;arch=compute_50,code=sm_50;-gencode;arch=compute_60,code=sm_60;-gencode;arch=compute_70,code=sm_70\n",
            "  - CuDNN 7.6.3\n",
            "  - Magma 2.5.2\n",
            "  - Build settings: BLAS_INFO=mkl, BUILD_TYPE=Release, CUDA_VERSION=10.1, CUDNN_VERSION=7.6.3, CXX_COMPILER=/opt/rh/devtoolset-7/root/usr/bin/c++, CXX_FLAGS= -Wno-deprecated -fvisibility-inlines-hidden -DUSE_PTHREADPOOL -fopenmp -DNDEBUG -DUSE_KINETO -DUSE_FBGEMM -DUSE_QNNPACK -DUSE_PYTORCH_QNNPACK -DUSE_XNNPACK -O2 -fPIC -Wno-narrowing -Wall -Wextra -Werror=return-type -Wno-missing-field-initializers -Wno-type-limits -Wno-array-bounds -Wno-unknown-pragmas -Wno-sign-compare -Wno-unused-parameter -Wno-unused-variable -Wno-unused-function -Wno-unused-result -Wno-unused-local-typedefs -Wno-strict-overflow -Wno-strict-aliasing -Wno-error=deprecated-declarations -Wno-stringop-overflow -Wno-psabi -Wno-error=pedantic -Wno-error=redundant-decls -Wno-error=old-style-cast -fdiagnostics-color=always -faligned-new -Wno-unused-but-set-variable -Wno-maybe-uninitialized -fno-math-errno -fno-trapping-math -Werror=format -Wno-stringop-overflow, LAPACK_INFO=mkl, PERF_WITH_AVX=1, PERF_WITH_AVX2=1, PERF_WITH_AVX512=1, TORCH_VERSION=1.8.1, USE_CUDA=ON, USE_CUDNN=ON, USE_EXCEPTION_PTR=1, USE_GFLAGS=OFF, USE_GLOG=OFF, USE_MKL=ON, USE_MKLDNN=ON, USE_MPI=OFF, USE_NCCL=ON, USE_NNPACK=ON, USE_OPENMP=ON, \n",
            "\n",
            "\u001b[32m[04/14 07:59:16 detectron2]: \u001b[0mCommand line arguments: Namespace(config_file='/content/gdrive/MyDrive/QueryDet-PyTorch/work_dirs/visdrone_querydet/config.yaml', dist_url='tcp://127.0.0.1:49152', eval_only=False, machine_rank=0, no_pretrain=False, num_gpus=1, num_machines=1, opts=['OUTPUT_DIR', 'work_dirs/visdrone_querydet'], resume=True)\n",
            "\u001b[32m[04/14 07:59:16 detectron2]: \u001b[0mContents of args.config_file=/content/gdrive/MyDrive/QueryDet-PyTorch/work_dirs/visdrone_querydet/config.yaml:\n",
            "CUDNN_BENCHMARK: false\n",
            "DATALOADER:\n",
            "  ASPECT_RATIO_GROUPING: true\n",
            "  FILTER_EMPTY_ANNOTATIONS: true\n",
            "  NUM_WORKERS: 4\n",
            "  REPEAT_THRESHOLD: 0.0\n",
            "  SAMPLER_TRAIN: TrainingSampler\n",
            "DATASETS:\n",
            "  PRECOMPUTED_PROPOSAL_TOPK_TEST: 1000\n",
            "  PRECOMPUTED_PROPOSAL_TOPK_TRAIN: 2000\n",
            "  PROPOSAL_FILES_TEST: []\n",
            "  PROPOSAL_FILES_TRAIN: []\n",
            "  TEST:\n",
            "  - coco_2017_val\n",
            "  TRAIN:\n",
            "  - coco_2017_train\n",
            "GLOBAL:\n",
            "  HACK: 1.0\n",
            "INPUT:\n",
            "  CROP:\n",
            "    ENABLED: false\n",
            "    SIZE:\n",
            "    - 0.9\n",
            "    - 0.9\n",
            "    TYPE: relative_range\n",
            "  FORMAT: BGR\n",
            "  MASK_FORMAT: polygon\n",
            "  MAX_SIZE_TEST: 1333\n",
            "  MAX_SIZE_TRAIN: 1333\n",
            "  MIN_SIZE_TEST: 800\n",
            "  MIN_SIZE_TRAIN:\n",
            "  - 640\n",
            "  - 672\n",
            "  - 704\n",
            "  - 736\n",
            "  - 768\n",
            "  - 800\n",
            "  MIN_SIZE_TRAIN_SAMPLING: choice\n",
            "  RANDOM_FLIP: horizontal\n",
            "META_INFO:\n",
            "  EVAL_AP: true\n",
            "  EVAL_GPU_TIME: true\n",
            "  VIS_ROOT: ''\n",
            "MODEL:\n",
            "  ANCHOR_GENERATOR:\n",
            "    ANGLES:\n",
            "    - - -90\n",
            "      - 0\n",
            "      - 90\n",
            "    ASPECT_RATIOS:\n",
            "    - - 0.5\n",
            "      - 1.0\n",
            "      - 2.0\n",
            "    NAME: AnchorGeneratorWithCenter\n",
            "    OFFSET: 0.0\n",
            "    SIZES:\n",
            "    - - 16\n",
            "      - 20.15873679831797\n",
            "      - 25.39841683149119\n",
            "    - - 32\n",
            "      - 40.31747359663594\n",
            "      - 50.79683366298238\n",
            "    - - 64\n",
            "      - 80.63494719327188\n",
            "      - 101.59366732596476\n",
            "    - - 128\n",
            "      - 161.26989438654377\n",
            "      - 203.18733465192952\n",
            "    - - 256\n",
            "      - 322.53978877308754\n",
            "      - 406.37466930385904\n",
            "    - - 512\n",
            "      - 645.0795775461751\n",
            "      - 812.7493386077181\n",
            "  BACKBONE:\n",
            "    FREEZE_AT: 2\n",
            "    NAME: build_retinanet_resnet_fpn_backbone\n",
            "  CUSTOM:\n",
            "    CLEAR_CUDA_CACHE: false\n",
            "    CLS_WEIGHTS:\n",
            "    - 1.0\n",
            "    - 1.4\n",
            "    - 1.8\n",
            "    - 2.2\n",
            "    - 2.6\n",
            "    - 2.6\n",
            "    FOCAL_LOSS_ALPHAS:\n",
            "    - 0.25\n",
            "    - 0.25\n",
            "    - 0.25\n",
            "    - 0.25\n",
            "    - 0.25\n",
            "    - 0.25\n",
            "    FOCAL_LOSS_GAMMAS:\n",
            "    - 2.0\n",
            "    - 2.0\n",
            "    - 2.0\n",
            "    - 2.0\n",
            "    - 2.0\n",
            "    - 2.0\n",
            "    GIOU_LOSS: false\n",
            "    GRADIENT_CHECKPOINT: false\n",
            "    HEAD_BN: false\n",
            "    REG_WEIGHTS:\n",
            "    - 1.0\n",
            "    - 1.4\n",
            "    - 1.8\n",
            "    - 2.2\n",
            "    - 2.6\n",
            "    - 2.6\n",
            "    SOFT_NMS_METHOD: linear\n",
            "    SOFT_NMS_PRUND: 0.001\n",
            "    SOFT_NMS_SIGMA: 0.5\n",
            "    SOFT_NMS_THRESHOLD: 0.5\n",
            "    USE_LOOP_MATCHER: true\n",
            "    USE_SOFT_NMS: false\n",
            "  DEVICE: cuda\n",
            "  FPN:\n",
            "    FUSE_TYPE: sum\n",
            "    IN_FEATURES:\n",
            "    - res2\n",
            "    - res3\n",
            "    - res4\n",
            "    - res5\n",
            "    NORM: ''\n",
            "    OUT_CHANNELS: 256\n",
            "    TOP_LEVELS: 2\n",
            "  KEYPOINT_ON: false\n",
            "  LOAD_PROPOSALS: false\n",
            "  MASK_ON: false\n",
            "  META_ARCHITECTURE: RetinaNetQueryDet\n",
            "  PANOPTIC_FPN:\n",
            "    COMBINE:\n",
            "      ENABLED: true\n",
            "      INSTANCES_CONFIDENCE_THRESH: 0.5\n",
            "      OVERLAP_THRESH: 0.5\n",
            "      STUFF_AREA_LIMIT: 4096\n",
            "    INSTANCE_LOSS_WEIGHT: 1.0\n",
            "  PIXEL_MEAN:\n",
            "  - 103.53\n",
            "  - 116.28\n",
            "  - 123.675\n",
            "  PIXEL_STD:\n",
            "  - 1.0\n",
            "  - 1.0\n",
            "  - 1.0\n",
            "  PROPOSAL_GENERATOR:\n",
            "    MIN_SIZE: 0\n",
            "    NAME: RPN\n",
            "  QUERY:\n",
            "    CONTEXT: 2\n",
            "    ENCODE_CENTER_DIS_COEFF:\n",
            "    - 1.0\n",
            "    - 1.0\n",
            "    ENCODE_SMALL_OBJ_SCALE:\n",
            "    - - 0\n",
            "      - 32\n",
            "    - - 0\n",
            "      - 64\n",
            "    FEATURES_VALUE_TEST:\n",
            "    - 0\n",
            "    - 1\n",
            "    FEATURES_VALUE_TRAIN:\n",
            "    - 0\n",
            "    - 1\n",
            "    FEATURES_WHOLE_TEST:\n",
            "    - 2\n",
            "    - 3\n",
            "    - 4\n",
            "    - 5\n",
            "    FEATURES_WHOLE_TRAIN:\n",
            "    - 2\n",
            "    - 3\n",
            "    - 4\n",
            "    - 5\n",
            "    QUERY_INFER: false\n",
            "    QUERY_LOSS_GAMMA:\n",
            "    - 1.3\n",
            "    - 1.3\n",
            "    QUERY_LOSS_WEIGHT:\n",
            "    - 10.0\n",
            "    - 10.0\n",
            "    Q_FEATURE_TEST:\n",
            "    - 1\n",
            "    - 2\n",
            "    Q_FEATURE_TRAIN:\n",
            "    - 1\n",
            "    - 2\n",
            "    THRESHOLD: 0.12\n",
            "  RESNETS:\n",
            "    DEFORM_MODULATED: false\n",
            "    DEFORM_NUM_GROUPS: 1\n",
            "    DEFORM_ON_PER_STAGE:\n",
            "    - false\n",
            "    - false\n",
            "    - false\n",
            "    - false\n",
            "    DEPTH: 50\n",
            "    NORM: FrozenBN\n",
            "    NUM_GROUPS: 1\n",
            "    OUT_FEATURES:\n",
            "    - res2\n",
            "    - res3\n",
            "    - res4\n",
            "    - res5\n",
            "    RES2_OUT_CHANNELS: 256\n",
            "    RES5_DILATION: 1\n",
            "    STEM_OUT_CHANNELS: 64\n",
            "    STRIDE_IN_1X1: true\n",
            "    WIDTH_PER_GROUP: 64\n",
            "  RETINANET:\n",
            "    BBOX_REG_LOSS_TYPE: smooth_l1\n",
            "    BBOX_REG_WEIGHTS:\n",
            "    - 1.0\n",
            "    - 1.0\n",
            "    - 1.0\n",
            "    - 1.0\n",
            "    FOCAL_LOSS_ALPHA: 0.25\n",
            "    FOCAL_LOSS_GAMMA: 2.0\n",
            "    IN_FEATURES:\n",
            "    - p2\n",
            "    - p3\n",
            "    - p4\n",
            "    - p5\n",
            "    - p6\n",
            "    - p7\n",
            "    IOU_LABELS:\n",
            "    - 0\n",
            "    - -1\n",
            "    - 1\n",
            "    IOU_THRESHOLDS:\n",
            "    - 0.4\n",
            "    - 0.5\n",
            "    NMS_THRESH_TEST: 0.5\n",
            "    NORM: ''\n",
            "    NUM_CLASSES: 10\n",
            "    NUM_CONVS: 4\n",
            "    PRIOR_PROB: 0.01\n",
            "    SCORE_THRESH_TEST: 0.05\n",
            "    SMOOTH_L1_LOSS_BETA: 0.1\n",
            "    TOPK_CANDIDATES_TEST: 1000\n",
            "  ROI_BOX_CASCADE_HEAD:\n",
            "    BBOX_REG_WEIGHTS:\n",
            "    - - 10.0\n",
            "      - 10.0\n",
            "      - 5.0\n",
            "      - 5.0\n",
            "    - - 20.0\n",
            "      - 20.0\n",
            "      - 10.0\n",
            "      - 10.0\n",
            "    - - 30.0\n",
            "      - 30.0\n",
            "      - 15.0\n",
            "      - 15.0\n",
            "    IOUS:\n",
            "    - 0.5\n",
            "    - 0.6\n",
            "    - 0.7\n",
            "  ROI_BOX_HEAD:\n",
            "    BBOX_REG_LOSS_TYPE: smooth_l1\n",
            "    BBOX_REG_LOSS_WEIGHT: 1.0\n",
            "    BBOX_REG_WEIGHTS:\n",
            "    - 10.0\n",
            "    - 10.0\n",
            "    - 5.0\n",
            "    - 5.0\n",
            "    CLS_AGNOSTIC_BBOX_REG: false\n",
            "    CONV_DIM: 256\n",
            "    FC_DIM: 1024\n",
            "    NAME: ''\n",
            "    NORM: ''\n",
            "    NUM_CONV: 0\n",
            "    NUM_FC: 0\n",
            "    POOLER_RESOLUTION: 14\n",
            "    POOLER_SAMPLING_RATIO: 0\n",
            "    POOLER_TYPE: ROIAlignV2\n",
            "    SMOOTH_L1_BETA: 0.0\n",
            "    TRAIN_ON_PRED_BOXES: false\n",
            "  ROI_HEADS:\n",
            "    BATCH_SIZE_PER_IMAGE: 512\n",
            "    IN_FEATURES:\n",
            "    - res4\n",
            "    IOU_LABELS:\n",
            "    - 0\n",
            "    - 1\n",
            "    IOU_THRESHOLDS:\n",
            "    - 0.5\n",
            "    NAME: Res5ROIHeads\n",
            "    NMS_THRESH_TEST: 0.5\n",
            "    NUM_CLASSES: 80\n",
            "    POSITIVE_FRACTION: 0.25\n",
            "    PROPOSAL_APPEND_GT: true\n",
            "    SCORE_THRESH_TEST: 0.05\n",
            "  ROI_KEYPOINT_HEAD:\n",
            "    CONV_DIMS:\n",
            "    - 512\n",
            "    - 512\n",
            "    - 512\n",
            "    - 512\n",
            "    - 512\n",
            "    - 512\n",
            "    - 512\n",
            "    - 512\n",
            "    LOSS_WEIGHT: 1.0\n",
            "    MIN_KEYPOINTS_PER_IMAGE: 1\n",
            "    NAME: KRCNNConvDeconvUpsampleHead\n",
            "    NORMALIZE_LOSS_BY_VISIBLE_KEYPOINTS: true\n",
            "    NUM_KEYPOINTS: 17\n",
            "    POOLER_RESOLUTION: 14\n",
            "    POOLER_SAMPLING_RATIO: 0\n",
            "    POOLER_TYPE: ROIAlignV2\n",
            "  ROI_MASK_HEAD:\n",
            "    CLS_AGNOSTIC_MASK: false\n",
            "    CONV_DIM: 256\n",
            "    NAME: MaskRCNNConvUpsampleHead\n",
            "    NORM: ''\n",
            "    NUM_CONV: 0\n",
            "    POOLER_RESOLUTION: 14\n",
            "    POOLER_SAMPLING_RATIO: 0\n",
            "    POOLER_TYPE: ROIAlignV2\n",
            "  RPN:\n",
            "    BATCH_SIZE_PER_IMAGE: 256\n",
            "    BBOX_REG_LOSS_TYPE: smooth_l1\n",
            "    BBOX_REG_LOSS_WEIGHT: 1.0\n",
            "    BBOX_REG_WEIGHTS:\n",
            "    - 1.0\n",
            "    - 1.0\n",
            "    - 1.0\n",
            "    - 1.0\n",
            "    BOUNDARY_THRESH: -1\n",
            "    HEAD_NAME: StandardRPNHead\n",
            "    IN_FEATURES:\n",
            "    - res4\n",
            "    IOU_LABELS:\n",
            "    - 0\n",
            "    - -1\n",
            "    - 1\n",
            "    IOU_THRESHOLDS:\n",
            "    - 0.3\n",
            "    - 0.7\n",
            "    LOSS_WEIGHT: 1.0\n",
            "    NMS_THRESH: 0.7\n",
            "    POSITIVE_FRACTION: 0.5\n",
            "    POST_NMS_TOPK_TEST: 1000\n",
            "    POST_NMS_TOPK_TRAIN: 2000\n",
            "    PRE_NMS_TOPK_TEST: 6000\n",
            "    PRE_NMS_TOPK_TRAIN: 12000\n",
            "    SMOOTH_L1_BETA: 0.0\n",
            "  SEM_SEG_HEAD:\n",
            "    COMMON_STRIDE: 4\n",
            "    CONVS_DIM: 128\n",
            "    IGNORE_VALUE: 255\n",
            "    IN_FEATURES:\n",
            "    - p2\n",
            "    - p3\n",
            "    - p4\n",
            "    - p5\n",
            "    LOSS_WEIGHT: 1.0\n",
            "    NAME: SemSegFPNHead\n",
            "    NORM: GN\n",
            "    NUM_CLASSES: 54\n",
            "  WEIGHTS: /content/gdrive/MyDrive/QueryDet-PyTorch/work_dirs/visdrone_querydet/model_0005999.pth\n",
            "OUTPUT_DIR: work_dirs/visdrone_querydet\n",
            "SEED: -1\n",
            "SOLVER:\n",
            "  AMP:\n",
            "    ENABLED: true\n",
            "  BASE_LR: 0.006\n",
            "  BIAS_LR_FACTOR: 1.0\n",
            "  CHECKPOINT_PERIOD: 2000\n",
            "  CLIP_GRADIENTS:\n",
            "    CLIP_TYPE: value\n",
            "    CLIP_VALUE: 35.0\n",
            "    ENABLED: true\n",
            "    NORM_TYPE: 2.0\n",
            "  GAMMA: 0.1\n",
            "  IMS_PER_BATCH: 3\n",
            "  LR_SCHEDULER_NAME: WarmupMultiStepLR\n",
            "  MAX_ITER: 60000\n",
            "  MOMENTUM: 0.9\n",
            "  NESTEROV: false\n",
            "  REFERENCE_WORLD_SIZE: 0\n",
            "  STEPS:\n",
            "  - 15000\n",
            "  - 20000\n",
            "  WARMUP_FACTOR: 0.001\n",
            "  WARMUP_ITERS: 1000\n",
            "  WARMUP_METHOD: linear\n",
            "  WEIGHT_DECAY: 0.0001\n",
            "  WEIGHT_DECAY_BIAS: 0.0001\n",
            "  WEIGHT_DECAY_NORM: 0.0\n",
            "TEST:\n",
            "  AUG:\n",
            "    ENABLED: false\n",
            "    FLIP: true\n",
            "    MAX_SIZE: 4000\n",
            "    MIN_SIZES:\n",
            "    - 400\n",
            "    - 500\n",
            "    - 600\n",
            "    - 700\n",
            "    - 800\n",
            "    - 900\n",
            "    - 1000\n",
            "    - 1100\n",
            "    - 1200\n",
            "  DETECTIONS_PER_IMAGE: 500\n",
            "  EVAL_PERIOD: 0\n",
            "  EXPECTED_RESULTS: []\n",
            "  KEYPOINT_OKS_SIGMAS: []\n",
            "  PRECISE_BN:\n",
            "    ENABLED: false\n",
            "    NUM_ITER: 200\n",
            "VERSION: 2\n",
            "VISDRONE:\n",
            "  MAX_LENGTH: 1999\n",
            "  SHORT_LENGTH:\n",
            "  - 1200\n",
            "  TEST_IMG_ROOT: data/visdrone/coco_format/val_images\n",
            "  TEST_JSON: data/visdrone/coco_format/annotations/val_label.json\n",
            "  TEST_LENGTH: 3999\n",
            "  TRAIN_JSON: data/visdrone/coco_format/annotations/train_label.json\n",
            "  TRING_IMG_ROOT: data//visdrone/coco_format/train_images\n",
            "VIS_PERIOD: 0\n",
            "\n",
            "\u001b[32m[04/14 07:59:16 detectron2]: \u001b[0mRunning with full config:\n",
            "CUDNN_BENCHMARK: False\n",
            "DATALOADER:\n",
            "  ASPECT_RATIO_GROUPING: True\n",
            "  FILTER_EMPTY_ANNOTATIONS: True\n",
            "  NUM_WORKERS: 4\n",
            "  REPEAT_THRESHOLD: 0.0\n",
            "  SAMPLER_TRAIN: TrainingSampler\n",
            "DATASETS:\n",
            "  PRECOMPUTED_PROPOSAL_TOPK_TEST: 1000\n",
            "  PRECOMPUTED_PROPOSAL_TOPK_TRAIN: 2000\n",
            "  PROPOSAL_FILES_TEST: ()\n",
            "  PROPOSAL_FILES_TRAIN: ()\n",
            "  TEST: ('coco_2017_val',)\n",
            "  TRAIN: ('coco_2017_train',)\n",
            "GLOBAL:\n",
            "  HACK: 1.0\n",
            "INPUT:\n",
            "  CROP:\n",
            "    ENABLED: False\n",
            "    SIZE: [0.9, 0.9]\n",
            "    TYPE: relative_range\n",
            "  FORMAT: BGR\n",
            "  MASK_FORMAT: polygon\n",
            "  MAX_SIZE_TEST: 1333\n",
            "  MAX_SIZE_TRAIN: 1333\n",
            "  MIN_SIZE_TEST: 800\n",
            "  MIN_SIZE_TRAIN: (640, 672, 704, 736, 768, 800)\n",
            "  MIN_SIZE_TRAIN_SAMPLING: choice\n",
            "  RANDOM_FLIP: horizontal\n",
            "META_INFO:\n",
            "  EVAL_AP: True\n",
            "  EVAL_GPU_TIME: True\n",
            "  VIS_ROOT: \n",
            "MODEL:\n",
            "  ANCHOR_GENERATOR:\n",
            "    ANGLES: [[-90, 0, 90]]\n",
            "    ASPECT_RATIOS: [[0.5, 1.0, 2.0]]\n",
            "    NAME: AnchorGeneratorWithCenter\n",
            "    OFFSET: 0.0\n",
            "    SIZES: [[16, 20.15873679831797, 25.39841683149119], [32, 40.31747359663594, 50.79683366298238], [64, 80.63494719327188, 101.59366732596476], [128, 161.26989438654377, 203.18733465192952], [256, 322.53978877308754, 406.37466930385904], [512, 645.0795775461751, 812.7493386077181]]\n",
            "  BACKBONE:\n",
            "    FREEZE_AT: 2\n",
            "    NAME: build_retinanet_resnet_fpn_backbone\n",
            "  CUSTOM:\n",
            "    CLEAR_CUDA_CACHE: False\n",
            "    CLS_WEIGHTS: [1.0, 1.4, 1.8, 2.2, 2.6, 2.6]\n",
            "    FOCAL_LOSS_ALPHAS: [0.25, 0.25, 0.25, 0.25, 0.25, 0.25]\n",
            "    FOCAL_LOSS_GAMMAS: [2.0, 2.0, 2.0, 2.0, 2.0, 2.0]\n",
            "    GIOU_LOSS: False\n",
            "    GRADIENT_CHECKPOINT: False\n",
            "    HEAD_BN: False\n",
            "    REG_WEIGHTS: [1.0, 1.4, 1.8, 2.2, 2.6, 2.6]\n",
            "    SOFT_NMS_METHOD: linear\n",
            "    SOFT_NMS_PRUND: 0.001\n",
            "    SOFT_NMS_SIGMA: 0.5\n",
            "    SOFT_NMS_THRESHOLD: 0.5\n",
            "    USE_LOOP_MATCHER: True\n",
            "    USE_SOFT_NMS: False\n",
            "  DEVICE: cuda\n",
            "  FPN:\n",
            "    FUSE_TYPE: sum\n",
            "    IN_FEATURES: ['res2', 'res3', 'res4', 'res5']\n",
            "    NORM: \n",
            "    OUT_CHANNELS: 256\n",
            "    TOP_LEVELS: 2\n",
            "  KEYPOINT_ON: False\n",
            "  LOAD_PROPOSALS: False\n",
            "  MASK_ON: False\n",
            "  META_ARCHITECTURE: RetinaNetQueryDet\n",
            "  PANOPTIC_FPN:\n",
            "    COMBINE:\n",
            "      ENABLED: True\n",
            "      INSTANCES_CONFIDENCE_THRESH: 0.5\n",
            "      OVERLAP_THRESH: 0.5\n",
            "      STUFF_AREA_LIMIT: 4096\n",
            "    INSTANCE_LOSS_WEIGHT: 1.0\n",
            "  PIXEL_MEAN: [103.53, 116.28, 123.675]\n",
            "  PIXEL_STD: [1.0, 1.0, 1.0]\n",
            "  PROPOSAL_GENERATOR:\n",
            "    MIN_SIZE: 0\n",
            "    NAME: RPN\n",
            "  QUERY:\n",
            "    CONTEXT: 2\n",
            "    ENCODE_CENTER_DIS_COEFF: [1.0, 1.0]\n",
            "    ENCODE_SMALL_OBJ_SCALE: [[0, 32], [0, 64]]\n",
            "    FEATURES_VALUE_TEST: [0, 1]\n",
            "    FEATURES_VALUE_TRAIN: [0, 1]\n",
            "    FEATURES_WHOLE_TEST: [2, 3, 4, 5]\n",
            "    FEATURES_WHOLE_TRAIN: [2, 3, 4, 5]\n",
            "    QUERY_INFER: False\n",
            "    QUERY_LOSS_GAMMA: [1.3, 1.3]\n",
            "    QUERY_LOSS_WEIGHT: [10.0, 10.0]\n",
            "    Q_FEATURE_TEST: [1, 2]\n",
            "    Q_FEATURE_TRAIN: [1, 2]\n",
            "    THRESHOLD: 0.12\n",
            "  RESNETS:\n",
            "    DEFORM_MODULATED: False\n",
            "    DEFORM_NUM_GROUPS: 1\n",
            "    DEFORM_ON_PER_STAGE: [False, False, False, False]\n",
            "    DEPTH: 50\n",
            "    NORM: FrozenBN\n",
            "    NUM_GROUPS: 1\n",
            "    OUT_FEATURES: ['res2', 'res3', 'res4', 'res5']\n",
            "    RES2_OUT_CHANNELS: 256\n",
            "    RES5_DILATION: 1\n",
            "    STEM_OUT_CHANNELS: 64\n",
            "    STRIDE_IN_1X1: True\n",
            "    WIDTH_PER_GROUP: 64\n",
            "  RETINANET:\n",
            "    BBOX_REG_LOSS_TYPE: smooth_l1\n",
            "    BBOX_REG_WEIGHTS: (1.0, 1.0, 1.0, 1.0)\n",
            "    FOCAL_LOSS_ALPHA: 0.25\n",
            "    FOCAL_LOSS_GAMMA: 2.0\n",
            "    IN_FEATURES: ['p2', 'p3', 'p4', 'p5', 'p6', 'p7']\n",
            "    IOU_LABELS: [0, -1, 1]\n",
            "    IOU_THRESHOLDS: [0.4, 0.5]\n",
            "    NMS_THRESH_TEST: 0.5\n",
            "    NORM: \n",
            "    NUM_CLASSES: 10\n",
            "    NUM_CONVS: 4\n",
            "    PRIOR_PROB: 0.01\n",
            "    SCORE_THRESH_TEST: 0.05\n",
            "    SMOOTH_L1_LOSS_BETA: 0.1\n",
            "    TOPK_CANDIDATES_TEST: 1000\n",
            "  ROI_BOX_CASCADE_HEAD:\n",
            "    BBOX_REG_WEIGHTS: ([10.0, 10.0, 5.0, 5.0], [20.0, 20.0, 10.0, 10.0], [30.0, 30.0, 15.0, 15.0])\n",
            "    IOUS: (0.5, 0.6, 0.7)\n",
            "  ROI_BOX_HEAD:\n",
            "    BBOX_REG_LOSS_TYPE: smooth_l1\n",
            "    BBOX_REG_LOSS_WEIGHT: 1.0\n",
            "    BBOX_REG_WEIGHTS: (10.0, 10.0, 5.0, 5.0)\n",
            "    CLS_AGNOSTIC_BBOX_REG: False\n",
            "    CONV_DIM: 256\n",
            "    FC_DIM: 1024\n",
            "    NAME: \n",
            "    NORM: \n",
            "    NUM_CONV: 0\n",
            "    NUM_FC: 0\n",
            "    POOLER_RESOLUTION: 14\n",
            "    POOLER_SAMPLING_RATIO: 0\n",
            "    POOLER_TYPE: ROIAlignV2\n",
            "    SMOOTH_L1_BETA: 0.0\n",
            "    TRAIN_ON_PRED_BOXES: False\n",
            "  ROI_HEADS:\n",
            "    BATCH_SIZE_PER_IMAGE: 512\n",
            "    IN_FEATURES: ['res4']\n",
            "    IOU_LABELS: [0, 1]\n",
            "    IOU_THRESHOLDS: [0.5]\n",
            "    NAME: Res5ROIHeads\n",
            "    NMS_THRESH_TEST: 0.5\n",
            "    NUM_CLASSES: 80\n",
            "    POSITIVE_FRACTION: 0.25\n",
            "    PROPOSAL_APPEND_GT: True\n",
            "    SCORE_THRESH_TEST: 0.05\n",
            "  ROI_KEYPOINT_HEAD:\n",
            "    CONV_DIMS: (512, 512, 512, 512, 512, 512, 512, 512)\n",
            "    LOSS_WEIGHT: 1.0\n",
            "    MIN_KEYPOINTS_PER_IMAGE: 1\n",
            "    NAME: KRCNNConvDeconvUpsampleHead\n",
            "    NORMALIZE_LOSS_BY_VISIBLE_KEYPOINTS: True\n",
            "    NUM_KEYPOINTS: 17\n",
            "    POOLER_RESOLUTION: 14\n",
            "    POOLER_SAMPLING_RATIO: 0\n",
            "    POOLER_TYPE: ROIAlignV2\n",
            "  ROI_MASK_HEAD:\n",
            "    CLS_AGNOSTIC_MASK: False\n",
            "    CONV_DIM: 256\n",
            "    NAME: MaskRCNNConvUpsampleHead\n",
            "    NORM: \n",
            "    NUM_CONV: 0\n",
            "    POOLER_RESOLUTION: 14\n",
            "    POOLER_SAMPLING_RATIO: 0\n",
            "    POOLER_TYPE: ROIAlignV2\n",
            "  RPN:\n",
            "    BATCH_SIZE_PER_IMAGE: 256\n",
            "    BBOX_REG_LOSS_TYPE: smooth_l1\n",
            "    BBOX_REG_LOSS_WEIGHT: 1.0\n",
            "    BBOX_REG_WEIGHTS: (1.0, 1.0, 1.0, 1.0)\n",
            "    BOUNDARY_THRESH: -1\n",
            "    HEAD_NAME: StandardRPNHead\n",
            "    IN_FEATURES: ['res4']\n",
            "    IOU_LABELS: [0, -1, 1]\n",
            "    IOU_THRESHOLDS: [0.3, 0.7]\n",
            "    LOSS_WEIGHT: 1.0\n",
            "    NMS_THRESH: 0.7\n",
            "    POSITIVE_FRACTION: 0.5\n",
            "    POST_NMS_TOPK_TEST: 1000\n",
            "    POST_NMS_TOPK_TRAIN: 2000\n",
            "    PRE_NMS_TOPK_TEST: 6000\n",
            "    PRE_NMS_TOPK_TRAIN: 12000\n",
            "    SMOOTH_L1_BETA: 0.0\n",
            "  SEM_SEG_HEAD:\n",
            "    COMMON_STRIDE: 4\n",
            "    CONVS_DIM: 128\n",
            "    IGNORE_VALUE: 255\n",
            "    IN_FEATURES: ['p2', 'p3', 'p4', 'p5']\n",
            "    LOSS_WEIGHT: 1.0\n",
            "    NAME: SemSegFPNHead\n",
            "    NORM: GN\n",
            "    NUM_CLASSES: 54\n",
            "  WEIGHTS: /content/gdrive/MyDrive/QueryDet-PyTorch/work_dirs/visdrone_querydet/model_0005999.pth\n",
            "OUTPUT_DIR: work_dirs/visdrone_querydet\n",
            "SEED: -1\n",
            "SOLVER:\n",
            "  AMP:\n",
            "    ENABLED: True\n",
            "  BASE_LR: 0.006\n",
            "  BIAS_LR_FACTOR: 1.0\n",
            "  CHECKPOINT_PERIOD: 2000\n",
            "  CLIP_GRADIENTS:\n",
            "    CLIP_TYPE: value\n",
            "    CLIP_VALUE: 35.0\n",
            "    ENABLED: True\n",
            "    NORM_TYPE: 2.0\n",
            "  GAMMA: 0.1\n",
            "  IMS_PER_BATCH: 3\n",
            "  LR_SCHEDULER_NAME: WarmupMultiStepLR\n",
            "  MAX_ITER: 60000\n",
            "  MOMENTUM: 0.9\n",
            "  NESTEROV: False\n",
            "  REFERENCE_WORLD_SIZE: 0\n",
            "  STEPS: (15000, 20000)\n",
            "  WARMUP_FACTOR: 0.001\n",
            "  WARMUP_ITERS: 1000\n",
            "  WARMUP_METHOD: linear\n",
            "  WEIGHT_DECAY: 0.0001\n",
            "  WEIGHT_DECAY_BIAS: 0.0001\n",
            "  WEIGHT_DECAY_NORM: 0.0\n",
            "TEST:\n",
            "  AUG:\n",
            "    ENABLED: False\n",
            "    FLIP: True\n",
            "    MAX_SIZE: 4000\n",
            "    MIN_SIZES: (400, 500, 600, 700, 800, 900, 1000, 1100, 1200)\n",
            "  DETECTIONS_PER_IMAGE: 500\n",
            "  EVAL_PERIOD: 0\n",
            "  EXPECTED_RESULTS: []\n",
            "  KEYPOINT_OKS_SIGMAS: []\n",
            "  PRECISE_BN:\n",
            "    ENABLED: False\n",
            "    NUM_ITER: 200\n",
            "VERSION: 2\n",
            "VISDRONE:\n",
            "  MAX_LENGTH: 1999\n",
            "  SHORT_LENGTH: [1200]\n",
            "  TEST_IMG_ROOT: data/visdrone/coco_format/val_images\n",
            "  TEST_JSON: data/visdrone/coco_format/annotations/val_label.json\n",
            "  TEST_LENGTH: 3999\n",
            "  TRAIN_JSON: data/visdrone/coco_format/annotations/train_label.json\n",
            "  TRING_IMG_ROOT: data//visdrone/coco_format/train_images\n",
            "VIS_PERIOD: 0\n",
            "\u001b[32m[04/14 07:59:18 detectron2]: \u001b[0mFull config saved to work_dirs/visdrone_querydet/config.yaml\n",
            "\u001b[32m[04/14 07:59:18 d2.utils.env]: \u001b[0mUsing a generated random seed 18635199\n",
            "\u001b[32m[04/14 07:59:23 d2.engine.defaults]: \u001b[0mModel:\n",
            "RetinaNetQueryDet(\n",
            "  (backbone): FPN(\n",
            "    (fpn_lateral2): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1))\n",
            "    (fpn_output2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (fpn_lateral3): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1))\n",
            "    (fpn_output3): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (fpn_lateral4): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1))\n",
            "    (fpn_output4): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (fpn_lateral5): Conv2d(2048, 256, kernel_size=(1, 1), stride=(1, 1))\n",
            "    (fpn_output5): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (top_block): LastLevelP6P7(\n",
            "      (p6): Conv2d(2048, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
            "      (p7): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
            "    )\n",
            "    (bottom_up): ResNet(\n",
            "      (stem): BasicStem(\n",
            "        (conv1): Conv2d(\n",
            "          3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)\n",
            "        )\n",
            "      )\n",
            "      (res2): Sequential(\n",
            "        (0): BottleneckBlock(\n",
            "          (shortcut): Conv2d(\n",
            "            64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
            "          )\n",
            "          (conv1): Conv2d(\n",
            "            64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)\n",
            "          )\n",
            "          (conv2): Conv2d(\n",
            "            64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)\n",
            "          )\n",
            "          (conv3): Conv2d(\n",
            "            64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
            "          )\n",
            "        )\n",
            "        (1): BottleneckBlock(\n",
            "          (conv1): Conv2d(\n",
            "            256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)\n",
            "          )\n",
            "          (conv2): Conv2d(\n",
            "            64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)\n",
            "          )\n",
            "          (conv3): Conv2d(\n",
            "            64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
            "          )\n",
            "        )\n",
            "        (2): BottleneckBlock(\n",
            "          (conv1): Conv2d(\n",
            "            256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)\n",
            "          )\n",
            "          (conv2): Conv2d(\n",
            "            64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)\n",
            "          )\n",
            "          (conv3): Conv2d(\n",
            "            64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
            "          )\n",
            "        )\n",
            "      )\n",
            "      (res3): Sequential(\n",
            "        (0): BottleneckBlock(\n",
            "          (shortcut): Conv2d(\n",
            "            256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
            "          )\n",
            "          (conv1): Conv2d(\n",
            "            256, 128, kernel_size=(1, 1), stride=(2, 2), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)\n",
            "          )\n",
            "          (conv2): Conv2d(\n",
            "            128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)\n",
            "          )\n",
            "          (conv3): Conv2d(\n",
            "            128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
            "          )\n",
            "        )\n",
            "        (1): BottleneckBlock(\n",
            "          (conv1): Conv2d(\n",
            "            512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)\n",
            "          )\n",
            "          (conv2): Conv2d(\n",
            "            128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)\n",
            "          )\n",
            "          (conv3): Conv2d(\n",
            "            128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
            "          )\n",
            "        )\n",
            "        (2): BottleneckBlock(\n",
            "          (conv1): Conv2d(\n",
            "            512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)\n",
            "          )\n",
            "          (conv2): Conv2d(\n",
            "            128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)\n",
            "          )\n",
            "          (conv3): Conv2d(\n",
            "            128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
            "          )\n",
            "        )\n",
            "        (3): BottleneckBlock(\n",
            "          (conv1): Conv2d(\n",
            "            512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)\n",
            "          )\n",
            "          (conv2): Conv2d(\n",
            "            128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)\n",
            "          )\n",
            "          (conv3): Conv2d(\n",
            "            128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
            "          )\n",
            "        )\n",
            "      )\n",
            "      (res4): Sequential(\n",
            "        (0): BottleneckBlock(\n",
            "          (shortcut): Conv2d(\n",
            "            512, 1024, kernel_size=(1, 1), stride=(2, 2), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n",
            "          )\n",
            "          (conv1): Conv2d(\n",
            "            512, 256, kernel_size=(1, 1), stride=(2, 2), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
            "          )\n",
            "          (conv2): Conv2d(\n",
            "            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
            "          )\n",
            "          (conv3): Conv2d(\n",
            "            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n",
            "          )\n",
            "        )\n",
            "        (1): BottleneckBlock(\n",
            "          (conv1): Conv2d(\n",
            "            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
            "          )\n",
            "          (conv2): Conv2d(\n",
            "            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
            "          )\n",
            "          (conv3): Conv2d(\n",
            "            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n",
            "          )\n",
            "        )\n",
            "        (2): BottleneckBlock(\n",
            "          (conv1): Conv2d(\n",
            "            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
            "          )\n",
            "          (conv2): Conv2d(\n",
            "            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
            "          )\n",
            "          (conv3): Conv2d(\n",
            "            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n",
            "          )\n",
            "        )\n",
            "        (3): BottleneckBlock(\n",
            "          (conv1): Conv2d(\n",
            "            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
            "          )\n",
            "          (conv2): Conv2d(\n",
            "            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
            "          )\n",
            "          (conv3): Conv2d(\n",
            "            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n",
            "          )\n",
            "        )\n",
            "        (4): BottleneckBlock(\n",
            "          (conv1): Conv2d(\n",
            "            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
            "          )\n",
            "          (conv2): Conv2d(\n",
            "            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
            "          )\n",
            "          (conv3): Conv2d(\n",
            "            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n",
            "          )\n",
            "        )\n",
            "        (5): BottleneckBlock(\n",
            "          (conv1): Conv2d(\n",
            "            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
            "          )\n",
            "          (conv2): Conv2d(\n",
            "            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
            "          )\n",
            "          (conv3): Conv2d(\n",
            "            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n",
            "          )\n",
            "        )\n",
            "      )\n",
            "      (res5): Sequential(\n",
            "        (0): BottleneckBlock(\n",
            "          (shortcut): Conv2d(\n",
            "            1024, 2048, kernel_size=(1, 1), stride=(2, 2), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=2048, eps=1e-05)\n",
            "          )\n",
            "          (conv1): Conv2d(\n",
            "            1024, 512, kernel_size=(1, 1), stride=(2, 2), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
            "          )\n",
            "          (conv2): Conv2d(\n",
            "            512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
            "          )\n",
            "          (conv3): Conv2d(\n",
            "            512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=2048, eps=1e-05)\n",
            "          )\n",
            "        )\n",
            "        (1): BottleneckBlock(\n",
            "          (conv1): Conv2d(\n",
            "            2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
            "          )\n",
            "          (conv2): Conv2d(\n",
            "            512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
            "          )\n",
            "          (conv3): Conv2d(\n",
            "            512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=2048, eps=1e-05)\n",
            "          )\n",
            "        )\n",
            "        (2): BottleneckBlock(\n",
            "          (conv1): Conv2d(\n",
            "            2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
            "          )\n",
            "          (conv2): Conv2d(\n",
            "            512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
            "          )\n",
            "          (conv3): Conv2d(\n",
            "            512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=2048, eps=1e-05)\n",
            "          )\n",
            "        )\n",
            "      )\n",
            "    )\n",
            "  )\n",
            "  (det_head): RetinaNetHead_3x3(\n",
            "    (cls_layer_0): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (bbox_layer_0): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (cls_layer_1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (bbox_layer_1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (cls_layer_2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (bbox_layer_2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (cls_layer_3): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (bbox_layer_3): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (cls_score): Conv2d(256, 90, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (bbox_pred): Conv2d(256, 36, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "  )\n",
            "  (query_head): Head_3x3(\n",
            "    (layer_0): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (layer_1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (layer_2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (layer_3): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (pred_net): Conv2d(256, 1, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "  )\n",
            "  (anchor_generator): AnchorGeneratorWithCenter(\n",
            "    (cell_anchors): BufferList()\n",
            "  )\n",
            "  (query_anchor_generator): AnchorGeneratorWithCenter(\n",
            "    (cell_anchors): BufferList()\n",
            "  )\n",
            ")\n",
            "\u001b[32m[04/14 07:59:23 fvcore.common.checkpoint]: \u001b[0mLoading checkpoint from /content/gdrive/MyDrive/QueryDet-PyTorch/work_dirs/visdrone_querydet/model_0005999.pth\n",
            "\u001b[32m[04/14 07:59:39 d2.data.common]: \u001b[0mSerializing 25884 elements to byte tensors and concatenating them all ...\n",
            "\u001b[32m[04/14 07:59:40 d2.data.common]: \u001b[0mSerialized dataset takes 15.78 MiB\n",
            "/usr/local/lib/python3.7/site-packages/torch/utils/data/dataloader.py:477: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  cpuset_checked))\n",
            "\u001b[32m[04/14 07:59:43 d2.engine.train_loop]: \u001b[0mStarting training from iteration 0\n",
            "\u001b[32m[04/14 08:00:49 d2.utils.events]: \u001b[0m eta: 1 day, 13:30:20  iter: 19  total_loss: 1.933  loss_cls: 1.16  loss_box_reg: 0.6791  loss_query: 0.0397  time: 2.4714  data_time: 0.4364  lr: 0.00011989  max_mem: 10555M\n",
            "\u001b[32m[04/14 08:01:39 d2.utils.events]: \u001b[0m eta: 1 day, 13:59:18  iter: 39  total_loss: 0.5892  loss_cls: 0.362  loss_box_reg: 0.2057  loss_query: 0.02851  time: 2.3954  data_time: 0.0085  lr: 0.00023977  max_mem: 10555M\n",
            "\u001b[32m[04/14 08:02:26 d2.utils.events]: \u001b[0m eta: 1 day, 14:09:55  iter: 59  total_loss: 0.7751  loss_cls: 0.4821  loss_box_reg: 0.2461  loss_query: 0.03813  time: 2.3481  data_time: 0.0060  lr: 0.00035965  max_mem: 10555M\n",
            "\u001b[32m[04/14 08:03:12 d2.utils.events]: \u001b[0m eta: 1 day, 14:06:13  iter: 79  total_loss: 0.5652  loss_cls: 0.3685  loss_box_reg: 0.1725  loss_query: 0.02895  time: 2.3217  data_time: 0.0089  lr: 0.00047953  max_mem: 10555M\n",
            "\u001b[32m[04/14 08:04:00 d2.utils.events]: \u001b[0m eta: 1 day, 14:12:38  iter: 99  total_loss: 0.8134  loss_cls: 0.4949  loss_box_reg: 0.295  loss_query: 0.0332  time: 2.3156  data_time: 0.0082  lr: 0.00059941  max_mem: 10555M\n",
            "\u001b[32m[04/14 08:04:45 d2.utils.events]: \u001b[0m eta: 1 day, 14:05:51  iter: 119  total_loss: 0.7023  loss_cls: 0.4193  loss_box_reg: 0.2482  loss_query: 0.03474  time: 2.2827  data_time: 0.0070  lr: 0.00071929  max_mem: 10555M\n",
            "\u001b[32m[04/14 08:05:32 d2.utils.events]: \u001b[0m eta: 1 day, 14:07:40  iter: 139  total_loss: 0.8412  loss_cls: 0.5232  loss_box_reg: 0.2868  loss_query: 0.02827  time: 2.2827  data_time: 0.0089  lr: 0.00083917  max_mem: 10555M\n",
            "\u001b[32m[04/14 08:06:19 d2.utils.events]: \u001b[0m eta: 1 day, 14:03:39  iter: 159  total_loss: 0.8331  loss_cls: 0.585  loss_box_reg: 0.2548  loss_query: 0.03322  time: 2.2749  data_time: 0.0093  lr: 0.00095905  max_mem: 10555M\n",
            "\u001b[32m[04/14 08:07:05 d2.utils.events]: \u001b[0m eta: 1 day, 14:03:46  iter: 179  total_loss: 0.7604  loss_cls: 0.4778  loss_box_reg: 0.2669  loss_query: 0.03021  time: 2.2707  data_time: 0.0090  lr: 0.0010789  max_mem: 10555M\n",
            "\u001b[32m[04/14 08:07:52 d2.utils.events]: \u001b[0m eta: 1 day, 14:03:16  iter: 199  total_loss: 0.5284  loss_cls: 0.3368  loss_box_reg: 0.1645  loss_query: 0.02422  time: 2.2711  data_time: 0.0069  lr: 0.0011988  max_mem: 10555M\n",
            "\u001b[32m[04/14 08:08:41 d2.utils.events]: \u001b[0m eta: 1 day, 14:06:02  iter: 219  total_loss: 0.7031  loss_cls: 0.4472  loss_box_reg: 0.2301  loss_query: 0.03523  time: 2.2716  data_time: 0.0067  lr: 0.0013187  max_mem: 10555M\n",
            "\u001b[32m[04/14 08:09:29 d2.utils.events]: \u001b[0m eta: 1 day, 14:09:22  iter: 239  total_loss: 0.8983  loss_cls: 0.5441  loss_box_reg: 0.3262  loss_query: 0.03007  time: 2.2739  data_time: 0.0064  lr: 0.0014386  max_mem: 10555M\n",
            "\u001b[32m[04/14 08:10:17 d2.utils.events]: \u001b[0m eta: 1 day, 14:08:54  iter: 259  total_loss: 0.5599  loss_cls: 0.3299  loss_box_reg: 0.2107  loss_query: 0.02308  time: 2.2783  data_time: 0.0066  lr: 0.0015584  max_mem: 10555M\n",
            "\u001b[32m[04/14 08:11:05 d2.utils.events]: \u001b[0m eta: 1 day, 14:08:53  iter: 279  total_loss: 0.5998  loss_cls: 0.3526  loss_box_reg: 0.214  loss_query: 0.02699  time: 2.2802  data_time: 0.0058  lr: 0.0016783  max_mem: 10556M\n",
            "\u001b[32m[04/14 08:11:52 d2.utils.events]: \u001b[0m eta: 1 day, 14:07:22  iter: 299  total_loss: 0.4626  loss_cls: 0.2996  loss_box_reg: 0.1497  loss_query: 0.02791  time: 2.2787  data_time: 0.0068  lr: 0.0017982  max_mem: 10556M\n",
            "\u001b[32m[04/14 08:12:40 d2.utils.events]: \u001b[0m eta: 1 day, 14:07:21  iter: 319  total_loss: 0.8827  loss_cls: 0.5164  loss_box_reg: 0.302  loss_query: 0.03808  time: 2.2777  data_time: 0.0078  lr: 0.0019181  max_mem: 10556M\n",
            "\u001b[32m[04/14 08:13:26 d2.utils.events]: \u001b[0m eta: 1 day, 14:06:42  iter: 339  total_loss: 0.8735  loss_cls: 0.4676  loss_box_reg: 0.3336  loss_query: 0.04403  time: 2.2729  data_time: 0.0073  lr: 0.002038  max_mem: 10556M\n",
            "\u001b[32m[04/14 08:14:11 d2.utils.events]: \u001b[0m eta: 1 day, 14:05:49  iter: 359  total_loss: 0.5422  loss_cls: 0.3467  loss_box_reg: 0.1721  loss_query: 0.02662  time: 2.2662  data_time: 0.0071  lr: 0.0021578  max_mem: 10556M\n",
            "\u001b[32m[04/14 08:14:57 d2.utils.events]: \u001b[0m eta: 1 day, 14:03:46  iter: 379  total_loss: 0.5375  loss_cls: 0.3473  loss_box_reg: 0.1597  loss_query: 0.0166  time: 2.2631  data_time: 0.0068  lr: 0.0022777  max_mem: 10556M\n",
            "\u001b[32m[04/14 08:15:45 d2.utils.events]: \u001b[0m eta: 1 day, 14:03:20  iter: 399  total_loss: 0.9353  loss_cls: 0.5547  loss_box_reg: 0.3401  loss_query: 0.04399  time: 2.2638  data_time: 0.0067  lr: 0.0023976  max_mem: 10556M\n",
            "\u001b[32m[04/14 08:16:30 d2.utils.events]: \u001b[0m eta: 1 day, 14:02:14  iter: 419  total_loss: 0.8644  loss_cls: 0.4874  loss_box_reg: 0.3107  loss_query: 0.03566  time: 2.2592  data_time: 0.0071  lr: 0.0025175  max_mem: 10556M\n",
            "\u001b[32m[04/14 08:17:19 d2.utils.events]: \u001b[0m eta: 1 day, 14:02:00  iter: 439  total_loss: 0.7671  loss_cls: 0.4826  loss_box_reg: 0.2273  loss_query: 0.0323  time: 2.2624  data_time: 0.0067  lr: 0.0026374  max_mem: 10556M\n",
            "\u001b[32m[04/14 08:18:07 d2.utils.events]: \u001b[0m eta: 1 day, 14:02:38  iter: 459  total_loss: 0.8507  loss_cls: 0.5241  loss_box_reg: 0.3165  loss_query: 0.03181  time: 2.2654  data_time: 0.0066  lr: 0.0027572  max_mem: 10556M\n",
            "\u001b[32m[04/14 08:18:56 d2.utils.events]: \u001b[0m eta: 1 day, 14:02:42  iter: 479  total_loss: 0.9247  loss_cls: 0.5718  loss_box_reg: 0.2868  loss_query: 0.05037  time: 2.2679  data_time: 0.0065  lr: 0.0028771  max_mem: 10556M\n",
            "\u001b[32m[04/14 08:19:44 d2.utils.events]: \u001b[0m eta: 1 day, 14:05:17  iter: 499  total_loss: 0.9097  loss_cls: 0.5075  loss_box_reg: 0.3044  loss_query: 0.03266  time: 2.2704  data_time: 0.0065  lr: 0.002997  max_mem: 10556M\n",
            "\u001b[32m[04/14 08:20:33 d2.utils.events]: \u001b[0m eta: 1 day, 14:08:43  iter: 519  total_loss: 0.8784  loss_cls: 0.5401  loss_box_reg: 0.305  loss_query: 0.04509  time: 2.2736  data_time: 0.0099  lr: 0.0031169  max_mem: 10556M\n",
            "\u001b[32m[04/14 08:21:20 d2.utils.events]: \u001b[0m eta: 1 day, 14:04:09  iter: 539  total_loss: 0.5308  loss_cls: 0.3005  loss_box_reg: 0.1857  loss_query: 0.0208  time: 2.2719  data_time: 0.0063  lr: 0.0032368  max_mem: 10556M\n",
            "\u001b[32m[04/14 08:22:08 d2.utils.events]: \u001b[0m eta: 1 day, 14:05:27  iter: 559  total_loss: 0.7751  loss_cls: 0.4777  loss_box_reg: 0.2826  loss_query: 0.03606  time: 2.2724  data_time: 0.0089  lr: 0.0033566  max_mem: 10556M\n",
            "\u001b[32m[04/14 08:22:56 d2.utils.events]: \u001b[0m eta: 1 day, 14:06:00  iter: 579  total_loss: 1.033  loss_cls: 0.6189  loss_box_reg: 0.3199  loss_query: 0.04832  time: 2.2734  data_time: 0.0078  lr: 0.0034765  max_mem: 10556M\n",
            "\u001b[32m[04/14 08:23:43 d2.utils.events]: \u001b[0m eta: 1 day, 14:05:06  iter: 599  total_loss: 0.6293  loss_cls: 0.3867  loss_box_reg: 0.2134  loss_query: 0.03652  time: 2.2734  data_time: 0.0077  lr: 0.0035964  max_mem: 10556M\n",
            "\u001b[32m[04/14 08:24:30 d2.utils.events]: \u001b[0m eta: 1 day, 14:04:34  iter: 619  total_loss: 0.843  loss_cls: 0.4732  loss_box_reg: 0.2964  loss_query: 0.03685  time: 2.2739  data_time: 0.0061  lr: 0.0037163  max_mem: 10556M\n",
            "\u001b[32m[04/14 08:25:18 d2.utils.events]: \u001b[0m eta: 1 day, 14:04:26  iter: 639  total_loss: 0.7357  loss_cls: 0.4297  loss_box_reg: 0.2454  loss_query: 0.03506  time: 2.2746  data_time: 0.0074  lr: 0.0038362  max_mem: 10556M\n",
            "\u001b[32m[04/14 08:26:05 d2.utils.events]: \u001b[0m eta: 1 day, 14:04:01  iter: 659  total_loss: 0.7021  loss_cls: 0.4229  loss_box_reg: 0.232  loss_query: 0.02505  time: 2.2740  data_time: 0.0083  lr: 0.003956  max_mem: 10556M\n",
            "\u001b[32m[04/14 08:26:52 d2.utils.events]: \u001b[0m eta: 1 day, 14:03:15  iter: 679  total_loss: 0.8045  loss_cls: 0.5276  loss_box_reg: 0.2585  loss_query: 0.0414  time: 2.2726  data_time: 0.0074  lr: 0.0040759  max_mem: 10556M\n",
            "\u001b[32m[04/14 08:27:39 d2.utils.events]: \u001b[0m eta: 1 day, 14:02:07  iter: 699  total_loss: 0.6189  loss_cls: 0.3726  loss_box_reg: 0.2207  loss_query: 0.02896  time: 2.2731  data_time: 0.0061  lr: 0.0041958  max_mem: 10556M\n",
            "\u001b[32m[04/14 08:28:26 d2.utils.events]: \u001b[0m eta: 1 day, 14:00:43  iter: 719  total_loss: 0.571  loss_cls: 0.3381  loss_box_reg: 0.2085  loss_query: 0.02787  time: 2.2725  data_time: 0.0078  lr: 0.0043157  max_mem: 10556M\n",
            "\u001b[32m[04/14 08:29:13 d2.utils.events]: \u001b[0m eta: 1 day, 13:59:51  iter: 739  total_loss: 0.6367  loss_cls: 0.4125  loss_box_reg: 0.2169  loss_query: 0.0308  time: 2.2716  data_time: 0.0072  lr: 0.0044356  max_mem: 10556M\n",
            "\u001b[32m[04/14 08:29:58 d2.utils.events]: \u001b[0m eta: 1 day, 13:57:41  iter: 759  total_loss: 0.6214  loss_cls: 0.3883  loss_box_reg: 0.2111  loss_query: 0.03621  time: 2.2694  data_time: 0.0072  lr: 0.0045554  max_mem: 10556M\n",
            "\u001b[32m[04/14 08:30:46 d2.utils.events]: \u001b[0m eta: 1 day, 13:57:24  iter: 779  total_loss: 0.8525  loss_cls: 0.5404  loss_box_reg: 0.2562  loss_query: 0.0253  time: 2.2693  data_time: 0.0087  lr: 0.0046753  max_mem: 10556M\n",
            "\u001b[32m[04/14 08:31:34 d2.utils.events]: \u001b[0m eta: 1 day, 13:57:32  iter: 799  total_loss: 0.8416  loss_cls: 0.4946  loss_box_reg: 0.2992  loss_query: 0.04436  time: 2.2704  data_time: 0.0067  lr: 0.0047952  max_mem: 10556M\n",
            "\u001b[32m[04/14 08:32:21 d2.utils.events]: \u001b[0m eta: 1 day, 13:56:38  iter: 819  total_loss: 0.6008  loss_cls: 0.3682  loss_box_reg: 0.1833  loss_query: 0.04948  time: 2.2690  data_time: 0.0061  lr: 0.0049151  max_mem: 10556M\n",
            "\u001b[32m[04/14 08:33:08 d2.utils.events]: \u001b[0m eta: 1 day, 13:55:59  iter: 839  total_loss: 0.8412  loss_cls: 0.4972  loss_box_reg: 0.2829  loss_query: 0.03542  time: 2.2692  data_time: 0.0072  lr: 0.005035  max_mem: 10556M\n",
            "\u001b[32m[04/14 08:34:00 d2.utils.events]: \u001b[0m eta: 1 day, 13:54:20  iter: 859  total_loss: 0.7544  loss_cls: 0.4325  loss_box_reg: 0.2325  loss_query: 0.04256  time: 2.2750  data_time: 0.3515  lr: 0.0051548  max_mem: 10556M\n",
            "\u001b[32m[04/14 08:34:48 d2.utils.events]: \u001b[0m eta: 1 day, 13:54:34  iter: 879  total_loss: 0.7489  loss_cls: 0.4677  loss_box_reg: 0.249  loss_query: 0.04418  time: 2.2758  data_time: 0.0081  lr: 0.0052747  max_mem: 10556M\n",
            "\u001b[32m[04/14 08:35:37 d2.utils.events]: \u001b[0m eta: 1 day, 13:53:56  iter: 899  total_loss: 0.8706  loss_cls: 0.5142  loss_box_reg: 0.3019  loss_query: 0.03484  time: 2.2755  data_time: 0.0074  lr: 0.0053946  max_mem: 10556M\n",
            "\u001b[32m[04/14 08:36:26 d2.utils.events]: \u001b[0m eta: 1 day, 13:53:26  iter: 919  total_loss: 0.7757  loss_cls: 0.5021  loss_box_reg: 0.2771  loss_query: 0.04292  time: 2.2762  data_time: 0.0083  lr: 0.0055145  max_mem: 10556M\n",
            "\u001b[32m[04/14 08:37:14 d2.utils.events]: \u001b[0m eta: 1 day, 13:52:38  iter: 939  total_loss: 0.6858  loss_cls: 0.4219  loss_box_reg: 0.2337  loss_query: 0.03139  time: 2.2751  data_time: 0.0055  lr: 0.0056344  max_mem: 10556M\n",
            "\u001b[32m[04/14 08:38:03 d2.utils.events]: \u001b[0m eta: 1 day, 13:52:19  iter: 959  total_loss: 0.8248  loss_cls: 0.5038  loss_box_reg: 0.2738  loss_query: 0.04373  time: 2.2765  data_time: 0.0073  lr: 0.0057542  max_mem: 10556M\n",
            "\u001b[32m[04/14 08:38:50 d2.utils.events]: \u001b[0m eta: 1 day, 13:52:07  iter: 979  total_loss: 0.7098  loss_cls: 0.4373  loss_box_reg: 0.2495  loss_query: 0.03089  time: 2.2768  data_time: 0.0073  lr: 0.0058741  max_mem: 10556M\n",
            "\u001b[32m[04/14 08:39:36 d2.utils.events]: \u001b[0m eta: 1 day, 13:50:33  iter: 999  total_loss: 0.611  loss_cls: 0.3705  loss_box_reg: 0.2148  loss_query: 0.02519  time: 2.2755  data_time: 0.0075  lr: 0.005994  max_mem: 10556M\n",
            "\u001b[32m[04/14 08:40:24 d2.utils.events]: \u001b[0m eta: 1 day, 13:51:02  iter: 1019  total_loss: 1.059  loss_cls: 0.6592  loss_box_reg: 0.3412  loss_query: 0.0395  time: 2.2756  data_time: 0.0071  lr: 0.006  max_mem: 10556M\n",
            "\u001b[32m[04/14 08:41:12 d2.utils.events]: \u001b[0m eta: 1 day, 13:50:12  iter: 1039  total_loss: 0.7912  loss_cls: 0.4952  loss_box_reg: 0.2581  loss_query: 0.03834  time: 2.2763  data_time: 0.0072  lr: 0.006  max_mem: 10556M\n",
            "\u001b[32m[04/14 08:42:02 d2.utils.events]: \u001b[0m eta: 1 day, 13:50:24  iter: 1059  total_loss: 0.8308  loss_cls: 0.4859  loss_box_reg: 0.2702  loss_query: 0.05869  time: 2.2786  data_time: 0.0075  lr: 0.006  max_mem: 10556M\n",
            "\u001b[32m[04/14 08:42:49 d2.utils.events]: \u001b[0m eta: 1 day, 13:51:03  iter: 1079  total_loss: 0.8642  loss_cls: 0.5283  loss_box_reg: 0.3013  loss_query: 0.04848  time: 2.2786  data_time: 0.0078  lr: 0.006  max_mem: 10556M\n",
            "\u001b[32m[04/14 08:43:36 d2.utils.events]: \u001b[0m eta: 1 day, 13:50:16  iter: 1099  total_loss: 0.7231  loss_cls: 0.4426  loss_box_reg: 0.2492  loss_query: 0.04134  time: 2.2784  data_time: 0.0089  lr: 0.006  max_mem: 10556M\n",
            "\u001b[32m[04/14 08:44:23 d2.utils.events]: \u001b[0m eta: 1 day, 13:49:47  iter: 1119  total_loss: 0.7121  loss_cls: 0.4322  loss_box_reg: 0.2361  loss_query: 0.03765  time: 2.2781  data_time: 0.0063  lr: 0.006  max_mem: 10556M\n",
            "\u001b[32m[04/14 08:45:11 d2.utils.events]: \u001b[0m eta: 1 day, 13:49:43  iter: 1139  total_loss: 0.8567  loss_cls: 0.5153  loss_box_reg: 0.3097  loss_query: 0.03903  time: 2.2785  data_time: 0.0066  lr: 0.006  max_mem: 10556M\n",
            "\u001b[32m[04/14 08:46:00 d2.utils.events]: \u001b[0m eta: 1 day, 13:49:27  iter: 1159  total_loss: 0.8585  loss_cls: 0.5234  loss_box_reg: 0.2735  loss_query: 0.02794  time: 2.2789  data_time: 0.0065  lr: 0.006  max_mem: 10556M\n",
            "\u001b[32m[04/14 08:46:47 d2.utils.events]: \u001b[0m eta: 1 day, 13:48:59  iter: 1179  total_loss: 0.6875  loss_cls: 0.4355  loss_box_reg: 0.217  loss_query: 0.04476  time: 2.2789  data_time: 0.0095  lr: 0.006  max_mem: 10556M\n",
            "\u001b[32m[04/14 08:47:35 d2.utils.events]: \u001b[0m eta: 1 day, 13:48:16  iter: 1199  total_loss: 0.915  loss_cls: 0.5433  loss_box_reg: 0.3331  loss_query: 0.05535  time: 2.2792  data_time: 0.0067  lr: 0.006  max_mem: 10556M\n",
            "\u001b[32m[04/14 08:48:23 d2.utils.events]: \u001b[0m eta: 1 day, 13:47:23  iter: 1219  total_loss: 0.8007  loss_cls: 0.4386  loss_box_reg: 0.2983  loss_query: 0.0537  time: 2.2798  data_time: 0.0073  lr: 0.006  max_mem: 10556M\n",
            "\u001b[32m[04/14 08:49:11 d2.utils.events]: \u001b[0m eta: 1 day, 13:46:37  iter: 1239  total_loss: 0.8045  loss_cls: 0.4805  loss_box_reg: 0.267  loss_query: 0.04465  time: 2.2807  data_time: 0.0069  lr: 0.006  max_mem: 10556M\n",
            "\u001b[32m[04/14 08:50:00 d2.utils.events]: \u001b[0m eta: 1 day, 13:45:45  iter: 1259  total_loss: 0.8233  loss_cls: 0.5118  loss_box_reg: 0.2589  loss_query: 0.04462  time: 2.2810  data_time: 0.0062  lr: 0.006  max_mem: 10556M\n",
            "\u001b[32m[04/14 08:50:48 d2.utils.events]: \u001b[0m eta: 1 day, 13:45:04  iter: 1279  total_loss: 0.7868  loss_cls: 0.4763  loss_box_reg: 0.2749  loss_query: 0.02955  time: 2.2812  data_time: 0.0075  lr: 0.006  max_mem: 10556M\n",
            "\u001b[32m[04/14 08:51:34 d2.utils.events]: \u001b[0m eta: 1 day, 13:44:29  iter: 1299  total_loss: 0.711  loss_cls: 0.417  loss_box_reg: 0.2573  loss_query: 0.03861  time: 2.2806  data_time: 0.0065  lr: 0.006  max_mem: 10556M\n",
            "\u001b[32m[04/14 08:52:22 d2.utils.events]: \u001b[0m eta: 1 day, 13:43:21  iter: 1319  total_loss: 0.6388  loss_cls: 0.4078  loss_box_reg: 0.2883  loss_query: 0.0327  time: 2.2806  data_time: 0.0068  lr: 0.006  max_mem: 10556M\n",
            "\u001b[32m[04/14 08:53:09 d2.utils.events]: \u001b[0m eta: 1 day, 13:42:46  iter: 1339  total_loss: 0.6641  loss_cls: 0.4318  loss_box_reg: 0.2136  loss_query: 0.04133  time: 2.2805  data_time: 0.0077  lr: 0.006  max_mem: 10556M\n",
            "\u001b[32m[04/14 08:53:57 d2.utils.events]: \u001b[0m eta: 1 day, 13:43:31  iter: 1359  total_loss: 0.9004  loss_cls: 0.5631  loss_box_reg: 0.3399  loss_query: 0.03835  time: 2.2813  data_time: 0.0071  lr: 0.006  max_mem: 10556M\n",
            "\u001b[32m[04/14 08:54:45 d2.utils.events]: \u001b[0m eta: 1 day, 13:44:43  iter: 1379  total_loss: 0.8788  loss_cls: 0.5165  loss_box_reg: 0.2972  loss_query: 0.05698  time: 2.2816  data_time: 0.0085  lr: 0.006  max_mem: 10556M\n",
            "\u001b[32m[04/14 08:55:33 d2.utils.events]: \u001b[0m eta: 1 day, 13:43:25  iter: 1399  total_loss: 0.7426  loss_cls: 0.4743  loss_box_reg: 0.2344  loss_query: 0.04573  time: 2.2816  data_time: 0.0066  lr: 0.006  max_mem: 10556M\n",
            "\u001b[32m[04/14 08:56:19 d2.utils.events]: \u001b[0m eta: 1 day, 13:42:39  iter: 1419  total_loss: 0.67  loss_cls: 0.3853  loss_box_reg: 0.2327  loss_query: 0.02663  time: 2.2807  data_time: 0.0061  lr: 0.006  max_mem: 10556M\n",
            "\u001b[32m[04/14 08:57:08 d2.utils.events]: \u001b[0m eta: 1 day, 13:41:52  iter: 1439  total_loss: 0.7833  loss_cls: 0.4479  loss_box_reg: 0.278  loss_query: 0.05111  time: 2.2819  data_time: 0.0077  lr: 0.006  max_mem: 10556M\n",
            "\u001b[32m[04/14 08:57:56 d2.utils.events]: \u001b[0m eta: 1 day, 13:40:54  iter: 1459  total_loss: 0.6199  loss_cls: 0.3857  loss_box_reg: 0.214  loss_query: 0.03236  time: 2.2818  data_time: 0.0077  lr: 0.006  max_mem: 10556M\n",
            "\u001b[32m[04/14 08:58:44 d2.utils.events]: \u001b[0m eta: 1 day, 13:40:20  iter: 1479  total_loss: 0.7057  loss_cls: 0.419  loss_box_reg: 0.2581  loss_query: 0.03097  time: 2.2824  data_time: 0.0062  lr: 0.006  max_mem: 10556M\n",
            "\u001b[32m[04/14 08:59:30 d2.utils.events]: \u001b[0m eta: 1 day, 13:37:27  iter: 1499  total_loss: 0.6272  loss_cls: 0.3827  loss_box_reg: 0.239  loss_query: 0.02323  time: 2.2815  data_time: 0.0065  lr: 0.006  max_mem: 10556M\n",
            "\u001b[32m[04/14 09:00:17 d2.utils.events]: \u001b[0m eta: 1 day, 13:35:31  iter: 1519  total_loss: 0.716  loss_cls: 0.4465  loss_box_reg: 0.2701  loss_query: 0.02855  time: 2.2807  data_time: 0.0059  lr: 0.006  max_mem: 10556M\n",
            "\u001b[32m[04/14 09:01:05 d2.utils.events]: \u001b[0m eta: 1 day, 13:36:34  iter: 1539  total_loss: 0.99  loss_cls: 0.6183  loss_box_reg: 0.3105  loss_query: 0.03993  time: 2.2811  data_time: 0.0085  lr: 0.006  max_mem: 10556M\n",
            "\u001b[32m[04/14 09:01:53 d2.utils.events]: \u001b[0m eta: 1 day, 13:37:02  iter: 1559  total_loss: 0.8193  loss_cls: 0.5198  loss_box_reg: 0.2737  loss_query: 0.0527  time: 2.2817  data_time: 0.0072  lr: 0.006  max_mem: 10556M\n",
            "\u001b[32m[04/14 09:02:40 d2.utils.events]: \u001b[0m eta: 1 day, 13:36:28  iter: 1579  total_loss: 1.067  loss_cls: 0.6163  loss_box_reg: 0.3827  loss_query: 0.05148  time: 2.2814  data_time: 0.0073  lr: 0.006  max_mem: 10556M\n",
            "\u001b[32m[04/14 09:03:26 d2.utils.events]: \u001b[0m eta: 1 day, 13:35:42  iter: 1599  total_loss: 0.8203  loss_cls: 0.5099  loss_box_reg: 0.3082  loss_query: 0.04699  time: 2.2803  data_time: 0.0068  lr: 0.006  max_mem: 10556M\n",
            "\u001b[32m[04/14 09:04:14 d2.utils.events]: \u001b[0m eta: 1 day, 13:34:29  iter: 1619  total_loss: 0.6229  loss_cls: 0.4029  loss_box_reg: 0.1889  loss_query: 0.02432  time: 2.2809  data_time: 0.0070  lr: 0.006  max_mem: 10556M\n",
            "\u001b[32m[04/14 09:05:01 d2.utils.events]: \u001b[0m eta: 1 day, 13:32:42  iter: 1639  total_loss: 0.7587  loss_cls: 0.5  loss_box_reg: 0.2592  loss_query: 0.03187  time: 2.2809  data_time: 0.0096  lr: 0.006  max_mem: 10556M\n",
            "\u001b[32m[04/14 09:05:50 d2.utils.events]: \u001b[0m eta: 1 day, 13:34:22  iter: 1659  total_loss: 0.8721  loss_cls: 0.5285  loss_box_reg: 0.2613  loss_query: 0.04387  time: 2.2816  data_time: 0.0065  lr: 0.006  max_mem: 10556M\n",
            "\u001b[32m[04/14 09:06:38 d2.utils.events]: \u001b[0m eta: 1 day, 13:35:12  iter: 1679  total_loss: 0.8227  loss_cls: 0.4796  loss_box_reg: 0.2716  loss_query: 0.05339  time: 2.2817  data_time: 0.0080  lr: 0.006  max_mem: 10556M\n",
            "\u001b[32m[04/14 09:07:26 d2.utils.events]: \u001b[0m eta: 1 day, 13:34:26  iter: 1699  total_loss: 0.6129  loss_cls: 0.3649  loss_box_reg: 0.2152  loss_query: 0.03691  time: 2.2823  data_time: 0.0059  lr: 0.006  max_mem: 10556M\n",
            "\u001b[32m[04/14 09:08:14 d2.utils.events]: \u001b[0m eta: 1 day, 13:34:04  iter: 1719  total_loss: 0.6615  loss_cls: 0.3635  loss_box_reg: 0.2528  loss_query: 0.04143  time: 2.2824  data_time: 0.0066  lr: 0.006  max_mem: 10556M\n",
            "\u001b[32m[04/14 09:09:03 d2.utils.events]: \u001b[0m eta: 1 day, 13:34:33  iter: 1739  total_loss: 0.7745  loss_cls: 0.4843  loss_box_reg: 0.2523  loss_query: 0.03616  time: 2.2834  data_time: 0.0073  lr: 0.006  max_mem: 10556M\n",
            "\u001b[32m[04/14 09:09:51 d2.utils.events]: \u001b[0m eta: 1 day, 13:35:24  iter: 1759  total_loss: 0.8397  loss_cls: 0.4679  loss_box_reg: 0.2859  loss_query: 0.03295  time: 2.2835  data_time: 0.0068  lr: 0.006  max_mem: 10556M\n",
            "\u001b[32m[04/14 09:10:38 d2.utils.events]: \u001b[0m eta: 1 day, 13:35:10  iter: 1779  total_loss: 0.6924  loss_cls: 0.3946  loss_box_reg: 0.2304  loss_query: 0.04864  time: 2.2829  data_time: 0.0072  lr: 0.006  max_mem: 10556M\n",
            "\u001b[32m[04/14 09:11:46 d2.utils.events]: \u001b[0m eta: 1 day, 13:33:41  iter: 1799  total_loss: 0.7675  loss_cls: 0.4131  loss_box_reg: 0.263  loss_query: 0.02753  time: 2.2945  data_time: 1.1053  lr: 0.006  max_mem: 10556M\n",
            "\u001b[32m[04/14 09:12:33 d2.utils.events]: \u001b[0m eta: 1 day, 13:33:37  iter: 1819  total_loss: 0.8714  loss_cls: 0.5615  loss_box_reg: 0.2876  loss_query: 0.02632  time: 2.2940  data_time: 0.0077  lr: 0.006  max_mem: 10556M\n",
            "\u001b[32m[04/14 09:13:20 d2.utils.events]: \u001b[0m eta: 1 day, 13:32:18  iter: 1839  total_loss: 0.9072  loss_cls: 0.5457  loss_box_reg: 0.3181  loss_query: 0.03714  time: 2.2935  data_time: 0.0072  lr: 0.006  max_mem: 10556M\n",
            "\u001b[32m[04/14 09:14:08 d2.utils.events]: \u001b[0m eta: 1 day, 13:32:04  iter: 1859  total_loss: 0.8161  loss_cls: 0.511  loss_box_reg: 0.2616  loss_query: 0.04076  time: 2.2929  data_time: 0.0102  lr: 0.006  max_mem: 10556M\n",
            "\u001b[32m[04/14 09:14:57 d2.utils.events]: \u001b[0m eta: 1 day, 13:30:25  iter: 1879  total_loss: 0.7468  loss_cls: 0.4675  loss_box_reg: 0.229  loss_query: 0.03509  time: 2.2928  data_time: 0.0072  lr: 0.006  max_mem: 10556M\n",
            "\u001b[32m[04/14 09:15:44 d2.utils.events]: \u001b[0m eta: 1 day, 13:29:24  iter: 1899  total_loss: 0.8141  loss_cls: 0.4875  loss_box_reg: 0.24  loss_query: 0.03349  time: 2.2922  data_time: 0.0074  lr: 0.006  max_mem: 10556M\n",
            "\u001b[32m[04/14 09:16:32 d2.utils.events]: \u001b[0m eta: 1 day, 13:26:14  iter: 1919  total_loss: 0.6093  loss_cls: 0.3949  loss_box_reg: 0.2128  loss_query: 0.02213  time: 2.2914  data_time: 0.0062  lr: 0.006  max_mem: 10556M\n",
            "\u001b[32m[04/14 09:17:19 d2.utils.events]: \u001b[0m eta: 1 day, 13:25:27  iter: 1939  total_loss: 0.8678  loss_cls: 0.5647  loss_box_reg: 0.2819  loss_query: 0.03607  time: 2.2907  data_time: 0.0091  lr: 0.006  max_mem: 10556M\n",
            "\u001b[32m[04/14 09:18:08 d2.utils.events]: \u001b[0m eta: 1 day, 13:24:02  iter: 1959  total_loss: 0.822  loss_cls: 0.4642  loss_box_reg: 0.2376  loss_query: 0.03041  time: 2.2907  data_time: 0.0094  lr: 0.006  max_mem: 10556M\n",
            "\u001b[32m[04/14 09:18:56 d2.utils.events]: \u001b[0m eta: 1 day, 13:24:01  iter: 1979  total_loss: 0.7771  loss_cls: 0.4657  loss_box_reg: 0.2767  loss_query: 0.03832  time: 2.2903  data_time: 0.0060  lr: 0.006  max_mem: 10556M\n",
            "\u001b[32m[04/14 09:19:45 fvcore.common.checkpoint]: \u001b[0mSaving checkpoint to work_dirs/visdrone_querydet/model_0001999.pth\n",
            "\u001b[32m[04/14 09:20:04 d2.utils.events]: \u001b[0m eta: 1 day, 13:25:19  iter: 1999  total_loss: 0.7394  loss_cls: 0.4659  loss_box_reg: 0.2383  loss_query: 0.02793  time: 2.2906  data_time: 0.0081  lr: 0.006  max_mem: 10556M\n",
            "\u001b[32m[04/14 09:20:54 d2.utils.events]: \u001b[0m eta: 1 day, 13:24:08  iter: 2019  total_loss: 0.5765  loss_cls: 0.347  loss_box_reg: 0.2069  loss_query: 0.03925  time: 2.2906  data_time: 0.0080  lr: 0.006  max_mem: 10556M\n",
            "\u001b[32m[04/14 09:21:43 d2.utils.events]: \u001b[0m eta: 1 day, 13:22:23  iter: 2039  total_loss: 0.5254  loss_cls: 0.3398  loss_box_reg: 0.1766  loss_query: 0.02472  time: 2.2907  data_time: 0.0106  lr: 0.006  max_mem: 10556M\n",
            "\u001b[32m[04/14 09:22:33 d2.utils.events]: \u001b[0m eta: 1 day, 13:20:10  iter: 2059  total_loss: 0.8821  loss_cls: 0.4969  loss_box_reg: 0.318  loss_query: 0.04995  time: 2.2911  data_time: 0.0092  lr: 0.006  max_mem: 10556M\n",
            "\u001b[32m[04/14 09:23:22 d2.utils.events]: \u001b[0m eta: 1 day, 13:19:43  iter: 2079  total_loss: 0.9981  loss_cls: 0.5519  loss_box_reg: 0.3327  loss_query: 0.03758  time: 2.2912  data_time: 0.0065  lr: 0.006  max_mem: 10556M\n",
            "\u001b[32m[04/14 09:24:11 d2.utils.events]: \u001b[0m eta: 1 day, 13:19:01  iter: 2099  total_loss: 0.6971  loss_cls: 0.4377  loss_box_reg: 0.2294  loss_query: 0.03847  time: 2.2908  data_time: 0.0081  lr: 0.006  max_mem: 10556M\n",
            "\u001b[32m[04/14 09:24:59 d2.utils.events]: \u001b[0m eta: 1 day, 13:18:15  iter: 2119  total_loss: 0.6582  loss_cls: 0.3783  loss_box_reg: 0.2077  loss_query: 0.02667  time: 2.2907  data_time: 0.0076  lr: 0.006  max_mem: 10556M\n",
            "\u001b[32m[04/14 09:25:49 d2.utils.events]: \u001b[0m eta: 1 day, 13:17:29  iter: 2139  total_loss: 0.7578  loss_cls: 0.4284  loss_box_reg: 0.2932  loss_query: 0.03976  time: 2.2914  data_time: 0.0082  lr: 0.006  max_mem: 10556M\n",
            "\u001b[32m[04/14 09:26:37 d2.utils.events]: \u001b[0m eta: 1 day, 13:16:44  iter: 2159  total_loss: 0.7694  loss_cls: 0.4402  loss_box_reg: 0.2628  loss_query: 0.03247  time: 2.2910  data_time: 0.0076  lr: 0.006  max_mem: 10556M\n",
            "\u001b[32m[04/14 09:27:27 d2.utils.events]: \u001b[0m eta: 1 day, 13:16:07  iter: 2179  total_loss: 0.7619  loss_cls: 0.4572  loss_box_reg: 0.274  loss_query: 0.03735  time: 2.2914  data_time: 0.0063  lr: 0.006  max_mem: 10556M\n",
            "\u001b[32m[04/14 09:28:34 d2.utils.events]: \u001b[0m eta: 1 day, 13:15:05  iter: 2199  total_loss: 0.7057  loss_cls: 0.4556  loss_box_reg: 0.199  loss_query: 0.0466  time: 2.2994  data_time: 0.9190  lr: 0.006  max_mem: 10556M\n",
            "\u001b[32m[04/14 09:29:23 d2.utils.events]: \u001b[0m eta: 1 day, 13:14:18  iter: 2219  total_loss: 0.5716  loss_cls: 0.3606  loss_box_reg: 0.1894  loss_query: 0.02177  time: 2.2992  data_time: 0.0081  lr: 0.006  max_mem: 10556M\n",
            "\u001b[32m[04/14 09:30:12 d2.utils.events]: \u001b[0m eta: 1 day, 13:13:20  iter: 2239  total_loss: 0.837  loss_cls: 0.5607  loss_box_reg: 0.268  loss_query: 0.0198  time: 2.2992  data_time: 0.0084  lr: 0.006  max_mem: 10556M\n",
            "\u001b[32m[04/14 09:31:00 d2.utils.events]: \u001b[0m eta: 1 day, 13:11:37  iter: 2259  total_loss: 0.8798  loss_cls: 0.5601  loss_box_reg: 0.2784  loss_query: 0.03561  time: 2.2988  data_time: 0.0073  lr: 0.006  max_mem: 10556M\n",
            "\u001b[32m[04/14 09:31:50 d2.utils.events]: \u001b[0m eta: 1 day, 13:09:50  iter: 2279  total_loss: 0.6539  loss_cls: 0.4222  loss_box_reg: 0.2027  loss_query: 0.02941  time: 2.2990  data_time: 0.0072  lr: 0.006  max_mem: 10556M\n",
            "\u001b[32m[04/14 09:32:37 d2.utils.events]: \u001b[0m eta: 1 day, 13:08:32  iter: 2299  total_loss: 0.7585  loss_cls: 0.4525  loss_box_reg: 0.2566  loss_query: 0.02574  time: 2.2985  data_time: 0.0083  lr: 0.006  max_mem: 10556M\n",
            "\u001b[32m[04/14 09:33:27 d2.utils.events]: \u001b[0m eta: 1 day, 13:08:18  iter: 2319  total_loss: 0.846  loss_cls: 0.4908  loss_box_reg: 0.311  loss_query: 0.04102  time: 2.2986  data_time: 0.0062  lr: 0.006  max_mem: 10556M\n",
            "\u001b[32m[04/14 09:34:17 d2.utils.events]: \u001b[0m eta: 1 day, 13:07:31  iter: 2339  total_loss: 0.6613  loss_cls: 0.4042  loss_box_reg: 0.2036  loss_query: 0.03247  time: 2.2988  data_time: 0.0071  lr: 0.006  max_mem: 10556M\n",
            "\u001b[32m[04/14 09:35:06 d2.utils.events]: \u001b[0m eta: 1 day, 13:06:27  iter: 2359  total_loss: 0.8412  loss_cls: 0.4676  loss_box_reg: 0.2657  loss_query: 0.03396  time: 2.2990  data_time: 0.0058  lr: 0.006  max_mem: 10556M\n",
            "\u001b[32m[04/14 09:35:56 d2.utils.events]: \u001b[0m eta: 1 day, 13:04:25  iter: 2379  total_loss: 0.665  loss_cls: 0.4526  loss_box_reg: 0.1904  loss_query: 0.02743  time: 2.2991  data_time: 0.0082  lr: 0.006  max_mem: 10556M\n",
            "\u001b[32m[04/14 09:36:45 d2.utils.events]: \u001b[0m eta: 1 day, 13:03:38  iter: 2399  total_loss: 0.6889  loss_cls: 0.4219  loss_box_reg: 0.2213  loss_query: 0.02996  time: 2.2987  data_time: 0.0078  lr: 0.006  max_mem: 10556M\n",
            "\u001b[32m[04/14 09:37:33 d2.utils.events]: \u001b[0m eta: 1 day, 13:03:54  iter: 2419  total_loss: 0.6025  loss_cls: 0.4042  loss_box_reg: 0.1769  loss_query: 0.03579  time: 2.2985  data_time: 0.0066  lr: 0.006  max_mem: 10556M\n",
            "\u001b[32m[04/14 09:38:22 d2.utils.events]: \u001b[0m eta: 1 day, 13:01:38  iter: 2439  total_loss: 0.9607  loss_cls: 0.5985  loss_box_reg: 0.3658  loss_query: 0.0274  time: 2.2982  data_time: 0.0095  lr: 0.006  max_mem: 10556M\n",
            "\u001b[32m[04/14 09:39:10 d2.utils.events]: \u001b[0m eta: 1 day, 13:00:52  iter: 2459  total_loss: 0.7338  loss_cls: 0.4832  loss_box_reg: 0.2433  loss_query: 0.03404  time: 2.2977  data_time: 0.0069  lr: 0.006  max_mem: 10556M\n",
            "\u001b[32m[04/14 09:39:59 d2.utils.events]: \u001b[0m eta: 1 day, 13:00:14  iter: 2479  total_loss: 0.8021  loss_cls: 0.483  loss_box_reg: 0.2845  loss_query: 0.03294  time: 2.2978  data_time: 0.0068  lr: 0.006  max_mem: 10556M\n",
            "\u001b[32m[04/14 09:40:49 d2.utils.events]: \u001b[0m eta: 1 day, 13:01:20  iter: 2499  total_loss: 0.9188  loss_cls: 0.5596  loss_box_reg: 0.321  loss_query: 0.04328  time: 2.2979  data_time: 0.0069  lr: 0.006  max_mem: 10556M\n",
            "\u001b[32m[04/14 09:41:37 d2.utils.events]: \u001b[0m eta: 1 day, 13:02:10  iter: 2519  total_loss: 0.8058  loss_cls: 0.5151  loss_box_reg: 0.2469  loss_query: 0.034  time: 2.2975  data_time: 0.0062  lr: 0.006  max_mem: 10556M\n",
            "\u001b[32m[04/14 09:42:25 d2.utils.events]: \u001b[0m eta: 1 day, 13:00:03  iter: 2539  total_loss: 0.6564  loss_cls: 0.3664  loss_box_reg: 0.2839  loss_query: 0.02217  time: 2.2973  data_time: 0.0070  lr: 0.006  max_mem: 10556M\n",
            "\u001b[32m[04/14 09:43:14 d2.utils.events]: \u001b[0m eta: 1 day, 12:58:44  iter: 2559  total_loss: 0.8679  loss_cls: 0.5053  loss_box_reg: 0.3207  loss_query: 0.03795  time: 2.2973  data_time: 0.0082  lr: 0.006  max_mem: 10556M\n",
            "\u001b[32m[04/14 09:44:53 d2.utils.events]: \u001b[0m eta: 1 day, 12:58:00  iter: 2599  total_loss: 0.8649  loss_cls: 0.5233  loss_box_reg: 0.2929  loss_query: 0.04364  time: 2.2978  data_time: 0.0082  lr: 0.006  max_mem: 10556M\n",
            "\u001b[32m[04/14 09:45:41 d2.utils.events]: \u001b[0m eta: 1 day, 12:57:42  iter: 2619  total_loss: 0.783  loss_cls: 0.4834  loss_box_reg: 0.2436  loss_query: 0.02867  time: 2.2973  data_time: 0.0079  lr: 0.006  max_mem: 10556M\n",
            "\u001b[32m[04/14 09:46:30 d2.utils.events]: \u001b[0m eta: 1 day, 12:57:15  iter: 2639  total_loss: 1.022  loss_cls: 0.5861  loss_box_reg: 0.3679  loss_query: 0.05175  time: 2.2976  data_time: 0.0073  lr: 0.006  max_mem: 10556M\n",
            "\u001b[32m[04/14 09:47:19 d2.utils.events]: \u001b[0m eta: 1 day, 12:54:52  iter: 2659  total_loss: 0.6774  loss_cls: 0.4092  loss_box_reg: 0.2342  loss_query: 0.03979  time: 2.2974  data_time: 0.0059  lr: 0.006  max_mem: 10556M\n",
            "\u001b[32m[04/14 09:48:09 d2.utils.events]: \u001b[0m eta: 1 day, 12:54:06  iter: 2679  total_loss: 0.8551  loss_cls: 0.5151  loss_box_reg: 0.2817  loss_query: 0.03851  time: 2.2979  data_time: 0.0070  lr: 0.006  max_mem: 10556M\n",
            "\u001b[32m[04/14 09:49:22 d2.utils.events]: \u001b[0m eta: 1 day, 12:54:08  iter: 2699  total_loss: 0.8203  loss_cls: 0.5068  loss_box_reg: 0.3091  loss_query: 0.03949  time: 2.3067  data_time: 1.1764  lr: 0.006  max_mem: 10556M\n",
            "\u001b[32m[04/14 09:50:13 d2.utils.events]: \u001b[0m eta: 1 day, 12:54:26  iter: 2719  total_loss: 0.8779  loss_cls: 0.5078  loss_box_reg: 0.2922  loss_query: 0.04404  time: 2.3072  data_time: 0.0094  lr: 0.006  max_mem: 10556M\n",
            "\u001b[32m[04/14 09:51:01 d2.utils.events]: \u001b[0m eta: 1 day, 12:51:52  iter: 2739  total_loss: 0.5598  loss_cls: 0.3705  loss_box_reg: 0.1976  loss_query: 0.02924  time: 2.3068  data_time: 0.0095  lr: 0.006  max_mem: 10556M\n",
            "\u001b[32m[04/14 09:51:49 d2.utils.events]: \u001b[0m eta: 1 day, 12:51:04  iter: 2759  total_loss: 0.7657  loss_cls: 0.5004  loss_box_reg: 0.2157  loss_query: 0.03245  time: 2.3065  data_time: 0.0075  lr: 0.006  max_mem: 10556M\n",
            "\u001b[32m[04/14 09:52:40 d2.utils.events]: \u001b[0m eta: 1 day, 12:51:31  iter: 2779  total_loss: 0.8376  loss_cls: 0.5276  loss_box_reg: 0.2559  loss_query: 0.04051  time: 2.3065  data_time: 0.0115  lr: 0.006  max_mem: 10556M\n",
            "\u001b[32m[04/14 09:53:31 d2.utils.events]: \u001b[0m eta: 1 day, 12:51:57  iter: 2799  total_loss: 0.7084  loss_cls: 0.4274  loss_box_reg: 0.254  loss_query: 0.03482  time: 2.3069  data_time: 0.0073  lr: 0.006  max_mem: 10556M\n",
            "\u001b[32m[04/14 09:54:21 d2.utils.events]: \u001b[0m eta: 1 day, 12:51:03  iter: 2819  total_loss: 0.6323  loss_cls: 0.3648  loss_box_reg: 0.2189  loss_query: 0.02961  time: 2.3073  data_time: 0.0065  lr: 0.006  max_mem: 10556M\n",
            "\u001b[32m[04/14 09:55:11 d2.utils.events]: \u001b[0m eta: 1 day, 12:51:47  iter: 2839  total_loss: 0.8867  loss_cls: 0.5434  loss_box_reg: 0.3027  loss_query: 0.04174  time: 2.3075  data_time: 0.0076  lr: 0.006  max_mem: 10556M\n",
            "\u001b[32m[04/14 09:56:00 d2.utils.events]: \u001b[0m eta: 1 day, 12:51:00  iter: 2859  total_loss: 0.8301  loss_cls: 0.4618  loss_box_reg: 0.3049  loss_query: 0.02494  time: 2.3076  data_time: 0.0062  lr: 0.006  max_mem: 10556M\n",
            "\u001b[32m[04/14 09:56:49 d2.utils.events]: \u001b[0m eta: 1 day, 12:50:42  iter: 2879  total_loss: 0.6933  loss_cls: 0.4458  loss_box_reg: 0.2494  loss_query: 0.03703  time: 2.3074  data_time: 0.0070  lr: 0.006  max_mem: 10556M\n",
            "\u001b[32m[04/14 09:57:37 d2.utils.events]: \u001b[0m eta: 1 day, 12:50:06  iter: 2899  total_loss: 0.6583  loss_cls: 0.3762  loss_box_reg: 0.2467  loss_query: 0.03354  time: 2.3070  data_time: 0.0066  lr: 0.006  max_mem: 10556M\n",
            "\u001b[32m[04/14 09:58:25 d2.utils.events]: \u001b[0m eta: 1 day, 12:50:09  iter: 2919  total_loss: 0.605  loss_cls: 0.4173  loss_box_reg: 0.1813  loss_query: 0.04023  time: 2.3066  data_time: 0.0067  lr: 0.006  max_mem: 10556M\n",
            "\u001b[32m[04/14 09:59:14 d2.utils.events]: \u001b[0m eta: 1 day, 12:50:43  iter: 2939  total_loss: 0.9033  loss_cls: 0.5473  loss_box_reg: 0.2801  loss_query: 0.03748  time: 2.3068  data_time: 0.0068  lr: 0.006  max_mem: 10556M\n",
            "\u001b[32m[04/14 10:00:04 d2.utils.events]: \u001b[0m eta: 1 day, 12:49:56  iter: 2959  total_loss: 0.7994  loss_cls: 0.4693  loss_box_reg: 0.2566  loss_query: 0.03885  time: 2.3069  data_time: 0.0061  lr: 0.006  max_mem: 10556M\n",
            "\u001b[32m[04/14 10:00:52 d2.utils.events]: \u001b[0m eta: 1 day, 12:47:53  iter: 2979  total_loss: 0.8773  loss_cls: 0.5111  loss_box_reg: 0.2719  loss_query: 0.03866  time: 2.3065  data_time: 0.0087  lr: 0.006  max_mem: 10556M\n",
            "\u001b[32m[04/14 10:01:41 d2.utils.events]: \u001b[0m eta: 1 day, 12:46:49  iter: 2999  total_loss: 0.7716  loss_cls: 0.459  loss_box_reg: 0.2697  loss_query: 0.04535  time: 2.3062  data_time: 0.0070  lr: 0.006  max_mem: 10556M\n",
            "\u001b[32m[04/14 10:02:32 d2.utils.events]: \u001b[0m eta: 1 day, 12:46:44  iter: 3019  total_loss: 0.7539  loss_cls: 0.4473  loss_box_reg: 0.2594  loss_query: 0.04497  time: 2.3067  data_time: 0.0075  lr: 0.006  max_mem: 10556M\n",
            "\u001b[32m[04/14 10:03:21 d2.utils.events]: \u001b[0m eta: 1 day, 12:47:06  iter: 3039  total_loss: 0.7739  loss_cls: 0.4581  loss_box_reg: 0.2534  loss_query: 0.0473  time: 2.3063  data_time: 0.0065  lr: 0.006  max_mem: 10556M\n",
            "\u001b[32m[04/14 10:04:09 d2.utils.events]: \u001b[0m eta: 1 day, 12:46:05  iter: 3059  total_loss: 0.6997  loss_cls: 0.4083  loss_box_reg: 0.2617  loss_query: 0.04274  time: 2.3060  data_time: 0.0080  lr: 0.006  max_mem: 10556M\n",
            "\u001b[32m[04/14 10:05:16 d2.utils.events]: \u001b[0m eta: 1 day, 12:44:05  iter: 3079  total_loss: 0.5363  loss_cls: 0.3484  loss_box_reg: 0.1709  loss_query: 0.01811  time: 2.3119  data_time: 0.9730  lr: 0.006  max_mem: 10556M\n",
            "\u001b[32m[04/14 10:06:07 d2.utils.events]: \u001b[0m eta: 1 day, 12:43:58  iter: 3099  total_loss: 1.043  loss_cls: 0.589  loss_box_reg: 0.3591  loss_query: 0.04561  time: 2.3121  data_time: 0.0084  lr: 0.006  max_mem: 10556M\n",
            "\u001b[32m[04/14 10:06:54 d2.utils.events]: \u001b[0m eta: 1 day, 12:42:54  iter: 3119  total_loss: 0.5683  loss_cls: 0.3771  loss_box_reg: 0.1806  loss_query: 0.03452  time: 2.3115  data_time: 0.0057  lr: 0.006  max_mem: 10556M\n",
            "\u001b[32m[04/14 10:07:42 d2.utils.events]: \u001b[0m eta: 1 day, 12:41:41  iter: 3139  total_loss: 0.7204  loss_cls: 0.4448  loss_box_reg: 0.2659  loss_query: 0.02812  time: 2.3111  data_time: 0.0076  lr: 0.006  max_mem: 10556M\n",
            "\u001b[32m[04/14 10:08:31 d2.utils.events]: \u001b[0m eta: 1 day, 12:40:55  iter: 3159  total_loss: 0.7902  loss_cls: 0.5301  loss_box_reg: 0.2549  loss_query: 0.0379  time: 2.3112  data_time: 0.0102  lr: 0.006  max_mem: 10556M\n",
            "\u001b[32m[04/14 10:09:20 d2.utils.events]: \u001b[0m eta: 1 day, 12:40:05  iter: 3179  total_loss: 0.5963  loss_cls: 0.3605  loss_box_reg: 0.1926  loss_query: 0.02912  time: 2.3109  data_time: 0.0091  lr: 0.006  max_mem: 10556M\n",
            "\u001b[32m[04/14 10:10:08 d2.utils.events]: \u001b[0m eta: 1 day, 12:39:48  iter: 3199  total_loss: 0.8928  loss_cls: 0.5383  loss_box_reg: 0.2602  loss_query: 0.04019  time: 2.3108  data_time: 0.0062  lr: 0.006  max_mem: 10556M\n",
            "\u001b[32m[04/14 10:10:55 d2.utils.events]: \u001b[0m eta: 1 day, 12:38:36  iter: 3219  total_loss: 0.6906  loss_cls: 0.3987  loss_box_reg: 0.2144  loss_query: 0.02638  time: 2.3101  data_time: 0.0076  lr: 0.006  max_mem: 10556M\n",
            "\u001b[32m[04/14 10:11:48 d2.utils.events]: \u001b[0m eta: 1 day, 12:38:33  iter: 3239  total_loss: 1.058  loss_cls: 0.5983  loss_box_reg: 0.3272  loss_query: 0.05205  time: 2.3102  data_time: 0.0097  lr: 0.006  max_mem: 10556M\n",
            "\u001b[32m[04/14 10:12:38 d2.utils.events]: \u001b[0m eta: 1 day, 12:38:40  iter: 3259  total_loss: 0.9423  loss_cls: 0.5518  loss_box_reg: 0.3309  loss_query: 0.05053  time: 2.3104  data_time: 0.0078  lr: 0.006  max_mem: 10556M\n",
            "\u001b[32m[04/14 10:13:26 d2.utils.events]: \u001b[0m eta: 1 day, 12:37:40  iter: 3279  total_loss: 0.6974  loss_cls: 0.4207  loss_box_reg: 0.2183  loss_query: 0.02974  time: 2.3100  data_time: 0.0078  lr: 0.006  max_mem: 10556M\n",
            "\u001b[32m[04/14 10:14:15 d2.utils.events]: \u001b[0m eta: 1 day, 12:37:55  iter: 3299  total_loss: 0.8038  loss_cls: 0.5038  loss_box_reg: 0.2623  loss_query: 0.04491  time: 2.3098  data_time: 0.0076  lr: 0.006  max_mem: 10556M\n",
            "\u001b[32m[04/14 10:15:05 d2.utils.events]: \u001b[0m eta: 1 day, 12:36:57  iter: 3319  total_loss: 0.8262  loss_cls: 0.4782  loss_box_reg: 0.2838  loss_query: 0.04502  time: 2.3098  data_time: 0.0075  lr: 0.006  max_mem: 10556M\n",
            "\u001b[32m[04/14 10:15:53 d2.utils.events]: \u001b[0m eta: 1 day, 12:35:31  iter: 3339  total_loss: 0.6985  loss_cls: 0.4351  loss_box_reg: 0.2403  loss_query: 0.04623  time: 2.3094  data_time: 0.0067  lr: 0.006  max_mem: 10556M\n",
            "\u001b[32m[04/14 10:16:42 d2.utils.events]: \u001b[0m eta: 1 day, 12:34:44  iter: 3359  total_loss: 0.8665  loss_cls: 0.4757  loss_box_reg: 0.3156  loss_query: 0.04406  time: 2.3094  data_time: 0.0064  lr: 0.006  max_mem: 10556M\n",
            "\u001b[32m[04/14 10:17:29 d2.utils.events]: \u001b[0m eta: 1 day, 12:33:20  iter: 3379  total_loss: 0.5516  loss_cls: 0.3246  loss_box_reg: 0.2033  loss_query: 0.02367  time: 2.3088  data_time: 0.0099  lr: 0.006  max_mem: 10556M\n",
            "\u001b[32m[04/14 10:18:17 d2.utils.events]: \u001b[0m eta: 1 day, 12:33:01  iter: 3399  total_loss: 0.7367  loss_cls: 0.4568  loss_box_reg: 0.2501  loss_query: 0.04221  time: 2.3085  data_time: 0.0077  lr: 0.006  max_mem: 10556M\n",
            "\u001b[32m[04/14 10:19:06 d2.utils.events]: \u001b[0m eta: 1 day, 12:32:28  iter: 3419  total_loss: 0.6673  loss_cls: 0.4016  loss_box_reg: 0.2317  loss_query: 0.02704  time: 2.3084  data_time: 0.0087  lr: 0.006  max_mem: 10556M\n",
            "\u001b[32m[04/14 10:19:56 d2.utils.events]: \u001b[0m eta: 1 day, 12:32:03  iter: 3439  total_loss: 1.026  loss_cls: 0.6019  loss_box_reg: 0.3464  loss_query: 0.05059  time: 2.3082  data_time: 0.0071  lr: 0.006  max_mem: 10556M\n",
            "\u001b[32m[04/14 10:20:44 d2.utils.events]: \u001b[0m eta: 1 day, 12:30:59  iter: 3459  total_loss: 0.7052  loss_cls: 0.4172  loss_box_reg: 0.2503  loss_query: 0.04246  time: 2.3080  data_time: 0.0081  lr: 0.006  max_mem: 10556M\n",
            "\u001b[32m[04/14 10:21:33 d2.utils.events]: \u001b[0m eta: 1 day, 12:29:27  iter: 3479  total_loss: 0.727  loss_cls: 0.4635  loss_box_reg: 0.2366  loss_query: 0.02891  time: 2.3077  data_time: 0.0127  lr: 0.006  max_mem: 10556M\n",
            "\u001b[32m[04/14 10:22:21 d2.utils.events]: \u001b[0m eta: 1 day, 12:28:41  iter: 3499  total_loss: 0.7573  loss_cls: 0.4777  loss_box_reg: 0.2639  loss_query: 0.0311  time: 2.3074  data_time: 0.0063  lr: 0.006  max_mem: 10556M\n",
            "\u001b[32m[04/14 10:23:11 d2.utils.events]: \u001b[0m eta: 1 day, 12:28:26  iter: 3519  total_loss: 1.021  loss_cls: 0.5899  loss_box_reg: 0.3512  loss_query: 0.05147  time: 2.3076  data_time: 0.0072  lr: 0.006  max_mem: 10556M\n",
            "\u001b[32m[04/14 10:23:59 d2.utils.events]: \u001b[0m eta: 1 day, 12:27:08  iter: 3539  total_loss: 0.8003  loss_cls: 0.4935  loss_box_reg: 0.2641  loss_query: 0.03371  time: 2.3073  data_time: 0.0073  lr: 0.006  max_mem: 10556M\n",
            "\u001b[32m[04/14 10:24:50 d2.utils.events]: \u001b[0m eta: 1 day, 12:27:03  iter: 3559  total_loss: 0.8696  loss_cls: 0.5126  loss_box_reg: 0.2691  loss_query: 0.03531  time: 2.3074  data_time: 0.0073  lr: 0.006  max_mem: 10556M\n",
            "\u001b[32m[04/14 10:25:39 d2.utils.events]: \u001b[0m eta: 1 day, 12:27:03  iter: 3579  total_loss: 0.656  loss_cls: 0.4314  loss_box_reg: 0.2077  loss_query: 0.03585  time: 2.3073  data_time: 0.0080  lr: 0.006  max_mem: 10556M\n",
            "\u001b[32m[04/14 10:26:28 d2.utils.events]: \u001b[0m eta: 1 day, 12:26:54  iter: 3599  total_loss: 0.7279  loss_cls: 0.4183  loss_box_reg: 0.2852  loss_query: 0.02714  time: 2.3073  data_time: 0.0084  lr: 0.006  max_mem: 10556M\n",
            "\u001b[32m[04/14 10:27:18 d2.utils.events]: \u001b[0m eta: 1 day, 12:26:46  iter: 3619  total_loss: 1.051  loss_cls: 0.5905  loss_box_reg: 0.3705  loss_query: 0.04976  time: 2.3075  data_time: 0.0087  lr: 0.006  max_mem: 10556M\n",
            "\u001b[32m[04/14 10:28:07 d2.utils.events]: \u001b[0m eta: 1 day, 12:25:55  iter: 3639  total_loss: 0.6162  loss_cls: 0.3376  loss_box_reg: 0.2119  loss_query: 0.02807  time: 2.3074  data_time: 0.0076  lr: 0.006  max_mem: 10556M\n",
            "\u001b[32m[04/14 10:28:58 d2.utils.events]: \u001b[0m eta: 1 day, 12:25:19  iter: 3659  total_loss: 0.7597  loss_cls: 0.4711  loss_box_reg: 0.2606  loss_query: 0.03619  time: 2.3076  data_time: 0.0072  lr: 0.006  max_mem: 10556M\n",
            "\u001b[32m[04/14 10:29:46 d2.utils.events]: \u001b[0m eta: 1 day, 12:24:22  iter: 3679  total_loss: 0.8925  loss_cls: 0.5395  loss_box_reg: 0.3075  loss_query: 0.04452  time: 2.3074  data_time: 0.0106  lr: 0.006  max_mem: 10556M\n",
            "\u001b[32m[04/14 10:30:34 d2.utils.events]: \u001b[0m eta: 1 day, 12:22:57  iter: 3699  total_loss: 0.7075  loss_cls: 0.4086  loss_box_reg: 0.2453  loss_query: 0.04202  time: 2.3070  data_time: 0.0086  lr: 0.006  max_mem: 10556M\n",
            "\u001b[32m[04/14 10:31:23 d2.utils.events]: \u001b[0m eta: 1 day, 12:21:48  iter: 3719  total_loss: 0.6704  loss_cls: 0.3955  loss_box_reg: 0.2257  loss_query: 0.02531  time: 2.3071  data_time: 0.0085  lr: 0.006  max_mem: 10556M\n",
            "\u001b[32m[04/14 10:32:10 d2.utils.events]: \u001b[0m eta: 1 day, 12:21:57  iter: 3739  total_loss: 0.9  loss_cls: 0.5755  loss_box_reg: 0.2976  loss_query: 0.0475  time: 2.3066  data_time: 0.0077  lr: 0.006  max_mem: 10556M\n",
            "\u001b[32m[04/14 10:33:02 d2.utils.events]: \u001b[0m eta: 1 day, 12:21:20  iter: 3759  total_loss: 0.7489  loss_cls: 0.4361  loss_box_reg: 0.2569  loss_query: 0.0371  time: 2.3068  data_time: 0.0092  lr: 0.006  max_mem: 10556M\n",
            "\u001b[32m[04/14 10:33:52 d2.utils.events]: \u001b[0m eta: 1 day, 12:20:24  iter: 3779  total_loss: 0.8193  loss_cls: 0.4946  loss_box_reg: 0.2771  loss_query: 0.02392  time: 2.3069  data_time: 0.0073  lr: 0.006  max_mem: 10556M\n",
            "\u001b[32m[04/14 10:34:40 d2.utils.events]: \u001b[0m eta: 1 day, 12:18:54  iter: 3799  total_loss: 0.7206  loss_cls: 0.4233  loss_box_reg: 0.2119  loss_query: 0.03467  time: 2.3066  data_time: 0.0080  lr: 0.006  max_mem: 10556M\n",
            "\u001b[32m[04/14 10:35:29 d2.utils.events]: \u001b[0m eta: 1 day, 12:17:46  iter: 3819  total_loss: 0.5509  loss_cls: 0.3035  loss_box_reg: 0.1818  loss_query: 0.03012  time: 2.3065  data_time: 0.0084  lr: 0.006  max_mem: 10556M\n",
            "\u001b[32m[04/14 10:36:17 d2.utils.events]: \u001b[0m eta: 1 day, 12:16:32  iter: 3839  total_loss: 0.7023  loss_cls: 0.4349  loss_box_reg: 0.2351  loss_query: 0.02902  time: 2.3064  data_time: 0.0073  lr: 0.006  max_mem: 10556M\n",
            "\u001b[32m[04/14 10:37:07 d2.utils.events]: \u001b[0m eta: 1 day, 12:16:25  iter: 3859  total_loss: 0.81  loss_cls: 0.4903  loss_box_reg: 0.268  loss_query: 0.03745  time: 2.3065  data_time: 0.0068  lr: 0.006  max_mem: 10556M\n",
            "\u001b[32m[04/14 10:37:56 d2.utils.events]: \u001b[0m eta: 1 day, 12:15:53  iter: 3879  total_loss: 0.562  loss_cls: 0.3675  loss_box_reg: 0.161  loss_query: 0.03756  time: 2.3066  data_time: 0.0080  lr: 0.006  max_mem: 10556M\n",
            "\u001b[32m[04/14 10:38:46 d2.utils.events]: \u001b[0m eta: 1 day, 12:15:06  iter: 3899  total_loss: 0.6728  loss_cls: 0.3567  loss_box_reg: 0.2444  loss_query: 0.03132  time: 2.3068  data_time: 0.0088  lr: 0.006  max_mem: 10556M\n",
            "\u001b[32m[04/14 10:39:36 d2.utils.events]: \u001b[0m eta: 1 day, 12:15:04  iter: 3919  total_loss: 0.8015  loss_cls: 0.4811  loss_box_reg: 0.2931  loss_query: 0.04836  time: 2.3069  data_time: 0.0084  lr: 0.006  max_mem: 10556M\n",
            "\u001b[32m[04/14 10:40:25 d2.utils.events]: \u001b[0m eta: 1 day, 12:13:47  iter: 3939  total_loss: 0.6698  loss_cls: 0.4016  loss_box_reg: 0.2134  loss_query: 0.03214  time: 2.3067  data_time: 0.0072  lr: 0.006  max_mem: 10556M\n",
            "\u001b[32m[04/14 10:41:12 d2.utils.events]: \u001b[0m eta: 1 day, 12:12:47  iter: 3959  total_loss: 0.6952  loss_cls: 0.4331  loss_box_reg: 0.2066  loss_query: 0.0351  time: 2.3062  data_time: 0.0093  lr: 0.006  max_mem: 10556M\n",
            "\u001b[32m[04/14 10:42:01 d2.utils.events]: \u001b[0m eta: 1 day, 12:13:11  iter: 3979  total_loss: 0.7911  loss_cls: 0.4041  loss_box_reg: 0.2613  loss_query: 0.02986  time: 2.3061  data_time: 0.0086  lr: 0.006  max_mem: 10556M\n",
            "\u001b[32m[04/14 10:42:49 fvcore.common.checkpoint]: \u001b[0mSaving checkpoint to work_dirs/visdrone_querydet/model_0003999.pth\n",
            "\u001b[32m[04/14 10:43:08 d2.utils.events]: \u001b[0m eta: 1 day, 12:12:24  iter: 3999  total_loss: 0.9439  loss_cls: 0.5808  loss_box_reg: 0.3428  loss_query: 0.03625  time: 2.3057  data_time: 0.0077  lr: 0.006  max_mem: 10556M\n",
            "\u001b[32m[04/14 10:43:57 d2.utils.events]: \u001b[0m eta: 1 day, 12:10:27  iter: 4019  total_loss: 0.5689  loss_cls: 0.3519  loss_box_reg: 0.1924  loss_query: 0.01674  time: 2.3056  data_time: 0.0070  lr: 0.006  max_mem: 10556M\n",
            "\u001b[32m[04/14 10:44:46 d2.utils.events]: \u001b[0m eta: 1 day, 12:09:55  iter: 4039  total_loss: 0.845  loss_cls: 0.5036  loss_box_reg: 0.279  loss_query: 0.03347  time: 2.3056  data_time: 0.0085  lr: 0.006  max_mem: 10556M\n",
            "\u001b[32m[04/14 10:45:36 d2.utils.events]: \u001b[0m eta: 1 day, 12:09:08  iter: 4059  total_loss: 0.5883  loss_cls: 0.3719  loss_box_reg: 0.1959  loss_query: 0.02649  time: 2.3058  data_time: 0.0092  lr: 0.006  max_mem: 10556M\n",
            "\u001b[32m[04/14 10:46:25 d2.utils.events]: \u001b[0m eta: 1 day, 12:08:49  iter: 4079  total_loss: 0.775  loss_cls: 0.4784  loss_box_reg: 0.2751  loss_query: 0.03657  time: 2.3058  data_time: 0.0089  lr: 0.006  max_mem: 10556M\n",
            "\u001b[32m[04/14 10:47:13 d2.utils.events]: \u001b[0m eta: 1 day, 12:06:57  iter: 4099  total_loss: 0.7052  loss_cls: 0.4228  loss_box_reg: 0.2363  loss_query: 0.03191  time: 2.3055  data_time: 0.0062  lr: 0.006  max_mem: 10556M\n",
            "\u001b[32m[04/14 10:48:00 d2.utils.events]: \u001b[0m eta: 1 day, 12:06:49  iter: 4119  total_loss: 0.7027  loss_cls: 0.4065  loss_box_reg: 0.2431  loss_query: 0.03886  time: 2.3050  data_time: 0.0079  lr: 0.006  max_mem: 10556M\n",
            "\u001b[32m[04/14 10:48:49 d2.utils.events]: \u001b[0m eta: 1 day, 12:06:33  iter: 4139  total_loss: 1.026  loss_cls: 0.6036  loss_box_reg: 0.3348  loss_query: 0.02833  time: 2.3051  data_time: 0.0084  lr: 0.006  max_mem: 10556M\n",
            "\u001b[32m[04/14 10:49:37 d2.utils.events]: \u001b[0m eta: 1 day, 12:05:15  iter: 4159  total_loss: 0.5961  loss_cls: 0.3702  loss_box_reg: 0.2043  loss_query: 0.03249  time: 2.3049  data_time: 0.0075  lr: 0.006  max_mem: 10556M\n",
            "\u001b[32m[04/14 10:50:26 d2.utils.events]: \u001b[0m eta: 1 day, 12:04:29  iter: 4179  total_loss: 0.7078  loss_cls: 0.4221  loss_box_reg: 0.2486  loss_query: 0.03052  time: 2.3049  data_time: 0.0098  lr: 0.006  max_mem: 10556M\n",
            "\u001b[32m[04/14 10:51:15 d2.utils.events]: \u001b[0m eta: 1 day, 12:03:15  iter: 4199  total_loss: 0.5678  loss_cls: 0.3738  loss_box_reg: 0.2136  loss_query: 0.02108  time: 2.3049  data_time: 0.0074  lr: 0.006  max_mem: 10556M\n",
            "\u001b[32m[04/14 10:52:05 d2.utils.events]: \u001b[0m eta: 1 day, 12:02:28  iter: 4219  total_loss: 0.6202  loss_cls: 0.4069  loss_box_reg: 0.2145  loss_query: 0.03106  time: 2.3048  data_time: 0.0088  lr: 0.006  max_mem: 10556M\n",
            "\u001b[32m[04/14 10:52:53 d2.utils.events]: \u001b[0m eta: 1 day, 12:01:38  iter: 4239  total_loss: 0.7715  loss_cls: 0.4734  loss_box_reg: 0.2959  loss_query: 0.03023  time: 2.3046  data_time: 0.0081  lr: 0.006  max_mem: 10556M\n",
            "\u001b[32m[04/14 10:53:41 d2.utils.events]: \u001b[0m eta: 1 day, 12:00:26  iter: 4259  total_loss: 0.623  loss_cls: 0.4336  loss_box_reg: 0.2303  loss_query: 0.03032  time: 2.3043  data_time: 0.0071  lr: 0.006  max_mem: 10556M\n",
            "\u001b[32m[04/14 10:54:32 d2.utils.events]: \u001b[0m eta: 1 day, 11:59:58  iter: 4279  total_loss: 0.7381  loss_cls: 0.4356  loss_box_reg: 0.2658  loss_query: 0.03583  time: 2.3045  data_time: 0.0078  lr: 0.006  max_mem: 10556M\n",
            "\u001b[32m[04/14 10:55:21 d2.utils.events]: \u001b[0m eta: 1 day, 11:59:03  iter: 4299  total_loss: 0.8532  loss_cls: 0.5201  loss_box_reg: 0.3053  loss_query: 0.04266  time: 2.3046  data_time: 0.0066  lr: 0.006  max_mem: 10556M\n",
            "\u001b[32m[04/14 10:56:09 d2.utils.events]: \u001b[0m eta: 1 day, 11:58:07  iter: 4319  total_loss: 0.838  loss_cls: 0.4961  loss_box_reg: 0.3066  loss_query: 0.04122  time: 2.3042  data_time: 0.0097  lr: 0.006  max_mem: 10556M\n",
            "\u001b[32m[04/14 10:56:57 d2.utils.events]: \u001b[0m eta: 1 day, 11:57:20  iter: 4339  total_loss: 0.8358  loss_cls: 0.5239  loss_box_reg: 0.2501  loss_query: 0.0306  time: 2.3040  data_time: 0.0082  lr: 0.006  max_mem: 10556M\n",
            "\u001b[32m[04/14 10:57:46 d2.utils.events]: \u001b[0m eta: 1 day, 11:56:14  iter: 4359  total_loss: 0.8749  loss_cls: 0.5385  loss_box_reg: 0.2671  loss_query: 0.04671  time: 2.3041  data_time: 0.0072  lr: 0.006  max_mem: 10556M\n",
            "\u001b[32m[04/14 10:58:35 d2.utils.events]: \u001b[0m eta: 1 day, 11:55:47  iter: 4379  total_loss: 0.4896  loss_cls: 0.2878  loss_box_reg: 0.1722  loss_query: 0.03167  time: 2.3039  data_time: 0.0079  lr: 0.006  max_mem: 10556M\n",
            "\u001b[32m[04/14 10:59:23 d2.utils.events]: \u001b[0m eta: 1 day, 11:55:01  iter: 4399  total_loss: 0.7311  loss_cls: 0.4583  loss_box_reg: 0.2462  loss_query: 0.04481  time: 2.3037  data_time: 0.0071  lr: 0.006  max_mem: 10556M\n",
            "\u001b[32m[04/14 11:00:12 d2.utils.events]: \u001b[0m eta: 1 day, 11:54:24  iter: 4419  total_loss: 0.9427  loss_cls: 0.5735  loss_box_reg: 0.3219  loss_query: 0.03964  time: 2.3036  data_time: 0.0080  lr: 0.006  max_mem: 10556M\n",
            "\u001b[32m[04/14 11:01:01 d2.utils.events]: \u001b[0m eta: 1 day, 11:54:07  iter: 4439  total_loss: 0.7234  loss_cls: 0.3954  loss_box_reg: 0.2504  loss_query: 0.03456  time: 2.3037  data_time: 0.0095  lr: 0.006  max_mem: 10556M\n",
            "\u001b[32m[04/14 11:01:49 d2.utils.events]: \u001b[0m eta: 1 day, 11:52:59  iter: 4459  total_loss: 0.9407  loss_cls: 0.5361  loss_box_reg: 0.3537  loss_query: 0.03329  time: 2.3035  data_time: 0.0061  lr: 0.006  max_mem: 10556M\n",
            "\u001b[32m[04/14 11:02:38 d2.utils.events]: \u001b[0m eta: 1 day, 11:52:30  iter: 4479  total_loss: 0.8613  loss_cls: 0.5265  loss_box_reg: 0.2742  loss_query: 0.0446  time: 2.3033  data_time: 0.0084  lr: 0.006  max_mem: 10556M\n",
            "\u001b[32m[04/14 11:03:27 d2.utils.events]: \u001b[0m eta: 1 day, 11:51:57  iter: 4499  total_loss: 0.7018  loss_cls: 0.4045  loss_box_reg: 0.2337  loss_query: 0.05701  time: 2.3034  data_time: 0.0063  lr: 0.006  max_mem: 10556M\n",
            "\u001b[32m[04/14 11:04:16 d2.utils.events]: \u001b[0m eta: 1 day, 11:50:40  iter: 4519  total_loss: 0.8403  loss_cls: 0.452  loss_box_reg: 0.3032  loss_query: 0.04454  time: 2.3032  data_time: 0.0071  lr: 0.006  max_mem: 10556M\n",
            "\u001b[32m[04/14 11:05:03 d2.utils.events]: \u001b[0m eta: 1 day, 11:50:10  iter: 4539  total_loss: 0.6752  loss_cls: 0.3956  loss_box_reg: 0.197  loss_query: 0.03911  time: 2.3029  data_time: 0.0068  lr: 0.006  max_mem: 10556M\n",
            "\u001b[32m[04/14 11:05:50 d2.utils.events]: \u001b[0m eta: 1 day, 11:48:38  iter: 4559  total_loss: 0.5943  loss_cls: 0.3455  loss_box_reg: 0.2164  loss_query: 0.03104  time: 2.3025  data_time: 0.0060  lr: 0.006  max_mem: 10556M\n",
            "\u001b[32m[04/14 11:06:39 d2.utils.events]: \u001b[0m eta: 1 day, 11:48:02  iter: 4579  total_loss: 0.773  loss_cls: 0.4761  loss_box_reg: 0.3069  loss_query: 0.02415  time: 2.3023  data_time: 0.0085  lr: 0.006  max_mem: 10556M\n",
            "\u001b[32m[04/14 11:07:28 d2.utils.events]: \u001b[0m eta: 1 day, 11:47:05  iter: 4599  total_loss: 0.9154  loss_cls: 0.5683  loss_box_reg: 0.3018  loss_query: 0.02962  time: 2.3024  data_time: 0.0066  lr: 0.006  max_mem: 10556M\n",
            "\u001b[4m\u001b[5m\u001b[31mERROR\u001b[0m \u001b[32m[04/14 11:08:04 d2.engine.train_loop]: \u001b[0mException during training:\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.7/site-packages/detectron2/engine/train_loop.py\", line 138, in train\n",
            "    self.run_step()\n",
            "  File \"/usr/local/lib/python3.7/site-packages/detectron2/engine/defaults.py\", line 441, in run_step\n",
            "    self._trainer.run_step()\n",
            "  File \"/usr/local/lib/python3.7/site-packages/detectron2/engine/train_loop.py\", line 328, in run_step\n",
            "    data = next(self._data_loader_iter)\n",
            "  File \"/usr/local/lib/python3.7/site-packages/torch/utils/data/dataloader.py\", line 517, in __next__\n",
            "    data = self._next_data()\n",
            "  File \"/usr/local/lib/python3.7/site-packages/torch/utils/data/dataloader.py\", line 1199, in _next_data\n",
            "    return self._process_data(data)\n",
            "  File \"/usr/local/lib/python3.7/site-packages/torch/utils/data/dataloader.py\", line 1225, in _process_data\n",
            "    data.reraise()\n",
            "  File \"/usr/local/lib/python3.7/site-packages/torch/_utils.py\", line 429, in reraise\n",
            "    raise self.exc_type(msg)\n",
            "OSError: Caught OSError in DataLoader worker process 2.\n",
            "Original Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.7/site-packages/torch/utils/data/_utils/worker.py\", line 202, in _worker_loop\n",
            "    data = fetcher.fetch(index)\n",
            "  File \"/usr/local/lib/python3.7/site-packages/torch/utils/data/_utils/fetch.py\", line 44, in fetch\n",
            "    data = [self.dataset[idx] for idx in possibly_batched_index]\n",
            "  File \"/usr/local/lib/python3.7/site-packages/torch/utils/data/_utils/fetch.py\", line 44, in <listcomp>\n",
            "    data = [self.dataset[idx] for idx in possibly_batched_index]\n",
            "  File \"/usr/local/lib/python3.7/site-packages/detectron2/data/common.py\", line 43, in __getitem__\n",
            "    data = self._map_func(self._dataset[cur_idx])\n",
            "  File \"/usr/local/lib/python3.7/site-packages/detectron2/utils/serialize.py\", line 23, in __call__\n",
            "    return self._obj(*args, **kwargs)\n",
            "  File \"/content/gdrive/MyDrive/.shared/QueryDet-PyTorch/visdrone/mapper.py\", line 53, in __call__\n",
            "    image = utils.read_image(dataset_dict[\"file_name\"], format=self.img_format)\n",
            "  File \"/usr/local/lib/python3.7/site-packages/detectron2/data/detection_utils.py\", line 183, in read_image\n",
            "    return convert_PIL_to_numpy(image, format)\n",
            "  File \"/usr/local/lib/python3.7/site-packages/detectron2/data/detection_utils.py\", line 75, in convert_PIL_to_numpy\n",
            "    image = image.convert(conversion_format)\n",
            "  File \"/usr/local/lib/python3.7/site-packages/PIL/Image.py\", line 933, in convert\n",
            "    self.load()\n",
            "  File \"/usr/local/lib/python3.7/site-packages/PIL/ImageFile.py\", line 266, in load\n",
            "    raise OSError(msg)\n",
            "OSError: image file is truncated (31 bytes not processed)\n",
            "\n",
            "\u001b[32m[04/14 11:08:04 d2.engine.hooks]: \u001b[0mOverall training speed: 4612 iterations in 2:56:58 (2.3024 s / it)\n",
            "\u001b[32m[04/14 11:08:04 d2.engine.hooks]: \u001b[0mTotal training time: 3:07:59 (0:11:00 on hooks)\n",
            "\u001b[32m[04/14 11:08:04 d2.utils.events]: \u001b[0m eta: 1 day, 11:46:09  iter: 4614  total_loss: 0.7138  loss_cls: 0.4354  loss_box_reg: 0.275  loss_query: 0.03066  time: 2.3024  data_time: 0.0062  lr: 0.006  max_mem: 10556M\n",
            "Traceback (most recent call last):\n",
            "  File \"train_visdrone.py\", line 18, in <module>\n",
            "    args=(args,),\n",
            "  File \"/usr/local/lib/python3.7/site-packages/detectron2/engine/launch.py\", line 62, in launch\n",
            "    main_func(*args)\n",
            "  File \"/content/gdrive/MyDrive/.shared/QueryDet-PyTorch/train_tools/visdrone_train.py\", line 250, in start_train\n",
            "    return trainer.train()\n",
            "  File \"/usr/local/lib/python3.7/site-packages/detectron2/engine/defaults.py\", line 431, in train\n",
            "    super().train(self.start_iter, self.max_iter)\n",
            "  File \"/usr/local/lib/python3.7/site-packages/detectron2/engine/train_loop.py\", line 138, in train\n",
            "    self.run_step()\n",
            "  File \"/usr/local/lib/python3.7/site-packages/detectron2/engine/defaults.py\", line 441, in run_step\n",
            "    self._trainer.run_step()\n",
            "  File \"/usr/local/lib/python3.7/site-packages/detectron2/engine/train_loop.py\", line 328, in run_step\n",
            "    data = next(self._data_loader_iter)\n",
            "  File \"/usr/local/lib/python3.7/site-packages/torch/utils/data/dataloader.py\", line 517, in __next__\n",
            "    data = self._next_data()\n",
            "  File \"/usr/local/lib/python3.7/site-packages/torch/utils/data/dataloader.py\", line 1199, in _next_data\n",
            "    return self._process_data(data)\n",
            "  File \"/usr/local/lib/python3.7/site-packages/torch/utils/data/dataloader.py\", line 1225, in _process_data\n",
            "    data.reraise()\n",
            "  File \"/usr/local/lib/python3.7/site-packages/torch/_utils.py\", line 429, in reraise\n",
            "    raise self.exc_type(msg)\n",
            "OSError: Caught OSError in DataLoader worker process 2.\n",
            "Original Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.7/site-packages/torch/utils/data/_utils/worker.py\", line 202, in _worker_loop\n",
            "    data = fetcher.fetch(index)\n",
            "  File \"/usr/local/lib/python3.7/site-packages/torch/utils/data/_utils/fetch.py\", line 44, in fetch\n",
            "    data = [self.dataset[idx] for idx in possibly_batched_index]\n",
            "  File \"/usr/local/lib/python3.7/site-packages/torch/utils/data/_utils/fetch.py\", line 44, in <listcomp>\n",
            "    data = [self.dataset[idx] for idx in possibly_batched_index]\n",
            "  File \"/usr/local/lib/python3.7/site-packages/detectron2/data/common.py\", line 43, in __getitem__\n",
            "    data = self._map_func(self._dataset[cur_idx])\n",
            "  File \"/usr/local/lib/python3.7/site-packages/detectron2/utils/serialize.py\", line 23, in __call__\n",
            "    return self._obj(*args, **kwargs)\n",
            "  File \"/content/gdrive/MyDrive/.shared/QueryDet-PyTorch/visdrone/mapper.py\", line 53, in __call__\n",
            "    image = utils.read_image(dataset_dict[\"file_name\"], format=self.img_format)\n",
            "  File \"/usr/local/lib/python3.7/site-packages/detectron2/data/detection_utils.py\", line 183, in read_image\n",
            "    return convert_PIL_to_numpy(image, format)\n",
            "  File \"/usr/local/lib/python3.7/site-packages/detectron2/data/detection_utils.py\", line 75, in convert_PIL_to_numpy\n",
            "    image = image.convert(conversion_format)\n",
            "  File \"/usr/local/lib/python3.7/site-packages/PIL/Image.py\", line 933, in convert\n",
            "    self.load()\n",
            "  File \"/usr/local/lib/python3.7/site-packages/PIL/ImageFile.py\", line 266, in load\n",
            "    raise OSError(msg)\n",
            "OSError: image file is truncated (31 bytes not processed)\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "w1UfcxPQEqlO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!python train_visdrone.py --resume --config /content/gdrive/MyDrive/QueryDet-PyTorch/work_dirs/visdrone_querydet/config.yaml --num-gpu 1 OUTPUT_DIR work_dirs/visdrone_querydet"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CyqUv1GoE7oM",
        "outputId": "2bd42dda-db20-477a-97cd-06982d3b1566",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "** fvcore version of PathManager will be deprecated soon. **\n",
            "** Please migrate to the version in iopath repo. **\n",
            "https://github.com/facebookresearch/iopath \n",
            "\n",
            "Command Line Args: Namespace(config_file='/content/gdrive/MyDrive/QueryDet-PyTorch/work_dirs/visdrone_querydet/config.yaml', dist_url='tcp://127.0.0.1:49152', eval_only=False, machine_rank=0, no_pretrain=False, num_gpus=1, num_machines=1, opts=['OUTPUT_DIR', 'work_dirs/visdrone_querydet'], resume=True)\n",
            "\u001b[32m[04/14 14:19:10 detectron2]: \u001b[0mRank of current process: 0. World size: 1\n",
            "\u001b[32m[04/14 14:19:12 detectron2]: \u001b[0mEnvironment info:\n",
            "----------------------  ---------------------------------------------------------------\n",
            "sys.platform            linux\n",
            "Python                  3.7.6 (default, Jan  8 2020, 19:59:22) [GCC 7.3.0]\n",
            "numpy                   1.21.6\n",
            "detectron2              0.4 @/usr/local/lib/python3.7/site-packages/detectron2\n",
            "Compiler                GCC 7.3\n",
            "CUDA compiler           CUDA 10.1\n",
            "detectron2 arch flags   3.7, 5.0, 5.2, 6.0, 6.1, 7.0, 7.5\n",
            "DETECTRON2_ENV_MODULE   <not set>\n",
            "PyTorch                 1.8.1+cu101 @/usr/local/lib/python3.7/site-packages/torch\n",
            "PyTorch debug build     False\n",
            "GPU available           True\n",
            "GPU 0                   Tesla T4 (arch=7.5)\n",
            "CUDA_HOME               /usr/local/cuda\n",
            "Pillow                  9.5.0\n",
            "torchvision             0.9.1+cu101 @/usr/local/lib/python3.7/site-packages/torchvision\n",
            "torchvision arch flags  3.5, 5.0, 6.0, 7.0, 7.5\n",
            "fvcore                  0.1.3.post20210317\n",
            "cv2                     4.7.0\n",
            "----------------------  ---------------------------------------------------------------\n",
            "PyTorch built with:\n",
            "  - GCC 7.3\n",
            "  - C++ Version: 201402\n",
            "  - Intel(R) Math Kernel Library Version 2020.0.0 Product Build 20191122 for Intel(R) 64 architecture applications\n",
            "  - Intel(R) MKL-DNN v1.7.0 (Git Hash 7aed236906b1f7a05c0917e5257a1af05e9ff683)\n",
            "  - OpenMP 201511 (a.k.a. OpenMP 4.5)\n",
            "  - NNPACK is enabled\n",
            "  - CPU capability usage: AVX2\n",
            "  - CUDA Runtime 10.1\n",
            "  - NVCC architecture flags: -gencode;arch=compute_37,code=sm_37;-gencode;arch=compute_50,code=sm_50;-gencode;arch=compute_60,code=sm_60;-gencode;arch=compute_70,code=sm_70\n",
            "  - CuDNN 7.6.3\n",
            "  - Magma 2.5.2\n",
            "  - Build settings: BLAS_INFO=mkl, BUILD_TYPE=Release, CUDA_VERSION=10.1, CUDNN_VERSION=7.6.3, CXX_COMPILER=/opt/rh/devtoolset-7/root/usr/bin/c++, CXX_FLAGS= -Wno-deprecated -fvisibility-inlines-hidden -DUSE_PTHREADPOOL -fopenmp -DNDEBUG -DUSE_KINETO -DUSE_FBGEMM -DUSE_QNNPACK -DUSE_PYTORCH_QNNPACK -DUSE_XNNPACK -O2 -fPIC -Wno-narrowing -Wall -Wextra -Werror=return-type -Wno-missing-field-initializers -Wno-type-limits -Wno-array-bounds -Wno-unknown-pragmas -Wno-sign-compare -Wno-unused-parameter -Wno-unused-variable -Wno-unused-function -Wno-unused-result -Wno-unused-local-typedefs -Wno-strict-overflow -Wno-strict-aliasing -Wno-error=deprecated-declarations -Wno-stringop-overflow -Wno-psabi -Wno-error=pedantic -Wno-error=redundant-decls -Wno-error=old-style-cast -fdiagnostics-color=always -faligned-new -Wno-unused-but-set-variable -Wno-maybe-uninitialized -fno-math-errno -fno-trapping-math -Werror=format -Wno-stringop-overflow, LAPACK_INFO=mkl, PERF_WITH_AVX=1, PERF_WITH_AVX2=1, PERF_WITH_AVX512=1, TORCH_VERSION=1.8.1, USE_CUDA=ON, USE_CUDNN=ON, USE_EXCEPTION_PTR=1, USE_GFLAGS=OFF, USE_GLOG=OFF, USE_MKL=ON, USE_MKLDNN=ON, USE_MPI=OFF, USE_NCCL=ON, USE_NNPACK=ON, USE_OPENMP=ON, \n",
            "\n",
            "\u001b[32m[04/14 14:19:12 detectron2]: \u001b[0mCommand line arguments: Namespace(config_file='/content/gdrive/MyDrive/QueryDet-PyTorch/work_dirs/visdrone_querydet/config.yaml', dist_url='tcp://127.0.0.1:49152', eval_only=False, machine_rank=0, no_pretrain=False, num_gpus=1, num_machines=1, opts=['OUTPUT_DIR', 'work_dirs/visdrone_querydet'], resume=True)\n",
            "\u001b[32m[04/14 14:19:12 detectron2]: \u001b[0mContents of args.config_file=/content/gdrive/MyDrive/QueryDet-PyTorch/work_dirs/visdrone_querydet/config.yaml:\n",
            "CUDNN_BENCHMARK: false\n",
            "DATALOADER:\n",
            "  ASPECT_RATIO_GROUPING: true\n",
            "  FILTER_EMPTY_ANNOTATIONS: true\n",
            "  NUM_WORKERS: 4\n",
            "  REPEAT_THRESHOLD: 0.0\n",
            "  SAMPLER_TRAIN: TrainingSampler\n",
            "DATASETS:\n",
            "  PRECOMPUTED_PROPOSAL_TOPK_TEST: 1000\n",
            "  PRECOMPUTED_PROPOSAL_TOPK_TRAIN: 2000\n",
            "  PROPOSAL_FILES_TEST: []\n",
            "  PROPOSAL_FILES_TRAIN: []\n",
            "  TEST:\n",
            "  - coco_2017_val\n",
            "  TRAIN:\n",
            "  - coco_2017_train\n",
            "GLOBAL:\n",
            "  HACK: 1.0\n",
            "INPUT:\n",
            "  CROP:\n",
            "    ENABLED: false\n",
            "    SIZE:\n",
            "    - 0.9\n",
            "    - 0.9\n",
            "    TYPE: relative_range\n",
            "  FORMAT: BGR\n",
            "  MASK_FORMAT: polygon\n",
            "  MAX_SIZE_TEST: 1333\n",
            "  MAX_SIZE_TRAIN: 1333\n",
            "  MIN_SIZE_TEST: 800\n",
            "  MIN_SIZE_TRAIN:\n",
            "  - 640\n",
            "  - 672\n",
            "  - 704\n",
            "  - 736\n",
            "  - 768\n",
            "  - 800\n",
            "  MIN_SIZE_TRAIN_SAMPLING: choice\n",
            "  RANDOM_FLIP: horizontal\n",
            "META_INFO:\n",
            "  EVAL_AP: true\n",
            "  EVAL_GPU_TIME: true\n",
            "  VIS_ROOT: ''\n",
            "MODEL:\n",
            "  ANCHOR_GENERATOR:\n",
            "    ANGLES:\n",
            "    - - -90\n",
            "      - 0\n",
            "      - 90\n",
            "    ASPECT_RATIOS:\n",
            "    - - 0.5\n",
            "      - 1.0\n",
            "      - 2.0\n",
            "    NAME: AnchorGeneratorWithCenter\n",
            "    OFFSET: 0.0\n",
            "    SIZES:\n",
            "    - - 16\n",
            "      - 20.15873679831797\n",
            "      - 25.39841683149119\n",
            "    - - 32\n",
            "      - 40.31747359663594\n",
            "      - 50.79683366298238\n",
            "    - - 64\n",
            "      - 80.63494719327188\n",
            "      - 101.59366732596476\n",
            "    - - 128\n",
            "      - 161.26989438654377\n",
            "      - 203.18733465192952\n",
            "    - - 256\n",
            "      - 322.53978877308754\n",
            "      - 406.37466930385904\n",
            "    - - 512\n",
            "      - 645.0795775461751\n",
            "      - 812.7493386077181\n",
            "  BACKBONE:\n",
            "    FREEZE_AT: 2\n",
            "    NAME: build_retinanet_resnet_fpn_backbone\n",
            "  CUSTOM:\n",
            "    CLEAR_CUDA_CACHE: false\n",
            "    CLS_WEIGHTS:\n",
            "    - 1.0\n",
            "    - 1.4\n",
            "    - 1.8\n",
            "    - 2.2\n",
            "    - 2.6\n",
            "    - 2.6\n",
            "    FOCAL_LOSS_ALPHAS:\n",
            "    - 0.25\n",
            "    - 0.25\n",
            "    - 0.25\n",
            "    - 0.25\n",
            "    - 0.25\n",
            "    - 0.25\n",
            "    FOCAL_LOSS_GAMMAS:\n",
            "    - 2.0\n",
            "    - 2.0\n",
            "    - 2.0\n",
            "    - 2.0\n",
            "    - 2.0\n",
            "    - 2.0\n",
            "    GIOU_LOSS: false\n",
            "    GRADIENT_CHECKPOINT: false\n",
            "    HEAD_BN: false\n",
            "    REG_WEIGHTS:\n",
            "    - 1.0\n",
            "    - 1.4\n",
            "    - 1.8\n",
            "    - 2.2\n",
            "    - 2.6\n",
            "    - 2.6\n",
            "    SOFT_NMS_METHOD: linear\n",
            "    SOFT_NMS_PRUND: 0.001\n",
            "    SOFT_NMS_SIGMA: 0.5\n",
            "    SOFT_NMS_THRESHOLD: 0.5\n",
            "    USE_LOOP_MATCHER: true\n",
            "    USE_SOFT_NMS: false\n",
            "  DEVICE: cuda\n",
            "  FPN:\n",
            "    FUSE_TYPE: sum\n",
            "    IN_FEATURES:\n",
            "    - res2\n",
            "    - res3\n",
            "    - res4\n",
            "    - res5\n",
            "    NORM: ''\n",
            "    OUT_CHANNELS: 256\n",
            "    TOP_LEVELS: 2\n",
            "  KEYPOINT_ON: false\n",
            "  LOAD_PROPOSALS: false\n",
            "  MASK_ON: false\n",
            "  META_ARCHITECTURE: RetinaNetQueryDet\n",
            "  PANOPTIC_FPN:\n",
            "    COMBINE:\n",
            "      ENABLED: true\n",
            "      INSTANCES_CONFIDENCE_THRESH: 0.5\n",
            "      OVERLAP_THRESH: 0.5\n",
            "      STUFF_AREA_LIMIT: 4096\n",
            "    INSTANCE_LOSS_WEIGHT: 1.0\n",
            "  PIXEL_MEAN:\n",
            "  - 103.53\n",
            "  - 116.28\n",
            "  - 123.675\n",
            "  PIXEL_STD:\n",
            "  - 1.0\n",
            "  - 1.0\n",
            "  - 1.0\n",
            "  PROPOSAL_GENERATOR:\n",
            "    MIN_SIZE: 0\n",
            "    NAME: RPN\n",
            "  QUERY:\n",
            "    CONTEXT: 2\n",
            "    ENCODE_CENTER_DIS_COEFF:\n",
            "    - 1.0\n",
            "    - 1.0\n",
            "    ENCODE_SMALL_OBJ_SCALE:\n",
            "    - - 0\n",
            "      - 32\n",
            "    - - 0\n",
            "      - 64\n",
            "    FEATURES_VALUE_TEST:\n",
            "    - 0\n",
            "    - 1\n",
            "    FEATURES_VALUE_TRAIN:\n",
            "    - 0\n",
            "    - 1\n",
            "    FEATURES_WHOLE_TEST:\n",
            "    - 2\n",
            "    - 3\n",
            "    - 4\n",
            "    - 5\n",
            "    FEATURES_WHOLE_TRAIN:\n",
            "    - 2\n",
            "    - 3\n",
            "    - 4\n",
            "    - 5\n",
            "    QUERY_INFER: false\n",
            "    QUERY_LOSS_GAMMA:\n",
            "    - 1.3\n",
            "    - 1.3\n",
            "    QUERY_LOSS_WEIGHT:\n",
            "    - 10.0\n",
            "    - 10.0\n",
            "    Q_FEATURE_TEST:\n",
            "    - 1\n",
            "    - 2\n",
            "    Q_FEATURE_TRAIN:\n",
            "    - 1\n",
            "    - 2\n",
            "    THRESHOLD: 0.12\n",
            "  RESNETS:\n",
            "    DEFORM_MODULATED: false\n",
            "    DEFORM_NUM_GROUPS: 1\n",
            "    DEFORM_ON_PER_STAGE:\n",
            "    - false\n",
            "    - false\n",
            "    - false\n",
            "    - false\n",
            "    DEPTH: 50\n",
            "    NORM: FrozenBN\n",
            "    NUM_GROUPS: 1\n",
            "    OUT_FEATURES:\n",
            "    - res2\n",
            "    - res3\n",
            "    - res4\n",
            "    - res5\n",
            "    RES2_OUT_CHANNELS: 256\n",
            "    RES5_DILATION: 1\n",
            "    STEM_OUT_CHANNELS: 64\n",
            "    STRIDE_IN_1X1: true\n",
            "    WIDTH_PER_GROUP: 64\n",
            "  RETINANET:\n",
            "    BBOX_REG_LOSS_TYPE: smooth_l1\n",
            "    BBOX_REG_WEIGHTS:\n",
            "    - 1.0\n",
            "    - 1.0\n",
            "    - 1.0\n",
            "    - 1.0\n",
            "    FOCAL_LOSS_ALPHA: 0.25\n",
            "    FOCAL_LOSS_GAMMA: 2.0\n",
            "    IN_FEATURES:\n",
            "    - p2\n",
            "    - p3\n",
            "    - p4\n",
            "    - p5\n",
            "    - p6\n",
            "    - p7\n",
            "    IOU_LABELS:\n",
            "    - 0\n",
            "    - -1\n",
            "    - 1\n",
            "    IOU_THRESHOLDS:\n",
            "    - 0.4\n",
            "    - 0.5\n",
            "    NMS_THRESH_TEST: 0.5\n",
            "    NORM: ''\n",
            "    NUM_CLASSES: 10\n",
            "    NUM_CONVS: 4\n",
            "    PRIOR_PROB: 0.01\n",
            "    SCORE_THRESH_TEST: 0.05\n",
            "    SMOOTH_L1_LOSS_BETA: 0.1\n",
            "    TOPK_CANDIDATES_TEST: 1000\n",
            "  ROI_BOX_CASCADE_HEAD:\n",
            "    BBOX_REG_WEIGHTS:\n",
            "    - - 10.0\n",
            "      - 10.0\n",
            "      - 5.0\n",
            "      - 5.0\n",
            "    - - 20.0\n",
            "      - 20.0\n",
            "      - 10.0\n",
            "      - 10.0\n",
            "    - - 30.0\n",
            "      - 30.0\n",
            "      - 15.0\n",
            "      - 15.0\n",
            "    IOUS:\n",
            "    - 0.5\n",
            "    - 0.6\n",
            "    - 0.7\n",
            "  ROI_BOX_HEAD:\n",
            "    BBOX_REG_LOSS_TYPE: smooth_l1\n",
            "    BBOX_REG_LOSS_WEIGHT: 1.0\n",
            "    BBOX_REG_WEIGHTS:\n",
            "    - 10.0\n",
            "    - 10.0\n",
            "    - 5.0\n",
            "    - 5.0\n",
            "    CLS_AGNOSTIC_BBOX_REG: false\n",
            "    CONV_DIM: 256\n",
            "    FC_DIM: 1024\n",
            "    NAME: ''\n",
            "    NORM: ''\n",
            "    NUM_CONV: 0\n",
            "    NUM_FC: 0\n",
            "    POOLER_RESOLUTION: 14\n",
            "    POOLER_SAMPLING_RATIO: 0\n",
            "    POOLER_TYPE: ROIAlignV2\n",
            "    SMOOTH_L1_BETA: 0.0\n",
            "    TRAIN_ON_PRED_BOXES: false\n",
            "  ROI_HEADS:\n",
            "    BATCH_SIZE_PER_IMAGE: 512\n",
            "    IN_FEATURES:\n",
            "    - res4\n",
            "    IOU_LABELS:\n",
            "    - 0\n",
            "    - 1\n",
            "    IOU_THRESHOLDS:\n",
            "    - 0.5\n",
            "    NAME: Res5ROIHeads\n",
            "    NMS_THRESH_TEST: 0.5\n",
            "    NUM_CLASSES: 80\n",
            "    POSITIVE_FRACTION: 0.25\n",
            "    PROPOSAL_APPEND_GT: true\n",
            "    SCORE_THRESH_TEST: 0.05\n",
            "  ROI_KEYPOINT_HEAD:\n",
            "    CONV_DIMS:\n",
            "    - 512\n",
            "    - 512\n",
            "    - 512\n",
            "    - 512\n",
            "    - 512\n",
            "    - 512\n",
            "    - 512\n",
            "    - 512\n",
            "    LOSS_WEIGHT: 1.0\n",
            "    MIN_KEYPOINTS_PER_IMAGE: 1\n",
            "    NAME: KRCNNConvDeconvUpsampleHead\n",
            "    NORMALIZE_LOSS_BY_VISIBLE_KEYPOINTS: true\n",
            "    NUM_KEYPOINTS: 17\n",
            "    POOLER_RESOLUTION: 14\n",
            "    POOLER_SAMPLING_RATIO: 0\n",
            "    POOLER_TYPE: ROIAlignV2\n",
            "  ROI_MASK_HEAD:\n",
            "    CLS_AGNOSTIC_MASK: false\n",
            "    CONV_DIM: 256\n",
            "    NAME: MaskRCNNConvUpsampleHead\n",
            "    NORM: ''\n",
            "    NUM_CONV: 0\n",
            "    POOLER_RESOLUTION: 14\n",
            "    POOLER_SAMPLING_RATIO: 0\n",
            "    POOLER_TYPE: ROIAlignV2\n",
            "  RPN:\n",
            "    BATCH_SIZE_PER_IMAGE: 256\n",
            "    BBOX_REG_LOSS_TYPE: smooth_l1\n",
            "    BBOX_REG_LOSS_WEIGHT: 1.0\n",
            "    BBOX_REG_WEIGHTS:\n",
            "    - 1.0\n",
            "    - 1.0\n",
            "    - 1.0\n",
            "    - 1.0\n",
            "    BOUNDARY_THRESH: -1\n",
            "    HEAD_NAME: StandardRPNHead\n",
            "    IN_FEATURES:\n",
            "    - res4\n",
            "    IOU_LABELS:\n",
            "    - 0\n",
            "    - -1\n",
            "    - 1\n",
            "    IOU_THRESHOLDS:\n",
            "    - 0.3\n",
            "    - 0.7\n",
            "    LOSS_WEIGHT: 1.0\n",
            "    NMS_THRESH: 0.7\n",
            "    POSITIVE_FRACTION: 0.5\n",
            "    POST_NMS_TOPK_TEST: 1000\n",
            "    POST_NMS_TOPK_TRAIN: 2000\n",
            "    PRE_NMS_TOPK_TEST: 6000\n",
            "    PRE_NMS_TOPK_TRAIN: 12000\n",
            "    SMOOTH_L1_BETA: 0.0\n",
            "  SEM_SEG_HEAD:\n",
            "    COMMON_STRIDE: 4\n",
            "    CONVS_DIM: 128\n",
            "    IGNORE_VALUE: 255\n",
            "    IN_FEATURES:\n",
            "    - p2\n",
            "    - p3\n",
            "    - p4\n",
            "    - p5\n",
            "    LOSS_WEIGHT: 1.0\n",
            "    NAME: SemSegFPNHead\n",
            "    NORM: GN\n",
            "    NUM_CLASSES: 54\n",
            "  WEIGHTS: /content/gdrive/MyDrive/QueryDet-PyTorch/work_dirs/visdrone_querydet/model_0003999.pth\n",
            "OUTPUT_DIR: work_dirs/visdrone_querydet\n",
            "SEED: -1\n",
            "SOLVER:\n",
            "  AMP:\n",
            "    ENABLED: true\n",
            "  BASE_LR: 0.006\n",
            "  BIAS_LR_FACTOR: 1.0\n",
            "  CHECKPOINT_PERIOD: 2000\n",
            "  CLIP_GRADIENTS:\n",
            "    CLIP_TYPE: value\n",
            "    CLIP_VALUE: 35.0\n",
            "    ENABLED: true\n",
            "    NORM_TYPE: 2.0\n",
            "  GAMMA: 0.1\n",
            "  IMS_PER_BATCH: 3\n",
            "  LR_SCHEDULER_NAME: WarmupMultiStepLR\n",
            "  MAX_ITER: 60000\n",
            "  MOMENTUM: 0.9\n",
            "  NESTEROV: false\n",
            "  REFERENCE_WORLD_SIZE: 0\n",
            "  STEPS:\n",
            "  - 15000\n",
            "  - 20000\n",
            "  WARMUP_FACTOR: 0.001\n",
            "  WARMUP_ITERS: 1000\n",
            "  WARMUP_METHOD: linear\n",
            "  WEIGHT_DECAY: 0.0001\n",
            "  WEIGHT_DECAY_BIAS: 0.0001\n",
            "  WEIGHT_DECAY_NORM: 0.0\n",
            "TEST:\n",
            "  AUG:\n",
            "    ENABLED: false\n",
            "    FLIP: true\n",
            "    MAX_SIZE: 4000\n",
            "    MIN_SIZES:\n",
            "    - 400\n",
            "    - 500\n",
            "    - 600\n",
            "    - 700\n",
            "    - 800\n",
            "    - 900\n",
            "    - 1000\n",
            "    - 1100\n",
            "    - 1200\n",
            "  DETECTIONS_PER_IMAGE: 500\n",
            "  EVAL_PERIOD: 0\n",
            "  EXPECTED_RESULTS: []\n",
            "  KEYPOINT_OKS_SIGMAS: []\n",
            "  PRECISE_BN:\n",
            "    ENABLED: false\n",
            "    NUM_ITER: 200\n",
            "VERSION: 2\n",
            "VISDRONE:\n",
            "  MAX_LENGTH: 1999\n",
            "  SHORT_LENGTH:\n",
            "  - 1200\n",
            "  TEST_IMG_ROOT: data/visdrone/coco_format/val_images\n",
            "  TEST_JSON: data/visdrone/coco_format/annotations/val_label.json\n",
            "  TEST_LENGTH: 3999\n",
            "  TRAIN_JSON: data/visdrone/coco_format/annotations/train_label.json\n",
            "  TRING_IMG_ROOT: data//visdrone/coco_format/train_images\n",
            "VIS_PERIOD: 0\n",
            "\n",
            "\u001b[32m[04/14 14:19:12 detectron2]: \u001b[0mRunning with full config:\n",
            "CUDNN_BENCHMARK: False\n",
            "DATALOADER:\n",
            "  ASPECT_RATIO_GROUPING: True\n",
            "  FILTER_EMPTY_ANNOTATIONS: True\n",
            "  NUM_WORKERS: 4\n",
            "  REPEAT_THRESHOLD: 0.0\n",
            "  SAMPLER_TRAIN: TrainingSampler\n",
            "DATASETS:\n",
            "  PRECOMPUTED_PROPOSAL_TOPK_TEST: 1000\n",
            "  PRECOMPUTED_PROPOSAL_TOPK_TRAIN: 2000\n",
            "  PROPOSAL_FILES_TEST: ()\n",
            "  PROPOSAL_FILES_TRAIN: ()\n",
            "  TEST: ('coco_2017_val',)\n",
            "  TRAIN: ('coco_2017_train',)\n",
            "GLOBAL:\n",
            "  HACK: 1.0\n",
            "INPUT:\n",
            "  CROP:\n",
            "    ENABLED: False\n",
            "    SIZE: [0.9, 0.9]\n",
            "    TYPE: relative_range\n",
            "  FORMAT: BGR\n",
            "  MASK_FORMAT: polygon\n",
            "  MAX_SIZE_TEST: 1333\n",
            "  MAX_SIZE_TRAIN: 1333\n",
            "  MIN_SIZE_TEST: 800\n",
            "  MIN_SIZE_TRAIN: (640, 672, 704, 736, 768, 800)\n",
            "  MIN_SIZE_TRAIN_SAMPLING: choice\n",
            "  RANDOM_FLIP: horizontal\n",
            "META_INFO:\n",
            "  EVAL_AP: True\n",
            "  EVAL_GPU_TIME: True\n",
            "  VIS_ROOT: \n",
            "MODEL:\n",
            "  ANCHOR_GENERATOR:\n",
            "    ANGLES: [[-90, 0, 90]]\n",
            "    ASPECT_RATIOS: [[0.5, 1.0, 2.0]]\n",
            "    NAME: AnchorGeneratorWithCenter\n",
            "    OFFSET: 0.0\n",
            "    SIZES: [[16, 20.15873679831797, 25.39841683149119], [32, 40.31747359663594, 50.79683366298238], [64, 80.63494719327188, 101.59366732596476], [128, 161.26989438654377, 203.18733465192952], [256, 322.53978877308754, 406.37466930385904], [512, 645.0795775461751, 812.7493386077181]]\n",
            "  BACKBONE:\n",
            "    FREEZE_AT: 2\n",
            "    NAME: build_retinanet_resnet_fpn_backbone\n",
            "  CUSTOM:\n",
            "    CLEAR_CUDA_CACHE: False\n",
            "    CLS_WEIGHTS: [1.0, 1.4, 1.8, 2.2, 2.6, 2.6]\n",
            "    FOCAL_LOSS_ALPHAS: [0.25, 0.25, 0.25, 0.25, 0.25, 0.25]\n",
            "    FOCAL_LOSS_GAMMAS: [2.0, 2.0, 2.0, 2.0, 2.0, 2.0]\n",
            "    GIOU_LOSS: False\n",
            "    GRADIENT_CHECKPOINT: False\n",
            "    HEAD_BN: False\n",
            "    REG_WEIGHTS: [1.0, 1.4, 1.8, 2.2, 2.6, 2.6]\n",
            "    SOFT_NMS_METHOD: linear\n",
            "    SOFT_NMS_PRUND: 0.001\n",
            "    SOFT_NMS_SIGMA: 0.5\n",
            "    SOFT_NMS_THRESHOLD: 0.5\n",
            "    USE_LOOP_MATCHER: True\n",
            "    USE_SOFT_NMS: False\n",
            "  DEVICE: cuda\n",
            "  FPN:\n",
            "    FUSE_TYPE: sum\n",
            "    IN_FEATURES: ['res2', 'res3', 'res4', 'res5']\n",
            "    NORM: \n",
            "    OUT_CHANNELS: 256\n",
            "    TOP_LEVELS: 2\n",
            "  KEYPOINT_ON: False\n",
            "  LOAD_PROPOSALS: False\n",
            "  MASK_ON: False\n",
            "  META_ARCHITECTURE: RetinaNetQueryDet\n",
            "  PANOPTIC_FPN:\n",
            "    COMBINE:\n",
            "      ENABLED: True\n",
            "      INSTANCES_CONFIDENCE_THRESH: 0.5\n",
            "      OVERLAP_THRESH: 0.5\n",
            "      STUFF_AREA_LIMIT: 4096\n",
            "    INSTANCE_LOSS_WEIGHT: 1.0\n",
            "  PIXEL_MEAN: [103.53, 116.28, 123.675]\n",
            "  PIXEL_STD: [1.0, 1.0, 1.0]\n",
            "  PROPOSAL_GENERATOR:\n",
            "    MIN_SIZE: 0\n",
            "    NAME: RPN\n",
            "  QUERY:\n",
            "    CONTEXT: 2\n",
            "    ENCODE_CENTER_DIS_COEFF: [1.0, 1.0]\n",
            "    ENCODE_SMALL_OBJ_SCALE: [[0, 32], [0, 64]]\n",
            "    FEATURES_VALUE_TEST: [0, 1]\n",
            "    FEATURES_VALUE_TRAIN: [0, 1]\n",
            "    FEATURES_WHOLE_TEST: [2, 3, 4, 5]\n",
            "    FEATURES_WHOLE_TRAIN: [2, 3, 4, 5]\n",
            "    QUERY_INFER: False\n",
            "    QUERY_LOSS_GAMMA: [1.3, 1.3]\n",
            "    QUERY_LOSS_WEIGHT: [10.0, 10.0]\n",
            "    Q_FEATURE_TEST: [1, 2]\n",
            "    Q_FEATURE_TRAIN: [1, 2]\n",
            "    THRESHOLD: 0.12\n",
            "  RESNETS:\n",
            "    DEFORM_MODULATED: False\n",
            "    DEFORM_NUM_GROUPS: 1\n",
            "    DEFORM_ON_PER_STAGE: [False, False, False, False]\n",
            "    DEPTH: 50\n",
            "    NORM: FrozenBN\n",
            "    NUM_GROUPS: 1\n",
            "    OUT_FEATURES: ['res2', 'res3', 'res4', 'res5']\n",
            "    RES2_OUT_CHANNELS: 256\n",
            "    RES5_DILATION: 1\n",
            "    STEM_OUT_CHANNELS: 64\n",
            "    STRIDE_IN_1X1: True\n",
            "    WIDTH_PER_GROUP: 64\n",
            "  RETINANET:\n",
            "    BBOX_REG_LOSS_TYPE: smooth_l1\n",
            "    BBOX_REG_WEIGHTS: (1.0, 1.0, 1.0, 1.0)\n",
            "    FOCAL_LOSS_ALPHA: 0.25\n",
            "    FOCAL_LOSS_GAMMA: 2.0\n",
            "    IN_FEATURES: ['p2', 'p3', 'p4', 'p5', 'p6', 'p7']\n",
            "    IOU_LABELS: [0, -1, 1]\n",
            "    IOU_THRESHOLDS: [0.4, 0.5]\n",
            "    NMS_THRESH_TEST: 0.5\n",
            "    NORM: \n",
            "    NUM_CLASSES: 10\n",
            "    NUM_CONVS: 4\n",
            "    PRIOR_PROB: 0.01\n",
            "    SCORE_THRESH_TEST: 0.05\n",
            "    SMOOTH_L1_LOSS_BETA: 0.1\n",
            "    TOPK_CANDIDATES_TEST: 1000\n",
            "  ROI_BOX_CASCADE_HEAD:\n",
            "    BBOX_REG_WEIGHTS: ([10.0, 10.0, 5.0, 5.0], [20.0, 20.0, 10.0, 10.0], [30.0, 30.0, 15.0, 15.0])\n",
            "    IOUS: (0.5, 0.6, 0.7)\n",
            "  ROI_BOX_HEAD:\n",
            "    BBOX_REG_LOSS_TYPE: smooth_l1\n",
            "    BBOX_REG_LOSS_WEIGHT: 1.0\n",
            "    BBOX_REG_WEIGHTS: (10.0, 10.0, 5.0, 5.0)\n",
            "    CLS_AGNOSTIC_BBOX_REG: False\n",
            "    CONV_DIM: 256\n",
            "    FC_DIM: 1024\n",
            "    NAME: \n",
            "    NORM: \n",
            "    NUM_CONV: 0\n",
            "    NUM_FC: 0\n",
            "    POOLER_RESOLUTION: 14\n",
            "    POOLER_SAMPLING_RATIO: 0\n",
            "    POOLER_TYPE: ROIAlignV2\n",
            "    SMOOTH_L1_BETA: 0.0\n",
            "    TRAIN_ON_PRED_BOXES: False\n",
            "  ROI_HEADS:\n",
            "    BATCH_SIZE_PER_IMAGE: 512\n",
            "    IN_FEATURES: ['res4']\n",
            "    IOU_LABELS: [0, 1]\n",
            "    IOU_THRESHOLDS: [0.5]\n",
            "    NAME: Res5ROIHeads\n",
            "    NMS_THRESH_TEST: 0.5\n",
            "    NUM_CLASSES: 80\n",
            "    POSITIVE_FRACTION: 0.25\n",
            "    PROPOSAL_APPEND_GT: True\n",
            "    SCORE_THRESH_TEST: 0.05\n",
            "  ROI_KEYPOINT_HEAD:\n",
            "    CONV_DIMS: (512, 512, 512, 512, 512, 512, 512, 512)\n",
            "    LOSS_WEIGHT: 1.0\n",
            "    MIN_KEYPOINTS_PER_IMAGE: 1\n",
            "    NAME: KRCNNConvDeconvUpsampleHead\n",
            "    NORMALIZE_LOSS_BY_VISIBLE_KEYPOINTS: True\n",
            "    NUM_KEYPOINTS: 17\n",
            "    POOLER_RESOLUTION: 14\n",
            "    POOLER_SAMPLING_RATIO: 0\n",
            "    POOLER_TYPE: ROIAlignV2\n",
            "  ROI_MASK_HEAD:\n",
            "    CLS_AGNOSTIC_MASK: False\n",
            "    CONV_DIM: 256\n",
            "    NAME: MaskRCNNConvUpsampleHead\n",
            "    NORM: \n",
            "    NUM_CONV: 0\n",
            "    POOLER_RESOLUTION: 14\n",
            "    POOLER_SAMPLING_RATIO: 0\n",
            "    POOLER_TYPE: ROIAlignV2\n",
            "  RPN:\n",
            "    BATCH_SIZE_PER_IMAGE: 256\n",
            "    BBOX_REG_LOSS_TYPE: smooth_l1\n",
            "    BBOX_REG_LOSS_WEIGHT: 1.0\n",
            "    BBOX_REG_WEIGHTS: (1.0, 1.0, 1.0, 1.0)\n",
            "    BOUNDARY_THRESH: -1\n",
            "    HEAD_NAME: StandardRPNHead\n",
            "    IN_FEATURES: ['res4']\n",
            "    IOU_LABELS: [0, -1, 1]\n",
            "    IOU_THRESHOLDS: [0.3, 0.7]\n",
            "    LOSS_WEIGHT: 1.0\n",
            "    NMS_THRESH: 0.7\n",
            "    POSITIVE_FRACTION: 0.5\n",
            "    POST_NMS_TOPK_TEST: 1000\n",
            "    POST_NMS_TOPK_TRAIN: 2000\n",
            "    PRE_NMS_TOPK_TEST: 6000\n",
            "    PRE_NMS_TOPK_TRAIN: 12000\n",
            "    SMOOTH_L1_BETA: 0.0\n",
            "  SEM_SEG_HEAD:\n",
            "    COMMON_STRIDE: 4\n",
            "    CONVS_DIM: 128\n",
            "    IGNORE_VALUE: 255\n",
            "    IN_FEATURES: ['p2', 'p3', 'p4', 'p5']\n",
            "    LOSS_WEIGHT: 1.0\n",
            "    NAME: SemSegFPNHead\n",
            "    NORM: GN\n",
            "    NUM_CLASSES: 54\n",
            "  WEIGHTS: /content/gdrive/MyDrive/QueryDet-PyTorch/work_dirs/visdrone_querydet/model_0003999.pth\n",
            "OUTPUT_DIR: work_dirs/visdrone_querydet\n",
            "SEED: -1\n",
            "SOLVER:\n",
            "  AMP:\n",
            "    ENABLED: True\n",
            "  BASE_LR: 0.006\n",
            "  BIAS_LR_FACTOR: 1.0\n",
            "  CHECKPOINT_PERIOD: 2000\n",
            "  CLIP_GRADIENTS:\n",
            "    CLIP_TYPE: value\n",
            "    CLIP_VALUE: 35.0\n",
            "    ENABLED: True\n",
            "    NORM_TYPE: 2.0\n",
            "  GAMMA: 0.1\n",
            "  IMS_PER_BATCH: 3\n",
            "  LR_SCHEDULER_NAME: WarmupMultiStepLR\n",
            "  MAX_ITER: 60000\n",
            "  MOMENTUM: 0.9\n",
            "  NESTEROV: False\n",
            "  REFERENCE_WORLD_SIZE: 0\n",
            "  STEPS: (15000, 20000)\n",
            "  WARMUP_FACTOR: 0.001\n",
            "  WARMUP_ITERS: 1000\n",
            "  WARMUP_METHOD: linear\n",
            "  WEIGHT_DECAY: 0.0001\n",
            "  WEIGHT_DECAY_BIAS: 0.0001\n",
            "  WEIGHT_DECAY_NORM: 0.0\n",
            "TEST:\n",
            "  AUG:\n",
            "    ENABLED: False\n",
            "    FLIP: True\n",
            "    MAX_SIZE: 4000\n",
            "    MIN_SIZES: (400, 500, 600, 700, 800, 900, 1000, 1100, 1200)\n",
            "  DETECTIONS_PER_IMAGE: 500\n",
            "  EVAL_PERIOD: 0\n",
            "  EXPECTED_RESULTS: []\n",
            "  KEYPOINT_OKS_SIGMAS: []\n",
            "  PRECISE_BN:\n",
            "    ENABLED: False\n",
            "    NUM_ITER: 200\n",
            "VERSION: 2\n",
            "VISDRONE:\n",
            "  MAX_LENGTH: 1999\n",
            "  SHORT_LENGTH: [1200]\n",
            "  TEST_IMG_ROOT: data/visdrone/coco_format/val_images\n",
            "  TEST_JSON: data/visdrone/coco_format/annotations/val_label.json\n",
            "  TEST_LENGTH: 3999\n",
            "  TRAIN_JSON: data/visdrone/coco_format/annotations/train_label.json\n",
            "  TRING_IMG_ROOT: data//visdrone/coco_format/train_images\n",
            "VIS_PERIOD: 0\n",
            "\u001b[32m[04/14 14:19:13 detectron2]: \u001b[0mFull config saved to work_dirs/visdrone_querydet/config.yaml\n",
            "\u001b[32m[04/14 14:19:13 d2.utils.env]: \u001b[0mUsing a generated random seed 13593855\n",
            "\u001b[32m[04/14 14:19:17 d2.engine.defaults]: \u001b[0mModel:\n",
            "RetinaNetQueryDet(\n",
            "  (backbone): FPN(\n",
            "    (fpn_lateral2): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1))\n",
            "    (fpn_output2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (fpn_lateral3): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1))\n",
            "    (fpn_output3): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (fpn_lateral4): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1))\n",
            "    (fpn_output4): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (fpn_lateral5): Conv2d(2048, 256, kernel_size=(1, 1), stride=(1, 1))\n",
            "    (fpn_output5): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (top_block): LastLevelP6P7(\n",
            "      (p6): Conv2d(2048, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
            "      (p7): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
            "    )\n",
            "    (bottom_up): ResNet(\n",
            "      (stem): BasicStem(\n",
            "        (conv1): Conv2d(\n",
            "          3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)\n",
            "        )\n",
            "      )\n",
            "      (res2): Sequential(\n",
            "        (0): BottleneckBlock(\n",
            "          (shortcut): Conv2d(\n",
            "            64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
            "          )\n",
            "          (conv1): Conv2d(\n",
            "            64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)\n",
            "          )\n",
            "          (conv2): Conv2d(\n",
            "            64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)\n",
            "          )\n",
            "          (conv3): Conv2d(\n",
            "            64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
            "          )\n",
            "        )\n",
            "        (1): BottleneckBlock(\n",
            "          (conv1): Conv2d(\n",
            "            256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)\n",
            "          )\n",
            "          (conv2): Conv2d(\n",
            "            64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)\n",
            "          )\n",
            "          (conv3): Conv2d(\n",
            "            64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
            "          )\n",
            "        )\n",
            "        (2): BottleneckBlock(\n",
            "          (conv1): Conv2d(\n",
            "            256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)\n",
            "          )\n",
            "          (conv2): Conv2d(\n",
            "            64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)\n",
            "          )\n",
            "          (conv3): Conv2d(\n",
            "            64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
            "          )\n",
            "        )\n",
            "      )\n",
            "      (res3): Sequential(\n",
            "        (0): BottleneckBlock(\n",
            "          (shortcut): Conv2d(\n",
            "            256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
            "          )\n",
            "          (conv1): Conv2d(\n",
            "            256, 128, kernel_size=(1, 1), stride=(2, 2), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)\n",
            "          )\n",
            "          (conv2): Conv2d(\n",
            "            128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)\n",
            "          )\n",
            "          (conv3): Conv2d(\n",
            "            128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
            "          )\n",
            "        )\n",
            "        (1): BottleneckBlock(\n",
            "          (conv1): Conv2d(\n",
            "            512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)\n",
            "          )\n",
            "          (conv2): Conv2d(\n",
            "            128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)\n",
            "          )\n",
            "          (conv3): Conv2d(\n",
            "            128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
            "          )\n",
            "        )\n",
            "        (2): BottleneckBlock(\n",
            "          (conv1): Conv2d(\n",
            "            512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)\n",
            "          )\n",
            "          (conv2): Conv2d(\n",
            "            128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)\n",
            "          )\n",
            "          (conv3): Conv2d(\n",
            "            128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
            "          )\n",
            "        )\n",
            "        (3): BottleneckBlock(\n",
            "          (conv1): Conv2d(\n",
            "            512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)\n",
            "          )\n",
            "          (conv2): Conv2d(\n",
            "            128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)\n",
            "          )\n",
            "          (conv3): Conv2d(\n",
            "            128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
            "          )\n",
            "        )\n",
            "      )\n",
            "      (res4): Sequential(\n",
            "        (0): BottleneckBlock(\n",
            "          (shortcut): Conv2d(\n",
            "            512, 1024, kernel_size=(1, 1), stride=(2, 2), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n",
            "          )\n",
            "          (conv1): Conv2d(\n",
            "            512, 256, kernel_size=(1, 1), stride=(2, 2), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
            "          )\n",
            "          (conv2): Conv2d(\n",
            "            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
            "          )\n",
            "          (conv3): Conv2d(\n",
            "            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n",
            "          )\n",
            "        )\n",
            "        (1): BottleneckBlock(\n",
            "          (conv1): Conv2d(\n",
            "            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
            "          )\n",
            "          (conv2): Conv2d(\n",
            "            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
            "          )\n",
            "          (conv3): Conv2d(\n",
            "            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n",
            "          )\n",
            "        )\n",
            "        (2): BottleneckBlock(\n",
            "          (conv1): Conv2d(\n",
            "            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
            "          )\n",
            "          (conv2): Conv2d(\n",
            "            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
            "          )\n",
            "          (conv3): Conv2d(\n",
            "            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n",
            "          )\n",
            "        )\n",
            "        (3): BottleneckBlock(\n",
            "          (conv1): Conv2d(\n",
            "            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
            "          )\n",
            "          (conv2): Conv2d(\n",
            "            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
            "          )\n",
            "          (conv3): Conv2d(\n",
            "            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n",
            "          )\n",
            "        )\n",
            "        (4): BottleneckBlock(\n",
            "          (conv1): Conv2d(\n",
            "            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
            "          )\n",
            "          (conv2): Conv2d(\n",
            "            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
            "          )\n",
            "          (conv3): Conv2d(\n",
            "            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n",
            "          )\n",
            "        )\n",
            "        (5): BottleneckBlock(\n",
            "          (conv1): Conv2d(\n",
            "            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
            "          )\n",
            "          (conv2): Conv2d(\n",
            "            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
            "          )\n",
            "          (conv3): Conv2d(\n",
            "            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n",
            "          )\n",
            "        )\n",
            "      )\n",
            "      (res5): Sequential(\n",
            "        (0): BottleneckBlock(\n",
            "          (shortcut): Conv2d(\n",
            "            1024, 2048, kernel_size=(1, 1), stride=(2, 2), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=2048, eps=1e-05)\n",
            "          )\n",
            "          (conv1): Conv2d(\n",
            "            1024, 512, kernel_size=(1, 1), stride=(2, 2), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
            "          )\n",
            "          (conv2): Conv2d(\n",
            "            512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
            "          )\n",
            "          (conv3): Conv2d(\n",
            "            512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=2048, eps=1e-05)\n",
            "          )\n",
            "        )\n",
            "        (1): BottleneckBlock(\n",
            "          (conv1): Conv2d(\n",
            "            2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
            "          )\n",
            "          (conv2): Conv2d(\n",
            "            512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
            "          )\n",
            "          (conv3): Conv2d(\n",
            "            512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=2048, eps=1e-05)\n",
            "          )\n",
            "        )\n",
            "        (2): BottleneckBlock(\n",
            "          (conv1): Conv2d(\n",
            "            2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
            "          )\n",
            "          (conv2): Conv2d(\n",
            "            512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
            "          )\n",
            "          (conv3): Conv2d(\n",
            "            512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=2048, eps=1e-05)\n",
            "          )\n",
            "        )\n",
            "      )\n",
            "    )\n",
            "  )\n",
            "  (det_head): RetinaNetHead_3x3(\n",
            "    (cls_layer_0): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (bbox_layer_0): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (cls_layer_1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (bbox_layer_1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (cls_layer_2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (bbox_layer_2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (cls_layer_3): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (bbox_layer_3): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (cls_score): Conv2d(256, 90, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (bbox_pred): Conv2d(256, 36, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "  )\n",
            "  (query_head): Head_3x3(\n",
            "    (layer_0): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (layer_1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (layer_2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (layer_3): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (pred_net): Conv2d(256, 1, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "  )\n",
            "  (anchor_generator): AnchorGeneratorWithCenter(\n",
            "    (cell_anchors): BufferList()\n",
            "  )\n",
            "  (query_anchor_generator): AnchorGeneratorWithCenter(\n",
            "    (cell_anchors): BufferList()\n",
            "  )\n",
            ")\n",
            "\u001b[32m[04/14 14:19:17 fvcore.common.checkpoint]: \u001b[0mLoading checkpoint from /content/gdrive/MyDrive/QueryDet-PyTorch/work_dirs/visdrone_querydet/model_0003999.pth\n",
            "\u001b[32m[04/14 14:19:21 d2.data.common]: \u001b[0mSerializing 25884 elements to byte tensors and concatenating them all ...\n",
            "\u001b[32m[04/14 14:19:21 d2.data.common]: \u001b[0mSerialized dataset takes 15.78 MiB\n",
            "/usr/local/lib/python3.7/site-packages/torch/utils/data/dataloader.py:477: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  cpuset_checked))\n",
            "\u001b[32m[04/14 14:19:24 d2.engine.train_loop]: \u001b[0mStarting training from iteration 0\n",
            "\u001b[32m[04/14 14:20:20 d2.utils.events]: \u001b[0m eta: 1 day, 12:16:00  iter: 19  total_loss: 1.49  loss_cls: 0.8937  loss_box_reg: 0.5337  loss_query: 0.02993  time: 2.3105  data_time: 0.0212  lr: 0.00011989  max_mem: 10548M\n",
            "\u001b[32m[04/14 14:21:04 d2.utils.events]: \u001b[0m eta: 1 day, 12:41:50  iter: 39  total_loss: 0.638  loss_cls: 0.4055  loss_box_reg: 0.2155  loss_query: 0.02376  time: 2.2316  data_time: 0.0069  lr: 0.00023977  max_mem: 10548M\n",
            "\u001b[32m[04/14 14:21:50 d2.utils.events]: \u001b[0m eta: 1 day, 13:06:51  iter: 59  total_loss: 0.7628  loss_cls: 0.4704  loss_box_reg: 0.2502  loss_query: 0.02892  time: 2.2361  data_time: 0.0068  lr: 0.00035965  max_mem: 10548M\n",
            "\u001b[32m[04/14 14:22:36 d2.utils.events]: \u001b[0m eta: 1 day, 13:25:10  iter: 79  total_loss: 0.7168  loss_cls: 0.4488  loss_box_reg: 0.2482  loss_query: 0.02871  time: 2.2366  data_time: 0.0058  lr: 0.00047953  max_mem: 10548M\n",
            "\u001b[32m[04/14 14:23:24 d2.utils.events]: \u001b[0m eta: 1 day, 13:49:41  iter: 99  total_loss: 0.5627  loss_cls: 0.3438  loss_box_reg: 0.191  loss_query: 0.02293  time: 2.2591  data_time: 0.0069  lr: 0.00059941  max_mem: 10548M\n",
            "\u001b[32m[04/14 14:24:11 d2.utils.events]: \u001b[0m eta: 1 day, 13:52:24  iter: 119  total_loss: 0.4399  loss_cls: 0.2781  loss_box_reg: 0.1553  loss_query: 0.0204  time: 2.2672  data_time: 0.0066  lr: 0.00071929  max_mem: 10549M\n",
            "\u001b[32m[04/14 14:25:00 d2.utils.events]: \u001b[0m eta: 1 day, 13:58:19  iter: 139  total_loss: 0.7787  loss_cls: 0.403  loss_box_reg: 0.2854  loss_query: 0.04205  time: 2.2813  data_time: 0.0061  lr: 0.00083917  max_mem: 10549M\n",
            "\u001b[32m[04/14 14:25:46 d2.utils.events]: \u001b[0m eta: 1 day, 14:05:36  iter: 159  total_loss: 0.4113  loss_cls: 0.2457  loss_box_reg: 0.1494  loss_query: 0.02269  time: 2.2766  data_time: 0.0080  lr: 0.00095905  max_mem: 10549M\n",
            "\u001b[32m[04/14 14:26:32 d2.utils.events]: \u001b[0m eta: 1 day, 14:01:52  iter: 179  total_loss: 0.7963  loss_cls: 0.477  loss_box_reg: 0.2667  loss_query: 0.03442  time: 2.2744  data_time: 0.0065  lr: 0.0010789  max_mem: 10549M\n",
            "\u001b[32m[04/14 14:27:19 d2.utils.events]: \u001b[0m eta: 1 day, 14:04:10  iter: 199  total_loss: 0.5596  loss_cls: 0.3322  loss_box_reg: 0.2225  loss_query: 0.03618  time: 2.2740  data_time: 0.0079  lr: 0.0011988  max_mem: 10549M\n",
            "\u001b[32m[04/14 14:28:06 d2.utils.events]: \u001b[0m eta: 1 day, 14:05:38  iter: 219  total_loss: 0.6499  loss_cls: 0.4193  loss_box_reg: 0.2101  loss_query: 0.04128  time: 2.2756  data_time: 0.0068  lr: 0.0013187  max_mem: 10549M\n",
            "\u001b[32m[04/14 14:28:53 d2.utils.events]: \u001b[0m eta: 1 day, 14:07:51  iter: 239  total_loss: 0.4697  loss_cls: 0.2976  loss_box_reg: 0.1569  loss_query: 0.02177  time: 2.2773  data_time: 0.0065  lr: 0.0014386  max_mem: 10549M\n",
            "\u001b[32m[04/14 14:29:37 d2.utils.events]: \u001b[0m eta: 1 day, 14:07:05  iter: 259  total_loss: 0.5142  loss_cls: 0.3192  loss_box_reg: 0.1509  loss_query: 0.01432  time: 2.2683  data_time: 0.0070  lr: 0.0015584  max_mem: 10549M\n",
            "\u001b[32m[04/14 14:30:24 d2.utils.events]: \u001b[0m eta: 1 day, 14:06:19  iter: 279  total_loss: 0.7197  loss_cls: 0.4038  loss_box_reg: 0.2837  loss_query: 0.03107  time: 2.2674  data_time: 0.0068  lr: 0.0016783  max_mem: 10549M\n",
            "\u001b[32m[04/14 14:31:11 d2.utils.events]: \u001b[0m eta: 1 day, 14:05:13  iter: 299  total_loss: 0.6576  loss_cls: 0.4361  loss_box_reg: 0.2013  loss_query: 0.02704  time: 2.2681  data_time: 0.0072  lr: 0.0017982  max_mem: 10549M\n",
            "\u001b[32m[04/14 14:31:57 d2.utils.events]: \u001b[0m eta: 1 day, 14:03:39  iter: 319  total_loss: 0.6206  loss_cls: 0.3704  loss_box_reg: 0.2025  loss_query: 0.02655  time: 2.2675  data_time: 0.0090  lr: 0.0019181  max_mem: 10549M\n",
            "\u001b[32m[04/14 14:32:44 d2.utils.events]: \u001b[0m eta: 1 day, 14:04:15  iter: 339  total_loss: 0.7687  loss_cls: 0.4802  loss_box_reg: 0.2536  loss_query: 0.02656  time: 2.2675  data_time: 0.0067  lr: 0.002038  max_mem: 10549M\n",
            "\u001b[32m[04/14 14:33:31 d2.utils.events]: \u001b[0m eta: 1 day, 14:10:24  iter: 359  total_loss: 0.6812  loss_cls: 0.4022  loss_box_reg: 0.2276  loss_query: 0.03696  time: 2.2710  data_time: 0.0070  lr: 0.0021578  max_mem: 10549M\n",
            "\u001b[32m[04/14 14:34:17 d2.utils.events]: \u001b[0m eta: 1 day, 14:04:24  iter: 379  total_loss: 0.6096  loss_cls: 0.3851  loss_box_reg: 0.205  loss_query: 0.02904  time: 2.2684  data_time: 0.0092  lr: 0.0022777  max_mem: 10549M\n",
            "\u001b[32m[04/14 14:35:03 d2.utils.events]: \u001b[0m eta: 1 day, 14:08:15  iter: 399  total_loss: 0.654  loss_cls: 0.3839  loss_box_reg: 0.2333  loss_query: 0.03036  time: 2.2675  data_time: 0.0083  lr: 0.0023976  max_mem: 10549M\n",
            "\u001b[32m[04/14 14:35:51 d2.utils.events]: \u001b[0m eta: 1 day, 14:08:55  iter: 419  total_loss: 0.7934  loss_cls: 0.4525  loss_box_reg: 0.2901  loss_query: 0.04289  time: 2.2688  data_time: 0.0067  lr: 0.0025175  max_mem: 10549M\n",
            "\u001b[32m[04/14 14:36:38 d2.utils.events]: \u001b[0m eta: 1 day, 14:04:33  iter: 439  total_loss: 0.3672  loss_cls: 0.2314  loss_box_reg: 0.1423  loss_query: 0.0293  time: 2.2687  data_time: 0.0061  lr: 0.0026374  max_mem: 10549M\n",
            "\u001b[32m[04/14 14:37:26 d2.utils.events]: \u001b[0m eta: 1 day, 14:07:35  iter: 459  total_loss: 0.6414  loss_cls: 0.3561  loss_box_reg: 0.1976  loss_query: 0.02913  time: 2.2719  data_time: 0.0066  lr: 0.0027572  max_mem: 10549M\n",
            "\u001b[32m[04/14 14:38:13 d2.utils.events]: \u001b[0m eta: 1 day, 14:06:36  iter: 479  total_loss: 0.5356  loss_cls: 0.3338  loss_box_reg: 0.1912  loss_query: 0.01981  time: 2.2721  data_time: 0.0073  lr: 0.0028771  max_mem: 10549M\n",
            "\u001b[32m[04/14 14:38:59 d2.utils.events]: \u001b[0m eta: 1 day, 14:06:02  iter: 499  total_loss: 0.9065  loss_cls: 0.5456  loss_box_reg: 0.3071  loss_query: 0.03191  time: 2.2717  data_time: 0.0058  lr: 0.002997  max_mem: 10549M\n",
            "\u001b[32m[04/14 14:39:48 d2.utils.events]: \u001b[0m eta: 1 day, 14:05:16  iter: 519  total_loss: 0.571  loss_cls: 0.359  loss_box_reg: 0.1989  loss_query: 0.02933  time: 2.2741  data_time: 0.0061  lr: 0.0031169  max_mem: 10549M\n",
            "\u001b[32m[04/14 14:40:33 d2.utils.events]: \u001b[0m eta: 1 day, 14:04:18  iter: 539  total_loss: 0.6662  loss_cls: 0.3846  loss_box_reg: 0.2174  loss_query: 0.03527  time: 2.2719  data_time: 0.0062  lr: 0.0032368  max_mem: 10549M\n",
            "\u001b[32m[04/14 14:41:20 d2.utils.events]: \u001b[0m eta: 1 day, 14:05:58  iter: 559  total_loss: 0.7792  loss_cls: 0.4403  loss_box_reg: 0.2622  loss_query: 0.05225  time: 2.2731  data_time: 0.0060  lr: 0.0033566  max_mem: 10549M\n",
            "\u001b[32m[04/14 14:42:07 d2.utils.events]: \u001b[0m eta: 1 day, 14:05:39  iter: 579  total_loss: 0.745  loss_cls: 0.4172  loss_box_reg: 0.277  loss_query: 0.04493  time: 2.2727  data_time: 0.0090  lr: 0.0034765  max_mem: 10549M\n",
            "\u001b[32m[04/14 14:42:54 d2.utils.events]: \u001b[0m eta: 1 day, 14:05:33  iter: 599  total_loss: 0.6936  loss_cls: 0.3695  loss_box_reg: 0.2576  loss_query: 0.04626  time: 2.2733  data_time: 0.0075  lr: 0.0035964  max_mem: 10549M\n",
            "\u001b[32m[04/14 14:43:42 d2.utils.events]: \u001b[0m eta: 1 day, 14:05:55  iter: 619  total_loss: 0.6345  loss_cls: 0.3703  loss_box_reg: 0.2478  loss_query: 0.03533  time: 2.2752  data_time: 0.0076  lr: 0.0037163  max_mem: 10549M\n",
            "\u001b[32m[04/14 14:44:27 d2.utils.events]: \u001b[0m eta: 1 day, 14:04:32  iter: 639  total_loss: 0.8218  loss_cls: 0.4546  loss_box_reg: 0.2725  loss_query: 0.0399  time: 2.2727  data_time: 0.0076  lr: 0.0038362  max_mem: 10549M\n",
            "\u001b[32m[04/14 14:45:14 d2.utils.events]: \u001b[0m eta: 1 day, 14:04:09  iter: 659  total_loss: 0.6736  loss_cls: 0.4184  loss_box_reg: 0.2191  loss_query: 0.04626  time: 2.2733  data_time: 0.0073  lr: 0.003956  max_mem: 10549M\n",
            "\u001b[32m[04/14 14:46:00 d2.utils.events]: \u001b[0m eta: 1 day, 14:03:23  iter: 679  total_loss: 0.8256  loss_cls: 0.5135  loss_box_reg: 0.2613  loss_query: 0.04185  time: 2.2730  data_time: 0.0066  lr: 0.0040759  max_mem: 10549M\n",
            "\u001b[32m[04/14 14:46:46 d2.utils.events]: \u001b[0m eta: 1 day, 14:02:37  iter: 699  total_loss: 0.7102  loss_cls: 0.4368  loss_box_reg: 0.265  loss_query: 0.02904  time: 2.2725  data_time: 0.0070  lr: 0.0041958  max_mem: 10549M\n",
            "\u001b[32m[04/14 14:47:33 d2.utils.events]: \u001b[0m eta: 1 day, 14:01:27  iter: 719  total_loss: 0.6526  loss_cls: 0.395  loss_box_reg: 0.2074  loss_query: 0.02007  time: 2.2718  data_time: 0.0070  lr: 0.0043157  max_mem: 10549M\n",
            "\u001b[32m[04/14 14:48:19 d2.utils.events]: \u001b[0m eta: 1 day, 14:00:41  iter: 739  total_loss: 0.842  loss_cls: 0.5067  loss_box_reg: 0.2437  loss_query: 0.02131  time: 2.2716  data_time: 0.0064  lr: 0.0044356  max_mem: 10549M\n",
            "\u001b[32m[04/14 14:49:05 d2.utils.events]: \u001b[0m eta: 1 day, 13:59:12  iter: 759  total_loss: 0.6328  loss_cls: 0.3781  loss_box_reg: 0.2162  loss_query: 0.0255  time: 2.2707  data_time: 0.0062  lr: 0.0045554  max_mem: 10549M\n",
            "\u001b[32m[04/14 14:49:52 d2.utils.events]: \u001b[0m eta: 1 day, 13:58:37  iter: 779  total_loss: 0.8034  loss_cls: 0.4837  loss_box_reg: 0.2841  loss_query: 0.03413  time: 2.2704  data_time: 0.0086  lr: 0.0046753  max_mem: 10549M\n",
            "\u001b[32m[04/14 14:50:39 d2.utils.events]: \u001b[0m eta: 1 day, 13:58:22  iter: 799  total_loss: 0.7783  loss_cls: 0.5156  loss_box_reg: 0.2399  loss_query: 0.02886  time: 2.2719  data_time: 0.0076  lr: 0.0047952  max_mem: 10549M\n",
            "\u001b[32m[04/14 14:51:25 d2.utils.events]: \u001b[0m eta: 1 day, 13:57:05  iter: 819  total_loss: 0.8003  loss_cls: 0.5043  loss_box_reg: 0.2804  loss_query: 0.03369  time: 2.2709  data_time: 0.0064  lr: 0.0049151  max_mem: 10549M\n",
            "\u001b[32m[04/14 14:52:12 d2.utils.events]: \u001b[0m eta: 1 day, 13:56:04  iter: 839  total_loss: 0.6645  loss_cls: 0.4053  loss_box_reg: 0.2621  loss_query: 0.02813  time: 2.2710  data_time: 0.0063  lr: 0.005035  max_mem: 10549M\n",
            "\u001b[32m[04/14 14:52:57 d2.utils.events]: \u001b[0m eta: 1 day, 13:54:29  iter: 859  total_loss: 0.5179  loss_cls: 0.3051  loss_box_reg: 0.1877  loss_query: 0.02961  time: 2.2696  data_time: 0.0069  lr: 0.0051548  max_mem: 10549M\n",
            "\u001b[32m[04/14 14:53:44 d2.utils.events]: \u001b[0m eta: 1 day, 13:53:46  iter: 879  total_loss: 1.043  loss_cls: 0.6546  loss_box_reg: 0.3533  loss_query: 0.02312  time: 2.2703  data_time: 0.0081  lr: 0.0052747  max_mem: 10549M\n",
            "\u001b[32m[04/14 14:54:31 d2.utils.events]: \u001b[0m eta: 1 day, 13:53:49  iter: 899  total_loss: 0.6473  loss_cls: 0.3786  loss_box_reg: 0.2356  loss_query: 0.03299  time: 2.2708  data_time: 0.0078  lr: 0.0053946  max_mem: 10549M\n",
            "\u001b[32m[04/14 14:55:19 d2.utils.events]: \u001b[0m eta: 1 day, 13:53:36  iter: 919  total_loss: 0.8466  loss_cls: 0.4968  loss_box_reg: 0.2716  loss_query: 0.04885  time: 2.2711  data_time: 0.0067  lr: 0.0055145  max_mem: 10549M\n",
            "\u001b[32m[04/14 14:56:09 d2.utils.events]: \u001b[0m eta: 1 day, 13:54:27  iter: 939  total_loss: 0.8038  loss_cls: 0.457  loss_box_reg: 0.2727  loss_query: 0.03885  time: 2.2740  data_time: 0.0069  lr: 0.0056344  max_mem: 10549M\n",
            "\u001b[32m[04/14 14:56:56 d2.utils.events]: \u001b[0m eta: 1 day, 13:54:11  iter: 959  total_loss: 0.5054  loss_cls: 0.3473  loss_box_reg: 0.1523  loss_query: 0.02244  time: 2.2748  data_time: 0.0071  lr: 0.0057542  max_mem: 10549M\n",
            "\u001b[32m[04/14 14:57:44 d2.utils.events]: \u001b[0m eta: 1 day, 13:54:09  iter: 979  total_loss: 0.7434  loss_cls: 0.4437  loss_box_reg: 0.2582  loss_query: 0.03436  time: 2.2753  data_time: 0.0073  lr: 0.0058741  max_mem: 10549M\n",
            "\u001b[32m[04/14 14:58:30 d2.utils.events]: \u001b[0m eta: 1 day, 13:53:23  iter: 999  total_loss: 0.5812  loss_cls: 0.3702  loss_box_reg: 0.1962  loss_query: 0.0164  time: 2.2748  data_time: 0.0058  lr: 0.005994  max_mem: 10549M\n",
            "\u001b[32m[04/14 14:59:17 d2.utils.events]: \u001b[0m eta: 1 day, 13:54:06  iter: 1019  total_loss: 0.9481  loss_cls: 0.5885  loss_box_reg: 0.3342  loss_query: 0.03464  time: 2.2756  data_time: 0.0065  lr: 0.006  max_mem: 10549M\n",
            "\u001b[32m[04/14 15:00:04 d2.utils.events]: \u001b[0m eta: 1 day, 13:53:59  iter: 1039  total_loss: 0.8092  loss_cls: 0.4783  loss_box_reg: 0.2683  loss_query: 0.03567  time: 2.2757  data_time: 0.0073  lr: 0.006  max_mem: 10549M\n",
            "\u001b[32m[04/14 15:00:51 d2.utils.events]: \u001b[0m eta: 1 day, 13:52:49  iter: 1059  total_loss: 0.8984  loss_cls: 0.525  loss_box_reg: 0.3144  loss_query: 0.02292  time: 2.2761  data_time: 0.0063  lr: 0.006  max_mem: 10549M\n",
            "\u001b[32m[04/14 15:01:38 d2.utils.events]: \u001b[0m eta: 1 day, 13:53:16  iter: 1079  total_loss: 0.7203  loss_cls: 0.4559  loss_box_reg: 0.2223  loss_query: 0.03254  time: 2.2758  data_time: 0.0071  lr: 0.006  max_mem: 10549M\n",
            "\u001b[32m[04/14 15:02:25 d2.utils.events]: \u001b[0m eta: 1 day, 13:53:15  iter: 1099  total_loss: 0.6499  loss_cls: 0.3859  loss_box_reg: 0.2144  loss_query: 0.03897  time: 2.2763  data_time: 0.0064  lr: 0.006  max_mem: 10549M\n",
            "\u001b[32m[04/14 15:03:12 d2.utils.events]: \u001b[0m eta: 1 day, 13:52:30  iter: 1119  total_loss: 0.7069  loss_cls: 0.3851  loss_box_reg: 0.2248  loss_query: 0.02822  time: 2.2765  data_time: 0.0062  lr: 0.006  max_mem: 10549M\n",
            "\u001b[32m[04/14 15:04:00 d2.utils.events]: \u001b[0m eta: 1 day, 13:52:36  iter: 1139  total_loss: 0.6255  loss_cls: 0.3958  loss_box_reg: 0.2238  loss_query: 0.03421  time: 2.2775  data_time: 0.0083  lr: 0.006  max_mem: 10549M\n",
            "\u001b[32m[04/14 15:04:48 d2.utils.events]: \u001b[0m eta: 1 day, 13:52:11  iter: 1159  total_loss: 0.8076  loss_cls: 0.4887  loss_box_reg: 0.2653  loss_query: 0.05583  time: 2.2783  data_time: 0.0069  lr: 0.006  max_mem: 10549M\n",
            "\u001b[32m[04/14 15:05:35 d2.utils.events]: \u001b[0m eta: 1 day, 13:53:18  iter: 1179  total_loss: 0.5851  loss_cls: 0.3668  loss_box_reg: 0.1826  loss_query: 0.03307  time: 2.2784  data_time: 0.0070  lr: 0.006  max_mem: 10549M\n",
            "\u001b[32m[04/14 15:06:21 d2.utils.events]: \u001b[0m eta: 1 day, 13:50:39  iter: 1199  total_loss: 0.6624  loss_cls: 0.3815  loss_box_reg: 0.2315  loss_query: 0.04867  time: 2.2778  data_time: 0.0065  lr: 0.006  max_mem: 10549M\n",
            "\u001b[32m[04/14 15:07:07 d2.utils.events]: \u001b[0m eta: 1 day, 13:49:50  iter: 1219  total_loss: 0.6668  loss_cls: 0.4333  loss_box_reg: 0.2342  loss_query: 0.04617  time: 2.2774  data_time: 0.0064  lr: 0.006  max_mem: 10549M\n",
            "\u001b[32m[04/14 15:07:55 d2.utils.events]: \u001b[0m eta: 1 day, 13:50:56  iter: 1239  total_loss: 1.062  loss_cls: 0.5972  loss_box_reg: 0.3867  loss_query: 0.04521  time: 2.2783  data_time: 0.0084  lr: 0.006  max_mem: 10549M\n",
            "\u001b[32m[04/14 15:08:41 d2.utils.events]: \u001b[0m eta: 1 day, 13:50:13  iter: 1259  total_loss: 0.5984  loss_cls: 0.3414  loss_box_reg: 0.2186  loss_query: 0.02373  time: 2.2781  data_time: 0.0070  lr: 0.006  max_mem: 10549M\n",
            "\u001b[32m[04/14 15:09:28 d2.utils.events]: \u001b[0m eta: 1 day, 13:49:36  iter: 1279  total_loss: 0.5568  loss_cls: 0.3468  loss_box_reg: 0.1805  loss_query: 0.02249  time: 2.2781  data_time: 0.0071  lr: 0.006  max_mem: 10549M\n",
            "\u001b[32m[04/14 15:10:17 d2.utils.events]: \u001b[0m eta: 1 day, 13:49:15  iter: 1299  total_loss: 1.019  loss_cls: 0.5855  loss_box_reg: 0.3291  loss_query: 0.04982  time: 2.2793  data_time: 0.0069  lr: 0.006  max_mem: 10549M\n",
            "\u001b[32m[04/14 15:11:04 d2.utils.events]: \u001b[0m eta: 1 day, 13:49:01  iter: 1319  total_loss: 0.4285  loss_cls: 0.2412  loss_box_reg: 0.165  loss_query: 0.0204  time: 2.2793  data_time: 0.0067  lr: 0.006  max_mem: 10549M\n",
            "\u001b[32m[04/14 15:11:51 d2.utils.events]: \u001b[0m eta: 1 day, 13:47:42  iter: 1339  total_loss: 0.6821  loss_cls: 0.437  loss_box_reg: 0.2268  loss_query: 0.03023  time: 2.2798  data_time: 0.0060  lr: 0.006  max_mem: 10549M\n",
            "\u001b[32m[04/14 15:12:37 d2.utils.events]: \u001b[0m eta: 1 day, 13:46:30  iter: 1359  total_loss: 0.676  loss_cls: 0.3896  loss_box_reg: 0.2511  loss_query: 0.04013  time: 2.2794  data_time: 0.0067  lr: 0.006  max_mem: 10549M\n",
            "\u001b[32m[04/14 15:13:24 d2.utils.events]: \u001b[0m eta: 1 day, 13:45:59  iter: 1379  total_loss: 0.6052  loss_cls: 0.4212  loss_box_reg: 0.194  loss_query: 0.02989  time: 2.2794  data_time: 0.0061  lr: 0.006  max_mem: 10549M\n",
            "\u001b[32m[04/14 15:14:10 d2.utils.events]: \u001b[0m eta: 1 day, 13:44:48  iter: 1399  total_loss: 0.606  loss_cls: 0.3946  loss_box_reg: 0.2179  loss_query: 0.01542  time: 2.2787  data_time: 0.0070  lr: 0.006  max_mem: 10549M\n",
            "\u001b[32m[04/14 15:14:58 d2.utils.events]: \u001b[0m eta: 1 day, 13:44:01  iter: 1419  total_loss: 0.9894  loss_cls: 0.6037  loss_box_reg: 0.288  loss_query: 0.04712  time: 2.2794  data_time: 0.0061  lr: 0.006  max_mem: 10549M\n",
            "\u001b[32m[04/14 15:15:45 d2.utils.events]: \u001b[0m eta: 1 day, 13:43:24  iter: 1439  total_loss: 0.9382  loss_cls: 0.5569  loss_box_reg: 0.3129  loss_query: 0.03239  time: 2.2795  data_time: 0.0061  lr: 0.006  max_mem: 10549M\n",
            "\u001b[32m[04/14 15:16:32 d2.utils.events]: \u001b[0m eta: 1 day, 13:42:20  iter: 1459  total_loss: 0.6423  loss_cls: 0.4047  loss_box_reg: 0.2143  loss_query: 0.04487  time: 2.2796  data_time: 0.0066  lr: 0.006  max_mem: 10549M\n",
            "\u001b[32m[04/14 15:17:19 d2.utils.events]: \u001b[0m eta: 1 day, 13:41:44  iter: 1479  total_loss: 0.7529  loss_cls: 0.4393  loss_box_reg: 0.2459  loss_query: 0.04027  time: 2.2797  data_time: 0.0057  lr: 0.006  max_mem: 10549M\n",
            "\u001b[32m[04/14 15:18:06 d2.utils.events]: \u001b[0m eta: 1 day, 13:40:57  iter: 1499  total_loss: 0.6602  loss_cls: 0.3864  loss_box_reg: 0.2209  loss_query: 0.02762  time: 2.2796  data_time: 0.0086  lr: 0.006  max_mem: 10549M\n",
            "\u001b[32m[04/14 15:18:52 d2.utils.events]: \u001b[0m eta: 1 day, 13:40:09  iter: 1519  total_loss: 0.8306  loss_cls: 0.4859  loss_box_reg: 0.2867  loss_query: 0.03028  time: 2.2789  data_time: 0.0081  lr: 0.006  max_mem: 10549M\n",
            "\u001b[32m[04/14 15:19:38 d2.utils.events]: \u001b[0m eta: 1 day, 13:39:45  iter: 1539  total_loss: 0.7986  loss_cls: 0.4814  loss_box_reg: 0.2741  loss_query: 0.02468  time: 2.2786  data_time: 0.0072  lr: 0.006  max_mem: 10549M\n",
            "\u001b[32m[04/14 15:20:25 d2.utils.events]: \u001b[0m eta: 1 day, 13:39:30  iter: 1559  total_loss: 0.9108  loss_cls: 0.5902  loss_box_reg: 0.2981  loss_query: 0.047  time: 2.2791  data_time: 0.0063  lr: 0.006  max_mem: 10549M\n",
            "\u001b[32m[04/14 15:21:12 d2.utils.events]: \u001b[0m eta: 1 day, 13:39:20  iter: 1579  total_loss: 0.756  loss_cls: 0.4525  loss_box_reg: 0.2773  loss_query: 0.03825  time: 2.2791  data_time: 0.0066  lr: 0.006  max_mem: 10549M\n",
            "\u001b[32m[04/14 15:21:59 d2.utils.events]: \u001b[0m eta: 1 day, 13:37:39  iter: 1599  total_loss: 0.5634  loss_cls: 0.3502  loss_box_reg: 0.19  loss_query: 0.02549  time: 2.2792  data_time: 0.0072  lr: 0.006  max_mem: 10549M\n",
            "\u001b[32m[04/14 15:22:46 d2.utils.events]: \u001b[0m eta: 1 day, 13:36:26  iter: 1619  total_loss: 0.755  loss_cls: 0.4814  loss_box_reg: 0.261  loss_query: 0.04502  time: 2.2793  data_time: 0.0058  lr: 0.006  max_mem: 10549M\n",
            "\u001b[32m[04/14 15:23:33 d2.utils.events]: \u001b[0m eta: 1 day, 13:35:53  iter: 1639  total_loss: 0.6853  loss_cls: 0.418  loss_box_reg: 0.2435  loss_query: 0.03409  time: 2.2793  data_time: 0.0078  lr: 0.006  max_mem: 10549M\n",
            "\u001b[32m[04/14 15:24:20 d2.utils.events]: \u001b[0m eta: 1 day, 13:35:01  iter: 1659  total_loss: 0.7221  loss_cls: 0.4572  loss_box_reg: 0.2175  loss_query: 0.03833  time: 2.2794  data_time: 0.0072  lr: 0.006  max_mem: 10549M\n",
            "\u001b[32m[04/14 15:25:07 d2.utils.events]: \u001b[0m eta: 1 day, 13:34:26  iter: 1679  total_loss: 0.7587  loss_cls: 0.4501  loss_box_reg: 0.2426  loss_query: 0.05137  time: 2.2797  data_time: 0.0077  lr: 0.006  max_mem: 10549M\n",
            "\u001b[32m[04/14 15:25:55 d2.utils.events]: \u001b[0m eta: 1 day, 13:33:53  iter: 1699  total_loss: 0.6904  loss_cls: 0.3921  loss_box_reg: 0.2281  loss_query: 0.04275  time: 2.2803  data_time: 0.0063  lr: 0.006  max_mem: 10549M\n",
            "\u001b[32m[04/14 15:26:41 d2.utils.events]: \u001b[0m eta: 1 day, 13:33:39  iter: 1719  total_loss: 0.6664  loss_cls: 0.4217  loss_box_reg: 0.2504  loss_query: 0.02431  time: 2.2801  data_time: 0.0073  lr: 0.006  max_mem: 10549M\n",
            "\u001b[32m[04/14 15:27:30 d2.utils.events]: \u001b[0m eta: 1 day, 13:34:42  iter: 1739  total_loss: 0.7916  loss_cls: 0.4641  loss_box_reg: 0.2765  loss_query: 0.02819  time: 2.2807  data_time: 0.0072  lr: 0.006  max_mem: 10549M\n",
            "\u001b[32m[04/14 15:28:17 d2.utils.events]: \u001b[0m eta: 1 day, 13:34:53  iter: 1759  total_loss: 0.9355  loss_cls: 0.5285  loss_box_reg: 0.314  loss_query: 0.0183  time: 2.2809  data_time: 0.0071  lr: 0.006  max_mem: 10549M\n",
            "\u001b[32m[04/14 15:29:03 d2.utils.events]: \u001b[0m eta: 1 day, 13:34:24  iter: 1779  total_loss: 0.8086  loss_cls: 0.457  loss_box_reg: 0.2618  loss_query: 0.04264  time: 2.2805  data_time: 0.0066  lr: 0.006  max_mem: 10549M\n",
            "\u001b[32m[04/14 15:29:50 d2.utils.events]: \u001b[0m eta: 1 day, 13:35:22  iter: 1799  total_loss: 0.5919  loss_cls: 0.3671  loss_box_reg: 0.2035  loss_query: 0.02897  time: 2.2809  data_time: 0.0070  lr: 0.006  max_mem: 10549M\n",
            "\u001b[32m[04/14 15:30:36 d2.utils.events]: \u001b[0m eta: 1 day, 13:34:35  iter: 1819  total_loss: 0.8349  loss_cls: 0.4879  loss_box_reg: 0.2818  loss_query: 0.04158  time: 2.2801  data_time: 0.0065  lr: 0.006  max_mem: 10549M\n",
            "\u001b[32m[04/14 15:31:22 d2.utils.events]: \u001b[0m eta: 1 day, 13:33:51  iter: 1839  total_loss: 0.9165  loss_cls: 0.5626  loss_box_reg: 0.309  loss_query: 0.03323  time: 2.2798  data_time: 0.0073  lr: 0.006  max_mem: 10549M\n",
            "\u001b[32m[04/14 15:32:08 d2.utils.events]: \u001b[0m eta: 1 day, 13:33:04  iter: 1859  total_loss: 0.5666  loss_cls: 0.37  loss_box_reg: 0.2054  loss_query: 0.02024  time: 2.2795  data_time: 0.0068  lr: 0.006  max_mem: 10549M\n",
            "\u001b[32m[04/14 15:32:55 d2.utils.events]: \u001b[0m eta: 1 day, 13:32:21  iter: 1879  total_loss: 0.7705  loss_cls: 0.4636  loss_box_reg: 0.2826  loss_query: 0.03057  time: 2.2797  data_time: 0.0079  lr: 0.006  max_mem: 10549M\n",
            "\u001b[32m[04/14 15:33:43 d2.utils.events]: \u001b[0m eta: 1 day, 13:31:46  iter: 1899  total_loss: 0.717  loss_cls: 0.4155  loss_box_reg: 0.2708  loss_query: 0.04707  time: 2.2802  data_time: 0.0065  lr: 0.006  max_mem: 10549M\n",
            "\u001b[32m[04/14 15:34:30 d2.utils.events]: \u001b[0m eta: 1 day, 13:30:52  iter: 1919  total_loss: 0.6604  loss_cls: 0.3925  loss_box_reg: 0.2171  loss_query: 0.02724  time: 2.2802  data_time: 0.0073  lr: 0.006  max_mem: 10549M\n",
            "\u001b[32m[04/14 15:35:17 d2.utils.events]: \u001b[0m eta: 1 day, 13:29:57  iter: 1939  total_loss: 0.858  loss_cls: 0.5195  loss_box_reg: 0.2889  loss_query: 0.02675  time: 2.2806  data_time: 0.0084  lr: 0.006  max_mem: 10549M\n",
            "\u001b[32m[04/14 15:36:05 d2.utils.events]: \u001b[0m eta: 1 day, 13:29:10  iter: 1959  total_loss: 0.553  loss_cls: 0.3565  loss_box_reg: 0.1577  loss_query: 0.02911  time: 2.2808  data_time: 0.0078  lr: 0.006  max_mem: 10549M\n",
            "\u001b[32m[04/14 15:36:51 d2.utils.events]: \u001b[0m eta: 1 day, 13:25:34  iter: 1979  total_loss: 0.6541  loss_cls: 0.4256  loss_box_reg: 0.2063  loss_query: 0.02837  time: 2.2808  data_time: 0.0058  lr: 0.006  max_mem: 10549M\n",
            "\u001b[32m[04/14 15:37:39 fvcore.common.checkpoint]: \u001b[0mSaving checkpoint to work_dirs/visdrone_querydet/model_0001999.pth\n",
            "\u001b[32m[04/14 15:37:59 d2.utils.events]: \u001b[0m eta: 1 day, 13:25:07  iter: 1999  total_loss: 0.7499  loss_cls: 0.4853  loss_box_reg: 0.264  loss_query: 0.03721  time: 2.2812  data_time: 0.0079  lr: 0.006  max_mem: 10549M\n",
            "\u001b[32m[04/14 15:38:46 d2.utils.events]: \u001b[0m eta: 1 day, 13:24:21  iter: 2019  total_loss: 0.8863  loss_cls: 0.4908  loss_box_reg: 0.3319  loss_query: 0.05151  time: 2.2814  data_time: 0.0076  lr: 0.006  max_mem: 10549M\n",
            "\u001b[32m[04/14 15:39:34 d2.utils.events]: \u001b[0m eta: 1 day, 13:24:31  iter: 2039  total_loss: 0.7766  loss_cls: 0.4531  loss_box_reg: 0.2431  loss_query: 0.03404  time: 2.2818  data_time: 0.0068  lr: 0.006  max_mem: 10549M\n",
            "\u001b[32m[04/14 15:40:22 d2.utils.events]: \u001b[0m eta: 1 day, 13:25:12  iter: 2059  total_loss: 0.6399  loss_cls: 0.393  loss_box_reg: 0.2341  loss_query: 0.03469  time: 2.2820  data_time: 0.0073  lr: 0.006  max_mem: 10549M\n",
            "\u001b[32m[04/14 15:41:09 d2.utils.events]: \u001b[0m eta: 1 day, 13:24:32  iter: 2079  total_loss: 0.6413  loss_cls: 0.3663  loss_box_reg: 0.2373  loss_query: 0.03462  time: 2.2821  data_time: 0.0073  lr: 0.006  max_mem: 10549M\n",
            "\u001b[32m[04/14 15:41:56 d2.utils.events]: \u001b[0m eta: 1 day, 13:22:45  iter: 2099  total_loss: 0.5687  loss_cls: 0.3457  loss_box_reg: 0.1951  loss_query: 0.02791  time: 2.2823  data_time: 0.0066  lr: 0.006  max_mem: 10549M\n",
            "\u001b[32m[04/14 15:42:41 d2.utils.events]: \u001b[0m eta: 1 day, 13:21:25  iter: 2119  total_loss: 0.5715  loss_cls: 0.3441  loss_box_reg: 0.1885  loss_query: 0.02466  time: 2.2815  data_time: 0.0085  lr: 0.006  max_mem: 10549M\n",
            "\u001b[32m[04/14 15:43:27 d2.utils.events]: \u001b[0m eta: 1 day, 13:20:16  iter: 2139  total_loss: 0.8136  loss_cls: 0.4965  loss_box_reg: 0.2505  loss_query: 0.02978  time: 2.2810  data_time: 0.0064  lr: 0.006  max_mem: 10549M\n",
            "\u001b[32m[04/14 15:44:12 d2.utils.events]: \u001b[0m eta: 1 day, 13:17:35  iter: 2159  total_loss: 0.6743  loss_cls: 0.3943  loss_box_reg: 0.2264  loss_query: 0.03283  time: 2.2800  data_time: 0.0070  lr: 0.006  max_mem: 10549M\n",
            "\u001b[32m[04/14 15:44:59 d2.utils.events]: \u001b[0m eta: 1 day, 13:18:08  iter: 2179  total_loss: 0.7972  loss_cls: 0.4878  loss_box_reg: 0.2859  loss_query: 0.03381  time: 2.2803  data_time: 0.0075  lr: 0.006  max_mem: 10549M\n",
            "\u001b[32m[04/14 15:45:46 d2.utils.events]: \u001b[0m eta: 1 day, 13:17:47  iter: 2199  total_loss: 0.9669  loss_cls: 0.5433  loss_box_reg: 0.3424  loss_query: 0.03972  time: 2.2804  data_time: 0.0060  lr: 0.006  max_mem: 10549M\n",
            "\u001b[32m[04/14 15:46:34 d2.utils.events]: \u001b[0m eta: 1 day, 13:18:06  iter: 2219  total_loss: 0.737  loss_cls: 0.4268  loss_box_reg: 0.2521  loss_query: 0.05005  time: 2.2809  data_time: 0.0073  lr: 0.006  max_mem: 10549M\n",
            "\u001b[32m[04/14 15:47:22 d2.utils.events]: \u001b[0m eta: 1 day, 13:15:44  iter: 2239  total_loss: 0.5707  loss_cls: 0.3518  loss_box_reg: 0.2214  loss_query: 0.02807  time: 2.2813  data_time: 0.0070  lr: 0.006  max_mem: 10549M\n",
            "\u001b[32m[04/14 15:48:08 d2.utils.events]: \u001b[0m eta: 1 day, 13:14:30  iter: 2259  total_loss: 0.4234  loss_cls: 0.2687  loss_box_reg: 0.1341  loss_query: 0.02056  time: 2.2810  data_time: 0.0062  lr: 0.006  max_mem: 10549M\n",
            "\u001b[32m[04/14 15:48:54 d2.utils.events]: \u001b[0m eta: 1 day, 13:13:44  iter: 2279  total_loss: 0.9081  loss_cls: 0.5474  loss_box_reg: 0.3043  loss_query: 0.03081  time: 2.2808  data_time: 0.0056  lr: 0.006  max_mem: 10549M\n",
            "\u001b[32m[04/14 15:49:41 d2.utils.events]: \u001b[0m eta: 1 day, 13:12:57  iter: 2299  total_loss: 0.761  loss_cls: 0.4575  loss_box_reg: 0.296  loss_query: 0.03965  time: 2.2810  data_time: 0.0065  lr: 0.006  max_mem: 10549M\n",
            "\u001b[32m[04/14 15:50:28 d2.utils.events]: \u001b[0m eta: 1 day, 13:11:45  iter: 2319  total_loss: 0.5763  loss_cls: 0.3594  loss_box_reg: 0.1808  loss_query: 0.03071  time: 2.2809  data_time: 0.0059  lr: 0.006  max_mem: 10549M\n",
            "\u001b[32m[04/14 15:51:16 d2.utils.events]: \u001b[0m eta: 1 day, 13:12:46  iter: 2339  total_loss: 0.8271  loss_cls: 0.4992  loss_box_reg: 0.2626  loss_query: 0.0505  time: 2.2814  data_time: 0.0070  lr: 0.006  max_mem: 10549M\n",
            "\u001b[32m[04/14 15:52:03 d2.utils.events]: \u001b[0m eta: 1 day, 13:13:15  iter: 2359  total_loss: 0.6823  loss_cls: 0.4251  loss_box_reg: 0.2476  loss_query: 0.02988  time: 2.2817  data_time: 0.0071  lr: 0.006  max_mem: 10549M\n",
            "\u001b[32m[04/14 15:52:50 d2.utils.events]: \u001b[0m eta: 1 day, 13:12:46  iter: 2379  total_loss: 0.8116  loss_cls: 0.4728  loss_box_reg: 0.2776  loss_query: 0.04574  time: 2.2817  data_time: 0.0068  lr: 0.006  max_mem: 10549M\n",
            "\u001b[32m[04/14 15:53:38 d2.utils.events]: \u001b[0m eta: 1 day, 13:13:08  iter: 2399  total_loss: 0.7867  loss_cls: 0.4793  loss_box_reg: 0.2799  loss_query: 0.02483  time: 2.2821  data_time: 0.0065  lr: 0.006  max_mem: 10549M\n",
            "\u001b[32m[04/14 15:54:25 d2.utils.events]: \u001b[0m eta: 1 day, 13:12:26  iter: 2419  total_loss: 0.8205  loss_cls: 0.4948  loss_box_reg: 0.2938  loss_query: 0.02923  time: 2.2819  data_time: 0.0063  lr: 0.006  max_mem: 10549M\n",
            "\u001b[32m[04/14 15:55:11 d2.utils.events]: \u001b[0m eta: 1 day, 13:11:27  iter: 2439  total_loss: 0.5831  loss_cls: 0.3678  loss_box_reg: 0.1947  loss_query: 0.02305  time: 2.2817  data_time: 0.0061  lr: 0.006  max_mem: 10549M\n",
            "\u001b[32m[04/14 15:55:57 d2.utils.events]: \u001b[0m eta: 1 day, 13:10:30  iter: 2459  total_loss: 0.8782  loss_cls: 0.5456  loss_box_reg: 0.2899  loss_query: 0.03758  time: 2.2816  data_time: 0.0061  lr: 0.006  max_mem: 10549M\n",
            "\u001b[32m[04/14 15:56:44 d2.utils.events]: \u001b[0m eta: 1 day, 13:09:23  iter: 2479  total_loss: 0.6517  loss_cls: 0.3579  loss_box_reg: 0.2738  loss_query: 0.01722  time: 2.2817  data_time: 0.0071  lr: 0.006  max_mem: 10549M\n",
            "\u001b[32m[04/14 15:57:32 d2.utils.events]: \u001b[0m eta: 1 day, 13:08:57  iter: 2499  total_loss: 0.8517  loss_cls: 0.5361  loss_box_reg: 0.3003  loss_query: 0.05552  time: 2.2821  data_time: 0.0058  lr: 0.006  max_mem: 10549M\n",
            "\u001b[32m[04/14 15:58:20 d2.utils.events]: \u001b[0m eta: 1 day, 13:08:37  iter: 2519  total_loss: 0.681  loss_cls: 0.4025  loss_box_reg: 0.2453  loss_query: 0.03768  time: 2.2824  data_time: 0.0064  lr: 0.006  max_mem: 10549M\n",
            "\u001b[32m[04/14 15:59:07 d2.utils.events]: \u001b[0m eta: 1 day, 13:07:50  iter: 2539  total_loss: 0.5823  loss_cls: 0.3584  loss_box_reg: 0.1843  loss_query: 0.03193  time: 2.2826  data_time: 0.0083  lr: 0.006  max_mem: 10549M\n",
            "\u001b[32m[04/14 15:59:52 d2.utils.events]: \u001b[0m eta: 1 day, 13:05:56  iter: 2559  total_loss: 0.6871  loss_cls: 0.4528  loss_box_reg: 0.2454  loss_query: 0.02962  time: 2.2820  data_time: 0.0075  lr: 0.006  max_mem: 10549M\n",
            "\u001b[32m[04/14 16:00:39 d2.utils.events]: \u001b[0m eta: 1 day, 13:05:10  iter: 2579  total_loss: 0.7642  loss_cls: 0.4787  loss_box_reg: 0.2527  loss_query: 0.04141  time: 2.2821  data_time: 0.0080  lr: 0.006  max_mem: 10549M\n",
            "\u001b[32m[04/14 16:01:24 d2.utils.events]: \u001b[0m eta: 1 day, 13:02:39  iter: 2599  total_loss: 0.7904  loss_cls: 0.4805  loss_box_reg: 0.2981  loss_query: 0.03832  time: 2.2815  data_time: 0.0057  lr: 0.006  max_mem: 10549M\n",
            "\u001b[32m[04/14 16:02:12 d2.utils.events]: \u001b[0m eta: 1 day, 13:03:22  iter: 2619  total_loss: 0.5079  loss_cls: 0.3114  loss_box_reg: 0.1573  loss_query: 0.04711  time: 2.2818  data_time: 0.0070  lr: 0.006  max_mem: 10549M\n",
            "\u001b[32m[04/14 16:02:58 d2.utils.events]: \u001b[0m eta: 1 day, 13:02:24  iter: 2639  total_loss: 0.5874  loss_cls: 0.3661  loss_box_reg: 0.2204  loss_query: 0.02865  time: 2.2815  data_time: 0.0055  lr: 0.006  max_mem: 10549M\n",
            "\u001b[32m[04/14 16:03:44 d2.utils.events]: \u001b[0m eta: 1 day, 13:02:09  iter: 2659  total_loss: 0.9089  loss_cls: 0.59  loss_box_reg: 0.2568  loss_query: 0.04782  time: 2.2811  data_time: 0.0088  lr: 0.006  max_mem: 10549M\n",
            "\u001b[32m[04/14 16:04:33 d2.utils.events]: \u001b[0m eta: 1 day, 13:01:38  iter: 2679  total_loss: 1.078  loss_cls: 0.6402  loss_box_reg: 0.3781  loss_query: 0.05263  time: 2.2819  data_time: 0.0072  lr: 0.006  max_mem: 10549M\n",
            "\u001b[32m[04/14 16:05:19 d2.utils.events]: \u001b[0m eta: 1 day, 13:00:57  iter: 2699  total_loss: 0.7643  loss_cls: 0.4518  loss_box_reg: 0.2349  loss_query: 0.04654  time: 2.2816  data_time: 0.0076  lr: 0.006  max_mem: 10549M\n",
            "\u001b[32m[04/14 16:06:06 d2.utils.events]: \u001b[0m eta: 1 day, 12:59:49  iter: 2719  total_loss: 0.6174  loss_cls: 0.386  loss_box_reg: 0.2024  loss_query: 0.03623  time: 2.2816  data_time: 0.0061  lr: 0.006  max_mem: 10549M\n",
            "\u001b[32m[04/14 16:06:54 d2.utils.events]: \u001b[0m eta: 1 day, 12:59:03  iter: 2739  total_loss: 0.7516  loss_cls: 0.429  loss_box_reg: 0.2557  loss_query: 0.03794  time: 2.2820  data_time: 0.0070  lr: 0.006  max_mem: 10549M\n",
            "\u001b[32m[04/14 16:07:41 d2.utils.events]: \u001b[0m eta: 1 day, 12:58:37  iter: 2759  total_loss: 0.6395  loss_cls: 0.3965  loss_box_reg: 0.2121  loss_query: 0.04224  time: 2.2820  data_time: 0.0063  lr: 0.006  max_mem: 10549M\n",
            "\u001b[32m[04/14 16:08:27 d2.utils.events]: \u001b[0m eta: 1 day, 12:57:51  iter: 2779  total_loss: 0.6196  loss_cls: 0.3936  loss_box_reg: 0.1826  loss_query: 0.02427  time: 2.2817  data_time: 0.0063  lr: 0.006  max_mem: 10549M\n",
            "\u001b[32m[04/14 16:09:23 d2.utils.events]: \u001b[0m eta: 1 day, 12:56:43  iter: 2799  total_loss: 0.7079  loss_cls: 0.4253  loss_box_reg: 0.2167  loss_query: 0.03054  time: 2.2850  data_time: 0.5247  lr: 0.006  max_mem: 10549M\n",
            "\u001b[32m[04/14 16:10:10 d2.utils.events]: \u001b[0m eta: 1 day, 12:56:18  iter: 2819  total_loss: 0.652  loss_cls: 0.399  loss_box_reg: 0.2502  loss_query: 0.01978  time: 2.2848  data_time: 0.0057  lr: 0.006  max_mem: 10549M\n",
            "\u001b[32m[04/14 16:10:56 d2.utils.events]: \u001b[0m eta: 1 day, 12:56:12  iter: 2839  total_loss: 0.9837  loss_cls: 0.5651  loss_box_reg: 0.3541  loss_query: 0.03865  time: 2.2847  data_time: 0.0066  lr: 0.006  max_mem: 10549M\n",
            "\u001b[32m[04/14 16:11:44 d2.utils.events]: \u001b[0m eta: 1 day, 12:55:28  iter: 2859  total_loss: 0.6579  loss_cls: 0.409  loss_box_reg: 0.2268  loss_query: 0.01775  time: 2.2849  data_time: 0.0066  lr: 0.006  max_mem: 10549M\n",
            "\u001b[32m[04/14 16:12:32 d2.utils.events]: \u001b[0m eta: 1 day, 12:54:24  iter: 2879  total_loss: 0.9725  loss_cls: 0.513  loss_box_reg: 0.3721  loss_query: 0.04179  time: 2.2853  data_time: 0.0059  lr: 0.006  max_mem: 10549M\n",
            "\u001b[32m[04/14 16:13:19 d2.utils.events]: \u001b[0m eta: 1 day, 12:52:31  iter: 2899  total_loss: 0.3658  loss_cls: 0.2163  loss_box_reg: 0.1278  loss_query: 0.0199  time: 2.2853  data_time: 0.0080  lr: 0.006  max_mem: 10549M\n",
            "\u001b[32m[04/14 16:14:06 d2.utils.events]: \u001b[0m eta: 1 day, 12:51:33  iter: 2919  total_loss: 0.6388  loss_cls: 0.4071  loss_box_reg: 0.2129  loss_query: 0.02563  time: 2.2853  data_time: 0.0068  lr: 0.006  max_mem: 10549M\n",
            "\u001b[32m[04/14 16:14:53 d2.utils.events]: \u001b[0m eta: 1 day, 12:48:33  iter: 2939  total_loss: 0.659  loss_cls: 0.4233  loss_box_reg: 0.235  loss_query: 0.03272  time: 2.2852  data_time: 0.0062  lr: 0.006  max_mem: 10549M\n",
            "\u001b[32m[04/14 16:15:40 d2.utils.events]: \u001b[0m eta: 1 day, 12:50:11  iter: 2959  total_loss: 0.686  loss_cls: 0.4259  loss_box_reg: 0.2409  loss_query: 0.03708  time: 2.2855  data_time: 0.0082  lr: 0.006  max_mem: 10549M\n",
            "\u001b[32m[04/14 16:16:28 d2.utils.events]: \u001b[0m eta: 1 day, 12:51:03  iter: 2979  total_loss: 0.7858  loss_cls: 0.4896  loss_box_reg: 0.2594  loss_query: 0.0376  time: 2.2858  data_time: 0.0063  lr: 0.006  max_mem: 10549M\n",
            "\u001b[32m[04/14 16:17:16 d2.utils.events]: \u001b[0m eta: 1 day, 12:50:44  iter: 2999  total_loss: 0.6212  loss_cls: 0.3822  loss_box_reg: 0.1929  loss_query: 0.04203  time: 2.2860  data_time: 0.0073  lr: 0.006  max_mem: 10549M\n",
            "\u001b[32m[04/14 16:18:04 d2.utils.events]: \u001b[0m eta: 1 day, 12:49:44  iter: 3019  total_loss: 0.5756  loss_cls: 0.3664  loss_box_reg: 0.1915  loss_query: 0.02345  time: 2.2863  data_time: 0.0061  lr: 0.006  max_mem: 10549M\n",
            "\u001b[32m[04/14 16:18:51 d2.utils.events]: \u001b[0m eta: 1 day, 12:49:42  iter: 3039  total_loss: 0.7806  loss_cls: 0.455  loss_box_reg: 0.27  loss_query: 0.0516  time: 2.2864  data_time: 0.0062  lr: 0.006  max_mem: 10549M\n",
            "\u001b[32m[04/14 16:19:38 d2.utils.events]: \u001b[0m eta: 1 day, 12:48:36  iter: 3059  total_loss: 0.7747  loss_cls: 0.4527  loss_box_reg: 0.2791  loss_query: 0.04288  time: 2.2866  data_time: 0.0057  lr: 0.006  max_mem: 10549M\n",
            "\u001b[32m[04/14 16:20:23 d2.utils.events]: \u001b[0m eta: 1 day, 12:47:24  iter: 3079  total_loss: 0.8325  loss_cls: 0.4935  loss_box_reg: 0.3048  loss_query: 0.0385  time: 2.2859  data_time: 0.0098  lr: 0.006  max_mem: 10549M\n",
            "\u001b[32m[04/14 16:21:09 d2.utils.events]: \u001b[0m eta: 1 day, 12:46:37  iter: 3099  total_loss: 0.7516  loss_cls: 0.4427  loss_box_reg: 0.2578  loss_query: 0.03498  time: 2.2855  data_time: 0.0065  lr: 0.006  max_mem: 10549M\n",
            "\u001b[32m[04/14 16:21:56 d2.utils.events]: \u001b[0m eta: 1 day, 12:46:16  iter: 3119  total_loss: 0.6434  loss_cls: 0.4063  loss_box_reg: 0.2219  loss_query: 0.03691  time: 2.2853  data_time: 0.0069  lr: 0.006  max_mem: 10549M\n",
            "\u001b[32m[04/14 16:22:43 d2.utils.events]: \u001b[0m eta: 1 day, 12:45:30  iter: 3139  total_loss: 0.6875  loss_cls: 0.4126  loss_box_reg: 0.2284  loss_query: 0.02549  time: 2.2853  data_time: 0.0062  lr: 0.006  max_mem: 10549M\n",
            "\u001b[32m[04/14 16:23:29 d2.utils.events]: \u001b[0m eta: 1 day, 12:44:43  iter: 3159  total_loss: 0.7656  loss_cls: 0.4512  loss_box_reg: 0.2806  loss_query: 0.02728  time: 2.2851  data_time: 0.0066  lr: 0.006  max_mem: 10549M\n",
            "\u001b[32m[04/14 16:24:17 d2.utils.events]: \u001b[0m eta: 1 day, 12:43:52  iter: 3179  total_loss: 0.7026  loss_cls: 0.4183  loss_box_reg: 0.2495  loss_query: 0.03795  time: 2.2853  data_time: 0.0071  lr: 0.006  max_mem: 10549M\n",
            "\u001b[32m[04/14 16:25:04 d2.utils.events]: \u001b[0m eta: 1 day, 12:42:59  iter: 3199  total_loss: 0.5993  loss_cls: 0.3943  loss_box_reg: 0.1872  loss_query: 0.03178  time: 2.2855  data_time: 0.0070  lr: 0.006  max_mem: 10549M\n",
            "\u001b[32m[04/14 16:25:50 d2.utils.events]: \u001b[0m eta: 1 day, 12:41:24  iter: 3219  total_loss: 0.5488  loss_cls: 0.3437  loss_box_reg: 0.1928  loss_query: 0.03268  time: 2.2851  data_time: 0.0082  lr: 0.006  max_mem: 10549M\n",
            "\u001b[32m[04/14 16:26:38 d2.utils.events]: \u001b[0m eta: 1 day, 12:40:19  iter: 3239  total_loss: 0.8287  loss_cls: 0.5222  loss_box_reg: 0.2706  loss_query: 0.02516  time: 2.2854  data_time: 0.0072  lr: 0.006  max_mem: 10549M\n",
            "\u001b[32m[04/14 16:27:25 d2.utils.events]: \u001b[0m eta: 1 day, 12:40:03  iter: 3259  total_loss: 0.8663  loss_cls: 0.5435  loss_box_reg: 0.2795  loss_query: 0.02877  time: 2.2854  data_time: 0.0069  lr: 0.006  max_mem: 10549M\n",
            "\u001b[32m[04/14 16:28:12 d2.utils.events]: \u001b[0m eta: 1 day, 12:38:55  iter: 3279  total_loss: 0.6304  loss_cls: 0.42  loss_box_reg: 0.2585  loss_query: 0.02288  time: 2.2854  data_time: 0.0075  lr: 0.006  max_mem: 10549M\n",
            "\u001b[32m[04/14 16:28:59 d2.utils.events]: \u001b[0m eta: 1 day, 12:38:09  iter: 3299  total_loss: 0.9169  loss_cls: 0.5854  loss_box_reg: 0.2823  loss_query: 0.03171  time: 2.2856  data_time: 0.0071  lr: 0.006  max_mem: 10549M\n",
            "\u001b[32m[04/14 16:29:45 d2.utils.events]: \u001b[0m eta: 1 day, 12:37:22  iter: 3319  total_loss: 0.6834  loss_cls: 0.4033  loss_box_reg: 0.2606  loss_query: 0.03157  time: 2.2852  data_time: 0.0073  lr: 0.006  max_mem: 10549M\n",
            "\u001b[32m[04/14 16:30:34 d2.utils.events]: \u001b[0m eta: 1 day, 12:36:27  iter: 3339  total_loss: 0.8648  loss_cls: 0.507  loss_box_reg: 0.3272  loss_query: 0.04943  time: 2.2856  data_time: 0.0077  lr: 0.006  max_mem: 10549M\n",
            "\u001b[32m[04/14 16:31:20 d2.utils.events]: \u001b[0m eta: 1 day, 12:35:58  iter: 3359  total_loss: 0.6943  loss_cls: 0.4265  loss_box_reg: 0.2115  loss_query: 0.04208  time: 2.2855  data_time: 0.0068  lr: 0.006  max_mem: 10549M\n",
            "\u001b[32m[04/14 16:32:07 d2.utils.events]: \u001b[0m eta: 1 day, 12:34:27  iter: 3379  total_loss: 0.5479  loss_cls: 0.3494  loss_box_reg: 0.1805  loss_query: 0.03274  time: 2.2855  data_time: 0.0067  lr: 0.006  max_mem: 10549M\n",
            "\u001b[32m[04/14 16:32:54 d2.utils.events]: \u001b[0m eta: 1 day, 12:33:17  iter: 3399  total_loss: 0.805  loss_cls: 0.4664  loss_box_reg: 0.2736  loss_query: 0.0517  time: 2.2857  data_time: 0.0063  lr: 0.006  max_mem: 10549M\n",
            "\u001b[32m[04/14 16:33:41 d2.utils.events]: \u001b[0m eta: 1 day, 12:31:19  iter: 3419  total_loss: 0.6314  loss_cls: 0.4225  loss_box_reg: 0.1763  loss_query: 0.02335  time: 2.2857  data_time: 0.0072  lr: 0.006  max_mem: 10549M\n",
            "\u001b[32m[04/14 16:34:27 d2.utils.events]: \u001b[0m eta: 1 day, 12:31:11  iter: 3439  total_loss: 0.7103  loss_cls: 0.4379  loss_box_reg: 0.2605  loss_query: 0.03851  time: 2.2851  data_time: 0.0072  lr: 0.006  max_mem: 10549M\n",
            "\u001b[32m[04/14 16:35:13 d2.utils.events]: \u001b[0m eta: 1 day, 12:30:35  iter: 3459  total_loss: 0.6467  loss_cls: 0.4339  loss_box_reg: 0.2305  loss_query: 0.02624  time: 2.2851  data_time: 0.0072  lr: 0.006  max_mem: 10549M\n",
            "\u001b[32m[04/14 16:36:01 d2.utils.events]: \u001b[0m eta: 1 day, 12:31:10  iter: 3479  total_loss: 0.8385  loss_cls: 0.4972  loss_box_reg: 0.3232  loss_query: 0.04515  time: 2.2852  data_time: 0.0066  lr: 0.006  max_mem: 10549M\n",
            "\u001b[32m[04/14 16:36:48 d2.utils.events]: \u001b[0m eta: 1 day, 12:29:02  iter: 3499  total_loss: 0.5451  loss_cls: 0.3287  loss_box_reg: 0.2066  loss_query: 0.04208  time: 2.2851  data_time: 0.0074  lr: 0.006  max_mem: 10549M\n",
            "\u001b[32m[04/14 16:37:33 d2.utils.events]: \u001b[0m eta: 1 day, 12:27:07  iter: 3519  total_loss: 0.7103  loss_cls: 0.4274  loss_box_reg: 0.2683  loss_query: 0.02981  time: 2.2846  data_time: 0.0063  lr: 0.006  max_mem: 10549M\n",
            "\u001b[32m[04/14 16:38:20 d2.utils.events]: \u001b[0m eta: 1 day, 12:27:08  iter: 3539  total_loss: 0.633  loss_cls: 0.3897  loss_box_reg: 0.2226  loss_query: 0.0333  time: 2.2847  data_time: 0.0078  lr: 0.006  max_mem: 10549M\n",
            "\u001b[32m[04/14 16:39:07 d2.utils.events]: \u001b[0m eta: 1 day, 12:28:04  iter: 3559  total_loss: 0.8194  loss_cls: 0.4873  loss_box_reg: 0.2643  loss_query: 0.04877  time: 2.2847  data_time: 0.0082  lr: 0.006  max_mem: 10549M\n",
            "\u001b[32m[04/14 16:39:55 d2.utils.events]: \u001b[0m eta: 1 day, 12:27:26  iter: 3579  total_loss: 0.8683  loss_cls: 0.505  loss_box_reg: 0.2707  loss_query: 0.04601  time: 2.2849  data_time: 0.0071  lr: 0.006  max_mem: 10549M\n",
            "\u001b[32m[04/14 16:40:44 d2.utils.events]: \u001b[0m eta: 1 day, 12:27:28  iter: 3599  total_loss: 0.647  loss_cls: 0.3954  loss_box_reg: 0.2339  loss_query: 0.03931  time: 2.2852  data_time: 0.0063  lr: 0.006  max_mem: 10549M\n",
            "\u001b[32m[04/14 16:41:31 d2.utils.events]: \u001b[0m eta: 1 day, 12:26:28  iter: 3619  total_loss: 0.5031  loss_cls: 0.3085  loss_box_reg: 0.1861  loss_query: 0.03175  time: 2.2853  data_time: 0.0068  lr: 0.006  max_mem: 10549M\n",
            "\u001b[32m[04/14 16:42:19 d2.utils.events]: \u001b[0m eta: 1 day, 12:26:07  iter: 3639  total_loss: 0.9159  loss_cls: 0.5348  loss_box_reg: 0.3361  loss_query: 0.03148  time: 2.2855  data_time: 0.0076  lr: 0.006  max_mem: 10549M\n",
            "\u001b[32m[04/14 16:43:06 d2.utils.events]: \u001b[0m eta: 1 day, 12:25:21  iter: 3659  total_loss: 0.5954  loss_cls: 0.3902  loss_box_reg: 0.1698  loss_query: 0.04261  time: 2.2856  data_time: 0.0068  lr: 0.006  max_mem: 10549M\n",
            "\u001b[32m[04/14 16:43:53 d2.utils.events]: \u001b[0m eta: 1 day, 12:24:34  iter: 3679  total_loss: 0.5913  loss_cls: 0.3616  loss_box_reg: 0.1965  loss_query: 0.02038  time: 2.2856  data_time: 0.0068  lr: 0.006  max_mem: 10549M\n",
            "\u001b[32m[04/14 16:44:40 d2.utils.events]: \u001b[0m eta: 1 day, 12:24:03  iter: 3699  total_loss: 0.8891  loss_cls: 0.5337  loss_box_reg: 0.2872  loss_query: 0.03631  time: 2.2856  data_time: 0.0088  lr: 0.006  max_mem: 10549M\n",
            "\u001b[32m[04/14 16:45:27 d2.utils.events]: \u001b[0m eta: 1 day, 12:23:59  iter: 3719  total_loss: 0.6104  loss_cls: 0.368  loss_box_reg: 0.1988  loss_query: 0.02572  time: 2.2857  data_time: 0.0073  lr: 0.006  max_mem: 10549M\n",
            "\u001b[32m[04/14 16:46:13 d2.utils.events]: \u001b[0m eta: 1 day, 12:21:56  iter: 3739  total_loss: 0.6956  loss_cls: 0.4238  loss_box_reg: 0.2352  loss_query: 0.03289  time: 2.2854  data_time: 0.0058  lr: 0.006  max_mem: 10549M\n",
            "\u001b[32m[04/14 16:47:00 d2.utils.events]: \u001b[0m eta: 1 day, 12:20:40  iter: 3759  total_loss: 0.8259  loss_cls: 0.4753  loss_box_reg: 0.3186  loss_query: 0.04211  time: 2.2854  data_time: 0.0069  lr: 0.006  max_mem: 10549M\n",
            "\u001b[32m[04/14 16:47:47 d2.utils.events]: \u001b[0m eta: 1 day, 12:19:54  iter: 3779  total_loss: 0.7219  loss_cls: 0.4316  loss_box_reg: 0.2441  loss_query: 0.04865  time: 2.2854  data_time: 0.0071  lr: 0.006  max_mem: 10549M\n",
            "\u001b[32m[04/14 16:48:37 d2.utils.events]: \u001b[0m eta: 1 day, 12:19:49  iter: 3799  total_loss: 0.8064  loss_cls: 0.5087  loss_box_reg: 0.2452  loss_query: 0.03369  time: 2.2860  data_time: 0.0078  lr: 0.006  max_mem: 10549M\n",
            "\u001b[32m[04/14 16:49:24 d2.utils.events]: \u001b[0m eta: 1 day, 12:19:02  iter: 3819  total_loss: 0.6521  loss_cls: 0.3891  loss_box_reg: 0.2253  loss_query: 0.02571  time: 2.2861  data_time: 0.0069  lr: 0.006  max_mem: 10549M\n",
            "\u001b[32m[04/14 16:50:12 d2.utils.events]: \u001b[0m eta: 1 day, 12:18:16  iter: 3839  total_loss: 0.6221  loss_cls: 0.3629  loss_box_reg: 0.2158  loss_query: 0.01841  time: 2.2862  data_time: 0.0093  lr: 0.006  max_mem: 10549M\n",
            "\u001b[32m[04/14 16:50:59 d2.utils.events]: \u001b[0m eta: 1 day, 12:17:35  iter: 3859  total_loss: 0.7496  loss_cls: 0.4945  loss_box_reg: 0.2248  loss_query: 0.02363  time: 2.2861  data_time: 0.0078  lr: 0.006  max_mem: 10549M\n",
            "\u001b[32m[04/14 16:52:05 d2.utils.events]: \u001b[0m eta: 1 day, 12:18:00  iter: 3879  total_loss: 0.8039  loss_cls: 0.4677  loss_box_reg: 0.288  loss_query: 0.05174  time: 2.2909  data_time: 0.8688  lr: 0.006  max_mem: 10549M\n",
            "\u001b[32m[04/14 16:52:51 d2.utils.events]: \u001b[0m eta: 1 day, 12:18:24  iter: 3899  total_loss: 0.7413  loss_cls: 0.4807  loss_box_reg: 0.2385  loss_query: 0.03173  time: 2.2905  data_time: 0.0081  lr: 0.006  max_mem: 10549M\n",
            "\u001b[32m[04/14 16:53:38 d2.utils.events]: \u001b[0m eta: 1 day, 12:17:44  iter: 3919  total_loss: 0.6786  loss_cls: 0.4066  loss_box_reg: 0.2482  loss_query: 0.03208  time: 2.2905  data_time: 0.0066  lr: 0.006  max_mem: 10549M\n",
            "\u001b[32m[04/14 16:54:26 d2.utils.events]: \u001b[0m eta: 1 day, 12:17:11  iter: 3939  total_loss: 0.6118  loss_cls: 0.3714  loss_box_reg: 0.1801  loss_query: 0.0329  time: 2.2906  data_time: 0.0070  lr: 0.006  max_mem: 10549M\n",
            "\u001b[32m[04/14 16:55:14 d2.utils.events]: \u001b[0m eta: 1 day, 12:15:16  iter: 3959  total_loss: 0.7274  loss_cls: 0.4486  loss_box_reg: 0.2399  loss_query: 0.03611  time: 2.2906  data_time: 0.0076  lr: 0.006  max_mem: 10549M\n",
            "\u001b[32m[04/14 16:56:00 d2.utils.events]: \u001b[0m eta: 1 day, 12:13:44  iter: 3979  total_loss: 0.743  loss_cls: 0.4364  loss_box_reg: 0.2613  loss_query: 0.02999  time: 2.2904  data_time: 0.0070  lr: 0.006  max_mem: 10549M\n",
            "\u001b[32m[04/14 16:56:46 fvcore.common.checkpoint]: \u001b[0mSaving checkpoint to work_dirs/visdrone_querydet/model_0003999.pth\n",
            "\u001b[32m[04/14 16:57:04 d2.utils.events]: \u001b[0m eta: 1 day, 12:11:43  iter: 3999  total_loss: 0.6743  loss_cls: 0.4165  loss_box_reg: 0.2575  loss_query: 0.02648  time: 2.2899  data_time: 0.0071  lr: 0.006  max_mem: 10549M\n",
            "\u001b[32m[04/14 16:57:52 d2.utils.events]: \u001b[0m eta: 1 day, 12:11:11  iter: 4019  total_loss: 0.7806  loss_cls: 0.495  loss_box_reg: 0.2542  loss_query: 0.03979  time: 2.2901  data_time: 0.0063  lr: 0.006  max_mem: 10549M\n",
            "\u001b[32m[04/14 16:58:38 d2.utils.events]: \u001b[0m eta: 1 day, 12:09:48  iter: 4039  total_loss: 0.5848  loss_cls: 0.3794  loss_box_reg: 0.2154  loss_query: 0.02288  time: 2.2897  data_time: 0.0090  lr: 0.006  max_mem: 10549M\n",
            "\u001b[32m[04/14 16:59:26 d2.utils.events]: \u001b[0m eta: 1 day, 12:08:50  iter: 4059  total_loss: 0.7987  loss_cls: 0.5105  loss_box_reg: 0.258  loss_query: 0.02951  time: 2.2899  data_time: 0.0059  lr: 0.006  max_mem: 10549M\n",
            "\u001b[32m[04/14 17:00:14 d2.utils.events]: \u001b[0m eta: 1 day, 12:08:17  iter: 4079  total_loss: 0.8091  loss_cls: 0.5165  loss_box_reg: 0.2457  loss_query: 0.03922  time: 2.2898  data_time: 0.0068  lr: 0.006  max_mem: 10549M\n",
            "\u001b[32m[04/14 17:01:01 d2.utils.events]: \u001b[0m eta: 1 day, 12:07:38  iter: 4099  total_loss: 0.7387  loss_cls: 0.4527  loss_box_reg: 0.2616  loss_query: 0.03214  time: 2.2897  data_time: 0.0063  lr: 0.006  max_mem: 10549M\n",
            "\u001b[32m[04/14 17:01:49 d2.utils.events]: \u001b[0m eta: 1 day, 12:07:12  iter: 4119  total_loss: 0.9316  loss_cls: 0.554  loss_box_reg: 0.2986  loss_query: 0.04027  time: 2.2898  data_time: 0.0079  lr: 0.006  max_mem: 10549M\n",
            "\u001b[32m[04/14 17:02:41 d2.utils.events]: \u001b[0m eta: 1 day, 12:06:35  iter: 4139  total_loss: 0.5753  loss_cls: 0.362  loss_box_reg: 0.1957  loss_query: 0.03683  time: 2.2900  data_time: 0.0088  lr: 0.006  max_mem: 10549M\n",
            "\u001b[32m[04/14 17:03:28 d2.utils.events]: \u001b[0m eta: 1 day, 12:06:17  iter: 4159  total_loss: 0.6003  loss_cls: 0.3594  loss_box_reg: 0.2046  loss_query: 0.02808  time: 2.2900  data_time: 0.0068  lr: 0.006  max_mem: 10549M\n",
            "\u001b[32m[04/14 17:04:14 d2.utils.events]: \u001b[0m eta: 1 day, 12:04:32  iter: 4179  total_loss: 0.713  loss_cls: 0.4219  loss_box_reg: 0.271  loss_query: 0.0392  time: 2.2896  data_time: 0.0086  lr: 0.006  max_mem: 10549M\n",
            "\u001b[32m[04/14 17:05:02 d2.utils.events]: \u001b[0m eta: 1 day, 12:03:33  iter: 4199  total_loss: 0.5992  loss_cls: 0.3624  loss_box_reg: 0.2531  loss_query: 0.02785  time: 2.2895  data_time: 0.0067  lr: 0.006  max_mem: 10549M\n",
            "\u001b[32m[04/14 17:05:49 d2.utils.events]: \u001b[0m eta: 1 day, 12:02:24  iter: 4219  total_loss: 0.5179  loss_cls: 0.3205  loss_box_reg: 0.1764  loss_query: 0.02162  time: 2.2896  data_time: 0.0066  lr: 0.006  max_mem: 10549M\n",
            "\u001b[32m[04/14 17:06:36 d2.utils.events]: \u001b[0m eta: 1 day, 12:01:47  iter: 4239  total_loss: 0.8084  loss_cls: 0.4793  loss_box_reg: 0.2765  loss_query: 0.04009  time: 2.2893  data_time: 0.0078  lr: 0.006  max_mem: 10549M\n",
            "\u001b[32m[04/14 17:07:22 d2.utils.events]: \u001b[0m eta: 1 day, 11:59:53  iter: 4259  total_loss: 0.6227  loss_cls: 0.3959  loss_box_reg: 0.1834  loss_query: 0.04103  time: 2.2891  data_time: 0.0075  lr: 0.006  max_mem: 10549M\n",
            "\u001b[32m[04/14 17:08:11 d2.utils.events]: \u001b[0m eta: 1 day, 12:00:27  iter: 4279  total_loss: 0.6386  loss_cls: 0.3758  loss_box_reg: 0.2149  loss_query: 0.0356  time: 2.2892  data_time: 0.0072  lr: 0.006  max_mem: 10549M\n",
            "\u001b[32m[04/14 17:08:59 d2.utils.events]: \u001b[0m eta: 1 day, 11:59:39  iter: 4299  total_loss: 0.7651  loss_cls: 0.4334  loss_box_reg: 0.2827  loss_query: 0.04455  time: 2.2895  data_time: 0.0066  lr: 0.006  max_mem: 10549M\n",
            "\u001b[32m[04/14 17:10:04 d2.utils.events]: \u001b[0m eta: 1 day, 11:58:41  iter: 4319  total_loss: 0.6377  loss_cls: 0.372  loss_box_reg: 0.2357  loss_query: 0.02769  time: 2.2934  data_time: 0.8578  lr: 0.006  max_mem: 10549M\n",
            "\u001b[32m[04/14 17:10:51 d2.utils.events]: \u001b[0m eta: 1 day, 11:57:23  iter: 4339  total_loss: 0.7974  loss_cls: 0.5051  loss_box_reg: 0.2922  loss_query: 0.03692  time: 2.2934  data_time: 0.0082  lr: 0.006  max_mem: 10549M\n",
            "\u001b[32m[04/14 17:11:39 d2.utils.events]: \u001b[0m eta: 1 day, 11:55:07  iter: 4359  total_loss: 0.8185  loss_cls: 0.4913  loss_box_reg: 0.2707  loss_query: 0.03598  time: 2.2935  data_time: 0.0083  lr: 0.006  max_mem: 10549M\n",
            "\u001b[32m[04/14 17:12:27 d2.utils.events]: \u001b[0m eta: 1 day, 11:54:08  iter: 4379  total_loss: 0.5915  loss_cls: 0.3675  loss_box_reg: 0.2158  loss_query: 0.02519  time: 2.2935  data_time: 0.0067  lr: 0.006  max_mem: 10549M\n",
            "\u001b[32m[04/14 17:13:16 d2.utils.events]: \u001b[0m eta: 1 day, 11:53:15  iter: 4399  total_loss: 0.7494  loss_cls: 0.4883  loss_box_reg: 0.2646  loss_query: 0.04317  time: 2.2937  data_time: 0.0068  lr: 0.006  max_mem: 10549M\n",
            "\u001b[32m[04/14 17:14:03 d2.utils.events]: \u001b[0m eta: 1 day, 11:52:48  iter: 4419  total_loss: 0.688  loss_cls: 0.3896  loss_box_reg: 0.2471  loss_query: 0.03398  time: 2.2937  data_time: 0.0060  lr: 0.006  max_mem: 10549M\n",
            "\u001b[32m[04/14 17:14:49 d2.utils.events]: \u001b[0m eta: 1 day, 11:52:22  iter: 4439  total_loss: 0.7534  loss_cls: 0.4242  loss_box_reg: 0.2662  loss_query: 0.03644  time: 2.2933  data_time: 0.0073  lr: 0.006  max_mem: 10549M\n",
            "\u001b[32m[04/14 17:15:38 d2.utils.events]: \u001b[0m eta: 1 day, 11:52:06  iter: 4459  total_loss: 0.7503  loss_cls: 0.4179  loss_box_reg: 0.2328  loss_query: 0.04135  time: 2.2936  data_time: 0.0069  lr: 0.006  max_mem: 10549M\n",
            "\u001b[32m[04/14 17:16:26 d2.utils.events]: \u001b[0m eta: 1 day, 11:50:49  iter: 4479  total_loss: 0.4672  loss_cls: 0.2678  loss_box_reg: 0.1649  loss_query: 0.02793  time: 2.2937  data_time: 0.0073  lr: 0.006  max_mem: 10549M\n",
            "\u001b[32m[04/14 17:18:00 d2.utils.events]: \u001b[0m eta: 1 day, 11:51:07  iter: 4519  total_loss: 0.7838  loss_cls: 0.4404  loss_box_reg: 0.293  loss_query: 0.03221  time: 2.2934  data_time: 0.0071  lr: 0.006  max_mem: 10549M\n",
            "\u001b[32m[04/14 17:18:49 d2.utils.events]: \u001b[0m eta: 1 day, 11:50:25  iter: 4539  total_loss: 0.6744  loss_cls: 0.4091  loss_box_reg: 0.2455  loss_query: 0.03165  time: 2.2937  data_time: 0.0069  lr: 0.006  max_mem: 10549M\n",
            "\u001b[32m[04/14 17:19:36 d2.utils.events]: \u001b[0m eta: 1 day, 11:48:50  iter: 4559  total_loss: 0.6811  loss_cls: 0.4397  loss_box_reg: 0.2063  loss_query: 0.02861  time: 2.2936  data_time: 0.0056  lr: 0.006  max_mem: 10549M\n",
            "\u001b[32m[04/14 17:20:24 d2.utils.events]: \u001b[0m eta: 1 day, 11:46:20  iter: 4579  total_loss: 0.7291  loss_cls: 0.4165  loss_box_reg: 0.2683  loss_query: 0.02672  time: 2.2935  data_time: 0.0081  lr: 0.006  max_mem: 10549M\n",
            "\u001b[32m[04/14 17:21:09 d2.utils.events]: \u001b[0m eta: 1 day, 11:44:57  iter: 4599  total_loss: 0.7737  loss_cls: 0.4549  loss_box_reg: 0.2738  loss_query: 0.0233  time: 2.2932  data_time: 0.0068  lr: 0.006  max_mem: 10549M\n",
            "\u001b[32m[04/14 17:21:57 d2.utils.events]: \u001b[0m eta: 1 day, 11:43:09  iter: 4619  total_loss: 0.6326  loss_cls: 0.3693  loss_box_reg: 0.2413  loss_query: 0.02505  time: 2.2933  data_time: 0.0080  lr: 0.006  max_mem: 10549M\n",
            "\u001b[32m[04/14 17:22:46 d2.utils.events]: \u001b[0m eta: 1 day, 11:41:13  iter: 4639  total_loss: 0.8213  loss_cls: 0.5104  loss_box_reg: 0.3059  loss_query: 0.03664  time: 2.2933  data_time: 0.0068  lr: 0.006  max_mem: 10549M\n",
            "\u001b[32m[04/14 17:23:32 d2.utils.events]: \u001b[0m eta: 1 day, 11:39:30  iter: 4659  total_loss: 0.6213  loss_cls: 0.3879  loss_box_reg: 0.2071  loss_query: 0.02785  time: 2.2930  data_time: 0.0069  lr: 0.006  max_mem: 10549M\n",
            "\u001b[32m[04/14 17:24:19 d2.utils.events]: \u001b[0m eta: 1 day, 11:37:07  iter: 4679  total_loss: 0.5573  loss_cls: 0.3602  loss_box_reg: 0.1622  loss_query: 0.02319  time: 2.2927  data_time: 0.0062  lr: 0.006  max_mem: 10549M\n",
            "\u001b[32m[04/14 17:25:07 d2.utils.events]: \u001b[0m eta: 1 day, 11:35:43  iter: 4699  total_loss: 0.9246  loss_cls: 0.5146  loss_box_reg: 0.3485  loss_query: 0.03827  time: 2.2928  data_time: 0.0071  lr: 0.006  max_mem: 10549M\n",
            "\u001b[32m[04/14 17:25:54 d2.utils.events]: \u001b[0m eta: 1 day, 11:34:16  iter: 4719  total_loss: 0.6152  loss_cls: 0.3518  loss_box_reg: 0.1988  loss_query: 0.03082  time: 2.2927  data_time: 0.0063  lr: 0.006  max_mem: 10549M\n",
            "\u001b[32m[04/14 17:26:42 d2.utils.events]: \u001b[0m eta: 1 day, 11:35:24  iter: 4739  total_loss: 0.9129  loss_cls: 0.5593  loss_box_reg: 0.3177  loss_query: 0.04157  time: 2.2929  data_time: 0.0067  lr: 0.006  max_mem: 10549M\n",
            "\u001b[32m[04/14 17:27:30 d2.utils.events]: \u001b[0m eta: 1 day, 11:34:38  iter: 4759  total_loss: 0.6411  loss_cls: 0.3988  loss_box_reg: 0.2043  loss_query: 0.02614  time: 2.2930  data_time: 0.0063  lr: 0.006  max_mem: 10549M\n",
            "\u001b[32m[04/14 17:28:18 d2.utils.events]: \u001b[0m eta: 1 day, 11:33:44  iter: 4779  total_loss: 0.5114  loss_cls: 0.3269  loss_box_reg: 0.1717  loss_query: 0.02709  time: 2.2929  data_time: 0.0063  lr: 0.006  max_mem: 10549M\n",
            "\u001b[32m[04/14 17:29:07 d2.utils.events]: \u001b[0m eta: 1 day, 11:31:52  iter: 4799  total_loss: 0.7006  loss_cls: 0.4351  loss_box_reg: 0.2352  loss_query: 0.03844  time: 2.2929  data_time: 0.0065  lr: 0.006  max_mem: 10549M\n",
            "\u001b[32m[04/14 17:29:55 d2.utils.events]: \u001b[0m eta: 1 day, 11:31:58  iter: 4819  total_loss: 1.018  loss_cls: 0.5635  loss_box_reg: 0.3625  loss_query: 0.05241  time: 2.2931  data_time: 0.0066  lr: 0.006  max_mem: 10549M\n",
            "\u001b[32m[04/14 17:30:43 d2.utils.events]: \u001b[0m eta: 1 day, 11:30:19  iter: 4839  total_loss: 0.4703  loss_cls: 0.2754  loss_box_reg: 0.1406  loss_query: 0.03045  time: 2.2933  data_time: 0.0084  lr: 0.006  max_mem: 10549M\n",
            "\u001b[32m[04/14 17:31:30 d2.utils.events]: \u001b[0m eta: 1 day, 11:29:33  iter: 4859  total_loss: 0.8562  loss_cls: 0.5395  loss_box_reg: 0.2935  loss_query: 0.0265  time: 2.2931  data_time: 0.0084  lr: 0.006  max_mem: 10549M\n",
            "\u001b[32m[04/14 17:32:18 d2.utils.events]: \u001b[0m eta: 1 day, 11:27:05  iter: 4879  total_loss: 0.5643  loss_cls: 0.3718  loss_box_reg: 0.1743  loss_query: 0.02765  time: 2.2930  data_time: 0.0074  lr: 0.006  max_mem: 10549M\n",
            "\u001b[32m[04/14 17:33:08 d2.utils.events]: \u001b[0m eta: 1 day, 11:26:46  iter: 4899  total_loss: 0.9545  loss_cls: 0.5362  loss_box_reg: 0.3147  loss_query: 0.04994  time: 2.2932  data_time: 0.0078  lr: 0.006  max_mem: 10549M\n",
            "\u001b[32m[04/14 17:33:56 d2.utils.events]: \u001b[0m eta: 1 day, 11:26:47  iter: 4919  total_loss: 0.7522  loss_cls: 0.4405  loss_box_reg: 0.2359  loss_query: 0.04417  time: 2.2934  data_time: 0.0084  lr: 0.006  max_mem: 10549M\n",
            "\u001b[32m[04/14 17:34:45 d2.utils.events]: \u001b[0m eta: 1 day, 11:26:27  iter: 4939  total_loss: 0.8714  loss_cls: 0.5009  loss_box_reg: 0.296  loss_query: 0.04124  time: 2.2937  data_time: 0.0063  lr: 0.006  max_mem: 10549M\n",
            "\u001b[32m[04/14 17:35:32 d2.utils.events]: \u001b[0m eta: 1 day, 11:25:41  iter: 4959  total_loss: 0.5247  loss_cls: 0.3029  loss_box_reg: 0.2104  loss_query: 0.02992  time: 2.2935  data_time: 0.0065  lr: 0.006  max_mem: 10549M\n",
            "\u001b[32m[04/14 17:36:19 d2.utils.events]: \u001b[0m eta: 1 day, 11:22:58  iter: 4979  total_loss: 0.7868  loss_cls: 0.4697  loss_box_reg: 0.2533  loss_query: 0.02867  time: 2.2934  data_time: 0.0062  lr: 0.006  max_mem: 10549M\n",
            "\u001b[32m[04/14 17:37:06 d2.utils.events]: \u001b[0m eta: 1 day, 11:24:08  iter: 4999  total_loss: 0.7524  loss_cls: 0.4208  loss_box_reg: 0.2906  loss_query: 0.03708  time: 2.2934  data_time: 0.0075  lr: 0.006  max_mem: 10549M\n",
            "\u001b[32m[04/14 17:37:53 d2.utils.events]: \u001b[0m eta: 1 day, 11:21:21  iter: 5019  total_loss: 0.8716  loss_cls: 0.5382  loss_box_reg: 0.308  loss_query: 0.03974  time: 2.2933  data_time: 0.0073  lr: 0.006  max_mem: 10549M\n",
            "\u001b[32m[04/14 17:38:40 d2.utils.events]: \u001b[0m eta: 1 day, 11:19:37  iter: 5039  total_loss: 0.6868  loss_cls: 0.413  loss_box_reg: 0.2553  loss_query: 0.02705  time: 2.2932  data_time: 0.0070  lr: 0.006  max_mem: 10549M\n",
            "\u001b[32m[04/14 17:39:27 d2.utils.events]: \u001b[0m eta: 1 day, 11:18:51  iter: 5059  total_loss: 0.7711  loss_cls: 0.4462  loss_box_reg: 0.2706  loss_query: 0.03228  time: 2.2931  data_time: 0.0067  lr: 0.006  max_mem: 10549M\n",
            "\u001b[32m[04/14 17:40:15 d2.utils.events]: \u001b[0m eta: 1 day, 11:17:54  iter: 5079  total_loss: 0.6539  loss_cls: 0.4116  loss_box_reg: 0.2223  loss_query: 0.0285  time: 2.2931  data_time: 0.0090  lr: 0.006  max_mem: 10549M\n",
            "\u001b[32m[04/14 17:41:04 d2.utils.events]: \u001b[0m eta: 1 day, 11:17:06  iter: 5099  total_loss: 0.5668  loss_cls: 0.3744  loss_box_reg: 0.2174  loss_query: 0.03828  time: 2.2933  data_time: 0.0075  lr: 0.006  max_mem: 10549M\n",
            "\u001b[32m[04/14 17:41:52 d2.utils.events]: \u001b[0m eta: 1 day, 11:16:18  iter: 5119  total_loss: 0.7952  loss_cls: 0.4234  loss_box_reg: 0.305  loss_query: 0.04256  time: 2.2934  data_time: 0.0078  lr: 0.006  max_mem: 10549M\n",
            "\u001b[32m[04/14 17:42:38 d2.utils.events]: \u001b[0m eta: 1 day, 11:15:02  iter: 5139  total_loss: 0.5303  loss_cls: 0.319  loss_box_reg: 0.1801  loss_query: 0.0402  time: 2.2932  data_time: 0.0071  lr: 0.006  max_mem: 10549M\n",
            "\u001b[32m[04/14 17:43:25 d2.utils.events]: \u001b[0m eta: 1 day, 11:13:05  iter: 5159  total_loss: 0.5503  loss_cls: 0.3463  loss_box_reg: 0.2055  loss_query: 0.03883  time: 2.2931  data_time: 0.0063  lr: 0.006  max_mem: 10549M\n",
            "\u001b[32m[04/14 17:44:14 d2.utils.events]: \u001b[0m eta: 1 day, 11:13:53  iter: 5179  total_loss: 0.6241  loss_cls: 0.3516  loss_box_reg: 0.2467  loss_query: 0.03942  time: 2.2932  data_time: 0.0073  lr: 0.006  max_mem: 10549M\n",
            "\u001b[32m[04/14 17:45:02 d2.utils.events]: \u001b[0m eta: 1 day, 11:12:44  iter: 5199  total_loss: 0.6112  loss_cls: 0.3543  loss_box_reg: 0.2227  loss_query: 0.03153  time: 2.2933  data_time: 0.0072  lr: 0.006  max_mem: 10549M\n",
            "\u001b[32m[04/14 17:45:49 d2.utils.events]: \u001b[0m eta: 1 day, 11:12:17  iter: 5219  total_loss: 0.7032  loss_cls: 0.4132  loss_box_reg: 0.243  loss_query: 0.03279  time: 2.2932  data_time: 0.0074  lr: 0.006  max_mem: 10549M\n",
            "\u001b[32m[04/14 17:46:37 d2.utils.events]: \u001b[0m eta: 1 day, 11:10:25  iter: 5239  total_loss: 0.6616  loss_cls: 0.4084  loss_box_reg: 0.211  loss_query: 0.02468  time: 2.2932  data_time: 0.0065  lr: 0.006  max_mem: 10549M\n",
            "\u001b[32m[04/14 17:47:22 d2.utils.events]: \u001b[0m eta: 1 day, 11:08:39  iter: 5259  total_loss: 0.6174  loss_cls: 0.3888  loss_box_reg: 0.2031  loss_query: 0.02593  time: 2.2928  data_time: 0.0069  lr: 0.006  max_mem: 10549M\n",
            "\u001b[32m[04/14 17:48:09 d2.utils.events]: \u001b[0m eta: 1 day, 11:07:38  iter: 5279  total_loss: 0.8729  loss_cls: 0.5274  loss_box_reg: 0.333  loss_query: 0.02278  time: 2.2927  data_time: 0.0081  lr: 0.006  max_mem: 10549M\n",
            "\u001b[32m[04/14 17:48:56 d2.utils.events]: \u001b[0m eta: 1 day, 11:06:38  iter: 5299  total_loss: 0.5977  loss_cls: 0.3575  loss_box_reg: 0.193  loss_query: 0.03313  time: 2.2925  data_time: 0.0069  lr: 0.006  max_mem: 10549M\n",
            "\u001b[32m[04/14 17:49:44 d2.utils.events]: \u001b[0m eta: 1 day, 11:06:13  iter: 5319  total_loss: 0.629  loss_cls: 0.3604  loss_box_reg: 0.2434  loss_query: 0.02736  time: 2.2926  data_time: 0.0080  lr: 0.006  max_mem: 10549M\n",
            "\u001b[32m[04/14 17:50:31 d2.utils.events]: \u001b[0m eta: 1 day, 11:05:27  iter: 5339  total_loss: 0.8955  loss_cls: 0.518  loss_box_reg: 0.2917  loss_query: 0.0348  time: 2.2921  data_time: 0.0072  lr: 0.006  max_mem: 10549M\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "SsqPl7lYEqkD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!python train_visdrone.py --resume --config /content/gdrive/MyDrive/QueryDet-PyTorch/work_dirs/visdrone_querydet/config.yaml --num-gpu 1 OUTPUT_DIR work_dirs/visdrone_querydet"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TiKmAcRQ-Mz6",
        "outputId": "183e2413-2c1a-4bb0-e2ac-8a40f6231d9d",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "** fvcore version of PathManager will be deprecated soon. **\n",
            "** Please migrate to the version in iopath repo. **\n",
            "https://github.com/facebookresearch/iopath \n",
            "\n",
            "Command Line Args: Namespace(config_file='/content/gdrive/MyDrive/QueryDet-PyTorch/work_dirs/visdrone_querydet/config.yaml', dist_url='tcp://127.0.0.1:49152', eval_only=False, machine_rank=0, no_pretrain=False, num_gpus=1, num_machines=1, opts=['OUTPUT_DIR', 'work_dirs/visdrone_querydet'], resume=True)\n",
            "\u001b[32m[04/14 18:33:10 detectron2]: \u001b[0mRank of current process: 0. World size: 1\n",
            "\u001b[32m[04/14 18:33:16 detectron2]: \u001b[0mEnvironment info:\n",
            "----------------------  ---------------------------------------------------------------\n",
            "sys.platform            linux\n",
            "Python                  3.7.6 (default, Jan  8 2020, 19:59:22) [GCC 7.3.0]\n",
            "numpy                   1.21.6\n",
            "detectron2              0.4 @/usr/local/lib/python3.7/site-packages/detectron2\n",
            "Compiler                GCC 7.3\n",
            "CUDA compiler           CUDA 10.1\n",
            "detectron2 arch flags   3.7, 5.0, 5.2, 6.0, 6.1, 7.0, 7.5\n",
            "DETECTRON2_ENV_MODULE   <not set>\n",
            "PyTorch                 1.8.1+cu101 @/usr/local/lib/python3.7/site-packages/torch\n",
            "PyTorch debug build     False\n",
            "GPU available           True\n",
            "GPU 0                   Tesla T4 (arch=7.5)\n",
            "CUDA_HOME               /usr/local/cuda\n",
            "Pillow                  9.5.0\n",
            "torchvision             0.9.1+cu101 @/usr/local/lib/python3.7/site-packages/torchvision\n",
            "torchvision arch flags  3.5, 5.0, 6.0, 7.0, 7.5\n",
            "fvcore                  0.1.3.post20210317\n",
            "cv2                     4.7.0\n",
            "----------------------  ---------------------------------------------------------------\n",
            "PyTorch built with:\n",
            "  - GCC 7.3\n",
            "  - C++ Version: 201402\n",
            "  - Intel(R) Math Kernel Library Version 2020.0.0 Product Build 20191122 for Intel(R) 64 architecture applications\n",
            "  - Intel(R) MKL-DNN v1.7.0 (Git Hash 7aed236906b1f7a05c0917e5257a1af05e9ff683)\n",
            "  - OpenMP 201511 (a.k.a. OpenMP 4.5)\n",
            "  - NNPACK is enabled\n",
            "  - CPU capability usage: AVX2\n",
            "  - CUDA Runtime 10.1\n",
            "  - NVCC architecture flags: -gencode;arch=compute_37,code=sm_37;-gencode;arch=compute_50,code=sm_50;-gencode;arch=compute_60,code=sm_60;-gencode;arch=compute_70,code=sm_70\n",
            "  - CuDNN 7.6.3\n",
            "  - Magma 2.5.2\n",
            "  - Build settings: BLAS_INFO=mkl, BUILD_TYPE=Release, CUDA_VERSION=10.1, CUDNN_VERSION=7.6.3, CXX_COMPILER=/opt/rh/devtoolset-7/root/usr/bin/c++, CXX_FLAGS= -Wno-deprecated -fvisibility-inlines-hidden -DUSE_PTHREADPOOL -fopenmp -DNDEBUG -DUSE_KINETO -DUSE_FBGEMM -DUSE_QNNPACK -DUSE_PYTORCH_QNNPACK -DUSE_XNNPACK -O2 -fPIC -Wno-narrowing -Wall -Wextra -Werror=return-type -Wno-missing-field-initializers -Wno-type-limits -Wno-array-bounds -Wno-unknown-pragmas -Wno-sign-compare -Wno-unused-parameter -Wno-unused-variable -Wno-unused-function -Wno-unused-result -Wno-unused-local-typedefs -Wno-strict-overflow -Wno-strict-aliasing -Wno-error=deprecated-declarations -Wno-stringop-overflow -Wno-psabi -Wno-error=pedantic -Wno-error=redundant-decls -Wno-error=old-style-cast -fdiagnostics-color=always -faligned-new -Wno-unused-but-set-variable -Wno-maybe-uninitialized -fno-math-errno -fno-trapping-math -Werror=format -Wno-stringop-overflow, LAPACK_INFO=mkl, PERF_WITH_AVX=1, PERF_WITH_AVX2=1, PERF_WITH_AVX512=1, TORCH_VERSION=1.8.1, USE_CUDA=ON, USE_CUDNN=ON, USE_EXCEPTION_PTR=1, USE_GFLAGS=OFF, USE_GLOG=OFF, USE_MKL=ON, USE_MKLDNN=ON, USE_MPI=OFF, USE_NCCL=ON, USE_NNPACK=ON, USE_OPENMP=ON, \n",
            "\n",
            "\u001b[32m[04/14 18:33:16 detectron2]: \u001b[0mCommand line arguments: Namespace(config_file='/content/gdrive/MyDrive/QueryDet-PyTorch/work_dirs/visdrone_querydet/config.yaml', dist_url='tcp://127.0.0.1:49152', eval_only=False, machine_rank=0, no_pretrain=False, num_gpus=1, num_machines=1, opts=['OUTPUT_DIR', 'work_dirs/visdrone_querydet'], resume=True)\n",
            "\u001b[32m[04/14 18:33:16 detectron2]: \u001b[0mContents of args.config_file=/content/gdrive/MyDrive/QueryDet-PyTorch/work_dirs/visdrone_querydet/config.yaml:\n",
            "CUDNN_BENCHMARK: false\n",
            "DATALOADER:\n",
            "  ASPECT_RATIO_GROUPING: true\n",
            "  FILTER_EMPTY_ANNOTATIONS: true\n",
            "  NUM_WORKERS: 4\n",
            "  REPEAT_THRESHOLD: 0.0\n",
            "  SAMPLER_TRAIN: TrainingSampler\n",
            "DATASETS:\n",
            "  PRECOMPUTED_PROPOSAL_TOPK_TEST: 1000\n",
            "  PRECOMPUTED_PROPOSAL_TOPK_TRAIN: 2000\n",
            "  PROPOSAL_FILES_TEST: []\n",
            "  PROPOSAL_FILES_TRAIN: []\n",
            "  TEST:\n",
            "  - coco_2017_val\n",
            "  TRAIN:\n",
            "  - coco_2017_train\n",
            "GLOBAL:\n",
            "  HACK: 1.0\n",
            "INPUT:\n",
            "  CROP:\n",
            "    ENABLED: false\n",
            "    SIZE:\n",
            "    - 0.9\n",
            "    - 0.9\n",
            "    TYPE: relative_range\n",
            "  FORMAT: BGR\n",
            "  MASK_FORMAT: polygon\n",
            "  MAX_SIZE_TEST: 1333\n",
            "  MAX_SIZE_TRAIN: 1333\n",
            "  MIN_SIZE_TEST: 800\n",
            "  MIN_SIZE_TRAIN:\n",
            "  - 640\n",
            "  - 672\n",
            "  - 704\n",
            "  - 736\n",
            "  - 768\n",
            "  - 800\n",
            "  MIN_SIZE_TRAIN_SAMPLING: choice\n",
            "  RANDOM_FLIP: horizontal\n",
            "META_INFO:\n",
            "  EVAL_AP: true\n",
            "  EVAL_GPU_TIME: true\n",
            "  VIS_ROOT: ''\n",
            "MODEL:\n",
            "  ANCHOR_GENERATOR:\n",
            "    ANGLES:\n",
            "    - - -90\n",
            "      - 0\n",
            "      - 90\n",
            "    ASPECT_RATIOS:\n",
            "    - - 0.5\n",
            "      - 1.0\n",
            "      - 2.0\n",
            "    NAME: AnchorGeneratorWithCenter\n",
            "    OFFSET: 0.0\n",
            "    SIZES:\n",
            "    - - 16\n",
            "      - 20.15873679831797\n",
            "      - 25.39841683149119\n",
            "    - - 32\n",
            "      - 40.31747359663594\n",
            "      - 50.79683366298238\n",
            "    - - 64\n",
            "      - 80.63494719327188\n",
            "      - 101.59366732596476\n",
            "    - - 128\n",
            "      - 161.26989438654377\n",
            "      - 203.18733465192952\n",
            "    - - 256\n",
            "      - 322.53978877308754\n",
            "      - 406.37466930385904\n",
            "    - - 512\n",
            "      - 645.0795775461751\n",
            "      - 812.7493386077181\n",
            "  BACKBONE:\n",
            "    FREEZE_AT: 2\n",
            "    NAME: build_retinanet_resnet_fpn_backbone\n",
            "  CUSTOM:\n",
            "    CLEAR_CUDA_CACHE: false\n",
            "    CLS_WEIGHTS:\n",
            "    - 1.0\n",
            "    - 1.4\n",
            "    - 1.8\n",
            "    - 2.2\n",
            "    - 2.6\n",
            "    - 2.6\n",
            "    FOCAL_LOSS_ALPHAS:\n",
            "    - 0.25\n",
            "    - 0.25\n",
            "    - 0.25\n",
            "    - 0.25\n",
            "    - 0.25\n",
            "    - 0.25\n",
            "    FOCAL_LOSS_GAMMAS:\n",
            "    - 2.0\n",
            "    - 2.0\n",
            "    - 2.0\n",
            "    - 2.0\n",
            "    - 2.0\n",
            "    - 2.0\n",
            "    GIOU_LOSS: false\n",
            "    GRADIENT_CHECKPOINT: false\n",
            "    HEAD_BN: false\n",
            "    REG_WEIGHTS:\n",
            "    - 1.0\n",
            "    - 1.4\n",
            "    - 1.8\n",
            "    - 2.2\n",
            "    - 2.6\n",
            "    - 2.6\n",
            "    SOFT_NMS_METHOD: linear\n",
            "    SOFT_NMS_PRUND: 0.001\n",
            "    SOFT_NMS_SIGMA: 0.5\n",
            "    SOFT_NMS_THRESHOLD: 0.5\n",
            "    USE_LOOP_MATCHER: true\n",
            "    USE_SOFT_NMS: false\n",
            "  DEVICE: cuda\n",
            "  FPN:\n",
            "    FUSE_TYPE: sum\n",
            "    IN_FEATURES:\n",
            "    - res2\n",
            "    - res3\n",
            "    - res4\n",
            "    - res5\n",
            "    NORM: ''\n",
            "    OUT_CHANNELS: 256\n",
            "    TOP_LEVELS: 2\n",
            "  KEYPOINT_ON: false\n",
            "  LOAD_PROPOSALS: false\n",
            "  MASK_ON: false\n",
            "  META_ARCHITECTURE: RetinaNetQueryDet\n",
            "  PANOPTIC_FPN:\n",
            "    COMBINE:\n",
            "      ENABLED: true\n",
            "      INSTANCES_CONFIDENCE_THRESH: 0.5\n",
            "      OVERLAP_THRESH: 0.5\n",
            "      STUFF_AREA_LIMIT: 4096\n",
            "    INSTANCE_LOSS_WEIGHT: 1.0\n",
            "  PIXEL_MEAN:\n",
            "  - 103.53\n",
            "  - 116.28\n",
            "  - 123.675\n",
            "  PIXEL_STD:\n",
            "  - 1.0\n",
            "  - 1.0\n",
            "  - 1.0\n",
            "  PROPOSAL_GENERATOR:\n",
            "    MIN_SIZE: 0\n",
            "    NAME: RPN\n",
            "  QUERY:\n",
            "    CONTEXT: 2\n",
            "    ENCODE_CENTER_DIS_COEFF:\n",
            "    - 1.0\n",
            "    - 1.0\n",
            "    ENCODE_SMALL_OBJ_SCALE:\n",
            "    - - 0\n",
            "      - 32\n",
            "    - - 0\n",
            "      - 64\n",
            "    FEATURES_VALUE_TEST:\n",
            "    - 0\n",
            "    - 1\n",
            "    FEATURES_VALUE_TRAIN:\n",
            "    - 0\n",
            "    - 1\n",
            "    FEATURES_WHOLE_TEST:\n",
            "    - 2\n",
            "    - 3\n",
            "    - 4\n",
            "    - 5\n",
            "    FEATURES_WHOLE_TRAIN:\n",
            "    - 2\n",
            "    - 3\n",
            "    - 4\n",
            "    - 5\n",
            "    QUERY_INFER: false\n",
            "    QUERY_LOSS_GAMMA:\n",
            "    - 1.3\n",
            "    - 1.3\n",
            "    QUERY_LOSS_WEIGHT:\n",
            "    - 10.0\n",
            "    - 10.0\n",
            "    Q_FEATURE_TEST:\n",
            "    - 1\n",
            "    - 2\n",
            "    Q_FEATURE_TRAIN:\n",
            "    - 1\n",
            "    - 2\n",
            "    THRESHOLD: 0.12\n",
            "  RESNETS:\n",
            "    DEFORM_MODULATED: false\n",
            "    DEFORM_NUM_GROUPS: 1\n",
            "    DEFORM_ON_PER_STAGE:\n",
            "    - false\n",
            "    - false\n",
            "    - false\n",
            "    - false\n",
            "    DEPTH: 50\n",
            "    NORM: FrozenBN\n",
            "    NUM_GROUPS: 1\n",
            "    OUT_FEATURES:\n",
            "    - res2\n",
            "    - res3\n",
            "    - res4\n",
            "    - res5\n",
            "    RES2_OUT_CHANNELS: 256\n",
            "    RES5_DILATION: 1\n",
            "    STEM_OUT_CHANNELS: 64\n",
            "    STRIDE_IN_1X1: true\n",
            "    WIDTH_PER_GROUP: 64\n",
            "  RETINANET:\n",
            "    BBOX_REG_LOSS_TYPE: smooth_l1\n",
            "    BBOX_REG_WEIGHTS:\n",
            "    - 1.0\n",
            "    - 1.0\n",
            "    - 1.0\n",
            "    - 1.0\n",
            "    FOCAL_LOSS_ALPHA: 0.25\n",
            "    FOCAL_LOSS_GAMMA: 2.0\n",
            "    IN_FEATURES:\n",
            "    - p2\n",
            "    - p3\n",
            "    - p4\n",
            "    - p5\n",
            "    - p6\n",
            "    - p7\n",
            "    IOU_LABELS:\n",
            "    - 0\n",
            "    - -1\n",
            "    - 1\n",
            "    IOU_THRESHOLDS:\n",
            "    - 0.4\n",
            "    - 0.5\n",
            "    NMS_THRESH_TEST: 0.5\n",
            "    NORM: ''\n",
            "    NUM_CLASSES: 10\n",
            "    NUM_CONVS: 4\n",
            "    PRIOR_PROB: 0.01\n",
            "    SCORE_THRESH_TEST: 0.05\n",
            "    SMOOTH_L1_LOSS_BETA: 0.1\n",
            "    TOPK_CANDIDATES_TEST: 1000\n",
            "  ROI_BOX_CASCADE_HEAD:\n",
            "    BBOX_REG_WEIGHTS:\n",
            "    - - 10.0\n",
            "      - 10.0\n",
            "      - 5.0\n",
            "      - 5.0\n",
            "    - - 20.0\n",
            "      - 20.0\n",
            "      - 10.0\n",
            "      - 10.0\n",
            "    - - 30.0\n",
            "      - 30.0\n",
            "      - 15.0\n",
            "      - 15.0\n",
            "    IOUS:\n",
            "    - 0.5\n",
            "    - 0.6\n",
            "    - 0.7\n",
            "  ROI_BOX_HEAD:\n",
            "    BBOX_REG_LOSS_TYPE: smooth_l1\n",
            "    BBOX_REG_LOSS_WEIGHT: 1.0\n",
            "    BBOX_REG_WEIGHTS:\n",
            "    - 10.0\n",
            "    - 10.0\n",
            "    - 5.0\n",
            "    - 5.0\n",
            "    CLS_AGNOSTIC_BBOX_REG: false\n",
            "    CONV_DIM: 256\n",
            "    FC_DIM: 1024\n",
            "    NAME: ''\n",
            "    NORM: ''\n",
            "    NUM_CONV: 0\n",
            "    NUM_FC: 0\n",
            "    POOLER_RESOLUTION: 14\n",
            "    POOLER_SAMPLING_RATIO: 0\n",
            "    POOLER_TYPE: ROIAlignV2\n",
            "    SMOOTH_L1_BETA: 0.0\n",
            "    TRAIN_ON_PRED_BOXES: false\n",
            "  ROI_HEADS:\n",
            "    BATCH_SIZE_PER_IMAGE: 512\n",
            "    IN_FEATURES:\n",
            "    - res4\n",
            "    IOU_LABELS:\n",
            "    - 0\n",
            "    - 1\n",
            "    IOU_THRESHOLDS:\n",
            "    - 0.5\n",
            "    NAME: Res5ROIHeads\n",
            "    NMS_THRESH_TEST: 0.5\n",
            "    NUM_CLASSES: 80\n",
            "    POSITIVE_FRACTION: 0.25\n",
            "    PROPOSAL_APPEND_GT: true\n",
            "    SCORE_THRESH_TEST: 0.05\n",
            "  ROI_KEYPOINT_HEAD:\n",
            "    CONV_DIMS:\n",
            "    - 512\n",
            "    - 512\n",
            "    - 512\n",
            "    - 512\n",
            "    - 512\n",
            "    - 512\n",
            "    - 512\n",
            "    - 512\n",
            "    LOSS_WEIGHT: 1.0\n",
            "    MIN_KEYPOINTS_PER_IMAGE: 1\n",
            "    NAME: KRCNNConvDeconvUpsampleHead\n",
            "    NORMALIZE_LOSS_BY_VISIBLE_KEYPOINTS: true\n",
            "    NUM_KEYPOINTS: 17\n",
            "    POOLER_RESOLUTION: 14\n",
            "    POOLER_SAMPLING_RATIO: 0\n",
            "    POOLER_TYPE: ROIAlignV2\n",
            "  ROI_MASK_HEAD:\n",
            "    CLS_AGNOSTIC_MASK: false\n",
            "    CONV_DIM: 256\n",
            "    NAME: MaskRCNNConvUpsampleHead\n",
            "    NORM: ''\n",
            "    NUM_CONV: 0\n",
            "    POOLER_RESOLUTION: 14\n",
            "    POOLER_SAMPLING_RATIO: 0\n",
            "    POOLER_TYPE: ROIAlignV2\n",
            "  RPN:\n",
            "    BATCH_SIZE_PER_IMAGE: 256\n",
            "    BBOX_REG_LOSS_TYPE: smooth_l1\n",
            "    BBOX_REG_LOSS_WEIGHT: 1.0\n",
            "    BBOX_REG_WEIGHTS:\n",
            "    - 1.0\n",
            "    - 1.0\n",
            "    - 1.0\n",
            "    - 1.0\n",
            "    BOUNDARY_THRESH: -1\n",
            "    HEAD_NAME: StandardRPNHead\n",
            "    IN_FEATURES:\n",
            "    - res4\n",
            "    IOU_LABELS:\n",
            "    - 0\n",
            "    - -1\n",
            "    - 1\n",
            "    IOU_THRESHOLDS:\n",
            "    - 0.3\n",
            "    - 0.7\n",
            "    LOSS_WEIGHT: 1.0\n",
            "    NMS_THRESH: 0.7\n",
            "    POSITIVE_FRACTION: 0.5\n",
            "    POST_NMS_TOPK_TEST: 1000\n",
            "    POST_NMS_TOPK_TRAIN: 2000\n",
            "    PRE_NMS_TOPK_TEST: 6000\n",
            "    PRE_NMS_TOPK_TRAIN: 12000\n",
            "    SMOOTH_L1_BETA: 0.0\n",
            "  SEM_SEG_HEAD:\n",
            "    COMMON_STRIDE: 4\n",
            "    CONVS_DIM: 128\n",
            "    IGNORE_VALUE: 255\n",
            "    IN_FEATURES:\n",
            "    - p2\n",
            "    - p3\n",
            "    - p4\n",
            "    - p5\n",
            "    LOSS_WEIGHT: 1.0\n",
            "    NAME: SemSegFPNHead\n",
            "    NORM: GN\n",
            "    NUM_CLASSES: 54\n",
            "  WEIGHTS: /content/gdrive/MyDrive/QueryDet-PyTorch/work_dirs/visdrone_querydet/model_0003999.pth\n",
            "OUTPUT_DIR: work_dirs/visdrone_querydet\n",
            "SEED: -1\n",
            "SOLVER:\n",
            "  AMP:\n",
            "    ENABLED: true\n",
            "  BASE_LR: 0.006\n",
            "  BIAS_LR_FACTOR: 1.0\n",
            "  CHECKPOINT_PERIOD: 2000\n",
            "  CLIP_GRADIENTS:\n",
            "    CLIP_TYPE: value\n",
            "    CLIP_VALUE: 35.0\n",
            "    ENABLED: true\n",
            "    NORM_TYPE: 2.0\n",
            "  GAMMA: 0.1\n",
            "  IMS_PER_BATCH: 3\n",
            "  LR_SCHEDULER_NAME: WarmupMultiStepLR\n",
            "  MAX_ITER: 60000\n",
            "  MOMENTUM: 0.9\n",
            "  NESTEROV: false\n",
            "  REFERENCE_WORLD_SIZE: 0\n",
            "  STEPS:\n",
            "  - 15000\n",
            "  - 20000\n",
            "  WARMUP_FACTOR: 0.001\n",
            "  WARMUP_ITERS: 1000\n",
            "  WARMUP_METHOD: linear\n",
            "  WEIGHT_DECAY: 0.0001\n",
            "  WEIGHT_DECAY_BIAS: 0.0001\n",
            "  WEIGHT_DECAY_NORM: 0.0\n",
            "TEST:\n",
            "  AUG:\n",
            "    ENABLED: false\n",
            "    FLIP: true\n",
            "    MAX_SIZE: 4000\n",
            "    MIN_SIZES:\n",
            "    - 400\n",
            "    - 500\n",
            "    - 600\n",
            "    - 700\n",
            "    - 800\n",
            "    - 900\n",
            "    - 1000\n",
            "    - 1100\n",
            "    - 1200\n",
            "  DETECTIONS_PER_IMAGE: 500\n",
            "  EVAL_PERIOD: 0\n",
            "  EXPECTED_RESULTS: []\n",
            "  KEYPOINT_OKS_SIGMAS: []\n",
            "  PRECISE_BN:\n",
            "    ENABLED: false\n",
            "    NUM_ITER: 200\n",
            "VERSION: 2\n",
            "VISDRONE:\n",
            "  MAX_LENGTH: 1999\n",
            "  SHORT_LENGTH:\n",
            "  - 1200\n",
            "  TEST_IMG_ROOT: data/visdrone/coco_format/val_images\n",
            "  TEST_JSON: data/visdrone/coco_format/annotations/val_label.json\n",
            "  TEST_LENGTH: 3999\n",
            "  TRAIN_JSON: data/visdrone/coco_format/annotations/train_label.json\n",
            "  TRING_IMG_ROOT: data//visdrone/coco_format/train_images\n",
            "VIS_PERIOD: 0\n",
            "\n",
            "\u001b[32m[04/14 18:33:16 detectron2]: \u001b[0mRunning with full config:\n",
            "CUDNN_BENCHMARK: False\n",
            "DATALOADER:\n",
            "  ASPECT_RATIO_GROUPING: True\n",
            "  FILTER_EMPTY_ANNOTATIONS: True\n",
            "  NUM_WORKERS: 4\n",
            "  REPEAT_THRESHOLD: 0.0\n",
            "  SAMPLER_TRAIN: TrainingSampler\n",
            "DATASETS:\n",
            "  PRECOMPUTED_PROPOSAL_TOPK_TEST: 1000\n",
            "  PRECOMPUTED_PROPOSAL_TOPK_TRAIN: 2000\n",
            "  PROPOSAL_FILES_TEST: ()\n",
            "  PROPOSAL_FILES_TRAIN: ()\n",
            "  TEST: ('coco_2017_val',)\n",
            "  TRAIN: ('coco_2017_train',)\n",
            "GLOBAL:\n",
            "  HACK: 1.0\n",
            "INPUT:\n",
            "  CROP:\n",
            "    ENABLED: False\n",
            "    SIZE: [0.9, 0.9]\n",
            "    TYPE: relative_range\n",
            "  FORMAT: BGR\n",
            "  MASK_FORMAT: polygon\n",
            "  MAX_SIZE_TEST: 1333\n",
            "  MAX_SIZE_TRAIN: 1333\n",
            "  MIN_SIZE_TEST: 800\n",
            "  MIN_SIZE_TRAIN: (640, 672, 704, 736, 768, 800)\n",
            "  MIN_SIZE_TRAIN_SAMPLING: choice\n",
            "  RANDOM_FLIP: horizontal\n",
            "META_INFO:\n",
            "  EVAL_AP: True\n",
            "  EVAL_GPU_TIME: True\n",
            "  VIS_ROOT: \n",
            "MODEL:\n",
            "  ANCHOR_GENERATOR:\n",
            "    ANGLES: [[-90, 0, 90]]\n",
            "    ASPECT_RATIOS: [[0.5, 1.0, 2.0]]\n",
            "    NAME: AnchorGeneratorWithCenter\n",
            "    OFFSET: 0.0\n",
            "    SIZES: [[16, 20.15873679831797, 25.39841683149119], [32, 40.31747359663594, 50.79683366298238], [64, 80.63494719327188, 101.59366732596476], [128, 161.26989438654377, 203.18733465192952], [256, 322.53978877308754, 406.37466930385904], [512, 645.0795775461751, 812.7493386077181]]\n",
            "  BACKBONE:\n",
            "    FREEZE_AT: 2\n",
            "    NAME: build_retinanet_resnet_fpn_backbone\n",
            "  CUSTOM:\n",
            "    CLEAR_CUDA_CACHE: False\n",
            "    CLS_WEIGHTS: [1.0, 1.4, 1.8, 2.2, 2.6, 2.6]\n",
            "    FOCAL_LOSS_ALPHAS: [0.25, 0.25, 0.25, 0.25, 0.25, 0.25]\n",
            "    FOCAL_LOSS_GAMMAS: [2.0, 2.0, 2.0, 2.0, 2.0, 2.0]\n",
            "    GIOU_LOSS: False\n",
            "    GRADIENT_CHECKPOINT: False\n",
            "    HEAD_BN: False\n",
            "    REG_WEIGHTS: [1.0, 1.4, 1.8, 2.2, 2.6, 2.6]\n",
            "    SOFT_NMS_METHOD: linear\n",
            "    SOFT_NMS_PRUND: 0.001\n",
            "    SOFT_NMS_SIGMA: 0.5\n",
            "    SOFT_NMS_THRESHOLD: 0.5\n",
            "    USE_LOOP_MATCHER: True\n",
            "    USE_SOFT_NMS: False\n",
            "  DEVICE: cuda\n",
            "  FPN:\n",
            "    FUSE_TYPE: sum\n",
            "    IN_FEATURES: ['res2', 'res3', 'res4', 'res5']\n",
            "    NORM: \n",
            "    OUT_CHANNELS: 256\n",
            "    TOP_LEVELS: 2\n",
            "  KEYPOINT_ON: False\n",
            "  LOAD_PROPOSALS: False\n",
            "  MASK_ON: False\n",
            "  META_ARCHITECTURE: RetinaNetQueryDet\n",
            "  PANOPTIC_FPN:\n",
            "    COMBINE:\n",
            "      ENABLED: True\n",
            "      INSTANCES_CONFIDENCE_THRESH: 0.5\n",
            "      OVERLAP_THRESH: 0.5\n",
            "      STUFF_AREA_LIMIT: 4096\n",
            "    INSTANCE_LOSS_WEIGHT: 1.0\n",
            "  PIXEL_MEAN: [103.53, 116.28, 123.675]\n",
            "  PIXEL_STD: [1.0, 1.0, 1.0]\n",
            "  PROPOSAL_GENERATOR:\n",
            "    MIN_SIZE: 0\n",
            "    NAME: RPN\n",
            "  QUERY:\n",
            "    CONTEXT: 2\n",
            "    ENCODE_CENTER_DIS_COEFF: [1.0, 1.0]\n",
            "    ENCODE_SMALL_OBJ_SCALE: [[0, 32], [0, 64]]\n",
            "    FEATURES_VALUE_TEST: [0, 1]\n",
            "    FEATURES_VALUE_TRAIN: [0, 1]\n",
            "    FEATURES_WHOLE_TEST: [2, 3, 4, 5]\n",
            "    FEATURES_WHOLE_TRAIN: [2, 3, 4, 5]\n",
            "    QUERY_INFER: False\n",
            "    QUERY_LOSS_GAMMA: [1.3, 1.3]\n",
            "    QUERY_LOSS_WEIGHT: [10.0, 10.0]\n",
            "    Q_FEATURE_TEST: [1, 2]\n",
            "    Q_FEATURE_TRAIN: [1, 2]\n",
            "    THRESHOLD: 0.12\n",
            "  RESNETS:\n",
            "    DEFORM_MODULATED: False\n",
            "    DEFORM_NUM_GROUPS: 1\n",
            "    DEFORM_ON_PER_STAGE: [False, False, False, False]\n",
            "    DEPTH: 50\n",
            "    NORM: FrozenBN\n",
            "    NUM_GROUPS: 1\n",
            "    OUT_FEATURES: ['res2', 'res3', 'res4', 'res5']\n",
            "    RES2_OUT_CHANNELS: 256\n",
            "    RES5_DILATION: 1\n",
            "    STEM_OUT_CHANNELS: 64\n",
            "    STRIDE_IN_1X1: True\n",
            "    WIDTH_PER_GROUP: 64\n",
            "  RETINANET:\n",
            "    BBOX_REG_LOSS_TYPE: smooth_l1\n",
            "    BBOX_REG_WEIGHTS: (1.0, 1.0, 1.0, 1.0)\n",
            "    FOCAL_LOSS_ALPHA: 0.25\n",
            "    FOCAL_LOSS_GAMMA: 2.0\n",
            "    IN_FEATURES: ['p2', 'p3', 'p4', 'p5', 'p6', 'p7']\n",
            "    IOU_LABELS: [0, -1, 1]\n",
            "    IOU_THRESHOLDS: [0.4, 0.5]\n",
            "    NMS_THRESH_TEST: 0.5\n",
            "    NORM: \n",
            "    NUM_CLASSES: 10\n",
            "    NUM_CONVS: 4\n",
            "    PRIOR_PROB: 0.01\n",
            "    SCORE_THRESH_TEST: 0.05\n",
            "    SMOOTH_L1_LOSS_BETA: 0.1\n",
            "    TOPK_CANDIDATES_TEST: 1000\n",
            "  ROI_BOX_CASCADE_HEAD:\n",
            "    BBOX_REG_WEIGHTS: ([10.0, 10.0, 5.0, 5.0], [20.0, 20.0, 10.0, 10.0], [30.0, 30.0, 15.0, 15.0])\n",
            "    IOUS: (0.5, 0.6, 0.7)\n",
            "  ROI_BOX_HEAD:\n",
            "    BBOX_REG_LOSS_TYPE: smooth_l1\n",
            "    BBOX_REG_LOSS_WEIGHT: 1.0\n",
            "    BBOX_REG_WEIGHTS: (10.0, 10.0, 5.0, 5.0)\n",
            "    CLS_AGNOSTIC_BBOX_REG: False\n",
            "    CONV_DIM: 256\n",
            "    FC_DIM: 1024\n",
            "    NAME: \n",
            "    NORM: \n",
            "    NUM_CONV: 0\n",
            "    NUM_FC: 0\n",
            "    POOLER_RESOLUTION: 14\n",
            "    POOLER_SAMPLING_RATIO: 0\n",
            "    POOLER_TYPE: ROIAlignV2\n",
            "    SMOOTH_L1_BETA: 0.0\n",
            "    TRAIN_ON_PRED_BOXES: False\n",
            "  ROI_HEADS:\n",
            "    BATCH_SIZE_PER_IMAGE: 512\n",
            "    IN_FEATURES: ['res4']\n",
            "    IOU_LABELS: [0, 1]\n",
            "    IOU_THRESHOLDS: [0.5]\n",
            "    NAME: Res5ROIHeads\n",
            "    NMS_THRESH_TEST: 0.5\n",
            "    NUM_CLASSES: 80\n",
            "    POSITIVE_FRACTION: 0.25\n",
            "    PROPOSAL_APPEND_GT: True\n",
            "    SCORE_THRESH_TEST: 0.05\n",
            "  ROI_KEYPOINT_HEAD:\n",
            "    CONV_DIMS: (512, 512, 512, 512, 512, 512, 512, 512)\n",
            "    LOSS_WEIGHT: 1.0\n",
            "    MIN_KEYPOINTS_PER_IMAGE: 1\n",
            "    NAME: KRCNNConvDeconvUpsampleHead\n",
            "    NORMALIZE_LOSS_BY_VISIBLE_KEYPOINTS: True\n",
            "    NUM_KEYPOINTS: 17\n",
            "    POOLER_RESOLUTION: 14\n",
            "    POOLER_SAMPLING_RATIO: 0\n",
            "    POOLER_TYPE: ROIAlignV2\n",
            "  ROI_MASK_HEAD:\n",
            "    CLS_AGNOSTIC_MASK: False\n",
            "    CONV_DIM: 256\n",
            "    NAME: MaskRCNNConvUpsampleHead\n",
            "    NORM: \n",
            "    NUM_CONV: 0\n",
            "    POOLER_RESOLUTION: 14\n",
            "    POOLER_SAMPLING_RATIO: 0\n",
            "    POOLER_TYPE: ROIAlignV2\n",
            "  RPN:\n",
            "    BATCH_SIZE_PER_IMAGE: 256\n",
            "    BBOX_REG_LOSS_TYPE: smooth_l1\n",
            "    BBOX_REG_LOSS_WEIGHT: 1.0\n",
            "    BBOX_REG_WEIGHTS: (1.0, 1.0, 1.0, 1.0)\n",
            "    BOUNDARY_THRESH: -1\n",
            "    HEAD_NAME: StandardRPNHead\n",
            "    IN_FEATURES: ['res4']\n",
            "    IOU_LABELS: [0, -1, 1]\n",
            "    IOU_THRESHOLDS: [0.3, 0.7]\n",
            "    LOSS_WEIGHT: 1.0\n",
            "    NMS_THRESH: 0.7\n",
            "    POSITIVE_FRACTION: 0.5\n",
            "    POST_NMS_TOPK_TEST: 1000\n",
            "    POST_NMS_TOPK_TRAIN: 2000\n",
            "    PRE_NMS_TOPK_TEST: 6000\n",
            "    PRE_NMS_TOPK_TRAIN: 12000\n",
            "    SMOOTH_L1_BETA: 0.0\n",
            "  SEM_SEG_HEAD:\n",
            "    COMMON_STRIDE: 4\n",
            "    CONVS_DIM: 128\n",
            "    IGNORE_VALUE: 255\n",
            "    IN_FEATURES: ['p2', 'p3', 'p4', 'p5']\n",
            "    LOSS_WEIGHT: 1.0\n",
            "    NAME: SemSegFPNHead\n",
            "    NORM: GN\n",
            "    NUM_CLASSES: 54\n",
            "  WEIGHTS: /content/gdrive/MyDrive/QueryDet-PyTorch/work_dirs/visdrone_querydet/model_0003999.pth\n",
            "OUTPUT_DIR: work_dirs/visdrone_querydet\n",
            "SEED: -1\n",
            "SOLVER:\n",
            "  AMP:\n",
            "    ENABLED: True\n",
            "  BASE_LR: 0.006\n",
            "  BIAS_LR_FACTOR: 1.0\n",
            "  CHECKPOINT_PERIOD: 2000\n",
            "  CLIP_GRADIENTS:\n",
            "    CLIP_TYPE: value\n",
            "    CLIP_VALUE: 35.0\n",
            "    ENABLED: True\n",
            "    NORM_TYPE: 2.0\n",
            "  GAMMA: 0.1\n",
            "  IMS_PER_BATCH: 3\n",
            "  LR_SCHEDULER_NAME: WarmupMultiStepLR\n",
            "  MAX_ITER: 60000\n",
            "  MOMENTUM: 0.9\n",
            "  NESTEROV: False\n",
            "  REFERENCE_WORLD_SIZE: 0\n",
            "  STEPS: (15000, 20000)\n",
            "  WARMUP_FACTOR: 0.001\n",
            "  WARMUP_ITERS: 1000\n",
            "  WARMUP_METHOD: linear\n",
            "  WEIGHT_DECAY: 0.0001\n",
            "  WEIGHT_DECAY_BIAS: 0.0001\n",
            "  WEIGHT_DECAY_NORM: 0.0\n",
            "TEST:\n",
            "  AUG:\n",
            "    ENABLED: False\n",
            "    FLIP: True\n",
            "    MAX_SIZE: 4000\n",
            "    MIN_SIZES: (400, 500, 600, 700, 800, 900, 1000, 1100, 1200)\n",
            "  DETECTIONS_PER_IMAGE: 500\n",
            "  EVAL_PERIOD: 0\n",
            "  EXPECTED_RESULTS: []\n",
            "  KEYPOINT_OKS_SIGMAS: []\n",
            "  PRECISE_BN:\n",
            "    ENABLED: False\n",
            "    NUM_ITER: 200\n",
            "VERSION: 2\n",
            "VISDRONE:\n",
            "  MAX_LENGTH: 1999\n",
            "  SHORT_LENGTH: [1200]\n",
            "  TEST_IMG_ROOT: data/visdrone/coco_format/val_images\n",
            "  TEST_JSON: data/visdrone/coco_format/annotations/val_label.json\n",
            "  TEST_LENGTH: 3999\n",
            "  TRAIN_JSON: data/visdrone/coco_format/annotations/train_label.json\n",
            "  TRING_IMG_ROOT: data//visdrone/coco_format/train_images\n",
            "VIS_PERIOD: 0\n",
            "\u001b[32m[04/14 18:33:18 detectron2]: \u001b[0mFull config saved to work_dirs/visdrone_querydet/config.yaml\n",
            "\u001b[32m[04/14 18:33:18 d2.utils.env]: \u001b[0mUsing a generated random seed 18398146\n",
            "\u001b[32m[04/14 18:33:22 d2.engine.defaults]: \u001b[0mModel:\n",
            "RetinaNetQueryDet(\n",
            "  (backbone): FPN(\n",
            "    (fpn_lateral2): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1))\n",
            "    (fpn_output2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (fpn_lateral3): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1))\n",
            "    (fpn_output3): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (fpn_lateral4): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1))\n",
            "    (fpn_output4): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (fpn_lateral5): Conv2d(2048, 256, kernel_size=(1, 1), stride=(1, 1))\n",
            "    (fpn_output5): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (top_block): LastLevelP6P7(\n",
            "      (p6): Conv2d(2048, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
            "      (p7): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
            "    )\n",
            "    (bottom_up): ResNet(\n",
            "      (stem): BasicStem(\n",
            "        (conv1): Conv2d(\n",
            "          3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)\n",
            "        )\n",
            "      )\n",
            "      (res2): Sequential(\n",
            "        (0): BottleneckBlock(\n",
            "          (shortcut): Conv2d(\n",
            "            64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
            "          )\n",
            "          (conv1): Conv2d(\n",
            "            64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)\n",
            "          )\n",
            "          (conv2): Conv2d(\n",
            "            64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)\n",
            "          )\n",
            "          (conv3): Conv2d(\n",
            "            64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
            "          )\n",
            "        )\n",
            "        (1): BottleneckBlock(\n",
            "          (conv1): Conv2d(\n",
            "            256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)\n",
            "          )\n",
            "          (conv2): Conv2d(\n",
            "            64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)\n",
            "          )\n",
            "          (conv3): Conv2d(\n",
            "            64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
            "          )\n",
            "        )\n",
            "        (2): BottleneckBlock(\n",
            "          (conv1): Conv2d(\n",
            "            256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)\n",
            "          )\n",
            "          (conv2): Conv2d(\n",
            "            64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)\n",
            "          )\n",
            "          (conv3): Conv2d(\n",
            "            64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
            "          )\n",
            "        )\n",
            "      )\n",
            "      (res3): Sequential(\n",
            "        (0): BottleneckBlock(\n",
            "          (shortcut): Conv2d(\n",
            "            256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
            "          )\n",
            "          (conv1): Conv2d(\n",
            "            256, 128, kernel_size=(1, 1), stride=(2, 2), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)\n",
            "          )\n",
            "          (conv2): Conv2d(\n",
            "            128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)\n",
            "          )\n",
            "          (conv3): Conv2d(\n",
            "            128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
            "          )\n",
            "        )\n",
            "        (1): BottleneckBlock(\n",
            "          (conv1): Conv2d(\n",
            "            512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)\n",
            "          )\n",
            "          (conv2): Conv2d(\n",
            "            128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)\n",
            "          )\n",
            "          (conv3): Conv2d(\n",
            "            128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
            "          )\n",
            "        )\n",
            "        (2): BottleneckBlock(\n",
            "          (conv1): Conv2d(\n",
            "            512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)\n",
            "          )\n",
            "          (conv2): Conv2d(\n",
            "            128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)\n",
            "          )\n",
            "          (conv3): Conv2d(\n",
            "            128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
            "          )\n",
            "        )\n",
            "        (3): BottleneckBlock(\n",
            "          (conv1): Conv2d(\n",
            "            512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)\n",
            "          )\n",
            "          (conv2): Conv2d(\n",
            "            128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)\n",
            "          )\n",
            "          (conv3): Conv2d(\n",
            "            128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
            "          )\n",
            "        )\n",
            "      )\n",
            "      (res4): Sequential(\n",
            "        (0): BottleneckBlock(\n",
            "          (shortcut): Conv2d(\n",
            "            512, 1024, kernel_size=(1, 1), stride=(2, 2), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n",
            "          )\n",
            "          (conv1): Conv2d(\n",
            "            512, 256, kernel_size=(1, 1), stride=(2, 2), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
            "          )\n",
            "          (conv2): Conv2d(\n",
            "            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
            "          )\n",
            "          (conv3): Conv2d(\n",
            "            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n",
            "          )\n",
            "        )\n",
            "        (1): BottleneckBlock(\n",
            "          (conv1): Conv2d(\n",
            "            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
            "          )\n",
            "          (conv2): Conv2d(\n",
            "            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
            "          )\n",
            "          (conv3): Conv2d(\n",
            "            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n",
            "          )\n",
            "        )\n",
            "        (2): BottleneckBlock(\n",
            "          (conv1): Conv2d(\n",
            "            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
            "          )\n",
            "          (conv2): Conv2d(\n",
            "            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
            "          )\n",
            "          (conv3): Conv2d(\n",
            "            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n",
            "          )\n",
            "        )\n",
            "        (3): BottleneckBlock(\n",
            "          (conv1): Conv2d(\n",
            "            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
            "          )\n",
            "          (conv2): Conv2d(\n",
            "            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
            "          )\n",
            "          (conv3): Conv2d(\n",
            "            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n",
            "          )\n",
            "        )\n",
            "        (4): BottleneckBlock(\n",
            "          (conv1): Conv2d(\n",
            "            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
            "          )\n",
            "          (conv2): Conv2d(\n",
            "            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
            "          )\n",
            "          (conv3): Conv2d(\n",
            "            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n",
            "          )\n",
            "        )\n",
            "        (5): BottleneckBlock(\n",
            "          (conv1): Conv2d(\n",
            "            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
            "          )\n",
            "          (conv2): Conv2d(\n",
            "            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
            "          )\n",
            "          (conv3): Conv2d(\n",
            "            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n",
            "          )\n",
            "        )\n",
            "      )\n",
            "      (res5): Sequential(\n",
            "        (0): BottleneckBlock(\n",
            "          (shortcut): Conv2d(\n",
            "            1024, 2048, kernel_size=(1, 1), stride=(2, 2), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=2048, eps=1e-05)\n",
            "          )\n",
            "          (conv1): Conv2d(\n",
            "            1024, 512, kernel_size=(1, 1), stride=(2, 2), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
            "          )\n",
            "          (conv2): Conv2d(\n",
            "            512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
            "          )\n",
            "          (conv3): Conv2d(\n",
            "            512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=2048, eps=1e-05)\n",
            "          )\n",
            "        )\n",
            "        (1): BottleneckBlock(\n",
            "          (conv1): Conv2d(\n",
            "            2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
            "          )\n",
            "          (conv2): Conv2d(\n",
            "            512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
            "          )\n",
            "          (conv3): Conv2d(\n",
            "            512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=2048, eps=1e-05)\n",
            "          )\n",
            "        )\n",
            "        (2): BottleneckBlock(\n",
            "          (conv1): Conv2d(\n",
            "            2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
            "          )\n",
            "          (conv2): Conv2d(\n",
            "            512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
            "          )\n",
            "          (conv3): Conv2d(\n",
            "            512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=2048, eps=1e-05)\n",
            "          )\n",
            "        )\n",
            "      )\n",
            "    )\n",
            "  )\n",
            "  (det_head): RetinaNetHead_3x3(\n",
            "    (cls_layer_0): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (bbox_layer_0): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (cls_layer_1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (bbox_layer_1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (cls_layer_2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (bbox_layer_2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (cls_layer_3): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (bbox_layer_3): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (cls_score): Conv2d(256, 90, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (bbox_pred): Conv2d(256, 36, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "  )\n",
            "  (query_head): Head_3x3(\n",
            "    (layer_0): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (layer_1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (layer_2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (layer_3): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (pred_net): Conv2d(256, 1, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "  )\n",
            "  (anchor_generator): AnchorGeneratorWithCenter(\n",
            "    (cell_anchors): BufferList()\n",
            "  )\n",
            "  (query_anchor_generator): AnchorGeneratorWithCenter(\n",
            "    (cell_anchors): BufferList()\n",
            "  )\n",
            ")\n",
            "\u001b[32m[04/14 18:33:22 fvcore.common.checkpoint]: \u001b[0mLoading checkpoint from /content/gdrive/MyDrive/QueryDet-PyTorch/work_dirs/visdrone_querydet/model_0003999.pth\n",
            "\u001b[32m[04/14 18:33:40 d2.data.common]: \u001b[0mSerializing 25884 elements to byte tensors and concatenating them all ...\n",
            "\u001b[32m[04/14 18:33:40 d2.data.common]: \u001b[0mSerialized dataset takes 15.78 MiB\n",
            "/usr/local/lib/python3.7/site-packages/torch/utils/data/dataloader.py:477: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  cpuset_checked))\n",
            "\u001b[32m[04/14 18:33:46 d2.engine.train_loop]: \u001b[0mStarting training from iteration 0\n",
            "\u001b[32m[04/14 18:34:47 d2.utils.events]: \u001b[0m eta: 1 day, 13:19:44  iter: 19  total_loss: 1.187  loss_cls: 0.7623  loss_box_reg: 0.427  loss_query: 0.02564  time: 2.4683  data_time: 0.1712  lr: 0.00011989  max_mem: 10545M\n",
            "\u001b[32m[04/14 18:35:37 d2.utils.events]: \u001b[0m eta: 1 day, 14:30:54  iter: 39  total_loss: 0.7659  loss_cls: 0.4545  loss_box_reg: 0.2669  loss_query: 0.03923  time: 2.3878  data_time: 0.0063  lr: 0.00023977  max_mem: 10545M\n",
            "\u001b[32m[04/14 18:36:27 d2.utils.events]: \u001b[0m eta: 1 day, 15:38:21  iter: 59  total_loss: 0.8673  loss_cls: 0.5134  loss_box_reg: 0.2931  loss_query: 0.04283  time: 2.3952  data_time: 0.0066  lr: 0.00035965  max_mem: 10545M\n",
            "\u001b[32m[04/14 18:37:17 d2.utils.events]: \u001b[0m eta: 1 day, 15:42:24  iter: 79  total_loss: 0.5577  loss_cls: 0.3196  loss_box_reg: 0.2105  loss_query: 0.02552  time: 2.3943  data_time: 0.0093  lr: 0.00047953  max_mem: 10545M\n",
            "\u001b[32m[04/14 18:38:08 d2.utils.events]: \u001b[0m eta: 1 day, 15:45:48  iter: 99  total_loss: 0.8016  loss_cls: 0.471  loss_box_reg: 0.2886  loss_query: 0.04211  time: 2.4015  data_time: 0.0081  lr: 0.00059941  max_mem: 10545M\n",
            "\u001b[32m[04/14 18:38:57 d2.utils.events]: \u001b[0m eta: 1 day, 15:45:58  iter: 119  total_loss: 0.5717  loss_cls: 0.3444  loss_box_reg: 0.1888  loss_query: 0.02601  time: 2.3931  data_time: 0.0065  lr: 0.00071929  max_mem: 10545M\n",
            "\u001b[32m[04/14 18:39:45 d2.utils.events]: \u001b[0m eta: 1 day, 15:48:03  iter: 139  total_loss: 0.704  loss_cls: 0.4142  loss_box_reg: 0.2287  loss_query: 0.03732  time: 2.3852  data_time: 0.0071  lr: 0.00083917  max_mem: 10545M\n",
            "\u001b[32m[04/14 18:40:35 d2.utils.events]: \u001b[0m eta: 1 day, 15:50:20  iter: 159  total_loss: 0.5729  loss_cls: 0.3379  loss_box_reg: 0.2399  loss_query: 0.02391  time: 2.3856  data_time: 0.0069  lr: 0.00095905  max_mem: 10545M\n",
            "\u001b[32m[04/14 18:41:23 d2.utils.events]: \u001b[0m eta: 1 day, 15:50:39  iter: 179  total_loss: 0.7484  loss_cls: 0.4338  loss_box_reg: 0.2467  loss_query: 0.03207  time: 2.3757  data_time: 0.0061  lr: 0.0010789  max_mem: 10545M\n",
            "\u001b[32m[04/14 18:42:11 d2.utils.events]: \u001b[0m eta: 1 day, 15:48:44  iter: 199  total_loss: 0.884  loss_cls: 0.509  loss_box_reg: 0.2932  loss_query: 0.04248  time: 2.3678  data_time: 0.0071  lr: 0.0011988  max_mem: 10545M\n",
            "\u001b[32m[04/14 18:43:00 d2.utils.events]: \u001b[0m eta: 1 day, 15:46:59  iter: 219  total_loss: 0.7316  loss_cls: 0.4306  loss_box_reg: 0.2573  loss_query: 0.02659  time: 2.3655  data_time: 0.0085  lr: 0.0013187  max_mem: 10545M\n",
            "\u001b[32m[04/14 18:43:49 d2.utils.events]: \u001b[0m eta: 1 day, 15:45:16  iter: 239  total_loss: 0.7218  loss_cls: 0.4441  loss_box_reg: 0.2818  loss_query: 0.0337  time: 2.3639  data_time: 0.0066  lr: 0.0014386  max_mem: 10545M\n",
            "\u001b[32m[04/14 18:44:38 d2.utils.events]: \u001b[0m eta: 1 day, 15:47:34  iter: 259  total_loss: 0.751  loss_cls: 0.4549  loss_box_reg: 0.2576  loss_query: 0.04304  time: 2.3658  data_time: 0.0069  lr: 0.0015584  max_mem: 10545M\n",
            "\u001b[32m[04/14 18:45:28 d2.utils.events]: \u001b[0m eta: 1 day, 15:47:03  iter: 279  total_loss: 0.7318  loss_cls: 0.3845  loss_box_reg: 0.3001  loss_query: 0.0429  time: 2.3680  data_time: 0.0063  lr: 0.0016783  max_mem: 10545M\n",
            "\u001b[32m[04/14 18:46:15 d2.utils.events]: \u001b[0m eta: 1 day, 15:43:13  iter: 299  total_loss: 0.4816  loss_cls: 0.2728  loss_box_reg: 0.1694  loss_query: 0.02581  time: 2.3587  data_time: 0.0087  lr: 0.0017982  max_mem: 10545M\n",
            "\u001b[32m[04/14 18:47:04 d2.utils.events]: \u001b[0m eta: 1 day, 15:41:34  iter: 319  total_loss: 0.6714  loss_cls: 0.4155  loss_box_reg: 0.22  loss_query: 0.03647  time: 2.3583  data_time: 0.0073  lr: 0.0019181  max_mem: 10545M\n",
            "\u001b[32m[04/14 18:47:54 d2.utils.events]: \u001b[0m eta: 1 day, 15:44:31  iter: 339  total_loss: 0.6256  loss_cls: 0.3694  loss_box_reg: 0.2353  loss_query: 0.03115  time: 2.3615  data_time: 0.0079  lr: 0.002038  max_mem: 10545M\n",
            "\u001b[32m[04/14 18:48:44 d2.utils.events]: \u001b[0m eta: 1 day, 15:43:51  iter: 359  total_loss: 0.6045  loss_cls: 0.347  loss_box_reg: 0.1817  loss_query: 0.03412  time: 2.3633  data_time: 0.0073  lr: 0.0021578  max_mem: 10545M\n",
            "\u001b[32m[04/14 18:49:34 d2.utils.events]: \u001b[0m eta: 1 day, 15:43:03  iter: 379  total_loss: 0.6221  loss_cls: 0.3694  loss_box_reg: 0.2094  loss_query: 0.03471  time: 2.3648  data_time: 0.0075  lr: 0.0022777  max_mem: 10545M\n",
            "\u001b[32m[04/14 18:50:24 d2.utils.events]: \u001b[0m eta: 1 day, 15:44:18  iter: 399  total_loss: 0.6112  loss_cls: 0.3325  loss_box_reg: 0.2199  loss_query: 0.0394  time: 2.3675  data_time: 0.0080  lr: 0.0023976  max_mem: 10545M\n",
            "\u001b[32m[04/14 18:51:13 d2.utils.events]: \u001b[0m eta: 1 day, 15:43:30  iter: 419  total_loss: 0.7107  loss_cls: 0.3831  loss_box_reg: 0.2539  loss_query: 0.02892  time: 2.3664  data_time: 0.0065  lr: 0.0025175  max_mem: 10545M\n",
            "\u001b[32m[04/14 18:52:03 d2.utils.events]: \u001b[0m eta: 1 day, 15:42:42  iter: 439  total_loss: 0.6945  loss_cls: 0.3758  loss_box_reg: 0.2501  loss_query: 0.04146  time: 2.3681  data_time: 0.0085  lr: 0.0026374  max_mem: 10545M\n",
            "\u001b[32m[04/14 18:52:52 d2.utils.events]: \u001b[0m eta: 1 day, 15:43:07  iter: 459  total_loss: 0.6488  loss_cls: 0.371  loss_box_reg: 0.1814  loss_query: 0.0395  time: 2.3671  data_time: 0.0080  lr: 0.0027572  max_mem: 10545M\n",
            "\u001b[32m[04/14 18:53:41 d2.utils.events]: \u001b[0m eta: 1 day, 15:42:19  iter: 479  total_loss: 0.5785  loss_cls: 0.357  loss_box_reg: 0.1976  loss_query: 0.02403  time: 2.3672  data_time: 0.0071  lr: 0.0028771  max_mem: 10545M\n",
            "\u001b[32m[04/14 18:54:31 d2.utils.events]: \u001b[0m eta: 1 day, 15:42:42  iter: 499  total_loss: 0.6778  loss_cls: 0.43  loss_box_reg: 0.24  loss_query: 0.03361  time: 2.3676  data_time: 0.0094  lr: 0.002997  max_mem: 10545M\n",
            "\u001b[32m[04/14 18:55:19 d2.utils.events]: \u001b[0m eta: 1 day, 15:42:30  iter: 519  total_loss: 0.537  loss_cls: 0.3458  loss_box_reg: 0.1859  loss_query: 0.02177  time: 2.3654  data_time: 0.0085  lr: 0.0031169  max_mem: 10545M\n",
            "\u001b[32m[04/14 18:56:06 d2.utils.events]: \u001b[0m eta: 1 day, 15:39:12  iter: 539  total_loss: 0.6928  loss_cls: 0.4228  loss_box_reg: 0.2593  loss_query: 0.02888  time: 2.3625  data_time: 0.0061  lr: 0.0032368  max_mem: 10545M\n",
            "\u001b[32m[04/14 18:57:30 d2.utils.events]: \u001b[0m eta: 1 day, 15:41:49  iter: 559  total_loss: 0.7223  loss_cls: 0.4068  loss_box_reg: 0.2805  loss_query: 0.03226  time: 2.4244  data_time: 1.6369  lr: 0.0033566  max_mem: 10545M\n",
            "\u001b[32m[04/14 18:58:19 d2.utils.events]: \u001b[0m eta: 1 day, 15:40:35  iter: 579  total_loss: 0.5625  loss_cls: 0.3626  loss_box_reg: 0.1943  loss_query: 0.02591  time: 2.4217  data_time: 0.0082  lr: 0.0034765  max_mem: 10545M\n",
            "\u001b[32m[04/14 18:59:08 d2.utils.events]: \u001b[0m eta: 1 day, 15:39:01  iter: 599  total_loss: 0.5807  loss_cls: 0.361  loss_box_reg: 0.2019  loss_query: 0.02479  time: 2.4185  data_time: 0.0070  lr: 0.0035964  max_mem: 10545M\n",
            "\u001b[32m[04/14 18:59:56 d2.utils.events]: \u001b[0m eta: 1 day, 15:36:53  iter: 619  total_loss: 0.578  loss_cls: 0.3512  loss_box_reg: 0.1965  loss_query: 0.02712  time: 2.4160  data_time: 0.0068  lr: 0.0037163  max_mem: 10545M\n",
            "\u001b[32m[04/14 19:00:46 d2.utils.events]: \u001b[0m eta: 1 day, 15:36:33  iter: 639  total_loss: 0.7524  loss_cls: 0.4727  loss_box_reg: 0.2425  loss_query: 0.02707  time: 2.4145  data_time: 0.0076  lr: 0.0038362  max_mem: 10545M\n",
            "\u001b[32m[04/14 19:01:35 d2.utils.events]: \u001b[0m eta: 1 day, 15:35:17  iter: 659  total_loss: 0.6714  loss_cls: 0.4193  loss_box_reg: 0.225  loss_query: 0.03481  time: 2.4133  data_time: 0.0084  lr: 0.003956  max_mem: 10545M\n",
            "\u001b[32m[04/14 19:02:24 d2.utils.events]: \u001b[0m eta: 1 day, 15:33:11  iter: 679  total_loss: 0.636  loss_cls: 0.414  loss_box_reg: 0.2122  loss_query: 0.0385  time: 2.4105  data_time: 0.0080  lr: 0.0040759  max_mem: 10545M\n",
            "\u001b[32m[04/14 19:03:13 d2.utils.events]: \u001b[0m eta: 1 day, 15:33:52  iter: 699  total_loss: 0.776  loss_cls: 0.4265  loss_box_reg: 0.3081  loss_query: 0.0515  time: 2.4099  data_time: 0.0069  lr: 0.0041958  max_mem: 10545M\n",
            "\u001b[32m[04/14 19:04:04 d2.utils.events]: \u001b[0m eta: 1 day, 15:34:05  iter: 719  total_loss: 0.4617  loss_cls: 0.2738  loss_box_reg: 0.1563  loss_query: 0.02376  time: 2.4110  data_time: 0.0069  lr: 0.0043157  max_mem: 10545M\n",
            "\u001b[32m[04/14 19:04:52 d2.utils.events]: \u001b[0m eta: 1 day, 15:32:37  iter: 739  total_loss: 0.6312  loss_cls: 0.404  loss_box_reg: 0.2174  loss_query: 0.03135  time: 2.4082  data_time: 0.0069  lr: 0.0044356  max_mem: 10545M\n",
            "\u001b[32m[04/14 19:05:40 d2.utils.events]: \u001b[0m eta: 1 day, 15:31:49  iter: 759  total_loss: 0.6799  loss_cls: 0.4035  loss_box_reg: 0.2187  loss_query: 0.02563  time: 2.4053  data_time: 0.0070  lr: 0.0045554  max_mem: 10545M\n",
            "\u001b[32m[04/14 19:06:29 d2.utils.events]: \u001b[0m eta: 1 day, 15:31:41  iter: 779  total_loss: 0.8051  loss_cls: 0.4938  loss_box_reg: 0.2603  loss_query: 0.04119  time: 2.4035  data_time: 0.0100  lr: 0.0046753  max_mem: 10545M\n",
            "\u001b[32m[04/14 19:07:18 d2.utils.events]: \u001b[0m eta: 1 day, 15:29:52  iter: 799  total_loss: 0.6255  loss_cls: 0.4215  loss_box_reg: 0.1977  loss_query: 0.0289  time: 2.4014  data_time: 0.0065  lr: 0.0047952  max_mem: 10545M\n",
            "\u001b[32m[04/14 19:08:08 d2.utils.events]: \u001b[0m eta: 1 day, 15:28:40  iter: 819  total_loss: 0.6669  loss_cls: 0.366  loss_box_reg: 0.2522  loss_query: 0.02944  time: 2.4012  data_time: 0.0075  lr: 0.0049151  max_mem: 10545M\n",
            "\u001b[32m[04/14 19:08:56 d2.utils.events]: \u001b[0m eta: 1 day, 15:26:47  iter: 839  total_loss: 0.5418  loss_cls: 0.3097  loss_box_reg: 0.1717  loss_query: 0.0237  time: 2.3999  data_time: 0.0073  lr: 0.005035  max_mem: 10545M\n",
            "\u001b[32m[04/14 19:09:46 d2.utils.events]: \u001b[0m eta: 1 day, 15:26:54  iter: 859  total_loss: 0.6441  loss_cls: 0.4062  loss_box_reg: 0.2188  loss_query: 0.03188  time: 2.3994  data_time: 0.0067  lr: 0.0051548  max_mem: 10545M\n",
            "\u001b[32m[04/14 19:10:36 d2.utils.events]: \u001b[0m eta: 1 day, 15:26:16  iter: 879  total_loss: 0.8714  loss_cls: 0.4841  loss_box_reg: 0.3068  loss_query: 0.03845  time: 2.3997  data_time: 0.0071  lr: 0.0052747  max_mem: 10545M\n",
            "\u001b[32m[04/14 19:11:25 d2.utils.events]: \u001b[0m eta: 1 day, 15:24:18  iter: 899  total_loss: 0.4921  loss_cls: 0.3156  loss_box_reg: 0.171  loss_query: 0.02913  time: 2.3985  data_time: 0.0083  lr: 0.0053946  max_mem: 10545M\n",
            "\u001b[32m[04/14 19:12:14 d2.utils.events]: \u001b[0m eta: 1 day, 15:23:35  iter: 919  total_loss: 0.7242  loss_cls: 0.4463  loss_box_reg: 0.2529  loss_query: 0.03151  time: 2.3977  data_time: 0.0083  lr: 0.0055145  max_mem: 10545M\n",
            "\u001b[32m[04/14 19:13:05 d2.utils.events]: \u001b[0m eta: 1 day, 15:24:04  iter: 939  total_loss: 0.8058  loss_cls: 0.5227  loss_box_reg: 0.2588  loss_query: 0.03345  time: 2.3979  data_time: 0.0061  lr: 0.0056344  max_mem: 10545M\n",
            "\u001b[32m[04/14 19:14:05 d2.utils.events]: \u001b[0m eta: 1 day, 15:23:49  iter: 959  total_loss: 0.4188  loss_cls: 0.2805  loss_box_reg: 0.1267  loss_query: 0.02618  time: 2.4089  data_time: 0.5024  lr: 0.0057542  max_mem: 10545M\n",
            "\u001b[32m[04/14 19:14:55 d2.utils.events]: \u001b[0m eta: 1 day, 15:23:41  iter: 979  total_loss: 0.7922  loss_cls: 0.4607  loss_box_reg: 0.275  loss_query: 0.03382  time: 2.4084  data_time: 0.0104  lr: 0.0058741  max_mem: 10545M\n",
            "\u001b[32m[04/14 19:15:42 d2.utils.events]: \u001b[0m eta: 1 day, 15:22:24  iter: 999  total_loss: 0.5845  loss_cls: 0.3459  loss_box_reg: 0.2  loss_query: 0.02411  time: 2.4058  data_time: 0.0073  lr: 0.005994  max_mem: 10545M\n",
            "\u001b[32m[04/14 19:16:31 d2.utils.events]: \u001b[0m eta: 1 day, 15:22:43  iter: 1019  total_loss: 0.7499  loss_cls: 0.4554  loss_box_reg: 0.2412  loss_query: 0.05212  time: 2.4044  data_time: 0.0069  lr: 0.006  max_mem: 10545M\n",
            "\u001b[32m[04/14 19:17:20 d2.utils.events]: \u001b[0m eta: 1 day, 15:21:55  iter: 1039  total_loss: 0.527  loss_cls: 0.2837  loss_box_reg: 0.1775  loss_query: 0.02511  time: 2.4031  data_time: 0.0086  lr: 0.006  max_mem: 10545M\n",
            "\u001b[32m[04/14 19:18:10 d2.utils.events]: \u001b[0m eta: 1 day, 15:21:25  iter: 1059  total_loss: 0.9121  loss_cls: 0.5567  loss_box_reg: 0.3207  loss_query: 0.0425  time: 2.4035  data_time: 0.0065  lr: 0.006  max_mem: 10545M\n",
            "\u001b[32m[04/14 19:19:00 d2.utils.events]: \u001b[0m eta: 1 day, 15:20:19  iter: 1079  total_loss: 0.6044  loss_cls: 0.3653  loss_box_reg: 0.2031  loss_query: 0.03596  time: 2.4030  data_time: 0.0075  lr: 0.006  max_mem: 10545M\n",
            "\u001b[32m[04/14 19:19:46 d2.utils.events]: \u001b[0m eta: 1 day, 15:18:29  iter: 1099  total_loss: 0.5943  loss_cls: 0.3594  loss_box_reg: 0.2154  loss_query: 0.02452  time: 2.3998  data_time: 0.0075  lr: 0.006  max_mem: 10545M\n",
            "\u001b[32m[04/14 19:20:36 d2.utils.events]: \u001b[0m eta: 1 day, 15:18:27  iter: 1119  total_loss: 0.9178  loss_cls: 0.5142  loss_box_reg: 0.3174  loss_query: 0.03631  time: 2.4000  data_time: 0.0075  lr: 0.006  max_mem: 10545M\n",
            "\u001b[32m[04/14 19:21:25 d2.utils.events]: \u001b[0m eta: 1 day, 15:17:47  iter: 1139  total_loss: 0.8434  loss_cls: 0.4814  loss_box_reg: 0.2978  loss_query: 0.0349  time: 2.3990  data_time: 0.0066  lr: 0.006  max_mem: 10545M\n",
            "\u001b[32m[04/14 19:22:14 d2.utils.events]: \u001b[0m eta: 1 day, 15:17:02  iter: 1159  total_loss: 0.6738  loss_cls: 0.3981  loss_box_reg: 0.2264  loss_query: 0.03465  time: 2.3981  data_time: 0.0069  lr: 0.006  max_mem: 10545M\n",
            "\u001b[32m[04/14 19:23:03 d2.utils.events]: \u001b[0m eta: 1 day, 15:16:11  iter: 1179  total_loss: 0.7728  loss_cls: 0.4619  loss_box_reg: 0.2299  loss_query: 0.02847  time: 2.3975  data_time: 0.0064  lr: 0.006  max_mem: 10545M\n",
            "\u001b[32m[04/14 19:23:53 d2.utils.events]: \u001b[0m eta: 1 day, 15:15:23  iter: 1199  total_loss: 0.6589  loss_cls: 0.3729  loss_box_reg: 0.2585  loss_query: 0.02688  time: 2.3967  data_time: 0.0075  lr: 0.006  max_mem: 10545M\n",
            "\u001b[32m[04/14 19:24:43 d2.utils.events]: \u001b[0m eta: 1 day, 15:14:35  iter: 1219  total_loss: 0.7103  loss_cls: 0.3696  loss_box_reg: 0.2447  loss_query: 0.02508  time: 2.3967  data_time: 0.0086  lr: 0.006  max_mem: 10545M\n",
            "\u001b[32m[04/14 19:25:32 d2.utils.events]: \u001b[0m eta: 1 day, 15:14:35  iter: 1239  total_loss: 0.8746  loss_cls: 0.4938  loss_box_reg: 0.3131  loss_query: 0.0301  time: 2.3961  data_time: 0.0071  lr: 0.006  max_mem: 10545M\n",
            "\u001b[32m[04/14 19:26:23 d2.utils.events]: \u001b[0m eta: 1 day, 15:13:24  iter: 1259  total_loss: 0.5241  loss_cls: 0.309  loss_box_reg: 0.2234  loss_query: 0.01938  time: 2.3964  data_time: 0.0067  lr: 0.006  max_mem: 10545M\n",
            "\u001b[32m[04/14 19:27:12 d2.utils.events]: \u001b[0m eta: 1 day, 15:11:56  iter: 1279  total_loss: 0.5678  loss_cls: 0.3492  loss_box_reg: 0.2188  loss_query: 0.02871  time: 2.3958  data_time: 0.0080  lr: 0.006  max_mem: 10545M\n",
            "\u001b[32m[04/14 19:28:02 d2.utils.events]: \u001b[0m eta: 1 day, 15:12:21  iter: 1299  total_loss: 1.037  loss_cls: 0.615  loss_box_reg: 0.3385  loss_query: 0.04906  time: 2.3961  data_time: 0.0068  lr: 0.006  max_mem: 10545M\n",
            "\u001b[32m[04/14 19:28:51 d2.utils.events]: \u001b[0m eta: 1 day, 15:11:33  iter: 1319  total_loss: 0.5742  loss_cls: 0.339  loss_box_reg: 0.1703  loss_query: 0.02386  time: 2.3960  data_time: 0.0071  lr: 0.006  max_mem: 10545M\n",
            "\u001b[32m[04/14 19:29:41 d2.utils.events]: \u001b[0m eta: 1 day, 15:09:46  iter: 1339  total_loss: 0.5956  loss_cls: 0.3499  loss_box_reg: 0.2076  loss_query: 0.03237  time: 2.3957  data_time: 0.0091  lr: 0.006  max_mem: 10545M\n",
            "\u001b[32m[04/14 19:30:31 d2.utils.events]: \u001b[0m eta: 1 day, 15:08:58  iter: 1359  total_loss: 0.6639  loss_cls: 0.3727  loss_box_reg: 0.1983  loss_query: 0.03747  time: 2.3955  data_time: 0.0071  lr: 0.006  max_mem: 10545M\n",
            "\u001b[32m[04/14 19:31:19 d2.utils.events]: \u001b[0m eta: 1 day, 15:07:51  iter: 1379  total_loss: 0.6101  loss_cls: 0.4027  loss_box_reg: 0.1727  loss_query: 0.0147  time: 2.3949  data_time: 0.0071  lr: 0.006  max_mem: 10545M\n",
            "\u001b[32m[04/14 19:32:10 d2.utils.events]: \u001b[0m eta: 1 day, 15:06:56  iter: 1399  total_loss: 0.7683  loss_cls: 0.4593  loss_box_reg: 0.2377  loss_query: 0.04499  time: 2.3952  data_time: 0.0095  lr: 0.006  max_mem: 10545M\n",
            "\u001b[32m[04/14 19:32:58 d2.utils.events]: \u001b[0m eta: 1 day, 15:06:08  iter: 1419  total_loss: 0.7123  loss_cls: 0.4317  loss_box_reg: 0.2295  loss_query: 0.03345  time: 2.3943  data_time: 0.0069  lr: 0.006  max_mem: 10545M\n",
            "\u001b[32m[04/14 19:33:47 d2.utils.events]: \u001b[0m eta: 1 day, 15:05:05  iter: 1439  total_loss: 0.742  loss_cls: 0.4434  loss_box_reg: 0.2539  loss_query: 0.03823  time: 2.3936  data_time: 0.0079  lr: 0.006  max_mem: 10545M\n",
            "\u001b[32m[04/14 19:34:36 d2.utils.events]: \u001b[0m eta: 1 day, 15:04:32  iter: 1459  total_loss: 0.7185  loss_cls: 0.4419  loss_box_reg: 0.2599  loss_query: 0.03422  time: 2.3932  data_time: 0.0082  lr: 0.006  max_mem: 10545M\n",
            "\u001b[32m[04/14 19:35:25 d2.utils.events]: \u001b[0m eta: 1 day, 15:03:51  iter: 1479  total_loss: 0.6636  loss_cls: 0.4139  loss_box_reg: 0.2198  loss_query: 0.02767  time: 2.3924  data_time: 0.0075  lr: 0.006  max_mem: 10545M\n",
            "\u001b[32m[04/14 19:36:14 d2.utils.events]: \u001b[0m eta: 1 day, 15:02:06  iter: 1499  total_loss: 0.7324  loss_cls: 0.4068  loss_box_reg: 0.2816  loss_query: 0.02975  time: 2.3918  data_time: 0.0082  lr: 0.006  max_mem: 10545M\n",
            "\u001b[32m[04/14 19:37:02 d2.utils.events]: \u001b[0m eta: 1 day, 15:00:35  iter: 1519  total_loss: 0.653  loss_cls: 0.3845  loss_box_reg: 0.2208  loss_query: 0.04018  time: 2.3905  data_time: 0.0076  lr: 0.006  max_mem: 10545M\n",
            "\u001b[32m[04/14 19:37:51 d2.utils.events]: \u001b[0m eta: 1 day, 15:00:30  iter: 1539  total_loss: 0.8153  loss_cls: 0.5006  loss_box_reg: 0.2524  loss_query: 0.03502  time: 2.3899  data_time: 0.0086  lr: 0.006  max_mem: 10545M\n",
            "\u001b[32m[04/14 19:38:41 d2.utils.events]: \u001b[0m eta: 1 day, 14:58:59  iter: 1559  total_loss: 0.6276  loss_cls: 0.3865  loss_box_reg: 0.2006  loss_query: 0.0361  time: 2.3900  data_time: 0.0079  lr: 0.006  max_mem: 10545M\n",
            "\u001b[32m[04/14 19:39:30 d2.utils.events]: \u001b[0m eta: 1 day, 14:58:28  iter: 1579  total_loss: 0.5911  loss_cls: 0.3484  loss_box_reg: 0.2073  loss_query: 0.03589  time: 2.3896  data_time: 0.0067  lr: 0.006  max_mem: 10545M\n",
            "\u001b[32m[04/14 19:40:20 d2.utils.events]: \u001b[0m eta: 1 day, 14:57:53  iter: 1599  total_loss: 0.6307  loss_cls: 0.381  loss_box_reg: 0.2268  loss_query: 0.03122  time: 2.3896  data_time: 0.0074  lr: 0.006  max_mem: 10545M\n",
            "\u001b[32m[04/14 19:41:10 d2.utils.events]: \u001b[0m eta: 1 day, 14:57:54  iter: 1619  total_loss: 0.6991  loss_cls: 0.4353  loss_box_reg: 0.2463  loss_query: 0.01881  time: 2.3899  data_time: 0.0079  lr: 0.006  max_mem: 10545M\n",
            "\u001b[32m[04/14 19:42:02 d2.utils.events]: \u001b[0m eta: 1 day, 14:58:02  iter: 1639  total_loss: 0.7617  loss_cls: 0.4347  loss_box_reg: 0.2679  loss_query: 0.05289  time: 2.3911  data_time: 0.0067  lr: 0.006  max_mem: 10545M\n",
            "\u001b[32m[04/14 19:42:52 d2.utils.events]: \u001b[0m eta: 1 day, 14:57:56  iter: 1659  total_loss: 0.6365  loss_cls: 0.386  loss_box_reg: 0.2359  loss_query: 0.03414  time: 2.3912  data_time: 0.0067  lr: 0.006  max_mem: 10545M\n",
            "\u001b[32m[04/14 19:43:40 d2.utils.events]: \u001b[0m eta: 1 day, 14:57:35  iter: 1679  total_loss: 0.6113  loss_cls: 0.3533  loss_box_reg: 0.197  loss_query: 0.02315  time: 2.3901  data_time: 0.0082  lr: 0.006  max_mem: 10545M\n",
            "\u001b[32m[04/14 19:44:28 d2.utils.events]: \u001b[0m eta: 1 day, 14:56:03  iter: 1699  total_loss: 0.8576  loss_cls: 0.4874  loss_box_reg: 0.29  loss_query: 0.03425  time: 2.3895  data_time: 0.0102  lr: 0.006  max_mem: 10545M\n",
            "\u001b[32m[04/14 19:45:18 d2.utils.events]: \u001b[0m eta: 1 day, 14:55:15  iter: 1719  total_loss: 0.797  loss_cls: 0.4736  loss_box_reg: 0.2968  loss_query: 0.0437  time: 2.3892  data_time: 0.0069  lr: 0.006  max_mem: 10545M\n",
            "\u001b[32m[04/14 19:46:07 d2.utils.events]: \u001b[0m eta: 1 day, 14:55:18  iter: 1739  total_loss: 0.6253  loss_cls: 0.3431  loss_box_reg: 0.2276  loss_query: 0.0297  time: 2.3888  data_time: 0.0084  lr: 0.006  max_mem: 10545M\n",
            "\u001b[32m[04/14 19:46:55 d2.utils.events]: \u001b[0m eta: 1 day, 14:53:39  iter: 1759  total_loss: 0.6865  loss_cls: 0.4304  loss_box_reg: 0.2049  loss_query: 0.02591  time: 2.3879  data_time: 0.0082  lr: 0.006  max_mem: 10545M\n",
            "\u001b[32m[04/14 19:47:44 d2.utils.events]: \u001b[0m eta: 1 day, 14:52:45  iter: 1779  total_loss: 0.7396  loss_cls: 0.4572  loss_box_reg: 0.2415  loss_query: 0.03972  time: 2.3874  data_time: 0.0077  lr: 0.006  max_mem: 10545M\n",
            "\u001b[32m[04/14 19:48:34 d2.utils.events]: \u001b[0m eta: 1 day, 14:52:42  iter: 1799  total_loss: 0.5078  loss_cls: 0.3035  loss_box_reg: 0.1779  loss_query: 0.03115  time: 2.3877  data_time: 0.0081  lr: 0.006  max_mem: 10545M\n",
            "\u001b[32m[04/14 19:49:24 d2.utils.events]: \u001b[0m eta: 1 day, 14:53:02  iter: 1819  total_loss: 0.6026  loss_cls: 0.334  loss_box_reg: 0.2335  loss_query: 0.04049  time: 2.3877  data_time: 0.0075  lr: 0.006  max_mem: 10545M\n",
            "\u001b[32m[04/14 19:50:12 d2.utils.events]: \u001b[0m eta: 1 day, 14:52:14  iter: 1839  total_loss: 0.5465  loss_cls: 0.3227  loss_box_reg: 0.1964  loss_query: 0.01959  time: 2.3871  data_time: 0.0072  lr: 0.006  max_mem: 10545M\n",
            "\u001b[32m[04/14 19:51:02 d2.utils.events]: \u001b[0m eta: 1 day, 14:51:26  iter: 1859  total_loss: 0.7498  loss_cls: 0.4296  loss_box_reg: 0.2711  loss_query: 0.03116  time: 2.3872  data_time: 0.0073  lr: 0.006  max_mem: 10545M\n",
            "\u001b[32m[04/14 19:51:52 d2.utils.events]: \u001b[0m eta: 1 day, 14:50:50  iter: 1879  total_loss: 0.7077  loss_cls: 0.4262  loss_box_reg: 0.2284  loss_query: 0.03722  time: 2.3872  data_time: 0.0073  lr: 0.006  max_mem: 10545M\n",
            "\u001b[32m[04/14 19:52:42 d2.utils.events]: \u001b[0m eta: 1 day, 14:50:41  iter: 1899  total_loss: 0.8203  loss_cls: 0.488  loss_box_reg: 0.2912  loss_query: 0.03991  time: 2.3875  data_time: 0.0078  lr: 0.006  max_mem: 10545M\n",
            "\u001b[32m[04/14 19:53:32 d2.utils.events]: \u001b[0m eta: 1 day, 14:49:59  iter: 1919  total_loss: 0.8167  loss_cls: 0.4988  loss_box_reg: 0.2959  loss_query: 0.03798  time: 2.3876  data_time: 0.0075  lr: 0.006  max_mem: 10545M\n",
            "\u001b[32m[04/14 19:54:23 d2.utils.events]: \u001b[0m eta: 1 day, 14:49:01  iter: 1939  total_loss: 0.7373  loss_cls: 0.44  loss_box_reg: 0.26  loss_query: 0.04521  time: 2.3882  data_time: 0.0065  lr: 0.006  max_mem: 10545M\n",
            "\u001b[32m[04/14 19:55:12 d2.utils.events]: \u001b[0m eta: 1 day, 14:47:52  iter: 1959  total_loss: 0.4194  loss_cls: 0.2467  loss_box_reg: 0.149  loss_query: 0.02018  time: 2.3880  data_time: 0.0076  lr: 0.006  max_mem: 10545M\n",
            "\u001b[32m[04/14 19:56:01 d2.utils.events]: \u001b[0m eta: 1 day, 14:46:54  iter: 1979  total_loss: 0.747  loss_cls: 0.415  loss_box_reg: 0.2717  loss_query: 0.0343  time: 2.3878  data_time: 0.0071  lr: 0.006  max_mem: 10545M\n",
            "\u001b[32m[04/14 19:56:50 fvcore.common.checkpoint]: \u001b[0mSaving checkpoint to work_dirs/visdrone_querydet/model_0001999.pth\n",
            "\u001b[32m[04/14 19:57:14 d2.utils.events]: \u001b[0m eta: 1 day, 14:46:30  iter: 1999  total_loss: 0.5899  loss_cls: 0.3476  loss_box_reg: 0.199  loss_query: 0.02593  time: 2.3875  data_time: 0.0069  lr: 0.006  max_mem: 10545M\n",
            "\u001b[32m[04/14 19:58:04 d2.utils.events]: \u001b[0m eta: 1 day, 14:45:28  iter: 2019  total_loss: 0.9382  loss_cls: 0.4951  loss_box_reg: 0.3708  loss_query: 0.03817  time: 2.3875  data_time: 0.0059  lr: 0.006  max_mem: 10545M\n",
            "\u001b[32m[04/14 19:58:53 d2.utils.events]: \u001b[0m eta: 1 day, 14:44:39  iter: 2039  total_loss: 0.6495  loss_cls: 0.3792  loss_box_reg: 0.2097  loss_query: 0.02632  time: 2.3873  data_time: 0.0085  lr: 0.006  max_mem: 10545M\n",
            "\u001b[32m[04/14 19:59:44 d2.utils.events]: \u001b[0m eta: 1 day, 14:44:02  iter: 2059  total_loss: 0.7874  loss_cls: 0.4777  loss_box_reg: 0.2774  loss_query: 0.03687  time: 2.3878  data_time: 0.0078  lr: 0.006  max_mem: 10545M\n",
            "\u001b[32m[04/14 20:00:33 d2.utils.events]: \u001b[0m eta: 1 day, 14:43:24  iter: 2079  total_loss: 0.7247  loss_cls: 0.4439  loss_box_reg: 0.2554  loss_query: 0.03851  time: 2.3876  data_time: 0.0075  lr: 0.006  max_mem: 10545M\n",
            "\u001b[32m[04/14 20:01:23 d2.utils.events]: \u001b[0m eta: 1 day, 14:43:40  iter: 2099  total_loss: 0.5454  loss_cls: 0.3422  loss_box_reg: 0.1766  loss_query: 0.02132  time: 2.3875  data_time: 0.0068  lr: 0.006  max_mem: 10545M\n",
            "\u001b[32m[04/14 20:02:12 d2.utils.events]: \u001b[0m eta: 1 day, 14:42:29  iter: 2119  total_loss: 0.6229  loss_cls: 0.3678  loss_box_reg: 0.2079  loss_query: 0.02834  time: 2.3872  data_time: 0.0087  lr: 0.006  max_mem: 10545M\n",
            "\u001b[32m[04/14 20:03:02 d2.utils.events]: \u001b[0m eta: 1 day, 14:41:41  iter: 2139  total_loss: 0.5898  loss_cls: 0.39  loss_box_reg: 0.1939  loss_query: 0.03257  time: 2.3873  data_time: 0.0067  lr: 0.006  max_mem: 10545M\n",
            "\u001b[32m[04/14 20:03:50 d2.utils.events]: \u001b[0m eta: 1 day, 14:40:27  iter: 2159  total_loss: 0.8978  loss_cls: 0.5327  loss_box_reg: 0.3081  loss_query: 0.03946  time: 2.3868  data_time: 0.0058  lr: 0.006  max_mem: 10545M\n",
            "\u001b[32m[04/14 20:04:39 d2.utils.events]: \u001b[0m eta: 1 day, 14:39:30  iter: 2179  total_loss: 0.7217  loss_cls: 0.4635  loss_box_reg: 0.2525  loss_query: 0.02843  time: 2.3862  data_time: 0.0088  lr: 0.006  max_mem: 10545M\n",
            "\u001b[32m[04/14 20:05:28 d2.utils.events]: \u001b[0m eta: 1 day, 14:38:54  iter: 2199  total_loss: 0.8231  loss_cls: 0.5041  loss_box_reg: 0.2777  loss_query: 0.03766  time: 2.3858  data_time: 0.0069  lr: 0.006  max_mem: 10545M\n",
            "\u001b[32m[04/14 20:06:35 d2.utils.events]: \u001b[0m eta: 1 day, 14:38:13  iter: 2219  total_loss: 0.7337  loss_cls: 0.4841  loss_box_reg: 0.2371  loss_query: 0.03176  time: 2.3934  data_time: 0.8649  lr: 0.006  max_mem: 10545M\n",
            "\u001b[32m[04/14 20:07:26 d2.utils.events]: \u001b[0m eta: 1 day, 14:38:03  iter: 2239  total_loss: 0.5369  loss_cls: 0.345  loss_box_reg: 0.1878  loss_query: 0.02168  time: 2.3940  data_time: 0.0084  lr: 0.006  max_mem: 10545M\n",
            "\u001b[32m[04/14 20:08:15 d2.utils.events]: \u001b[0m eta: 1 day, 14:36:26  iter: 2259  total_loss: 0.694  loss_cls: 0.426  loss_box_reg: 0.2254  loss_query: 0.03858  time: 2.3938  data_time: 0.0076  lr: 0.006  max_mem: 10545M\n",
            "\u001b[32m[04/14 20:09:03 d2.utils.events]: \u001b[0m eta: 1 day, 14:35:38  iter: 2279  total_loss: 0.583  loss_cls: 0.3744  loss_box_reg: 0.2264  loss_query: 0.02783  time: 2.3929  data_time: 0.0081  lr: 0.006  max_mem: 10545M\n",
            "\u001b[32m[04/14 20:09:52 d2.utils.events]: \u001b[0m eta: 1 day, 14:34:54  iter: 2299  total_loss: 0.6226  loss_cls: 0.3612  loss_box_reg: 0.2226  loss_query: 0.02858  time: 2.3928  data_time: 0.0093  lr: 0.006  max_mem: 10545M\n",
            "\u001b[32m[04/14 20:10:42 d2.utils.events]: \u001b[0m eta: 1 day, 14:33:50  iter: 2319  total_loss: 0.4695  loss_cls: 0.2664  loss_box_reg: 0.1524  loss_query: 0.01212  time: 2.3917  data_time: 0.0074  lr: 0.006  max_mem: 10545M\n",
            "\u001b[32m[04/14 20:11:33 d2.utils.events]: \u001b[0m eta: 1 day, 14:33:14  iter: 2339  total_loss: 0.5885  loss_cls: 0.3454  loss_box_reg: 0.2192  loss_query: 0.02943  time: 2.3922  data_time: 0.0081  lr: 0.006  max_mem: 10545M\n",
            "\u001b[32m[04/14 20:12:22 d2.utils.events]: \u001b[0m eta: 1 day, 14:31:57  iter: 2359  total_loss: 0.6966  loss_cls: 0.4259  loss_box_reg: 0.2389  loss_query: 0.0371  time: 2.3921  data_time: 0.0078  lr: 0.006  max_mem: 10545M\n",
            "\u001b[32m[04/14 20:13:11 d2.utils.events]: \u001b[0m eta: 1 day, 14:31:26  iter: 2379  total_loss: 0.8483  loss_cls: 0.4757  loss_box_reg: 0.2818  loss_query: 0.03862  time: 2.3918  data_time: 0.0074  lr: 0.006  max_mem: 10545M\n",
            "\u001b[32m[04/14 20:13:59 d2.utils.events]: \u001b[0m eta: 1 day, 14:29:59  iter: 2399  total_loss: 0.6881  loss_cls: 0.4162  loss_box_reg: 0.2134  loss_query: 0.03295  time: 2.3911  data_time: 0.0079  lr: 0.006  max_mem: 10545M\n",
            "\u001b[32m[04/14 20:14:49 d2.utils.events]: \u001b[0m eta: 1 day, 14:29:25  iter: 2419  total_loss: 0.621  loss_cls: 0.3493  loss_box_reg: 0.2078  loss_query: 0.0466  time: 2.3910  data_time: 0.0073  lr: 0.006  max_mem: 10545M\n",
            "\u001b[32m[04/14 20:15:38 d2.utils.events]: \u001b[0m eta: 1 day, 14:28:33  iter: 2439  total_loss: 0.5643  loss_cls: 0.3421  loss_box_reg: 0.1948  loss_query: 0.02817  time: 2.3907  data_time: 0.0078  lr: 0.006  max_mem: 10545M\n",
            "\u001b[32m[04/14 20:16:28 d2.utils.events]: \u001b[0m eta: 1 day, 14:28:00  iter: 2459  total_loss: 0.7739  loss_cls: 0.4532  loss_box_reg: 0.2882  loss_query: 0.0338  time: 2.3908  data_time: 0.0072  lr: 0.006  max_mem: 10545M\n",
            "\u001b[32m[04/14 20:17:16 d2.utils.events]: \u001b[0m eta: 1 day, 14:27:17  iter: 2479  total_loss: 0.7512  loss_cls: 0.4752  loss_box_reg: 0.2449  loss_query: 0.0568  time: 2.3902  data_time: 0.0080  lr: 0.006  max_mem: 10545M\n",
            "\u001b[32m[04/14 20:18:05 d2.utils.events]: \u001b[0m eta: 1 day, 14:26:44  iter: 2499  total_loss: 0.58  loss_cls: 0.333  loss_box_reg: 0.2101  loss_query: 0.03457  time: 2.3899  data_time: 0.0077  lr: 0.006  max_mem: 10545M\n",
            "\u001b[32m[04/14 20:18:54 d2.utils.events]: \u001b[0m eta: 1 day, 14:26:21  iter: 2519  total_loss: 0.8527  loss_cls: 0.4921  loss_box_reg: 0.2981  loss_query: 0.04256  time: 2.3894  data_time: 0.0076  lr: 0.006  max_mem: 10545M\n",
            "\u001b[32m[04/14 20:19:43 d2.utils.events]: \u001b[0m eta: 1 day, 14:25:14  iter: 2539  total_loss: 0.6084  loss_cls: 0.3501  loss_box_reg: 0.209  loss_query: 0.03397  time: 2.3893  data_time: 0.0060  lr: 0.006  max_mem: 10545M\n",
            "\u001b[32m[04/14 20:20:32 d2.utils.events]: \u001b[0m eta: 1 day, 14:24:20  iter: 2559  total_loss: 0.5443  loss_cls: 0.3201  loss_box_reg: 0.1922  loss_query: 0.01752  time: 2.3891  data_time: 0.0083  lr: 0.006  max_mem: 10545M\n",
            "\u001b[32m[04/14 20:21:20 d2.utils.events]: \u001b[0m eta: 1 day, 14:23:16  iter: 2579  total_loss: 0.5702  loss_cls: 0.3503  loss_box_reg: 0.1699  loss_query: 0.02385  time: 2.3886  data_time: 0.0087  lr: 0.006  max_mem: 10545M\n",
            "\u001b[32m[04/14 20:22:08 d2.utils.events]: \u001b[0m eta: 1 day, 14:22:28  iter: 2599  total_loss: 0.6984  loss_cls: 0.4392  loss_box_reg: 0.2253  loss_query: 0.03119  time: 2.3881  data_time: 0.0074  lr: 0.006  max_mem: 10545M\n",
            "\u001b[32m[04/14 20:22:57 d2.utils.events]: \u001b[0m eta: 1 day, 14:20:49  iter: 2619  total_loss: 0.8146  loss_cls: 0.524  loss_box_reg: 0.2712  loss_query: 0.02234  time: 2.3877  data_time: 0.0074  lr: 0.006  max_mem: 10545M\n",
            "\u001b[32m[04/14 20:23:47 d2.utils.events]: \u001b[0m eta: 1 day, 14:19:12  iter: 2639  total_loss: 0.7645  loss_cls: 0.4486  loss_box_reg: 0.2654  loss_query: 0.03348  time: 2.3878  data_time: 0.0059  lr: 0.006  max_mem: 10545M\n",
            "\u001b[32m[04/14 20:24:37 d2.utils.events]: \u001b[0m eta: 1 day, 14:18:11  iter: 2659  total_loss: 0.4576  loss_cls: 0.3085  loss_box_reg: 0.1331  loss_query: 0.01958  time: 2.3877  data_time: 0.0076  lr: 0.006  max_mem: 10545M\n",
            "\u001b[32m[04/14 20:25:26 d2.utils.events]: \u001b[0m eta: 1 day, 14:17:30  iter: 2679  total_loss: 0.5954  loss_cls: 0.3472  loss_box_reg: 0.1924  loss_query: 0.02656  time: 2.3873  data_time: 0.0080  lr: 0.006  max_mem: 10545M\n",
            "\u001b[32m[04/14 20:26:15 d2.utils.events]: \u001b[0m eta: 1 day, 14:16:42  iter: 2699  total_loss: 0.7059  loss_cls: 0.4236  loss_box_reg: 0.2338  loss_query: 0.03576  time: 2.3871  data_time: 0.0072  lr: 0.006  max_mem: 10545M\n",
            "\u001b[32m[04/14 20:27:05 d2.utils.events]: \u001b[0m eta: 1 day, 14:15:54  iter: 2719  total_loss: 0.8588  loss_cls: 0.4698  loss_box_reg: 0.2922  loss_query: 0.03306  time: 2.3873  data_time: 0.0082  lr: 0.006  max_mem: 10545M\n",
            "\u001b[32m[04/14 20:27:55 d2.utils.events]: \u001b[0m eta: 1 day, 14:14:58  iter: 2739  total_loss: 0.6223  loss_cls: 0.3844  loss_box_reg: 0.2111  loss_query: 0.04061  time: 2.3871  data_time: 0.0073  lr: 0.006  max_mem: 10545M\n",
            "\u001b[32m[04/14 20:28:45 d2.utils.events]: \u001b[0m eta: 1 day, 14:14:33  iter: 2759  total_loss: 0.6436  loss_cls: 0.4003  loss_box_reg: 0.2257  loss_query: 0.03377  time: 2.3875  data_time: 0.0072  lr: 0.006  max_mem: 10545M\n",
            "\u001b[32m[04/14 20:29:35 d2.utils.events]: \u001b[0m eta: 1 day, 14:13:34  iter: 2779  total_loss: 0.5792  loss_cls: 0.3911  loss_box_reg: 0.1865  loss_query: 0.02835  time: 2.3873  data_time: 0.0077  lr: 0.006  max_mem: 10545M\n",
            "\u001b[32m[04/14 20:30:25 d2.utils.events]: \u001b[0m eta: 1 day, 14:12:34  iter: 2799  total_loss: 0.4926  loss_cls: 0.3171  loss_box_reg: 0.1781  loss_query: 0.02605  time: 2.3870  data_time: 0.0078  lr: 0.006  max_mem: 10545M\n",
            "\u001b[32m[04/14 20:31:15 d2.utils.events]: \u001b[0m eta: 1 day, 14:10:44  iter: 2819  total_loss: 0.7612  loss_cls: 0.4723  loss_box_reg: 0.2695  loss_query: 0.02855  time: 2.3863  data_time: 0.0075  lr: 0.006  max_mem: 10545M\n",
            "\u001b[32m[04/14 20:32:03 d2.utils.events]: \u001b[0m eta: 1 day, 14:10:39  iter: 2839  total_loss: 0.7822  loss_cls: 0.4746  loss_box_reg: 0.2463  loss_query: 0.04631  time: 2.3861  data_time: 0.0073  lr: 0.006  max_mem: 10545M\n",
            "\u001b[32m[04/14 20:32:51 d2.utils.events]: \u001b[0m eta: 1 day, 14:09:08  iter: 2859  total_loss: 0.8314  loss_cls: 0.4932  loss_box_reg: 0.2778  loss_query: 0.03926  time: 2.3855  data_time: 0.0093  lr: 0.006  max_mem: 10545M\n",
            "\u001b[32m[04/14 20:33:42 d2.utils.events]: \u001b[0m eta: 1 day, 14:08:20  iter: 2879  total_loss: 0.8622  loss_cls: 0.566  loss_box_reg: 0.2627  loss_query: 0.04762  time: 2.3855  data_time: 0.0071  lr: 0.006  max_mem: 10545M\n",
            "\u001b[32m[04/14 20:34:34 d2.utils.events]: \u001b[0m eta: 1 day, 14:07:32  iter: 2899  total_loss: 0.6294  loss_cls: 0.3878  loss_box_reg: 0.2284  loss_query: 0.03022  time: 2.3858  data_time: 0.0076  lr: 0.006  max_mem: 10545M\n",
            "\u001b[32m[04/14 20:35:25 d2.utils.events]: \u001b[0m eta: 1 day, 14:06:38  iter: 2919  total_loss: 0.7158  loss_cls: 0.4434  loss_box_reg: 0.227  loss_query: 0.05312  time: 2.3858  data_time: 0.0076  lr: 0.006  max_mem: 10545M\n",
            "\u001b[32m[04/14 20:36:14 d2.utils.events]: \u001b[0m eta: 1 day, 14:05:15  iter: 2939  total_loss: 0.6375  loss_cls: 0.3793  loss_box_reg: 0.2155  loss_query: 0.0341  time: 2.3855  data_time: 0.0062  lr: 0.006  max_mem: 10545M\n",
            "\u001b[32m[04/14 20:37:03 d2.utils.events]: \u001b[0m eta: 1 day, 14:04:27  iter: 2959  total_loss: 0.652  loss_cls: 0.3754  loss_box_reg: 0.2395  loss_query: 0.04095  time: 2.3853  data_time: 0.0069  lr: 0.006  max_mem: 10545M\n",
            "\u001b[32m[04/14 20:37:53 d2.utils.events]: \u001b[0m eta: 1 day, 14:03:39  iter: 2979  total_loss: 0.7108  loss_cls: 0.4299  loss_box_reg: 0.2467  loss_query: 0.0502  time: 2.3853  data_time: 0.0063  lr: 0.006  max_mem: 10545M\n",
            "\u001b[32m[04/14 20:38:44 d2.utils.events]: \u001b[0m eta: 1 day, 14:03:00  iter: 2999  total_loss: 0.5104  loss_cls: 0.3046  loss_box_reg: 0.1782  loss_query: 0.03019  time: 2.3855  data_time: 0.0101  lr: 0.006  max_mem: 10545M\n",
            "\u001b[32m[04/14 20:39:34 d2.utils.events]: \u001b[0m eta: 1 day, 14:02:25  iter: 3019  total_loss: 0.7883  loss_cls: 0.467  loss_box_reg: 0.263  loss_query: 0.04218  time: 2.3853  data_time: 0.0074  lr: 0.006  max_mem: 10545M\n",
            "\u001b[32m[04/14 20:40:23 d2.utils.events]: \u001b[0m eta: 1 day, 14:02:19  iter: 3039  total_loss: 0.7667  loss_cls: 0.4762  loss_box_reg: 0.2735  loss_query: 0.03068  time: 2.3851  data_time: 0.0074  lr: 0.006  max_mem: 10545M\n",
            "\u001b[32m[04/14 20:41:12 d2.utils.events]: \u001b[0m eta: 1 day, 14:00:40  iter: 3059  total_loss: 0.6532  loss_cls: 0.4058  loss_box_reg: 0.2564  loss_query: 0.03114  time: 2.3849  data_time: 0.0089  lr: 0.006  max_mem: 10545M\n",
            "\u001b[32m[04/14 20:42:03 d2.utils.events]: \u001b[0m eta: 1 day, 14:00:11  iter: 3079  total_loss: 0.5811  loss_cls: 0.3363  loss_box_reg: 0.2202  loss_query: 0.02271  time: 2.3849  data_time: 0.0070  lr: 0.006  max_mem: 10545M\n",
            "\u001b[32m[04/14 20:42:52 d2.utils.events]: \u001b[0m eta: 1 day, 13:57:53  iter: 3099  total_loss: 0.8258  loss_cls: 0.4861  loss_box_reg: 0.2988  loss_query: 0.03559  time: 2.3845  data_time: 0.0079  lr: 0.006  max_mem: 10545M\n",
            "\u001b[32m[04/14 20:43:41 d2.utils.events]: \u001b[0m eta: 1 day, 13:56:19  iter: 3119  total_loss: 0.7564  loss_cls: 0.4547  loss_box_reg: 0.2702  loss_query: 0.03666  time: 2.3841  data_time: 0.0062  lr: 0.006  max_mem: 10545M\n",
            "\u001b[32m[04/14 20:44:35 d2.utils.events]: \u001b[0m eta: 1 day, 13:55:53  iter: 3139  total_loss: 0.8131  loss_cls: 0.4525  loss_box_reg: 0.2932  loss_query: 0.04093  time: 2.3851  data_time: 0.1436  lr: 0.006  max_mem: 10545M\n",
            "\u001b[32m[04/14 20:45:25 d2.utils.events]: \u001b[0m eta: 1 day, 13:56:39  iter: 3159  total_loss: 0.6909  loss_cls: 0.4139  loss_box_reg: 0.218  loss_query: 0.04249  time: 2.3854  data_time: 0.0075  lr: 0.006  max_mem: 10545M\n",
            "\u001b[32m[04/14 20:46:16 d2.utils.events]: \u001b[0m eta: 1 day, 13:56:10  iter: 3179  total_loss: 0.6138  loss_cls: 0.3644  loss_box_reg: 0.2135  loss_query: 0.02402  time: 2.3855  data_time: 0.0092  lr: 0.006  max_mem: 10545M\n",
            "\u001b[32m[04/14 20:47:03 d2.utils.events]: \u001b[0m eta: 1 day, 13:55:28  iter: 3199  total_loss: 0.7781  loss_cls: 0.4552  loss_box_reg: 0.3004  loss_query: 0.03241  time: 2.3849  data_time: 0.0069  lr: 0.006  max_mem: 10545M\n",
            "\u001b[32m[04/14 20:47:53 d2.utils.events]: \u001b[0m eta: 1 day, 13:55:33  iter: 3219  total_loss: 0.8184  loss_cls: 0.45  loss_box_reg: 0.2859  loss_query: 0.03618  time: 2.3850  data_time: 0.0074  lr: 0.006  max_mem: 10545M\n",
            "\u001b[32m[04/14 20:48:44 d2.utils.events]: \u001b[0m eta: 1 day, 13:53:57  iter: 3239  total_loss: 0.7076  loss_cls: 0.4349  loss_box_reg: 0.2405  loss_query: 0.04041  time: 2.3852  data_time: 0.0084  lr: 0.006  max_mem: 10545M\n",
            "\u001b[32m[04/14 20:49:34 d2.utils.events]: \u001b[0m eta: 1 day, 13:53:04  iter: 3259  total_loss: 0.5733  loss_cls: 0.3309  loss_box_reg: 0.2132  loss_query: 0.02121  time: 2.3848  data_time: 0.0060  lr: 0.006  max_mem: 10545M\n",
            "\u001b[32m[04/14 20:50:23 d2.utils.events]: \u001b[0m eta: 1 day, 13:52:39  iter: 3279  total_loss: 0.7394  loss_cls: 0.4545  loss_box_reg: 0.2473  loss_query: 0.03123  time: 2.3847  data_time: 0.0079  lr: 0.006  max_mem: 10545M\n",
            "\u001b[32m[04/14 20:51:13 d2.utils.events]: \u001b[0m eta: 1 day, 13:51:51  iter: 3299  total_loss: 0.6988  loss_cls: 0.4228  loss_box_reg: 0.2163  loss_query: 0.02762  time: 2.3848  data_time: 0.0070  lr: 0.006  max_mem: 10545M\n",
            "\u001b[32m[04/14 20:52:01 d2.utils.events]: \u001b[0m eta: 1 day, 13:51:45  iter: 3319  total_loss: 0.7356  loss_cls: 0.413  loss_box_reg: 0.2751  loss_query: 0.02726  time: 2.3844  data_time: 0.0073  lr: 0.006  max_mem: 10545M\n",
            "\u001b[32m[04/14 20:52:54 d2.utils.events]: \u001b[0m eta: 1 day, 13:51:44  iter: 3339  total_loss: 0.5494  loss_cls: 0.3374  loss_box_reg: 0.1945  loss_query: 0.01689  time: 2.3849  data_time: 0.0080  lr: 0.006  max_mem: 10545M\n",
            "\u001b[32m[04/14 20:53:45 d2.utils.events]: \u001b[0m eta: 1 day, 13:51:36  iter: 3359  total_loss: 0.7628  loss_cls: 0.4454  loss_box_reg: 0.2693  loss_query: 0.03559  time: 2.3850  data_time: 0.0077  lr: 0.006  max_mem: 10545M\n",
            "\u001b[32m[04/14 20:54:34 d2.utils.events]: \u001b[0m eta: 1 day, 13:50:00  iter: 3379  total_loss: 0.713  loss_cls: 0.4255  loss_box_reg: 0.2543  loss_query: 0.03004  time: 2.3846  data_time: 0.0058  lr: 0.006  max_mem: 10545M\n",
            "\u001b[32m[04/14 20:55:23 d2.utils.events]: \u001b[0m eta: 1 day, 13:49:59  iter: 3399  total_loss: 0.8582  loss_cls: 0.4873  loss_box_reg: 0.3002  loss_query: 0.02739  time: 2.3844  data_time: 0.0083  lr: 0.006  max_mem: 10545M\n",
            "\u001b[32m[04/14 20:56:11 d2.utils.events]: \u001b[0m eta: 1 day, 13:49:04  iter: 3419  total_loss: 0.6094  loss_cls: 0.3675  loss_box_reg: 0.2149  loss_query: 0.03479  time: 2.3840  data_time: 0.0103  lr: 0.006  max_mem: 10545M\n",
            "\u001b[32m[04/14 20:57:00 d2.utils.events]: \u001b[0m eta: 1 day, 13:49:15  iter: 3439  total_loss: 0.6771  loss_cls: 0.4229  loss_box_reg: 0.239  loss_query: 0.03023  time: 2.3838  data_time: 0.0082  lr: 0.006  max_mem: 10545M\n",
            "\u001b[32m[04/14 20:57:52 d2.utils.events]: \u001b[0m eta: 1 day, 13:48:08  iter: 3459  total_loss: 0.6165  loss_cls: 0.4081  loss_box_reg: 0.1821  loss_query: 0.02694  time: 2.3841  data_time: 0.0076  lr: 0.006  max_mem: 10545M\n",
            "\u001b[32m[04/14 20:58:41 d2.utils.events]: \u001b[0m eta: 1 day, 13:47:04  iter: 3479  total_loss: 0.7401  loss_cls: 0.4531  loss_box_reg: 0.2745  loss_query: 0.03777  time: 2.3837  data_time: 0.0067  lr: 0.006  max_mem: 10545M\n",
            "\u001b[32m[04/14 20:59:32 d2.utils.events]: \u001b[0m eta: 1 day, 13:46:47  iter: 3499  total_loss: 0.7051  loss_cls: 0.4449  loss_box_reg: 0.2211  loss_query: 0.02052  time: 2.3838  data_time: 0.0086  lr: 0.006  max_mem: 10545M\n",
            "\u001b[32m[04/14 21:00:20 d2.utils.events]: \u001b[0m eta: 1 day, 13:45:11  iter: 3519  total_loss: 0.593  loss_cls: 0.3882  loss_box_reg: 0.1905  loss_query: 0.02226  time: 2.3833  data_time: 0.0074  lr: 0.006  max_mem: 10545M\n",
            "\u001b[32m[04/14 21:01:10 d2.utils.events]: \u001b[0m eta: 1 day, 13:44:15  iter: 3539  total_loss: 0.7015  loss_cls: 0.3983  loss_box_reg: 0.2787  loss_query: 0.02247  time: 2.3833  data_time: 0.0086  lr: 0.006  max_mem: 10545M\n",
            "\u001b[32m[04/14 21:01:59 d2.utils.events]: \u001b[0m eta: 1 day, 13:43:27  iter: 3559  total_loss: 0.55  loss_cls: 0.3379  loss_box_reg: 0.1942  loss_query: 0.03087  time: 2.3833  data_time: 0.0070  lr: 0.006  max_mem: 10545M\n",
            "\u001b[32m[04/14 21:02:50 d2.utils.events]: \u001b[0m eta: 1 day, 13:43:43  iter: 3579  total_loss: 0.6777  loss_cls: 0.4046  loss_box_reg: 0.2404  loss_query: 0.03639  time: 2.3834  data_time: 0.0069  lr: 0.006  max_mem: 10545M\n",
            "\u001b[32m[04/14 21:03:40 d2.utils.events]: \u001b[0m eta: 1 day, 13:43:26  iter: 3599  total_loss: 0.6325  loss_cls: 0.4013  loss_box_reg: 0.2329  loss_query: 0.02925  time: 2.3833  data_time: 0.0073  lr: 0.006  max_mem: 10545M\n",
            "\u001b[32m[04/14 21:04:30 d2.utils.events]: \u001b[0m eta: 1 day, 13:43:15  iter: 3619  total_loss: 0.597  loss_cls: 0.3728  loss_box_reg: 0.1916  loss_query: 0.03895  time: 2.3831  data_time: 0.0087  lr: 0.006  max_mem: 10545M\n",
            "\u001b[32m[04/14 21:05:19 d2.utils.events]: \u001b[0m eta: 1 day, 13:42:28  iter: 3639  total_loss: 0.6704  loss_cls: 0.4203  loss_box_reg: 0.2289  loss_query: 0.02438  time: 2.3832  data_time: 0.0079  lr: 0.006  max_mem: 10545M\n",
            "\u001b[32m[04/14 21:06:09 d2.utils.events]: \u001b[0m eta: 1 day, 13:43:02  iter: 3659  total_loss: 0.7788  loss_cls: 0.4546  loss_box_reg: 0.2561  loss_query: 0.04024  time: 2.3832  data_time: 0.0067  lr: 0.006  max_mem: 10545M\n",
            "\u001b[32m[04/14 21:06:59 d2.utils.events]: \u001b[0m eta: 1 day, 13:42:40  iter: 3679  total_loss: 0.8376  loss_cls: 0.5097  loss_box_reg: 0.2774  loss_query: 0.05001  time: 2.3829  data_time: 0.0071  lr: 0.006  max_mem: 10545M\n",
            "\u001b[32m[04/14 21:07:48 d2.utils.events]: \u001b[0m eta: 1 day, 13:41:52  iter: 3699  total_loss: 0.668  loss_cls: 0.4097  loss_box_reg: 0.2126  loss_query: 0.04247  time: 2.3826  data_time: 0.0095  lr: 0.006  max_mem: 10545M\n",
            "\u001b[32m[04/14 21:08:38 d2.utils.events]: \u001b[0m eta: 1 day, 13:40:00  iter: 3719  total_loss: 0.545  loss_cls: 0.3219  loss_box_reg: 0.1975  loss_query: 0.03447  time: 2.3825  data_time: 0.0079  lr: 0.006  max_mem: 10545M\n",
            "\u001b[32m[04/14 21:09:27 d2.utils.events]: \u001b[0m eta: 1 day, 13:40:15  iter: 3739  total_loss: 0.7574  loss_cls: 0.4675  loss_box_reg: 0.2416  loss_query: 0.04128  time: 2.3826  data_time: 0.0074  lr: 0.006  max_mem: 10545M\n",
            "\u001b[32m[04/14 21:10:17 d2.utils.events]: \u001b[0m eta: 1 day, 13:39:27  iter: 3759  total_loss: 0.7874  loss_cls: 0.4472  loss_box_reg: 0.2794  loss_query: 0.0318  time: 2.3824  data_time: 0.0067  lr: 0.006  max_mem: 10545M\n",
            "\u001b[32m[04/14 21:11:08 d2.utils.events]: \u001b[0m eta: 1 day, 13:40:29  iter: 3779  total_loss: 0.6986  loss_cls: 0.4072  loss_box_reg: 0.2627  loss_query: 0.02904  time: 2.3830  data_time: 0.0075  lr: 0.006  max_mem: 10545M\n",
            "\u001b[32m[04/14 21:12:00 d2.utils.events]: \u001b[0m eta: 1 day, 13:40:06  iter: 3799  total_loss: 0.653  loss_cls: 0.3885  loss_box_reg: 0.2388  loss_query: 0.02204  time: 2.3828  data_time: 0.0088  lr: 0.006  max_mem: 10545M\n",
            "\u001b[32m[04/14 21:12:50 d2.utils.events]: \u001b[0m eta: 1 day, 13:39:18  iter: 3819  total_loss: 0.6361  loss_cls: 0.352  loss_box_reg: 0.2186  loss_query: 0.02691  time: 2.3828  data_time: 0.0069  lr: 0.006  max_mem: 10545M\n",
            "\u001b[32m[04/14 21:13:42 d2.utils.events]: \u001b[0m eta: 1 day, 13:38:39  iter: 3839  total_loss: 0.9441  loss_cls: 0.586  loss_box_reg: 0.3132  loss_query: 0.05199  time: 2.3829  data_time: 0.0073  lr: 0.006  max_mem: 10545M\n",
            "\u001b[32m[04/14 21:14:31 d2.utils.events]: \u001b[0m eta: 1 day, 13:37:42  iter: 3859  total_loss: 0.5191  loss_cls: 0.3146  loss_box_reg: 0.1646  loss_query: 0.01968  time: 2.3828  data_time: 0.0071  lr: 0.006  max_mem: 10545M\n",
            "\u001b[32m[04/14 21:15:19 d2.utils.events]: \u001b[0m eta: 1 day, 13:36:39  iter: 3879  total_loss: 0.8292  loss_cls: 0.499  loss_box_reg: 0.2889  loss_query: 0.04054  time: 2.3826  data_time: 0.0076  lr: 0.006  max_mem: 10545M\n",
            "\u001b[32m[04/14 21:16:08 d2.utils.events]: \u001b[0m eta: 1 day, 13:35:22  iter: 3899  total_loss: 0.5123  loss_cls: 0.3144  loss_box_reg: 0.1702  loss_query: 0.02679  time: 2.3824  data_time: 0.0091  lr: 0.006  max_mem: 10545M\n",
            "\u001b[32m[04/14 21:16:58 d2.utils.events]: \u001b[0m eta: 1 day, 13:34:09  iter: 3919  total_loss: 0.5953  loss_cls: 0.3457  loss_box_reg: 0.1901  loss_query: 0.02947  time: 2.3822  data_time: 0.0114  lr: 0.006  max_mem: 10545M\n",
            "\u001b[32m[04/14 21:17:49 d2.utils.events]: \u001b[0m eta: 1 day, 13:34:14  iter: 3939  total_loss: 0.7058  loss_cls: 0.4084  loss_box_reg: 0.1829  loss_query: 0.045  time: 2.3822  data_time: 0.0111  lr: 0.006  max_mem: 10545M\n",
            "\u001b[32m[04/14 21:18:39 d2.utils.events]: \u001b[0m eta: 1 day, 13:33:11  iter: 3959  total_loss: 0.6527  loss_cls: 0.3996  loss_box_reg: 0.23  loss_query: 0.0237  time: 2.3821  data_time: 0.0102  lr: 0.006  max_mem: 10545M\n",
            "\u001b[32m[04/14 21:19:28 d2.utils.events]: \u001b[0m eta: 1 day, 13:31:36  iter: 3979  total_loss: 0.782  loss_cls: 0.4471  loss_box_reg: 0.2754  loss_query: 0.02958  time: 2.3821  data_time: 0.0073  lr: 0.006  max_mem: 10545M\n",
            "\u001b[32m[04/14 21:20:18 fvcore.common.checkpoint]: \u001b[0mSaving checkpoint to work_dirs/visdrone_querydet/model_0003999.pth\n",
            "\u001b[32m[04/14 21:20:41 d2.utils.events]: \u001b[0m eta: 1 day, 13:30:43  iter: 3999  total_loss: 0.7891  loss_cls: 0.4262  loss_box_reg: 0.2766  loss_query: 0.02887  time: 2.3821  data_time: 0.0078  lr: 0.006  max_mem: 10545M\n",
            "\u001b[32m[04/14 21:21:32 d2.utils.events]: \u001b[0m eta: 1 day, 13:30:00  iter: 4019  total_loss: 0.5458  loss_cls: 0.3333  loss_box_reg: 0.2012  loss_query: 0.02906  time: 2.3820  data_time: 0.0068  lr: 0.006  max_mem: 10545M\n",
            "\u001b[32m[04/14 21:22:20 d2.utils.events]: \u001b[0m eta: 1 day, 13:29:06  iter: 4039  total_loss: 0.7056  loss_cls: 0.4671  loss_box_reg: 0.2344  loss_query: 0.03243  time: 2.3815  data_time: 0.0084  lr: 0.006  max_mem: 10545M\n",
            "\u001b[32m[04/14 21:23:09 d2.utils.events]: \u001b[0m eta: 1 day, 13:28:31  iter: 4059  total_loss: 0.7542  loss_cls: 0.4788  loss_box_reg: 0.2406  loss_query: 0.03724  time: 2.3813  data_time: 0.0082  lr: 0.006  max_mem: 10545M\n",
            "\u001b[32m[04/14 21:23:57 d2.utils.events]: \u001b[0m eta: 1 day, 13:27:23  iter: 4079  total_loss: 0.7162  loss_cls: 0.4881  loss_box_reg: 0.2476  loss_query: 0.03542  time: 2.3810  data_time: 0.0075  lr: 0.006  max_mem: 10545M\n",
            "\u001b[32m[04/14 21:24:46 d2.utils.events]: \u001b[0m eta: 1 day, 13:26:47  iter: 4099  total_loss: 0.7197  loss_cls: 0.4804  loss_box_reg: 0.222  loss_query: 0.03334  time: 2.3809  data_time: 0.0090  lr: 0.006  max_mem: 10545M\n",
            "\u001b[32m[04/14 21:25:35 d2.utils.events]: \u001b[0m eta: 1 day, 13:26:32  iter: 4119  total_loss: 0.6254  loss_cls: 0.3936  loss_box_reg: 0.2132  loss_query: 0.03586  time: 2.3809  data_time: 0.0080  lr: 0.006  max_mem: 10545M\n",
            "\u001b[32m[04/14 21:26:25 d2.utils.events]: \u001b[0m eta: 1 day, 13:25:18  iter: 4139  total_loss: 0.7653  loss_cls: 0.4862  loss_box_reg: 0.2618  loss_query: 0.0346  time: 2.3810  data_time: 0.0078  lr: 0.006  max_mem: 10545M\n",
            "\u001b[32m[04/14 21:27:14 d2.utils.events]: \u001b[0m eta: 1 day, 13:22:53  iter: 4159  total_loss: 0.7886  loss_cls: 0.4625  loss_box_reg: 0.2392  loss_query: 0.03837  time: 2.3810  data_time: 0.0081  lr: 0.006  max_mem: 10545M\n",
            "\u001b[32m[04/14 21:28:01 d2.utils.events]: \u001b[0m eta: 1 day, 13:21:43  iter: 4179  total_loss: 0.6086  loss_cls: 0.3427  loss_box_reg: 0.2381  loss_query: 0.02513  time: 2.3804  data_time: 0.0072  lr: 0.006  max_mem: 10545M\n",
            "\u001b[32m[04/14 21:28:50 d2.utils.events]: \u001b[0m eta: 1 day, 13:20:55  iter: 4199  total_loss: 0.7069  loss_cls: 0.4086  loss_box_reg: 0.2751  loss_query: 0.0364  time: 2.3804  data_time: 0.0069  lr: 0.006  max_mem: 10545M\n",
            "\u001b[32m[04/14 21:29:39 d2.utils.events]: \u001b[0m eta: 1 day, 13:19:29  iter: 4219  total_loss: 0.699  loss_cls: 0.4022  loss_box_reg: 0.2191  loss_query: 0.04746  time: 2.3802  data_time: 0.0067  lr: 0.006  max_mem: 10545M\n",
            "\u001b[32m[04/14 21:30:27 d2.utils.events]: \u001b[0m eta: 1 day, 13:17:49  iter: 4239  total_loss: 0.6333  loss_cls: 0.4009  loss_box_reg: 0.2006  loss_query: 0.04443  time: 2.3798  data_time: 0.0067  lr: 0.006  max_mem: 10545M\n",
            "\u001b[32m[04/14 21:31:20 d2.utils.events]: \u001b[0m eta: 1 day, 13:18:19  iter: 4259  total_loss: 0.8649  loss_cls: 0.4878  loss_box_reg: 0.3142  loss_query: 0.03092  time: 2.3805  data_time: 0.1941  lr: 0.006  max_mem: 10545M\n",
            "\u001b[32m[04/14 21:32:10 d2.utils.events]: \u001b[0m eta: 1 day, 13:18:04  iter: 4279  total_loss: 0.5348  loss_cls: 0.325  loss_box_reg: 0.1912  loss_query: 0.02695  time: 2.3808  data_time: 0.0067  lr: 0.006  max_mem: 10545M\n",
            "\u001b[32m[04/14 21:33:00 d2.utils.events]: \u001b[0m eta: 1 day, 13:17:15  iter: 4299  total_loss: 0.5122  loss_cls: 0.3092  loss_box_reg: 0.1695  loss_query: 0.02373  time: 2.3807  data_time: 0.0078  lr: 0.006  max_mem: 10545M\n",
            "\u001b[32m[04/14 21:33:49 d2.utils.events]: \u001b[0m eta: 1 day, 13:16:59  iter: 4319  total_loss: 0.7539  loss_cls: 0.4695  loss_box_reg: 0.2503  loss_query: 0.04011  time: 2.3808  data_time: 0.0086  lr: 0.006  max_mem: 10545M\n",
            "\u001b[32m[04/14 21:34:39 d2.utils.events]: \u001b[0m eta: 1 day, 13:15:06  iter: 4339  total_loss: 0.6625  loss_cls: 0.3848  loss_box_reg: 0.2297  loss_query: 0.03178  time: 2.3809  data_time: 0.0067  lr: 0.006  max_mem: 10545M\n",
            "\u001b[32m[04/14 21:35:29 d2.utils.events]: \u001b[0m eta: 1 day, 13:14:34  iter: 4359  total_loss: 0.9044  loss_cls: 0.5397  loss_box_reg: 0.3278  loss_query: 0.04357  time: 2.3809  data_time: 0.0071  lr: 0.006  max_mem: 10545M\n",
            "\u001b[32m[04/14 21:36:17 d2.utils.events]: \u001b[0m eta: 1 day, 13:14:16  iter: 4379  total_loss: 0.5888  loss_cls: 0.3554  loss_box_reg: 0.1924  loss_query: 0.02951  time: 2.3806  data_time: 0.0074  lr: 0.006  max_mem: 10545M\n",
            "\u001b[32m[04/14 21:37:05 d2.utils.events]: \u001b[0m eta: 1 day, 13:12:16  iter: 4399  total_loss: 0.472  loss_cls: 0.3205  loss_box_reg: 0.1525  loss_query: 0.01915  time: 2.3803  data_time: 0.0076  lr: 0.006  max_mem: 10545M\n",
            "\u001b[32m[04/14 21:37:53 d2.utils.events]: \u001b[0m eta: 1 day, 13:11:15  iter: 4419  total_loss: 0.7592  loss_cls: 0.4508  loss_box_reg: 0.2498  loss_query: 0.03102  time: 2.3800  data_time: 0.0059  lr: 0.006  max_mem: 10545M\n",
            "\u001b[32m[04/14 21:38:43 d2.utils.events]: \u001b[0m eta: 1 day, 13:10:39  iter: 4439  total_loss: 0.7078  loss_cls: 0.4319  loss_box_reg: 0.2462  loss_query: 0.02254  time: 2.3801  data_time: 0.0078  lr: 0.006  max_mem: 10545M\n",
            "\u001b[32m[04/14 21:39:31 d2.utils.events]: \u001b[0m eta: 1 day, 13:09:17  iter: 4459  total_loss: 0.6018  loss_cls: 0.3709  loss_box_reg: 0.1987  loss_query: 0.03342  time: 2.3800  data_time: 0.0083  lr: 0.006  max_mem: 10545M\n",
            "\u001b[32m[04/14 21:40:20 d2.utils.events]: \u001b[0m eta: 1 day, 13:08:49  iter: 4479  total_loss: 0.8131  loss_cls: 0.4804  loss_box_reg: 0.2983  loss_query: 0.03831  time: 2.3798  data_time: 0.0069  lr: 0.006  max_mem: 10545M\n",
            "\u001b[32m[04/14 21:41:09 d2.utils.events]: \u001b[0m eta: 1 day, 13:07:41  iter: 4499  total_loss: 0.7484  loss_cls: 0.4492  loss_box_reg: 0.2607  loss_query: 0.03549  time: 2.3797  data_time: 0.0083  lr: 0.006  max_mem: 10545M\n",
            "\u001b[32m[04/14 21:42:00 d2.utils.events]: \u001b[0m eta: 1 day, 13:07:35  iter: 4519  total_loss: 0.6368  loss_cls: 0.393  loss_box_reg: 0.2354  loss_query: 0.04164  time: 2.3799  data_time: 0.0077  lr: 0.006  max_mem: 10545M\n",
            "\u001b[32m[04/14 21:42:48 d2.utils.events]: \u001b[0m eta: 1 day, 13:06:47  iter: 4539  total_loss: 0.5598  loss_cls: 0.3372  loss_box_reg: 0.1914  loss_query: 0.01722  time: 2.3798  data_time: 0.0087  lr: 0.006  max_mem: 10545M\n",
            "\u001b[32m[04/14 21:43:38 d2.utils.events]: \u001b[0m eta: 1 day, 13:05:59  iter: 4559  total_loss: 0.6267  loss_cls: 0.3932  loss_box_reg: 0.2266  loss_query: 0.0259  time: 2.3799  data_time: 0.0083  lr: 0.006  max_mem: 10545M\n",
            "\u001b[32m[04/14 21:44:28 d2.utils.events]: \u001b[0m eta: 1 day, 13:05:18  iter: 4579  total_loss: 0.6087  loss_cls: 0.365  loss_box_reg: 0.2143  loss_query: 0.02438  time: 2.3800  data_time: 0.0063  lr: 0.006  max_mem: 10545M\n",
            "\u001b[32m[04/14 21:45:16 d2.utils.events]: \u001b[0m eta: 1 day, 13:04:22  iter: 4599  total_loss: 0.7948  loss_cls: 0.4972  loss_box_reg: 0.2656  loss_query: 0.04484  time: 2.3797  data_time: 0.0066  lr: 0.006  max_mem: 10545M\n",
            "\u001b[32m[04/14 21:46:06 d2.utils.events]: \u001b[0m eta: 1 day, 13:03:26  iter: 4619  total_loss: 0.654  loss_cls: 0.3843  loss_box_reg: 0.2294  loss_query: 0.02913  time: 2.3798  data_time: 0.0080  lr: 0.006  max_mem: 10545M\n",
            "\u001b[32m[04/14 21:46:52 d2.utils.events]: \u001b[0m eta: 1 day, 13:01:34  iter: 4639  total_loss: 0.5025  loss_cls: 0.3153  loss_box_reg: 0.1592  loss_query: 0.02166  time: 2.3792  data_time: 0.0095  lr: 0.006  max_mem: 10545M\n",
            "\u001b[32m[04/14 21:47:42 d2.utils.events]: \u001b[0m eta: 1 day, 13:00:55  iter: 4659  total_loss: 0.9924  loss_cls: 0.571  loss_box_reg: 0.3292  loss_query: 0.04502  time: 2.3792  data_time: 0.0119  lr: 0.006  max_mem: 10545M\n",
            "\u001b[32m[04/14 21:48:30 d2.utils.events]: \u001b[0m eta: 1 day, 12:59:45  iter: 4679  total_loss: 0.6657  loss_cls: 0.3916  loss_box_reg: 0.2393  loss_query: 0.02925  time: 2.3790  data_time: 0.0069  lr: 0.006  max_mem: 10545M\n",
            "\u001b[32m[04/14 21:49:20 d2.utils.events]: \u001b[0m eta: 1 day, 12:58:30  iter: 4699  total_loss: 0.8718  loss_cls: 0.5087  loss_box_reg: 0.3108  loss_query: 0.0403  time: 2.3790  data_time: 0.0084  lr: 0.006  max_mem: 10545M\n",
            "\u001b[32m[04/14 21:50:09 d2.utils.events]: \u001b[0m eta: 1 day, 12:57:22  iter: 4719  total_loss: 0.4442  loss_cls: 0.2568  loss_box_reg: 0.1398  loss_query: 0.01807  time: 2.3789  data_time: 0.0083  lr: 0.006  max_mem: 10545M\n",
            "\u001b[32m[04/14 21:50:58 d2.utils.events]: \u001b[0m eta: 1 day, 12:55:51  iter: 4739  total_loss: 0.7023  loss_cls: 0.4378  loss_box_reg: 0.2345  loss_query: 0.025  time: 2.3789  data_time: 0.0064  lr: 0.006  max_mem: 10545M\n",
            "\u001b[32m[04/14 21:51:46 d2.utils.events]: \u001b[0m eta: 1 day, 12:53:53  iter: 4759  total_loss: 0.7401  loss_cls: 0.4273  loss_box_reg: 0.2744  loss_query: 0.02309  time: 2.3786  data_time: 0.0087  lr: 0.006  max_mem: 10545M\n",
            "\u001b[32m[04/14 21:52:34 d2.utils.events]: \u001b[0m eta: 1 day, 12:51:57  iter: 4779  total_loss: 0.7937  loss_cls: 0.4727  loss_box_reg: 0.2666  loss_query: 0.02943  time: 2.3782  data_time: 0.0072  lr: 0.006  max_mem: 10545M\n",
            "\u001b[32m[04/14 21:53:23 d2.utils.events]: \u001b[0m eta: 1 day, 12:51:09  iter: 4799  total_loss: 0.5423  loss_cls: 0.3326  loss_box_reg: 0.1973  loss_query: 0.02499  time: 2.3782  data_time: 0.0075  lr: 0.006  max_mem: 10545M\n",
            "\u001b[32m[04/14 21:54:13 d2.utils.events]: \u001b[0m eta: 1 day, 12:50:43  iter: 4819  total_loss: 0.6439  loss_cls: 0.3891  loss_box_reg: 0.2437  loss_query: 0.03718  time: 2.3783  data_time: 0.0086  lr: 0.006  max_mem: 10545M\n",
            "\u001b[32m[04/14 21:55:02 d2.utils.events]: \u001b[0m eta: 1 day, 12:49:47  iter: 4839  total_loss: 0.5655  loss_cls: 0.3741  loss_box_reg: 0.2047  loss_query: 0.01946  time: 2.3781  data_time: 0.0085  lr: 0.006  max_mem: 10545M\n",
            "\u001b[32m[04/14 21:55:52 d2.utils.events]: \u001b[0m eta: 1 day, 12:49:39  iter: 4859  total_loss: 0.6662  loss_cls: 0.4078  loss_box_reg: 0.2114  loss_query: 0.02991  time: 2.3783  data_time: 0.0081  lr: 0.006  max_mem: 10545M\n",
            "\u001b[32m[04/14 21:56:42 d2.utils.events]: \u001b[0m eta: 1 day, 12:48:51  iter: 4879  total_loss: 0.7917  loss_cls: 0.4837  loss_box_reg: 0.2895  loss_query: 0.0491  time: 2.3784  data_time: 0.0078  lr: 0.006  max_mem: 10545M\n",
            "\u001b[32m[04/14 21:57:32 d2.utils.events]: \u001b[0m eta: 1 day, 12:48:55  iter: 4899  total_loss: 0.5834  loss_cls: 0.3762  loss_box_reg: 0.1882  loss_query: 0.02274  time: 2.3784  data_time: 0.0068  lr: 0.006  max_mem: 10545M\n",
            "\u001b[32m[04/14 21:58:20 d2.utils.events]: \u001b[0m eta: 1 day, 12:47:05  iter: 4919  total_loss: 0.6925  loss_cls: 0.417  loss_box_reg: 0.2777  loss_query: 0.02095  time: 2.3783  data_time: 0.0115  lr: 0.006  max_mem: 10545M\n",
            "\u001b[32m[04/14 21:59:10 d2.utils.events]: \u001b[0m eta: 1 day, 12:45:47  iter: 4939  total_loss: 0.8103  loss_cls: 0.5038  loss_box_reg: 0.2952  loss_query: 0.03278  time: 2.3783  data_time: 0.0065  lr: 0.006  max_mem: 10545M\n",
            "\u001b[32m[04/14 21:59:58 d2.utils.events]: \u001b[0m eta: 1 day, 12:44:39  iter: 4959  total_loss: 0.5251  loss_cls: 0.3536  loss_box_reg: 0.1979  loss_query: 0.02005  time: 2.3780  data_time: 0.0062  lr: 0.006  max_mem: 10545M\n",
            "\u001b[32m[04/14 22:00:47 d2.utils.events]: \u001b[0m eta: 1 day, 12:44:01  iter: 4979  total_loss: 0.5691  loss_cls: 0.3545  loss_box_reg: 0.1924  loss_query: 0.0376  time: 2.3780  data_time: 0.0085  lr: 0.006  max_mem: 10545M\n",
            "\u001b[32m[04/14 22:01:37 d2.utils.events]: \u001b[0m eta: 1 day, 12:43:04  iter: 4999  total_loss: 0.549  loss_cls: 0.3427  loss_box_reg: 0.2072  loss_query: 0.02422  time: 2.3781  data_time: 0.0070  lr: 0.006  max_mem: 10545M\n",
            "\u001b[32m[04/14 22:02:28 d2.utils.events]: \u001b[0m eta: 1 day, 12:42:21  iter: 5019  total_loss: 0.6671  loss_cls: 0.422  loss_box_reg: 0.2457  loss_query: 0.03436  time: 2.3784  data_time: 0.0073  lr: 0.006  max_mem: 10545M\n",
            "\u001b[32m[04/14 22:03:18 d2.utils.events]: \u001b[0m eta: 1 day, 12:42:00  iter: 5039  total_loss: 0.6124  loss_cls: 0.3473  loss_box_reg: 0.167  loss_query: 0.02866  time: 2.3784  data_time: 0.0109  lr: 0.006  max_mem: 10545M\n",
            "\u001b[32m[04/14 22:04:06 d2.utils.events]: \u001b[0m eta: 1 day, 12:40:09  iter: 5059  total_loss: 0.6393  loss_cls: 0.3866  loss_box_reg: 0.2149  loss_query: 0.02191  time: 2.3781  data_time: 0.0088  lr: 0.006  max_mem: 10545M\n",
            "\u001b[32m[04/14 22:04:54 d2.utils.events]: \u001b[0m eta: 1 day, 12:39:52  iter: 5079  total_loss: 0.7454  loss_cls: 0.4424  loss_box_reg: 0.2455  loss_query: 0.03593  time: 2.3779  data_time: 0.0093  lr: 0.006  max_mem: 10545M\n",
            "\u001b[32m[04/14 22:05:43 d2.utils.events]: \u001b[0m eta: 1 day, 12:39:15  iter: 5099  total_loss: 0.907  loss_cls: 0.538  loss_box_reg: 0.2954  loss_query: 0.04401  time: 2.3778  data_time: 0.0076  lr: 0.006  max_mem: 10545M\n",
            "\u001b[32m[04/14 22:06:32 d2.utils.events]: \u001b[0m eta: 1 day, 12:38:20  iter: 5119  total_loss: 0.6685  loss_cls: 0.4001  loss_box_reg: 0.2371  loss_query: 0.04187  time: 2.3778  data_time: 0.0103  lr: 0.006  max_mem: 10545M\n",
            "\u001b[32m[04/14 22:07:22 d2.utils.events]: \u001b[0m eta: 1 day, 12:37:20  iter: 5139  total_loss: 0.4523  loss_cls: 0.2795  loss_box_reg: 0.1361  loss_query: 0.02564  time: 2.3779  data_time: 0.0082  lr: 0.006  max_mem: 10545M\n",
            "\u001b[32m[04/14 22:08:12 d2.utils.events]: \u001b[0m eta: 1 day, 12:36:49  iter: 5159  total_loss: 0.739  loss_cls: 0.4691  loss_box_reg: 0.2633  loss_query: 0.03686  time: 2.3779  data_time: 0.0068  lr: 0.006  max_mem: 10545M\n",
            "\u001b[32m[04/14 22:09:01 d2.utils.events]: \u001b[0m eta: 1 day, 12:36:50  iter: 5179  total_loss: 0.4342  loss_cls: 0.3123  loss_box_reg: 0.131  loss_query: 0.02151  time: 2.3780  data_time: 0.0077  lr: 0.006  max_mem: 10545M\n",
            "\u001b[32m[04/14 22:09:50 d2.utils.events]: \u001b[0m eta: 1 day, 12:35:36  iter: 5199  total_loss: 0.692  loss_cls: 0.423  loss_box_reg: 0.2244  loss_query: 0.02929  time: 2.3779  data_time: 0.0076  lr: 0.006  max_mem: 10545M\n",
            "\u001b[32m[04/14 22:10:39 d2.utils.events]: \u001b[0m eta: 1 day, 12:34:48  iter: 5219  total_loss: 0.5829  loss_cls: 0.3746  loss_box_reg: 0.1679  loss_query: 0.02207  time: 2.3778  data_time: 0.0072  lr: 0.006  max_mem: 10545M\n",
            "\u001b[32m[04/14 22:11:30 d2.utils.events]: \u001b[0m eta: 1 day, 12:35:17  iter: 5239  total_loss: 0.5699  loss_cls: 0.382  loss_box_reg: 0.2049  loss_query: 0.02963  time: 2.3780  data_time: 0.0072  lr: 0.006  max_mem: 10545M\n",
            "\u001b[32m[04/14 22:12:19 d2.utils.events]: \u001b[0m eta: 1 day, 12:33:37  iter: 5259  total_loss: 0.6265  loss_cls: 0.3613  loss_box_reg: 0.2106  loss_query: 0.02469  time: 2.3780  data_time: 0.0107  lr: 0.006  max_mem: 10545M\n",
            "\u001b[32m[04/14 22:13:08 d2.utils.events]: \u001b[0m eta: 1 day, 12:31:56  iter: 5279  total_loss: 0.7071  loss_cls: 0.4355  loss_box_reg: 0.2231  loss_query: 0.03766  time: 2.3779  data_time: 0.0073  lr: 0.006  max_mem: 10545M\n",
            "\u001b[32m[04/14 22:13:57 d2.utils.events]: \u001b[0m eta: 1 day, 12:30:33  iter: 5299  total_loss: 0.5459  loss_cls: 0.3298  loss_box_reg: 0.2003  loss_query: 0.02715  time: 2.3779  data_time: 0.0071  lr: 0.006  max_mem: 10545M\n",
            "\u001b[32m[04/14 22:14:46 d2.utils.events]: \u001b[0m eta: 1 day, 12:29:03  iter: 5319  total_loss: 0.5944  loss_cls: 0.3646  loss_box_reg: 0.2171  loss_query: 0.03924  time: 2.3779  data_time: 0.0086  lr: 0.006  max_mem: 10545M\n",
            "\u001b[32m[04/14 22:15:36 d2.utils.events]: \u001b[0m eta: 1 day, 12:27:59  iter: 5339  total_loss: 0.6711  loss_cls: 0.4174  loss_box_reg: 0.2092  loss_query: 0.02808  time: 2.3778  data_time: 0.0072  lr: 0.006  max_mem: 10545M\n",
            "\u001b[32m[04/14 22:16:24 d2.utils.events]: \u001b[0m eta: 1 day, 12:26:46  iter: 5359  total_loss: 0.6259  loss_cls: 0.3603  loss_box_reg: 0.2235  loss_query: 0.02867  time: 2.3777  data_time: 0.0082  lr: 0.006  max_mem: 10545M\n",
            "\u001b[32m[04/14 22:17:14 d2.utils.events]: \u001b[0m eta: 1 day, 12:26:09  iter: 5379  total_loss: 0.5766  loss_cls: 0.3377  loss_box_reg: 0.1746  loss_query: 0.02779  time: 2.3778  data_time: 0.0070  lr: 0.006  max_mem: 10545M\n",
            "\u001b[32m[04/14 22:18:04 d2.utils.events]: \u001b[0m eta: 1 day, 12:25:35  iter: 5399  total_loss: 0.6947  loss_cls: 0.4436  loss_box_reg: 0.2156  loss_query: 0.03182  time: 2.3778  data_time: 0.0073  lr: 0.006  max_mem: 10545M\n",
            "\u001b[32m[04/14 22:18:53 d2.utils.events]: \u001b[0m eta: 1 day, 12:25:02  iter: 5419  total_loss: 0.6274  loss_cls: 0.3693  loss_box_reg: 0.2293  loss_query: 0.03521  time: 2.3776  data_time: 0.0066  lr: 0.006  max_mem: 10545M\n",
            "\u001b[32m[04/14 22:19:41 d2.utils.events]: \u001b[0m eta: 1 day, 12:23:24  iter: 5439  total_loss: 0.5874  loss_cls: 0.3764  loss_box_reg: 0.1834  loss_query: 0.01839  time: 2.3774  data_time: 0.0080  lr: 0.006  max_mem: 10545M\n",
            "\u001b[32m[04/14 22:20:30 d2.utils.events]: \u001b[0m eta: 1 day, 12:22:56  iter: 5459  total_loss: 0.6689  loss_cls: 0.4268  loss_box_reg: 0.258  loss_query: 0.02173  time: 2.3772  data_time: 0.0083  lr: 0.006  max_mem: 10545M\n",
            "\u001b[32m[04/14 22:21:18 d2.utils.events]: \u001b[0m eta: 1 day, 12:22:16  iter: 5479  total_loss: 0.5482  loss_cls: 0.3787  loss_box_reg: 0.1404  loss_query: 0.02336  time: 2.3770  data_time: 0.0091  lr: 0.006  max_mem: 10545M\n",
            "\u001b[32m[04/14 22:22:07 d2.utils.events]: \u001b[0m eta: 1 day, 12:21:29  iter: 5499  total_loss: 0.9072  loss_cls: 0.5328  loss_box_reg: 0.2933  loss_query: 0.02841  time: 2.3770  data_time: 0.0081  lr: 0.006  max_mem: 10545M\n",
            "\u001b[32m[04/14 22:22:58 d2.utils.events]: \u001b[0m eta: 1 day, 12:20:40  iter: 5519  total_loss: 0.5921  loss_cls: 0.3623  loss_box_reg: 0.1837  loss_query: 0.03732  time: 2.3772  data_time: 0.0083  lr: 0.006  max_mem: 10545M\n",
            "\u001b[32m[04/14 22:23:46 d2.utils.events]: \u001b[0m eta: 1 day, 12:19:52  iter: 5539  total_loss: 0.5394  loss_cls: 0.3088  loss_box_reg: 0.1769  loss_query: 0.03012  time: 2.3770  data_time: 0.0074  lr: 0.006  max_mem: 10545M\n",
            "\u001b[32m[04/14 22:24:35 d2.utils.events]: \u001b[0m eta: 1 day, 12:18:29  iter: 5559  total_loss: 0.6659  loss_cls: 0.3638  loss_box_reg: 0.2162  loss_query: 0.01744  time: 2.3769  data_time: 0.0089  lr: 0.006  max_mem: 10545M\n",
            "\u001b[32m[04/14 22:25:23 d2.utils.events]: \u001b[0m eta: 1 day, 12:15:12  iter: 5579  total_loss: 0.712  loss_cls: 0.4178  loss_box_reg: 0.2554  loss_query: 0.01599  time: 2.3766  data_time: 0.0080  lr: 0.006  max_mem: 10545M\n",
            "\u001b[32m[04/14 22:26:13 d2.utils.events]: \u001b[0m eta: 1 day, 12:15:19  iter: 5599  total_loss: 0.9613  loss_cls: 0.5168  loss_box_reg: 0.3244  loss_query: 0.04121  time: 2.3767  data_time: 0.0081  lr: 0.006  max_mem: 10545M\n",
            "\u001b[32m[04/14 22:27:01 d2.utils.events]: \u001b[0m eta: 1 day, 12:15:05  iter: 5619  total_loss: 0.7045  loss_cls: 0.4021  loss_box_reg: 0.2349  loss_query: 0.0368  time: 2.3765  data_time: 0.0078  lr: 0.006  max_mem: 10545M\n",
            "\u001b[32m[04/14 22:27:50 d2.utils.events]: \u001b[0m eta: 1 day, 12:15:08  iter: 5639  total_loss: 0.5442  loss_cls: 0.3257  loss_box_reg: 0.2042  loss_query: 0.03354  time: 2.3765  data_time: 0.0084  lr: 0.006  max_mem: 10545M\n",
            "\u001b[32m[04/14 22:28:39 d2.utils.events]: \u001b[0m eta: 1 day, 12:14:03  iter: 5659  total_loss: 0.7983  loss_cls: 0.477  loss_box_reg: 0.3054  loss_query: 0.04022  time: 2.3763  data_time: 0.0075  lr: 0.006  max_mem: 10545M\n",
            "\u001b[32m[04/14 22:29:28 d2.utils.events]: \u001b[0m eta: 1 day, 12:13:41  iter: 5679  total_loss: 0.6011  loss_cls: 0.3784  loss_box_reg: 0.1824  loss_query: 0.03604  time: 2.3763  data_time: 0.0090  lr: 0.006  max_mem: 10545M\n",
            "\u001b[32m[04/14 22:30:18 d2.utils.events]: \u001b[0m eta: 1 day, 12:12:53  iter: 5699  total_loss: 0.5375  loss_cls: 0.365  loss_box_reg: 0.1652  loss_query: 0.0251  time: 2.3764  data_time: 0.0088  lr: 0.006  max_mem: 10545M\n",
            "\u001b[32m[04/14 22:31:07 d2.utils.events]: \u001b[0m eta: 1 day, 12:12:21  iter: 5719  total_loss: 0.6091  loss_cls: 0.3976  loss_box_reg: 0.1863  loss_query: 0.02394  time: 2.3763  data_time: 0.0078  lr: 0.006  max_mem: 10545M\n",
            "\u001b[32m[04/14 22:31:55 d2.utils.events]: \u001b[0m eta: 1 day, 12:11:33  iter: 5739  total_loss: 0.5927  loss_cls: 0.3371  loss_box_reg: 0.2207  loss_query: 0.03198  time: 2.3761  data_time: 0.0089  lr: 0.006  max_mem: 10545M\n",
            "\u001b[32m[04/14 22:32:43 d2.utils.events]: \u001b[0m eta: 1 day, 12:10:51  iter: 5759  total_loss: 0.5463  loss_cls: 0.3235  loss_box_reg: 0.2013  loss_query: 0.01896  time: 2.3760  data_time: 0.0088  lr: 0.006  max_mem: 10545M\n",
            "\u001b[32m[04/14 22:33:34 d2.utils.events]: \u001b[0m eta: 1 day, 12:10:18  iter: 5779  total_loss: 0.8707  loss_cls: 0.5077  loss_box_reg: 0.3108  loss_query: 0.03981  time: 2.3762  data_time: 0.0081  lr: 0.006  max_mem: 10545M\n",
            "\u001b[32m[04/14 22:34:24 d2.utils.events]: \u001b[0m eta: 1 day, 12:09:30  iter: 5799  total_loss: 0.6964  loss_cls: 0.4104  loss_box_reg: 0.2378  loss_query: 0.03884  time: 2.3762  data_time: 0.0114  lr: 0.006  max_mem: 10545M\n",
            "\u001b[32m[04/14 22:35:13 d2.utils.events]: \u001b[0m eta: 1 day, 12:08:40  iter: 5819  total_loss: 0.6887  loss_cls: 0.4194  loss_box_reg: 0.2428  loss_query: 0.03993  time: 2.3762  data_time: 0.0096  lr: 0.006  max_mem: 10545M\n",
            "\u001b[32m[04/14 22:36:03 d2.utils.events]: \u001b[0m eta: 1 day, 12:08:58  iter: 5839  total_loss: 0.7171  loss_cls: 0.4192  loss_box_reg: 0.2607  loss_query: 0.0346  time: 2.3764  data_time: 0.0078  lr: 0.006  max_mem: 10545M\n",
            "\u001b[32m[04/14 22:36:50 d2.utils.events]: \u001b[0m eta: 1 day, 12:07:03  iter: 5859  total_loss: 0.4071  loss_cls: 0.2505  loss_box_reg: 0.1548  loss_query: 0.02112  time: 2.3759  data_time: 0.0089  lr: 0.006  max_mem: 10545M\n",
            "\u001b[32m[04/14 22:37:39 d2.utils.events]: \u001b[0m eta: 1 day, 12:06:17  iter: 5879  total_loss: 0.8814  loss_cls: 0.4693  loss_box_reg: 0.3298  loss_query: 0.04443  time: 2.3758  data_time: 0.0083  lr: 0.006  max_mem: 10545M\n",
            "\u001b[32m[04/14 22:38:28 d2.utils.events]: \u001b[0m eta: 1 day, 12:05:55  iter: 5899  total_loss: 0.5191  loss_cls: 0.3095  loss_box_reg: 0.1723  loss_query: 0.02407  time: 2.3758  data_time: 0.0075  lr: 0.006  max_mem: 10545M\n",
            "\u001b[32m[04/14 22:39:18 d2.utils.events]: \u001b[0m eta: 1 day, 12:06:17  iter: 5919  total_loss: 0.7606  loss_cls: 0.4315  loss_box_reg: 0.2649  loss_query: 0.03645  time: 2.3759  data_time: 0.0075  lr: 0.006  max_mem: 10545M\n",
            "\u001b[32m[04/14 22:40:06 d2.utils.events]: \u001b[0m eta: 1 day, 12:05:22  iter: 5939  total_loss: 0.6847  loss_cls: 0.412  loss_box_reg: 0.2597  loss_query: 0.02635  time: 2.3757  data_time: 0.0077  lr: 0.006  max_mem: 10545M\n",
            "\u001b[32m[04/14 22:40:55 d2.utils.events]: \u001b[0m eta: 1 day, 12:05:41  iter: 5959  total_loss: 0.6111  loss_cls: 0.3697  loss_box_reg: 0.1961  loss_query: 0.02176  time: 2.3757  data_time: 0.0081  lr: 0.006  max_mem: 10545M\n",
            "\u001b[32m[04/14 22:41:43 d2.utils.events]: \u001b[0m eta: 1 day, 12:04:16  iter: 5979  total_loss: 0.7713  loss_cls: 0.4062  loss_box_reg: 0.2736  loss_query: 0.03675  time: 2.3755  data_time: 0.0064  lr: 0.006  max_mem: 10545M\n",
            "\u001b[32m[04/14 22:42:33 fvcore.common.checkpoint]: \u001b[0mSaving checkpoint to work_dirs/visdrone_querydet/model_0005999.pth\n",
            "\u001b[32m[04/14 22:42:56 d2.utils.events]: \u001b[0m eta: 1 day, 12:03:45  iter: 5999  total_loss: 0.6641  loss_cls: 0.3794  loss_box_reg: 0.2255  loss_query: 0.03194  time: 2.3756  data_time: 0.0076  lr: 0.006  max_mem: 10545M\n",
            "\u001b[32m[04/14 22:43:48 d2.utils.events]: \u001b[0m eta: 1 day, 12:02:57  iter: 6019  total_loss: 0.7689  loss_cls: 0.4613  loss_box_reg: 0.2499  loss_query: 0.0358  time: 2.3758  data_time: 0.0073  lr: 0.006  max_mem: 10545M\n",
            "\u001b[32m[04/14 22:44:35 d2.utils.events]: \u001b[0m eta: 1 day, 12:00:58  iter: 6039  total_loss: 0.5602  loss_cls: 0.346  loss_box_reg: 0.1799  loss_query: 0.0182  time: 2.3754  data_time: 0.0078  lr: 0.006  max_mem: 10545M\n",
            "\u001b[32m[04/14 22:45:24 d2.utils.events]: \u001b[0m eta: 1 day, 12:01:41  iter: 6059  total_loss: 0.6525  loss_cls: 0.3658  loss_box_reg: 0.2441  loss_query: 0.01694  time: 2.3755  data_time: 0.0071  lr: 0.006  max_mem: 10545M\n",
            "\u001b[32m[04/14 22:46:14 d2.utils.events]: \u001b[0m eta: 1 day, 12:00:53  iter: 6079  total_loss: 0.7192  loss_cls: 0.447  loss_box_reg: 0.2341  loss_query: 0.02294  time: 2.3755  data_time: 0.0078  lr: 0.006  max_mem: 10545M\n",
            "\u001b[32m[04/14 22:47:02 d2.utils.events]: \u001b[0m eta: 1 day, 12:00:10  iter: 6099  total_loss: 0.7948  loss_cls: 0.5142  loss_box_reg: 0.2535  loss_query: 0.02716  time: 2.3754  data_time: 0.0097  lr: 0.006  max_mem: 10545M\n",
            "\u001b[32m[04/14 22:47:52 d2.utils.events]: \u001b[0m eta: 1 day, 11:59:36  iter: 6119  total_loss: 0.6767  loss_cls: 0.4028  loss_box_reg: 0.2471  loss_query: 0.02639  time: 2.3755  data_time: 0.0074  lr: 0.006  max_mem: 10545M\n",
            "\u001b[32m[04/14 22:48:42 d2.utils.events]: \u001b[0m eta: 1 day, 11:59:04  iter: 6139  total_loss: 0.8152  loss_cls: 0.5013  loss_box_reg: 0.2257  loss_query: 0.04118  time: 2.3756  data_time: 0.0073  lr: 0.006  max_mem: 10545M\n",
            "\u001b[32m[04/14 22:49:30 d2.utils.events]: \u001b[0m eta: 1 day, 11:57:46  iter: 6159  total_loss: 0.6541  loss_cls: 0.4171  loss_box_reg: 0.2153  loss_query: 0.03408  time: 2.3754  data_time: 0.0067  lr: 0.006  max_mem: 10545M\n",
            "\u001b[32m[04/14 22:50:21 d2.utils.events]: \u001b[0m eta: 1 day, 11:57:48  iter: 6179  total_loss: 0.7761  loss_cls: 0.4898  loss_box_reg: 0.2533  loss_query: 0.04031  time: 2.3756  data_time: 0.0078  lr: 0.006  max_mem: 10545M\n",
            "\u001b[32m[04/14 22:51:13 d2.utils.events]: \u001b[0m eta: 1 day, 11:58:09  iter: 6199  total_loss: 0.8229  loss_cls: 0.4821  loss_box_reg: 0.28  loss_query: 0.04695  time: 2.3759  data_time: 0.0082  lr: 0.006  max_mem: 10545M\n",
            "\u001b[32m[04/14 22:52:02 d2.utils.events]: \u001b[0m eta: 1 day, 11:57:37  iter: 6219  total_loss: 0.5445  loss_cls: 0.3298  loss_box_reg: 0.1614  loss_query: 0.02617  time: 2.3758  data_time: 0.0066  lr: 0.006  max_mem: 10545M\n",
            "\u001b[32m[04/14 22:52:53 d2.utils.events]: \u001b[0m eta: 1 day, 11:56:49  iter: 6239  total_loss: 0.5816  loss_cls: 0.3516  loss_box_reg: 0.1741  loss_query: 0.01674  time: 2.3760  data_time: 0.0081  lr: 0.006  max_mem: 10545M\n",
            "\u001b[32m[04/14 22:53:42 d2.utils.events]: \u001b[0m eta: 1 day, 11:56:14  iter: 6259  total_loss: 0.8805  loss_cls: 0.514  loss_box_reg: 0.3473  loss_query: 0.04924  time: 2.3759  data_time: 0.0096  lr: 0.006  max_mem: 10545M\n",
            "\u001b[32m[04/14 22:54:30 d2.utils.events]: \u001b[0m eta: 1 day, 11:55:35  iter: 6279  total_loss: 0.8062  loss_cls: 0.4822  loss_box_reg: 0.2939  loss_query: 0.04565  time: 2.3758  data_time: 0.0079  lr: 0.006  max_mem: 10545M\n",
            "\u001b[32m[04/14 22:55:35 d2.utils.events]: \u001b[0m eta: 1 day, 11:54:50  iter: 6299  total_loss: 0.5654  loss_cls: 0.3408  loss_box_reg: 0.1975  loss_query: 0.02254  time: 2.3783  data_time: 0.8075  lr: 0.006  max_mem: 10545M\n",
            "\u001b[32m[04/14 22:56:26 d2.utils.events]: \u001b[0m eta: 1 day, 11:54:44  iter: 6319  total_loss: 0.7923  loss_cls: 0.4945  loss_box_reg: 0.2629  loss_query: 0.03099  time: 2.3785  data_time: 0.0067  lr: 0.006  max_mem: 10545M\n",
            "\u001b[32m[04/14 22:57:15 d2.utils.events]: \u001b[0m eta: 1 day, 11:53:52  iter: 6339  total_loss: 0.624  loss_cls: 0.39  loss_box_reg: 0.1921  loss_query: 0.02563  time: 2.3784  data_time: 0.0062  lr: 0.006  max_mem: 10545M\n",
            "\u001b[32m[04/14 22:58:04 d2.utils.events]: \u001b[0m eta: 1 day, 11:52:58  iter: 6359  total_loss: 0.6376  loss_cls: 0.3587  loss_box_reg: 0.194  loss_query: 0.01874  time: 2.3782  data_time: 0.0078  lr: 0.006  max_mem: 10545M\n",
            "\u001b[32m[04/14 22:58:52 d2.utils.events]: \u001b[0m eta: 1 day, 11:52:10  iter: 6379  total_loss: 0.7052  loss_cls: 0.4116  loss_box_reg: 0.2612  loss_query: 0.03257  time: 2.3781  data_time: 0.0093  lr: 0.006  max_mem: 10545M\n",
            "\u001b[32m[04/14 22:59:51 d2.utils.events]: \u001b[0m eta: 1 day, 11:51:22  iter: 6399  total_loss: 0.6471  loss_cls: 0.402  loss_box_reg: 0.216  loss_query: 0.03718  time: 2.3797  data_time: 0.5697  lr: 0.006  max_mem: 10545M\n",
            "\u001b[32m[04/14 23:00:42 d2.utils.events]: \u001b[0m eta: 1 day, 11:50:43  iter: 6419  total_loss: 0.6743  loss_cls: 0.378  loss_box_reg: 0.2274  loss_query: 0.032  time: 2.3799  data_time: 0.0086  lr: 0.006  max_mem: 10545M\n",
            "\u001b[32m[04/14 23:01:33 d2.utils.events]: \u001b[0m eta: 1 day, 11:51:16  iter: 6439  total_loss: 0.6606  loss_cls: 0.4034  loss_box_reg: 0.2302  loss_query: 0.0325  time: 2.3801  data_time: 0.0082  lr: 0.006  max_mem: 10545M\n",
            "\u001b[32m[04/14 23:02:22 d2.utils.events]: \u001b[0m eta: 1 day, 11:50:08  iter: 6459  total_loss: 0.564  loss_cls: 0.299  loss_box_reg: 0.2254  loss_query: 0.04056  time: 2.3799  data_time: 0.0092  lr: 0.006  max_mem: 10545M\n",
            "\u001b[32m[04/14 23:03:10 d2.utils.events]: \u001b[0m eta: 1 day, 11:49:40  iter: 6479  total_loss: 0.5993  loss_cls: 0.379  loss_box_reg: 0.1638  loss_query: 0.02636  time: 2.3798  data_time: 0.0082  lr: 0.006  max_mem: 10545M\n",
            "\u001b[32m[04/14 23:03:58 d2.utils.events]: \u001b[0m eta: 1 day, 11:48:13  iter: 6499  total_loss: 0.9697  loss_cls: 0.5839  loss_box_reg: 0.3323  loss_query: 0.03819  time: 2.3795  data_time: 0.0075  lr: 0.006  max_mem: 10545M\n",
            "\u001b[32m[04/14 23:04:48 d2.utils.events]: \u001b[0m eta: 1 day, 11:47:44  iter: 6519  total_loss: 0.744  loss_cls: 0.4462  loss_box_reg: 0.2713  loss_query: 0.03237  time: 2.3796  data_time: 0.0082  lr: 0.006  max_mem: 10545M\n",
            "\u001b[32m[04/14 23:05:37 d2.utils.events]: \u001b[0m eta: 1 day, 11:46:56  iter: 6539  total_loss: 0.6823  loss_cls: 0.4042  loss_box_reg: 0.2246  loss_query: 0.03818  time: 2.3796  data_time: 0.0084  lr: 0.006  max_mem: 10545M\n",
            "\u001b[32m[04/14 23:06:26 d2.utils.events]: \u001b[0m eta: 1 day, 11:46:49  iter: 6559  total_loss: 0.6686  loss_cls: 0.4069  loss_box_reg: 0.2214  loss_query: 0.03243  time: 2.3795  data_time: 0.0074  lr: 0.006  max_mem: 10545M\n",
            "\u001b[32m[04/14 23:07:17 d2.utils.events]: \u001b[0m eta: 1 day, 11:47:16  iter: 6579  total_loss: 0.7524  loss_cls: 0.4166  loss_box_reg: 0.2705  loss_query: 0.04833  time: 2.3796  data_time: 0.0093  lr: 0.006  max_mem: 10545M\n",
            "\u001b[32m[04/14 23:08:06 d2.utils.events]: \u001b[0m eta: 1 day, 11:46:17  iter: 6599  total_loss: 0.5167  loss_cls: 0.2957  loss_box_reg: 0.1957  loss_query: 0.02326  time: 2.3795  data_time: 0.0077  lr: 0.006  max_mem: 10545M\n",
            "\u001b[32m[04/14 23:08:55 d2.utils.events]: \u001b[0m eta: 1 day, 11:45:02  iter: 6619  total_loss: 0.8018  loss_cls: 0.4634  loss_box_reg: 0.2677  loss_query: 0.04143  time: 2.3794  data_time: 0.0079  lr: 0.006  max_mem: 10545M\n",
            "\u001b[32m[04/14 23:09:44 d2.utils.events]: \u001b[0m eta: 1 day, 11:44:29  iter: 6639  total_loss: 0.6154  loss_cls: 0.4068  loss_box_reg: 0.2142  loss_query: 0.02657  time: 2.3793  data_time: 0.0072  lr: 0.006  max_mem: 10545M\n",
            "\u001b[32m[04/14 23:10:32 d2.utils.events]: \u001b[0m eta: 1 day, 11:42:48  iter: 6659  total_loss: 0.6476  loss_cls: 0.3454  loss_box_reg: 0.2601  loss_query: 0.03543  time: 2.3791  data_time: 0.0070  lr: 0.006  max_mem: 10545M\n",
            "\u001b[32m[04/14 23:11:22 d2.utils.events]: \u001b[0m eta: 1 day, 11:42:13  iter: 6679  total_loss: 0.667  loss_cls: 0.3949  loss_box_reg: 0.2188  loss_query: 0.03449  time: 2.3790  data_time: 0.0087  lr: 0.006  max_mem: 10545M\n",
            "\u001b[32m[04/14 23:12:13 d2.utils.events]: \u001b[0m eta: 1 day, 11:42:20  iter: 6699  total_loss: 0.6275  loss_cls: 0.3792  loss_box_reg: 0.2202  loss_query: 0.04009  time: 2.3792  data_time: 0.0070  lr: 0.006  max_mem: 10545M\n",
            "\u001b[32m[04/14 23:13:02 d2.utils.events]: \u001b[0m eta: 1 day, 11:41:28  iter: 6719  total_loss: 0.6738  loss_cls: 0.3859  loss_box_reg: 0.266  loss_query: 0.03723  time: 2.3792  data_time: 0.0082  lr: 0.006  max_mem: 10545M\n",
            "\u001b[32m[04/14 23:13:51 d2.utils.events]: \u001b[0m eta: 1 day, 11:40:33  iter: 6739  total_loss: 0.4993  loss_cls: 0.3122  loss_box_reg: 0.1848  loss_query: 0.0239  time: 2.3792  data_time: 0.0065  lr: 0.006  max_mem: 10545M\n",
            "\u001b[32m[04/14 23:14:41 d2.utils.events]: \u001b[0m eta: 1 day, 11:40:05  iter: 6759  total_loss: 0.6978  loss_cls: 0.4439  loss_box_reg: 0.2053  loss_query: 0.01899  time: 2.3792  data_time: 0.0075  lr: 0.006  max_mem: 10545M\n",
            "\u001b[32m[04/14 23:15:30 d2.utils.events]: \u001b[0m eta: 1 day, 11:38:12  iter: 6779  total_loss: 0.6699  loss_cls: 0.421  loss_box_reg: 0.2607  loss_query: 0.02119  time: 2.3791  data_time: 0.0064  lr: 0.006  max_mem: 10545M\n",
            "\u001b[32m[04/14 23:16:20 d2.utils.events]: \u001b[0m eta: 1 day, 11:37:29  iter: 6799  total_loss: 0.6836  loss_cls: 0.3957  loss_box_reg: 0.2584  loss_query: 0.02158  time: 2.3793  data_time: 0.0075  lr: 0.006  max_mem: 10545M\n",
            "\u001b[32m[04/14 23:17:10 d2.utils.events]: \u001b[0m eta: 1 day, 11:36:22  iter: 6819  total_loss: 0.6153  loss_cls: 0.3658  loss_box_reg: 0.2297  loss_query: 0.03753  time: 2.3792  data_time: 0.0074  lr: 0.006  max_mem: 10545M\n",
            "\u001b[32m[04/14 23:17:58 d2.utils.events]: \u001b[0m eta: 1 day, 11:35:20  iter: 6839  total_loss: 0.773  loss_cls: 0.3831  loss_box_reg: 0.2865  loss_query: 0.02665  time: 2.3791  data_time: 0.0084  lr: 0.006  max_mem: 10545M\n",
            "\u001b[32m[04/14 23:18:48 d2.utils.events]: \u001b[0m eta: 1 day, 11:34:59  iter: 6859  total_loss: 0.7711  loss_cls: 0.4599  loss_box_reg: 0.2543  loss_query: 0.03761  time: 2.3792  data_time: 0.0077  lr: 0.006  max_mem: 10545M\n",
            "\u001b[32m[04/14 23:19:37 d2.utils.events]: \u001b[0m eta: 1 day, 11:33:43  iter: 6879  total_loss: 0.6048  loss_cls: 0.371  loss_box_reg: 0.1978  loss_query: 0.03654  time: 2.3791  data_time: 0.0076  lr: 0.006  max_mem: 10545M\n",
            "\u001b[32m[04/14 23:20:24 d2.utils.events]: \u001b[0m eta: 1 day, 11:32:05  iter: 6899  total_loss: 0.795  loss_cls: 0.4797  loss_box_reg: 0.269  loss_query: 0.04052  time: 2.3787  data_time: 0.0071  lr: 0.006  max_mem: 10545M\n",
            "\u001b[32m[04/14 23:21:12 d2.utils.events]: \u001b[0m eta: 1 day, 11:31:04  iter: 6919  total_loss: 0.6734  loss_cls: 0.3981  loss_box_reg: 0.2379  loss_query: 0.03152  time: 2.3786  data_time: 0.0085  lr: 0.006  max_mem: 10545M\n",
            "\u001b[32m[04/14 23:22:02 d2.utils.events]: \u001b[0m eta: 1 day, 11:30:50  iter: 6939  total_loss: 0.6234  loss_cls: 0.3837  loss_box_reg: 0.2509  loss_query: 0.03127  time: 2.3786  data_time: 0.0098  lr: 0.006  max_mem: 10545M\n",
            "\u001b[32m[04/14 23:22:50 d2.utils.events]: \u001b[0m eta: 1 day, 11:29:01  iter: 6959  total_loss: 0.6382  loss_cls: 0.3702  loss_box_reg: 0.229  loss_query: 0.03505  time: 2.3785  data_time: 0.0073  lr: 0.006  max_mem: 10545M\n",
            "\u001b[32m[04/14 23:23:39 d2.utils.events]: \u001b[0m eta: 1 day, 11:28:08  iter: 6979  total_loss: 0.645  loss_cls: 0.3884  loss_box_reg: 0.2207  loss_query: 0.04012  time: 2.3784  data_time: 0.0074  lr: 0.006  max_mem: 10545M\n",
            "\u001b[32m[04/14 23:24:29 d2.utils.events]: \u001b[0m eta: 1 day, 11:28:04  iter: 6999  total_loss: 0.6301  loss_cls: 0.3773  loss_box_reg: 0.1803  loss_query: 0.05178  time: 2.3784  data_time: 0.0082  lr: 0.006  max_mem: 10545M\n",
            "\u001b[32m[04/14 23:25:19 d2.utils.events]: \u001b[0m eta: 1 day, 11:27:03  iter: 7019  total_loss: 0.5961  loss_cls: 0.3446  loss_box_reg: 0.2194  loss_query: 0.02993  time: 2.3786  data_time: 0.0075  lr: 0.006  max_mem: 10545M\n",
            "\u001b[32m[04/14 23:26:07 d2.utils.events]: \u001b[0m eta: 1 day, 11:27:41  iter: 7039  total_loss: 0.6046  loss_cls: 0.3313  loss_box_reg: 0.2317  loss_query: 0.02873  time: 2.3784  data_time: 0.0113  lr: 0.006  max_mem: 10545M\n",
            "\u001b[32m[04/14 23:26:57 d2.utils.events]: \u001b[0m eta: 1 day, 11:26:50  iter: 7059  total_loss: 0.7561  loss_cls: 0.435  loss_box_reg: 0.2878  loss_query: 0.04193  time: 2.3785  data_time: 0.0074  lr: 0.006  max_mem: 10545M\n",
            "\u001b[32m[04/14 23:27:46 d2.utils.events]: \u001b[0m eta: 1 day, 11:26:10  iter: 7079  total_loss: 0.6759  loss_cls: 0.3806  loss_box_reg: 0.2652  loss_query: 0.03713  time: 2.3784  data_time: 0.0085  lr: 0.006  max_mem: 10545M\n",
            "\u001b[32m[04/14 23:28:36 d2.utils.events]: \u001b[0m eta: 1 day, 11:25:29  iter: 7099  total_loss: 0.7442  loss_cls: 0.4054  loss_box_reg: 0.2932  loss_query: 0.03849  time: 2.3785  data_time: 0.0103  lr: 0.006  max_mem: 10545M\n",
            "\u001b[32m[04/14 23:29:25 d2.utils.events]: \u001b[0m eta: 1 day, 11:24:51  iter: 7119  total_loss: 0.7305  loss_cls: 0.3869  loss_box_reg: 0.2728  loss_query: 0.05853  time: 2.3784  data_time: 0.0092  lr: 0.006  max_mem: 10545M\n",
            "\u001b[32m[04/14 23:30:14 d2.utils.events]: \u001b[0m eta: 1 day, 11:23:52  iter: 7139  total_loss: 0.6862  loss_cls: 0.4157  loss_box_reg: 0.2433  loss_query: 0.03077  time: 2.3783  data_time: 0.0090  lr: 0.006  max_mem: 10545M\n",
            "\u001b[32m[04/14 23:31:04 d2.utils.events]: \u001b[0m eta: 1 day, 11:23:41  iter: 7159  total_loss: 0.8208  loss_cls: 0.4934  loss_box_reg: 0.2867  loss_query: 0.03718  time: 2.3785  data_time: 0.0089  lr: 0.006  max_mem: 10545M\n",
            "\u001b[32m[04/14 23:31:54 d2.utils.events]: \u001b[0m eta: 1 day, 11:22:16  iter: 7179  total_loss: 0.6099  loss_cls: 0.346  loss_box_reg: 0.2322  loss_query: 0.0281  time: 2.3785  data_time: 0.0081  lr: 0.006  max_mem: 10545M\n",
            "\u001b[32m[04/14 23:32:44 d2.utils.events]: \u001b[0m eta: 1 day, 11:20:02  iter: 7199  total_loss: 0.6974  loss_cls: 0.4129  loss_box_reg: 0.2616  loss_query: 0.03764  time: 2.3786  data_time: 0.0092  lr: 0.006  max_mem: 10545M\n",
            "\u001b[32m[04/14 23:33:35 d2.utils.events]: \u001b[0m eta: 1 day, 11:20:16  iter: 7219  total_loss: 0.7549  loss_cls: 0.4401  loss_box_reg: 0.276  loss_query: 0.03923  time: 2.3788  data_time: 0.0069  lr: 0.006  max_mem: 10545M\n",
            "\u001b[32m[04/14 23:34:24 d2.utils.events]: \u001b[0m eta: 1 day, 11:19:24  iter: 7239  total_loss: 0.7193  loss_cls: 0.4161  loss_box_reg: 0.259  loss_query: 0.04699  time: 2.3787  data_time: 0.0083  lr: 0.006  max_mem: 10545M\n",
            "\u001b[32m[04/14 23:35:12 d2.utils.events]: \u001b[0m eta: 1 day, 11:18:22  iter: 7259  total_loss: 0.4975  loss_cls: 0.2924  loss_box_reg: 0.1832  loss_query: 0.03249  time: 2.3786  data_time: 0.0068  lr: 0.006  max_mem: 10545M\n",
            "\u001b[32m[04/14 23:36:01 d2.utils.events]: \u001b[0m eta: 1 day, 11:16:50  iter: 7279  total_loss: 0.6861  loss_cls: 0.4292  loss_box_reg: 0.2203  loss_query: 0.02981  time: 2.3785  data_time: 0.0084  lr: 0.006  max_mem: 10545M\n",
            "\u001b[32m[04/14 23:36:50 d2.utils.events]: \u001b[0m eta: 1 day, 11:15:57  iter: 7299  total_loss: 0.562  loss_cls: 0.3625  loss_box_reg: 0.1724  loss_query: 0.02486  time: 2.3784  data_time: 0.0067  lr: 0.006  max_mem: 10545M\n",
            "\u001b[32m[04/14 23:37:40 d2.utils.events]: \u001b[0m eta: 1 day, 11:14:17  iter: 7319  total_loss: 0.6215  loss_cls: 0.3618  loss_box_reg: 0.2153  loss_query: 0.03263  time: 2.3786  data_time: 0.0085  lr: 0.006  max_mem: 10545M\n",
            "\u001b[32m[04/14 23:38:29 d2.utils.events]: \u001b[0m eta: 1 day, 11:13:29  iter: 7339  total_loss: 0.5999  loss_cls: 0.3857  loss_box_reg: 0.2096  loss_query: 0.03215  time: 2.3784  data_time: 0.0078  lr: 0.006  max_mem: 10545M\n",
            "\u001b[32m[04/14 23:39:17 d2.utils.events]: \u001b[0m eta: 1 day, 11:12:41  iter: 7359  total_loss: 0.7606  loss_cls: 0.3667  loss_box_reg: 0.2807  loss_query: 0.02238  time: 2.3783  data_time: 0.0066  lr: 0.006  max_mem: 10545M\n",
            "\u001b[32m[04/14 23:40:07 d2.utils.events]: \u001b[0m eta: 1 day, 11:12:14  iter: 7379  total_loss: 0.7507  loss_cls: 0.4126  loss_box_reg: 0.2641  loss_query: 0.04021  time: 2.3783  data_time: 0.0077  lr: 0.006  max_mem: 10545M\n",
            "\u001b[32m[04/14 23:40:57 d2.utils.events]: \u001b[0m eta: 1 day, 11:11:26  iter: 7399  total_loss: 0.687  loss_cls: 0.4203  loss_box_reg: 0.2299  loss_query: 0.03363  time: 2.3784  data_time: 0.0073  lr: 0.006  max_mem: 10545M\n",
            "\u001b[32m[04/14 23:41:46 d2.utils.events]: \u001b[0m eta: 1 day, 11:10:18  iter: 7419  total_loss: 0.5539  loss_cls: 0.3448  loss_box_reg: 0.1985  loss_query: 0.02676  time: 2.3783  data_time: 0.0076  lr: 0.006  max_mem: 10545M\n",
            "\u001b[32m[04/14 23:42:37 d2.utils.events]: \u001b[0m eta: 1 day, 11:09:13  iter: 7439  total_loss: 0.9199  loss_cls: 0.5205  loss_box_reg: 0.3212  loss_query: 0.03714  time: 2.3784  data_time: 0.0066  lr: 0.006  max_mem: 10545M\n",
            "\u001b[32m[04/14 23:43:27 d2.utils.events]: \u001b[0m eta: 1 day, 11:08:03  iter: 7459  total_loss: 0.457  loss_cls: 0.2515  loss_box_reg: 0.1619  loss_query: 0.0255  time: 2.3784  data_time: 0.0066  lr: 0.006  max_mem: 10545M\n",
            "\u001b[32m[04/14 23:44:14 d2.utils.events]: \u001b[0m eta: 1 day, 11:07:15  iter: 7479  total_loss: 0.5813  loss_cls: 0.3806  loss_box_reg: 0.1855  loss_query: 0.04076  time: 2.3781  data_time: 0.0091  lr: 0.006  max_mem: 10545M\n",
            "\u001b[32m[04/14 23:45:03 d2.utils.events]: \u001b[0m eta: 1 day, 11:06:35  iter: 7499  total_loss: 0.634  loss_cls: 0.401  loss_box_reg: 0.2226  loss_query: 0.02425  time: 2.3780  data_time: 0.0081  lr: 0.006  max_mem: 10545M\n",
            "\u001b[32m[04/14 23:45:52 d2.utils.events]: \u001b[0m eta: 1 day, 11:05:24  iter: 7519  total_loss: 0.5218  loss_cls: 0.2906  loss_box_reg: 0.1758  loss_query: 0.02256  time: 2.3781  data_time: 0.0083  lr: 0.006  max_mem: 10545M\n",
            "\u001b[32m[04/14 23:46:42 d2.utils.events]: \u001b[0m eta: 1 day, 11:05:08  iter: 7539  total_loss: 0.6892  loss_cls: 0.3897  loss_box_reg: 0.221  loss_query: 0.0321  time: 2.3780  data_time: 0.0080  lr: 0.006  max_mem: 10545M\n",
            "\u001b[32m[04/14 23:47:32 d2.utils.events]: \u001b[0m eta: 1 day, 11:04:29  iter: 7559  total_loss: 0.5879  loss_cls: 0.3516  loss_box_reg: 0.2106  loss_query: 0.02653  time: 2.3781  data_time: 0.0074  lr: 0.006  max_mem: 10545M\n",
            "\u001b[32m[04/14 23:48:22 d2.utils.events]: \u001b[0m eta: 1 day, 11:03:45  iter: 7579  total_loss: 0.5926  loss_cls: 0.334  loss_box_reg: 0.1792  loss_query: 0.01576  time: 2.3781  data_time: 0.0071  lr: 0.006  max_mem: 10545M\n",
            "\u001b[32m[04/14 23:49:29 d2.utils.events]: \u001b[0m eta: 1 day, 11:03:09  iter: 7599  total_loss: 0.8657  loss_cls: 0.5074  loss_box_reg: 0.2961  loss_query: 0.04187  time: 2.3805  data_time: 0.8826  lr: 0.006  max_mem: 10545M\n",
            "\u001b[32m[04/14 23:50:35 d2.utils.events]: \u001b[0m eta: 1 day, 11:03:03  iter: 7619  total_loss: 0.6921  loss_cls: 0.4327  loss_box_reg: 0.2289  loss_query: 0.02174  time: 2.3826  data_time: 0.7974  lr: 0.006  max_mem: 10545M\n",
            "\u001b[32m[04/14 23:51:25 d2.utils.events]: \u001b[0m eta: 1 day, 11:03:06  iter: 7639  total_loss: 0.4681  loss_cls: 0.2951  loss_box_reg: 0.1504  loss_query: 0.0252  time: 2.3827  data_time: 0.0060  lr: 0.006  max_mem: 10545M\n",
            "\u001b[32m[04/14 23:52:12 d2.utils.events]: \u001b[0m eta: 1 day, 11:02:04  iter: 7659  total_loss: 0.6882  loss_cls: 0.4166  loss_box_reg: 0.2192  loss_query: 0.03202  time: 2.3824  data_time: 0.0076  lr: 0.006  max_mem: 10545M\n",
            "\u001b[32m[04/14 23:53:02 d2.utils.events]: \u001b[0m eta: 1 day, 11:01:40  iter: 7679  total_loss: 0.8658  loss_cls: 0.5079  loss_box_reg: 0.2878  loss_query: 0.04423  time: 2.3825  data_time: 0.0076  lr: 0.006  max_mem: 10545M\n",
            "\u001b[32m[04/14 23:53:52 d2.utils.events]: \u001b[0m eta: 1 day, 11:00:28  iter: 7699  total_loss: 0.8166  loss_cls: 0.4787  loss_box_reg: 0.2824  loss_query: 0.03672  time: 2.3826  data_time: 0.0076  lr: 0.006  max_mem: 10545M\n",
            "\u001b[32m[04/14 23:54:43 d2.utils.events]: \u001b[0m eta: 1 day, 11:00:04  iter: 7719  total_loss: 0.731  loss_cls: 0.4528  loss_box_reg: 0.2466  loss_query: 0.04347  time: 2.3827  data_time: 0.0089  lr: 0.006  max_mem: 10545M\n",
            "\u001b[32m[04/14 23:55:33 d2.utils.events]: \u001b[0m eta: 1 day, 10:59:05  iter: 7739  total_loss: 0.8469  loss_cls: 0.4861  loss_box_reg: 0.2883  loss_query: 0.03944  time: 2.3827  data_time: 0.0080  lr: 0.006  max_mem: 10545M\n",
            "\u001b[32m[04/14 23:56:23 d2.utils.events]: \u001b[0m eta: 1 day, 10:58:40  iter: 7759  total_loss: 0.5708  loss_cls: 0.3442  loss_box_reg: 0.1966  loss_query: 0.02852  time: 2.3827  data_time: 0.0090  lr: 0.006  max_mem: 10545M\n",
            "\u001b[32m[04/14 23:57:13 d2.utils.events]: \u001b[0m eta: 1 day, 10:58:28  iter: 7779  total_loss: 0.7523  loss_cls: 0.4654  loss_box_reg: 0.2555  loss_query: 0.03299  time: 2.3828  data_time: 0.0074  lr: 0.006  max_mem: 10545M\n",
            "\u001b[32m[04/14 23:58:18 d2.utils.events]: \u001b[0m eta: 1 day, 10:57:24  iter: 7799  total_loss: 0.5684  loss_cls: 0.3569  loss_box_reg: 0.1915  loss_query: 0.04247  time: 2.3848  data_time: 0.7689  lr: 0.006  max_mem: 10545M\n",
            "\u001b[32m[04/14 23:59:11 d2.utils.events]: \u001b[0m eta: 1 day, 10:56:29  iter: 7819  total_loss: 0.6939  loss_cls: 0.3795  loss_box_reg: 0.2065  loss_query: 0.03293  time: 2.3852  data_time: 0.2571  lr: 0.006  max_mem: 10545M\n",
            "\u001b[32m[04/15 00:00:24 d2.utils.events]: \u001b[0m eta: 1 day, 10:56:34  iter: 7839  total_loss: 0.4714  loss_cls: 0.2998  loss_box_reg: 0.1526  loss_query: 0.02269  time: 2.3883  data_time: 1.1725  lr: 0.006  max_mem: 10545M\n",
            "\u001b[32m[04/15 00:01:17 d2.utils.events]: \u001b[0m eta: 1 day, 10:55:11  iter: 7859  total_loss: 0.7775  loss_cls: 0.4511  loss_box_reg: 0.2644  loss_query: 0.05328  time: 2.3886  data_time: 0.1922  lr: 0.006  max_mem: 10545M\n",
            "\u001b[32m[04/15 00:02:06 d2.utils.events]: \u001b[0m eta: 1 day, 10:55:14  iter: 7879  total_loss: 0.5044  loss_cls: 0.3168  loss_box_reg: 0.1524  loss_query: 0.0312  time: 2.3886  data_time: 0.0080  lr: 0.006  max_mem: 10545M\n",
            "\u001b[32m[04/15 00:02:57 d2.utils.events]: \u001b[0m eta: 1 day, 10:54:51  iter: 7899  total_loss: 0.7112  loss_cls: 0.4  loss_box_reg: 0.2393  loss_query: 0.03056  time: 2.3886  data_time: 0.0084  lr: 0.006  max_mem: 10545M\n",
            "\u001b[32m[04/15 00:04:02 d2.utils.events]: \u001b[0m eta: 1 day, 10:54:17  iter: 7919  total_loss: 0.7322  loss_cls: 0.468  loss_box_reg: 0.2398  loss_query: 0.03337  time: 2.3904  data_time: 0.6911  lr: 0.006  max_mem: 10545M\n",
            "\u001b[32m[04/15 00:05:10 d2.utils.events]: \u001b[0m eta: 1 day, 10:54:26  iter: 7939  total_loss: 0.6158  loss_cls: 0.3857  loss_box_reg: 0.2084  loss_query: 0.03065  time: 2.3927  data_time: 0.9367  lr: 0.006  max_mem: 10545M\n",
            "\u001b[32m[04/15 00:06:09 d2.utils.events]: \u001b[0m eta: 1 day, 10:54:04  iter: 7959  total_loss: 0.744  loss_cls: 0.4796  loss_box_reg: 0.2563  loss_query: 0.03348  time: 2.3938  data_time: 0.4847  lr: 0.006  max_mem: 10545M\n",
            "\u001b[32m[04/15 00:06:58 d2.utils.events]: \u001b[0m eta: 1 day, 10:53:16  iter: 7979  total_loss: 0.5053  loss_cls: 0.3272  loss_box_reg: 0.1596  loss_query: 0.02959  time: 2.3938  data_time: 0.0058  lr: 0.006  max_mem: 10545M\n",
            "\u001b[32m[04/15 00:07:49 fvcore.common.checkpoint]: \u001b[0mSaving checkpoint to work_dirs/visdrone_querydet/model_0007999.pth\n",
            "\u001b[32m[04/15 00:08:19 d2.utils.events]: \u001b[0m eta: 1 day, 10:52:08  iter: 7999  total_loss: 0.8022  loss_cls: 0.4836  loss_box_reg: 0.2624  loss_query: 0.02381  time: 2.3939  data_time: 0.0100  lr: 0.006  max_mem: 10545M\n",
            "\u001b[32m[04/15 00:09:11 d2.utils.events]: \u001b[0m eta: 1 day, 10:51:08  iter: 8019  total_loss: 0.6858  loss_cls: 0.4232  loss_box_reg: 0.2342  loss_query: 0.0413  time: 2.3939  data_time: 0.0099  lr: 0.006  max_mem: 10545M\n",
            "\u001b[32m[04/15 00:10:00 d2.utils.events]: \u001b[0m eta: 1 day, 10:50:20  iter: 8039  total_loss: 0.6031  loss_cls: 0.3766  loss_box_reg: 0.1928  loss_query: 0.03275  time: 2.3939  data_time: 0.0071  lr: 0.006  max_mem: 10545M\n",
            "\u001b[32m[04/15 00:10:49 d2.utils.events]: \u001b[0m eta: 1 day, 10:49:13  iter: 8059  total_loss: 0.7328  loss_cls: 0.4192  loss_box_reg: 0.2433  loss_query: 0.03333  time: 2.3938  data_time: 0.0089  lr: 0.006  max_mem: 10545M\n",
            "\u001b[32m[04/15 00:11:39 d2.utils.events]: \u001b[0m eta: 1 day, 10:47:20  iter: 8079  total_loss: 0.5777  loss_cls: 0.34  loss_box_reg: 0.2166  loss_query: 0.03834  time: 2.3938  data_time: 0.0075  lr: 0.006  max_mem: 10545M\n",
            "\u001b[32m[04/15 00:12:30 d2.utils.events]: \u001b[0m eta: 1 day, 10:46:36  iter: 8099  total_loss: 0.6509  loss_cls: 0.3842  loss_box_reg: 0.2255  loss_query: 0.02907  time: 2.3939  data_time: 0.0066  lr: 0.006  max_mem: 10545M\n",
            "\u001b[32m[04/15 00:13:21 d2.utils.events]: \u001b[0m eta: 1 day, 10:46:01  iter: 8119  total_loss: 0.4527  loss_cls: 0.2653  loss_box_reg: 0.14  loss_query: 0.0217  time: 2.3940  data_time: 0.0102  lr: 0.006  max_mem: 10545M\n",
            "\u001b[32m[04/15 00:14:10 d2.utils.events]: \u001b[0m eta: 1 day, 10:44:44  iter: 8139  total_loss: 0.6932  loss_cls: 0.3995  loss_box_reg: 0.2541  loss_query: 0.04197  time: 2.3940  data_time: 0.0075  lr: 0.006  max_mem: 10545M\n",
            "\u001b[32m[04/15 00:14:58 d2.utils.events]: \u001b[0m eta: 1 day, 10:43:00  iter: 8159  total_loss: 0.5652  loss_cls: 0.3196  loss_box_reg: 0.1974  loss_query: 0.02802  time: 2.3938  data_time: 0.0083  lr: 0.006  max_mem: 10545M\n",
            "\u001b[32m[04/15 00:15:48 d2.utils.events]: \u001b[0m eta: 1 day, 10:42:12  iter: 8179  total_loss: 0.7384  loss_cls: 0.4715  loss_box_reg: 0.2174  loss_query: 0.03357  time: 2.3938  data_time: 0.0098  lr: 0.006  max_mem: 10545M\n",
            "\u001b[32m[04/15 00:16:39 d2.utils.events]: \u001b[0m eta: 1 day, 10:42:24  iter: 8199  total_loss: 0.4965  loss_cls: 0.3141  loss_box_reg: 0.1566  loss_query: 0.02897  time: 2.3939  data_time: 0.0088  lr: 0.006  max_mem: 10545M\n",
            "\u001b[32m[04/15 00:17:29 d2.utils.events]: \u001b[0m eta: 1 day, 10:41:01  iter: 8219  total_loss: 0.716  loss_cls: 0.4333  loss_box_reg: 0.2244  loss_query: 0.03377  time: 2.3940  data_time: 0.0065  lr: 0.006  max_mem: 10545M\n",
            "\u001b[32m[04/15 00:18:20 d2.utils.events]: \u001b[0m eta: 1 day, 10:40:27  iter: 8239  total_loss: 0.5676  loss_cls: 0.3353  loss_box_reg: 0.2276  loss_query: 0.03582  time: 2.3942  data_time: 0.0096  lr: 0.006  max_mem: 10545M\n",
            "\u001b[32m[04/15 00:19:11 d2.utils.events]: \u001b[0m eta: 1 day, 10:40:23  iter: 8259  total_loss: 0.7179  loss_cls: 0.4267  loss_box_reg: 0.2246  loss_query: 0.04779  time: 2.3943  data_time: 0.0085  lr: 0.006  max_mem: 10545M\n",
            "\u001b[32m[04/15 00:20:00 d2.utils.events]: \u001b[0m eta: 1 day, 10:39:48  iter: 8279  total_loss: 0.5644  loss_cls: 0.3208  loss_box_reg: 0.1897  loss_query: 0.03326  time: 2.3942  data_time: 0.0075  lr: 0.006  max_mem: 10545M\n",
            "\u001b[32m[04/15 00:20:49 d2.utils.events]: \u001b[0m eta: 1 day, 10:39:52  iter: 8299  total_loss: 0.5823  loss_cls: 0.3744  loss_box_reg: 0.1952  loss_query: 0.02746  time: 2.3941  data_time: 0.0063  lr: 0.006  max_mem: 10545M\n",
            "\u001b[32m[04/15 00:21:39 d2.utils.events]: \u001b[0m eta: 1 day, 10:39:40  iter: 8319  total_loss: 0.7014  loss_cls: 0.3817  loss_box_reg: 0.2677  loss_query: 0.03423  time: 2.3942  data_time: 0.0072  lr: 0.006  max_mem: 10545M\n",
            "\u001b[32m[04/15 00:22:28 d2.utils.events]: \u001b[0m eta: 1 day, 10:39:26  iter: 8339  total_loss: 0.7423  loss_cls: 0.4199  loss_box_reg: 0.2701  loss_query: 0.03555  time: 2.3940  data_time: 0.0087  lr: 0.006  max_mem: 10545M\n",
            "\u001b[32m[04/15 00:23:17 d2.utils.events]: \u001b[0m eta: 1 day, 10:38:17  iter: 8359  total_loss: 0.4779  loss_cls: 0.3026  loss_box_reg: 0.1671  loss_query: 0.02492  time: 2.3940  data_time: 0.0075  lr: 0.006  max_mem: 10545M\n",
            "\u001b[32m[04/15 00:24:07 d2.utils.events]: \u001b[0m eta: 1 day, 10:37:08  iter: 8379  total_loss: 0.6383  loss_cls: 0.3844  loss_box_reg: 0.2451  loss_query: 0.02822  time: 2.3940  data_time: 0.0077  lr: 0.006  max_mem: 10545M\n",
            "\u001b[32m[04/15 00:24:56 d2.utils.events]: \u001b[0m eta: 1 day, 10:36:03  iter: 8399  total_loss: 0.6673  loss_cls: 0.4113  loss_box_reg: 0.2133  loss_query: 0.02071  time: 2.3939  data_time: 0.0096  lr: 0.006  max_mem: 10545M\n",
            "\u001b[32m[04/15 00:25:43 d2.utils.events]: \u001b[0m eta: 1 day, 10:35:23  iter: 8419  total_loss: 0.5069  loss_cls: 0.3061  loss_box_reg: 0.2058  loss_query: 0.02111  time: 2.3936  data_time: 0.0093  lr: 0.006  max_mem: 10545M\n",
            "\u001b[32m[04/15 00:26:33 d2.utils.events]: \u001b[0m eta: 1 day, 10:34:43  iter: 8439  total_loss: 0.9612  loss_cls: 0.5611  loss_box_reg: 0.3393  loss_query: 0.04489  time: 2.3936  data_time: 0.0073  lr: 0.006  max_mem: 10545M\n",
            "\u001b[32m[04/15 00:27:22 d2.utils.events]: \u001b[0m eta: 1 day, 10:33:47  iter: 8459  total_loss: 0.468  loss_cls: 0.3394  loss_box_reg: 0.1362  loss_query: 0.01607  time: 2.3935  data_time: 0.0072  lr: 0.006  max_mem: 10545M\n",
            "\u001b[32m[04/15 00:28:12 d2.utils.events]: \u001b[0m eta: 1 day, 10:32:45  iter: 8479  total_loss: 0.7377  loss_cls: 0.4448  loss_box_reg: 0.2425  loss_query: 0.02709  time: 2.3935  data_time: 0.0064  lr: 0.006  max_mem: 10545M\n",
            "\u001b[32m[04/15 00:29:02 d2.utils.events]: \u001b[0m eta: 1 day, 10:31:57  iter: 8499  total_loss: 0.7652  loss_cls: 0.445  loss_box_reg: 0.2439  loss_query: 0.04299  time: 2.3935  data_time: 0.0072  lr: 0.006  max_mem: 10545M\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "SFJrFa8DjJ0V"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "4xRtFEfZjJj-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#!pip uninstall detectron2"
      ],
      "metadata": {
        "id": "tMggM2sQk9xt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!bash eval_visdrone.sh work_dirs/visdrone_querydet/visdrone_infer.json"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KMgCzpuHpcsO",
        "outputId": "7c729d48-d4eb-4db8-d32f-9fa167514b25"
      },
      "execution_count": null,
      "outputs": [
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Json to txt: .visdrone_det_txt\n",
            "  0% 0/548 [00:00<?, ?it/s]0\n",
            "0\n",
            "  0% 1/548 [00:01<11:15,  1.23s/it]1\n",
            "1\n",
            "  0% 2/548 [00:02<11:00,  1.21s/it]2\n",
            "2\n",
            "  1% 3/548 [00:04<13:15,  1.46s/it]3\n",
            "3\n",
            "  1% 4/548 [00:05<12:40,  1.40s/it]4\n",
            "4\n",
            "  1% 5/548 [00:07<12:34,  1.39s/it]5\n",
            "5\n",
            "  1% 6/548 [00:08<11:54,  1.32s/it]6\n",
            "6\n",
            "  1% 7/548 [00:09<11:26,  1.27s/it]7\n",
            "7\n",
            "  1% 8/548 [00:10<11:48,  1.31s/it]8\n",
            "8\n",
            "  2% 9/548 [00:12<11:42,  1.30s/it]9\n",
            "9\n",
            "  2% 10/548 [00:13<11:28,  1.28s/it]10\n",
            "10\n",
            "  2% 11/548 [00:14<11:56,  1.33s/it]11\n",
            "11\n",
            "  2% 12/548 [00:16<11:51,  1.33s/it]12\n",
            "12\n",
            "  2% 13/548 [00:17<11:58,  1.34s/it]13\n",
            "13\n",
            "  3% 14/548 [00:18<11:51,  1.33s/it]14\n",
            "14\n",
            "  3% 15/548 [00:20<13:28,  1.52s/it]15\n",
            "15\n",
            "  3% 16/548 [00:21<12:36,  1.42s/it]16\n",
            "16\n",
            "  3% 17/548 [00:23<11:47,  1.33s/it]17\n",
            "17\n",
            "  3% 18/548 [00:24<11:51,  1.34s/it]18\n",
            "18\n",
            "  3% 19/548 [00:25<11:34,  1.31s/it]19\n",
            "19\n",
            "  4% 20/548 [00:26<11:17,  1.28s/it]20\n",
            "20\n",
            "  4% 21/548 [00:28<13:33,  1.54s/it]21\n",
            "21\n",
            "  4% 22/548 [00:30<12:41,  1.45s/it]22\n",
            "22\n",
            "  4% 23/548 [00:31<13:30,  1.54s/it]23\n",
            "23\n",
            "  4% 24/548 [00:33<12:17,  1.41s/it]24\n",
            "24\n",
            "  5% 25/548 [00:35<14:02,  1.61s/it]25\n",
            "25\n",
            "  5% 26/548 [00:37<15:07,  1.74s/it]26\n",
            "26\n",
            "  5% 27/548 [00:38<13:19,  1.53s/it]27\n",
            "27\n",
            "  5% 28/548 [00:39<11:46,  1.36s/it]28\n",
            "28\n",
            "  5% 29/548 [00:40<11:06,  1.28s/it]29\n",
            "29\n",
            "  5% 30/548 [00:41<10:04,  1.17s/it]30\n",
            "30\n",
            "  6% 31/548 [00:42<09:46,  1.13s/it]31\n",
            "31\n",
            "  6% 32/548 [00:43<10:03,  1.17s/it]32\n",
            "32\n",
            "  6% 33/548 [00:44<09:37,  1.12s/it]33\n",
            "33\n",
            "  6% 34/548 [00:45<09:29,  1.11s/it]34\n",
            "34\n",
            "  6% 35/548 [00:46<09:02,  1.06s/it]35\n",
            "35\n",
            "  7% 36/548 [00:48<11:44,  1.38s/it]36\n",
            "36\n",
            "  7% 37/548 [00:49<10:46,  1.27s/it]37\n",
            "37\n",
            "  7% 38/548 [00:50<10:30,  1.24s/it]38\n",
            "38\n",
            "  7% 39/548 [00:51<10:04,  1.19s/it]39\n",
            "39\n",
            "  7% 40/548 [00:52<09:15,  1.09s/it]40\n",
            "40\n",
            "  7% 41/548 [00:55<12:26,  1.47s/it]41\n",
            "41\n",
            "  8% 42/548 [00:56<11:08,  1.32s/it]42\n",
            "42\n",
            "  8% 43/548 [00:57<10:35,  1.26s/it]43\n",
            "43\n",
            "  8% 44/548 [00:58<09:51,  1.17s/it]44\n",
            "44\n",
            "  8% 45/548 [00:59<09:32,  1.14s/it]45\n",
            "45\n",
            "  8% 46/548 [01:00<09:02,  1.08s/it]46\n",
            "46\n",
            "  9% 47/548 [01:01<08:54,  1.07s/it]47\n",
            "47\n",
            "  9% 48/548 [01:03<10:58,  1.32s/it]48\n",
            "48\n",
            "  9% 49/548 [01:04<09:58,  1.20s/it]49\n",
            "49\n",
            "  9% 50/548 [01:05<09:31,  1.15s/it]50\n",
            "50\n",
            "  9% 51/548 [01:06<09:14,  1.11s/it]51\n",
            "51\n",
            "  9% 52/548 [01:07<09:03,  1.10s/it]52\n",
            "52\n",
            " 10% 53/548 [01:08<08:36,  1.04s/it]53\n",
            "53\n",
            " 10% 54/548 [01:10<10:54,  1.33s/it]54\n",
            "54\n",
            " 10% 55/548 [01:11<10:14,  1.25s/it]55\n",
            "55\n",
            " 10% 56/548 [01:12<09:29,  1.16s/it]56\n",
            "56\n",
            " 10% 57/548 [01:13<09:05,  1.11s/it]57\n",
            "57\n",
            " 11% 58/548 [01:14<10:56,  1.34s/it]58\n",
            "58\n",
            " 11% 59/548 [01:17<12:52,  1.58s/it]59\n",
            "59\n",
            " 11% 60/548 [01:18<11:36,  1.43s/it]60\n",
            "60\n",
            " 11% 61/548 [01:19<10:28,  1.29s/it]61\n",
            "61\n",
            " 11% 62/548 [01:20<09:47,  1.21s/it]62\n",
            "62\n",
            " 11% 63/548 [01:21<09:33,  1.18s/it]63\n",
            "63\n",
            " 12% 64/548 [01:22<08:53,  1.10s/it]64\n",
            "64\n",
            " 12% 65/548 [01:23<08:55,  1.11s/it]65\n",
            "65\n",
            " 12% 66/548 [01:24<09:02,  1.12s/it]66\n",
            "66\n",
            " 12% 67/548 [01:25<09:13,  1.15s/it]67\n",
            "67\n",
            " 12% 68/548 [01:26<08:55,  1.11s/it]68\n",
            "68\n",
            " 13% 69/548 [01:27<08:43,  1.09s/it]69\n",
            "69\n",
            " 13% 70/548 [01:28<08:29,  1.07s/it]70\n",
            "70\n",
            " 13% 71/548 [01:29<08:24,  1.06s/it]71\n",
            "71\n",
            " 13% 72/548 [01:31<08:58,  1.13s/it]72\n",
            "72\n",
            " 13% 73/548 [01:32<08:26,  1.07s/it]73\n",
            "73\n",
            " 14% 74/548 [01:33<08:55,  1.13s/it]74\n",
            "74\n",
            " 14% 75/548 [01:34<08:48,  1.12s/it]75\n",
            "75\n",
            " 14% 76/548 [01:36<10:07,  1.29s/it]76\n",
            "76\n",
            " 14% 77/548 [01:37<10:40,  1.36s/it]77\n",
            "77\n",
            " 14% 78/548 [01:38<10:34,  1.35s/it]78\n",
            "78\n",
            " 14% 79/548 [01:40<10:23,  1.33s/it]79\n",
            "79\n",
            " 15% 80/548 [01:41<10:43,  1.38s/it]80\n",
            "80\n",
            " 15% 81/548 [01:43<10:36,  1.36s/it]81\n",
            "81\n",
            " 15% 82/548 [01:44<10:17,  1.33s/it]82\n",
            "82\n",
            " 15% 83/548 [01:46<12:19,  1.59s/it]83\n",
            "83\n",
            " 15% 84/548 [01:47<11:20,  1.47s/it]84\n",
            "84\n",
            " 16% 85/548 [01:48<10:35,  1.37s/it]85\n",
            "85\n",
            " 16% 86/548 [01:50<11:57,  1.55s/it]86\n",
            "86\n",
            " 16% 87/548 [01:52<11:18,  1.47s/it]87\n",
            "87\n",
            " 16% 88/548 [01:53<10:59,  1.43s/it]88\n",
            "88\n",
            " 16% 89/548 [01:54<10:51,  1.42s/it]89\n",
            "89\n",
            " 16% 90/548 [01:56<10:41,  1.40s/it]90\n",
            "90\n",
            " 17% 91/548 [01:57<10:35,  1.39s/it]91\n",
            "91\n",
            " 17% 92/548 [01:58<10:19,  1.36s/it]92\n",
            "92\n",
            " 17% 93/548 [02:00<11:30,  1.52s/it]93\n",
            "93\n",
            " 17% 94/548 [02:01<10:37,  1.40s/it]94\n",
            "94\n",
            " 17% 95/548 [02:03<10:23,  1.38s/it]95\n",
            "95\n",
            " 18% 96/548 [02:04<10:16,  1.36s/it]96\n",
            "96\n",
            " 18% 97/548 [02:05<10:07,  1.35s/it]97\n",
            "97\n",
            " 18% 98/548 [02:07<10:03,  1.34s/it]98\n",
            "98\n",
            " 18% 99/548 [02:08<09:58,  1.33s/it]99\n",
            "99\n",
            " 18% 100/548 [02:09<10:06,  1.35s/it]100\n",
            "100\n",
            " 18% 101/548 [02:11<11:33,  1.55s/it]101\n",
            "101\n",
            " 19% 102/548 [02:13<10:57,  1.47s/it]102\n",
            "102\n",
            " 19% 103/548 [02:14<10:19,  1.39s/it]103\n",
            "103\n",
            " 19% 104/548 [02:15<10:47,  1.46s/it]104\n",
            "104\n",
            " 19% 105/548 [02:17<10:08,  1.37s/it]105\n",
            "105\n",
            " 19% 106/548 [02:18<09:29,  1.29s/it]106\n",
            "106\n",
            " 20% 107/548 [02:19<09:33,  1.30s/it]107\n",
            "107\n",
            " 20% 108/548 [02:20<09:11,  1.25s/it]108\n",
            "108\n",
            " 20% 109/548 [02:22<10:52,  1.49s/it]109\n",
            "109\n",
            " 20% 110/548 [02:24<10:36,  1.45s/it]110\n",
            "110\n",
            " 20% 111/548 [02:25<10:09,  1.40s/it]111\n",
            "111\n",
            " 20% 112/548 [02:26<09:53,  1.36s/it]112\n",
            "112\n",
            " 21% 113/548 [02:28<09:59,  1.38s/it]113\n",
            "113\n",
            " 21% 114/548 [02:29<09:47,  1.35s/it]114\n",
            "114\n",
            " 21% 115/548 [02:30<09:16,  1.29s/it]115\n",
            "115\n",
            " 21% 116/548 [02:32<09:55,  1.38s/it]116\n",
            "116\n",
            " 21% 117/548 [02:33<09:28,  1.32s/it]117\n",
            "117\n",
            " 22% 118/548 [02:34<09:22,  1.31s/it]118\n",
            "118\n",
            " 22% 119/548 [02:36<10:18,  1.44s/it]119\n",
            "119\n",
            " 22% 120/548 [02:38<11:10,  1.57s/it]120\n",
            "120\n",
            " 22% 121/548 [02:39<09:53,  1.39s/it]121\n",
            "121\n",
            " 22% 122/548 [02:41<10:55,  1.54s/it]122\n",
            "122\n",
            " 22% 123/548 [02:42<10:08,  1.43s/it]123\n",
            "123\n",
            " 23% 124/548 [02:43<09:35,  1.36s/it]124\n",
            "124\n",
            " 23% 125/548 [02:44<08:47,  1.25s/it]125\n",
            "125\n",
            " 23% 126/548 [02:45<08:05,  1.15s/it]126\n",
            "126\n",
            " 23% 127/548 [02:46<08:23,  1.20s/it]127\n",
            "127\n",
            " 23% 128/548 [02:47<08:11,  1.17s/it]128\n",
            "128\n",
            " 24% 129/548 [02:48<08:15,  1.18s/it]129\n",
            "129\n",
            " 24% 130/548 [02:49<07:44,  1.11s/it]130\n",
            "130\n",
            " 24% 131/548 [02:50<07:34,  1.09s/it]131\n",
            "131\n",
            " 24% 132/548 [02:51<07:31,  1.08s/it]132\n",
            "132\n",
            " 24% 133/548 [02:53<07:25,  1.07s/it]133\n",
            "133\n",
            " 24% 134/548 [02:54<07:17,  1.06s/it]134\n",
            "134\n",
            " 25% 135/548 [02:55<07:28,  1.09s/it]135\n",
            "135\n",
            " 25% 136/548 [02:56<07:33,  1.10s/it]136\n",
            "136\n",
            " 25% 137/548 [02:57<07:39,  1.12s/it]137\n",
            "137\n",
            " 25% 138/548 [02:58<07:44,  1.13s/it]138\n",
            "138\n",
            " 25% 139/548 [02:59<07:39,  1.12s/it]139\n",
            "139\n",
            " 26% 140/548 [03:00<07:49,  1.15s/it]140\n",
            "140\n",
            " 26% 141/548 [03:02<07:55,  1.17s/it]141\n",
            "141\n",
            " 26% 142/548 [03:03<07:39,  1.13s/it]142\n",
            "142\n",
            " 26% 143/548 [03:04<07:17,  1.08s/it]143\n",
            "143\n",
            " 26% 144/548 [03:05<07:09,  1.06s/it]144\n",
            "144\n",
            " 26% 145/548 [03:06<07:22,  1.10s/it]145\n",
            "145\n",
            " 27% 146/548 [03:07<07:15,  1.08s/it]146\n",
            "146\n",
            " 27% 147/548 [03:09<09:18,  1.39s/it]147\n",
            "147\n",
            " 27% 148/548 [03:10<08:35,  1.29s/it]148\n",
            "148\n",
            " 27% 149/548 [03:11<08:09,  1.23s/it]149\n",
            "149\n",
            " 27% 150/548 [03:12<07:59,  1.20s/it]150\n",
            "150\n",
            " 28% 151/548 [03:13<07:43,  1.17s/it]151\n",
            "151\n",
            " 28% 152/548 [03:15<07:48,  1.18s/it]152\n",
            "152\n",
            " 28% 153/548 [03:16<07:34,  1.15s/it]153\n",
            "153\n",
            " 28% 154/548 [03:17<07:35,  1.16s/it]154\n",
            "154\n",
            " 28% 155/548 [03:18<07:08,  1.09s/it]155\n",
            "155\n",
            " 28% 156/548 [03:19<07:02,  1.08s/it]156\n",
            "156\n",
            " 29% 157/548 [03:20<07:02,  1.08s/it]157\n",
            "157\n",
            " 29% 158/548 [03:21<07:08,  1.10s/it]158\n",
            "158\n",
            " 29% 159/548 [03:22<06:44,  1.04s/it]159\n",
            "159\n",
            " 29% 160/548 [03:23<06:36,  1.02s/it]160\n",
            "160\n",
            " 29% 161/548 [03:24<06:42,  1.04s/it]161\n",
            "161\n",
            " 30% 162/548 [03:25<06:52,  1.07s/it]162\n",
            "162\n",
            " 30% 163/548 [03:26<06:55,  1.08s/it]163\n",
            "163\n",
            " 30% 164/548 [03:27<07:04,  1.11s/it]164\n",
            "164\n",
            " 30% 165/548 [03:29<07:06,  1.11s/it]165\n",
            "165\n",
            " 30% 166/548 [03:30<07:28,  1.17s/it]166\n",
            "166\n",
            " 30% 167/548 [03:32<09:02,  1.43s/it]167\n",
            "167\n",
            " 31% 168/548 [03:33<08:33,  1.35s/it]168\n",
            "168\n",
            " 31% 169/548 [03:34<07:53,  1.25s/it]169\n",
            "169\n",
            " 31% 170/548 [03:35<07:39,  1.22s/it]170\n",
            "170\n",
            " 31% 171/548 [03:36<07:11,  1.15s/it]171\n",
            "171\n",
            " 31% 172/548 [03:38<08:33,  1.37s/it]172\n",
            "172\n",
            " 32% 173/548 [03:40<08:50,  1.42s/it]173\n",
            "173\n",
            " 32% 174/548 [03:41<08:55,  1.43s/it]174\n",
            "174\n",
            " 32% 175/548 [03:42<08:25,  1.36s/it]175\n",
            "175\n",
            " 32% 176/548 [03:44<08:30,  1.37s/it]176\n",
            "176\n",
            " 32% 177/548 [03:45<08:34,  1.39s/it]177\n",
            "177\n",
            " 32% 178/548 [03:46<08:13,  1.33s/it]178\n",
            "178\n",
            " 33% 179/548 [03:48<08:03,  1.31s/it]179\n",
            "179\n",
            " 33% 180/548 [03:49<08:12,  1.34s/it]180\n",
            "180\n",
            " 33% 181/548 [03:50<08:21,  1.37s/it]181\n",
            "181\n",
            " 33% 182/548 [03:52<08:17,  1.36s/it]182\n",
            "182\n",
            " 33% 183/548 [03:53<07:46,  1.28s/it]183\n",
            "183\n",
            " 34% 184/548 [03:54<07:37,  1.26s/it]184\n",
            "184\n",
            " 34% 185/548 [03:55<07:41,  1.27s/it]185\n",
            "185\n",
            " 34% 186/548 [03:57<07:56,  1.32s/it]186\n",
            "186\n",
            " 34% 187/548 [03:58<07:44,  1.29s/it]187\n",
            "187\n",
            " 34% 188/548 [03:59<07:48,  1.30s/it]188\n",
            "188\n",
            " 34% 189/548 [04:01<07:34,  1.26s/it]189\n",
            "189\n",
            " 35% 190/548 [04:02<07:34,  1.27s/it]190\n",
            "190\n",
            " 35% 191/548 [04:03<07:51,  1.32s/it]191\n",
            "191\n",
            " 35% 192/548 [04:04<07:39,  1.29s/it]192\n",
            "192\n",
            " 35% 193/548 [04:06<07:36,  1.29s/it]193\n",
            "193\n",
            " 35% 194/548 [04:07<07:31,  1.27s/it]194\n",
            "194\n",
            " 36% 195/548 [04:09<08:51,  1.51s/it]195\n",
            "195\n",
            " 36% 196/548 [04:10<08:05,  1.38s/it]196\n",
            "196\n",
            " 36% 197/548 [04:11<07:56,  1.36s/it]197\n",
            "197\n",
            " 36% 198/548 [04:13<07:47,  1.33s/it]198\n",
            "198\n",
            " 36% 199/548 [04:14<07:27,  1.28s/it]199\n",
            "199\n",
            " 36% 200/548 [04:15<07:29,  1.29s/it]200\n",
            "200\n",
            " 37% 201/548 [04:16<07:21,  1.27s/it]201\n",
            "201\n",
            " 37% 202/548 [04:18<07:13,  1.25s/it]202\n",
            "202\n",
            " 37% 203/548 [04:19<07:07,  1.24s/it]203\n",
            "203\n",
            " 37% 204/548 [04:20<07:25,  1.29s/it]204\n",
            "204\n",
            " 37% 205/548 [04:22<07:26,  1.30s/it]205\n",
            "205\n",
            " 38% 206/548 [04:24<08:35,  1.51s/it]206\n",
            "206\n",
            " 38% 207/548 [04:25<08:10,  1.44s/it]207\n",
            "207\n",
            " 38% 208/548 [04:26<08:01,  1.42s/it]208\n",
            "208\n",
            " 38% 209/548 [04:28<07:58,  1.41s/it]209\n",
            "209\n",
            " 38% 210/548 [04:29<07:41,  1.36s/it]210\n",
            "210\n",
            " 39% 211/548 [04:31<08:47,  1.57s/it]211\n",
            "211\n",
            " 39% 212/548 [04:32<08:26,  1.51s/it]212\n",
            "212\n",
            " 39% 213/548 [04:33<07:42,  1.38s/it]213\n",
            "213\n",
            " 39% 214/548 [04:35<07:31,  1.35s/it]214\n",
            "214\n",
            " 39% 215/548 [04:36<07:00,  1.26s/it]215\n",
            "215\n",
            " 39% 216/548 [04:37<07:28,  1.35s/it]216\n",
            "216\n",
            " 40% 217/548 [04:39<07:29,  1.36s/it]217\n",
            "217\n",
            " 40% 218/548 [04:40<07:15,  1.32s/it]218\n",
            "218\n",
            " 40% 219/548 [04:41<07:10,  1.31s/it]219\n",
            "219\n",
            " 40% 220/548 [04:42<07:07,  1.30s/it]220\n",
            "220\n",
            " 40% 221/548 [04:44<06:48,  1.25s/it]221\n",
            "221\n",
            " 41% 222/548 [04:45<06:37,  1.22s/it]222\n",
            "222\n",
            " 41% 223/548 [04:46<06:32,  1.21s/it]223\n",
            "223\n",
            " 41% 224/548 [04:48<07:56,  1.47s/it]224\n",
            "224\n",
            " 41% 225/548 [04:49<07:36,  1.41s/it]225\n",
            "225\n",
            " 41% 226/548 [04:50<07:10,  1.34s/it]226\n",
            "226\n",
            " 41% 227/548 [04:52<06:51,  1.28s/it]227\n",
            "227\n",
            " 42% 228/548 [04:53<06:46,  1.27s/it]228\n",
            "228\n",
            " 42% 229/548 [04:54<06:56,  1.31s/it]229\n",
            "229\n",
            " 42% 230/548 [04:56<07:18,  1.38s/it]230\n",
            "230\n",
            " 42% 231/548 [04:57<06:53,  1.30s/it]231\n",
            "231\n",
            " 42% 232/548 [04:58<06:50,  1.30s/it]232\n",
            "232\n",
            " 43% 233/548 [05:00<06:58,  1.33s/it]233\n",
            "233\n",
            " 43% 234/548 [05:01<06:48,  1.30s/it]234\n",
            "234\n",
            " 43% 235/548 [05:02<07:09,  1.37s/it]235\n",
            "235\n",
            " 43% 236/548 [05:03<06:40,  1.28s/it]236\n",
            "236\n",
            " 43% 237/548 [05:05<06:47,  1.31s/it]237\n",
            "237\n",
            " 43% 238/548 [05:06<06:48,  1.32s/it]238\n",
            "238\n",
            " 44% 239/548 [05:07<06:26,  1.25s/it]239\n",
            "239\n",
            " 44% 240/548 [05:08<06:03,  1.18s/it]240\n",
            "240\n",
            " 44% 241/548 [05:10<06:12,  1.21s/it]241\n",
            "241\n",
            " 44% 242/548 [05:11<06:05,  1.19s/it]242\n",
            "242\n",
            " 44% 243/548 [05:12<05:59,  1.18s/it]243\n",
            "243\n",
            " 45% 244/548 [05:13<06:01,  1.19s/it]244\n",
            "244\n",
            " 45% 245/548 [05:14<06:03,  1.20s/it]245\n",
            "245\n",
            " 45% 246/548 [05:16<06:10,  1.23s/it]246\n",
            "246\n",
            " 45% 247/548 [05:17<06:13,  1.24s/it]247\n",
            "247\n",
            " 45% 248/548 [05:18<06:09,  1.23s/it]248\n",
            "248\n",
            " 45% 249/548 [05:19<06:13,  1.25s/it]249\n",
            "249\n",
            " 46% 250/548 [05:21<06:12,  1.25s/it]250\n",
            "250\n",
            " 46% 251/548 [05:22<06:17,  1.27s/it]251\n",
            "251\n",
            " 46% 252/548 [05:24<07:01,  1.42s/it]252\n",
            "252\n",
            " 46% 253/548 [05:25<06:44,  1.37s/it]253\n",
            "253\n",
            " 46% 254/548 [05:27<07:09,  1.46s/it]254\n",
            "254\n",
            " 47% 255/548 [05:28<06:52,  1.41s/it]255\n",
            "255\n",
            " 47% 256/548 [05:29<06:47,  1.40s/it]256\n",
            "256\n",
            " 47% 257/548 [05:30<06:27,  1.33s/it]257\n",
            "257\n",
            " 47% 258/548 [05:32<06:49,  1.41s/it]258\n",
            "258\n",
            " 47% 259/548 [05:33<06:36,  1.37s/it]259\n",
            "259\n",
            " 47% 260/548 [05:35<06:29,  1.35s/it]260\n",
            "260\n",
            " 48% 261/548 [05:36<06:32,  1.37s/it]261\n",
            "261\n",
            " 48% 262/548 [05:38<06:44,  1.41s/it]262\n",
            "262\n",
            " 48% 263/548 [05:39<06:37,  1.39s/it]263\n",
            "263\n",
            " 48% 264/548 [05:40<06:40,  1.41s/it]264\n",
            "264\n",
            " 48% 265/548 [05:42<07:18,  1.55s/it]265\n",
            "265\n",
            " 49% 266/548 [05:43<06:49,  1.45s/it]266\n",
            "266\n",
            " 49% 267/548 [05:45<06:52,  1.47s/it]267\n",
            "267\n",
            " 49% 268/548 [05:46<06:48,  1.46s/it]268\n",
            "268\n",
            " 49% 269/548 [05:48<06:42,  1.44s/it]269\n",
            "269\n",
            " 49% 270/548 [05:49<06:22,  1.37s/it]270\n",
            "270\n",
            " 49% 271/548 [05:50<06:15,  1.36s/it]271\n",
            "271\n",
            " 50% 272/548 [05:52<06:06,  1.33s/it]272\n",
            "272\n",
            " 50% 273/548 [05:53<05:58,  1.30s/it]273\n",
            "273\n",
            " 50% 274/548 [05:54<05:57,  1.31s/it]274\n",
            "274\n",
            " 50% 275/548 [05:55<05:56,  1.31s/it]275\n",
            "275\n",
            " 50% 276/548 [05:57<05:55,  1.31s/it]276\n",
            "276\n",
            " 51% 277/548 [05:58<05:35,  1.24s/it]277\n",
            "277\n",
            " 51% 278/548 [05:59<05:31,  1.23s/it]278\n",
            "278\n",
            " 51% 279/548 [06:00<05:36,  1.25s/it]279\n",
            "279\n",
            " 51% 280/548 [06:02<05:34,  1.25s/it]280\n",
            "280\n",
            " 51% 281/548 [06:03<05:41,  1.28s/it]281\n",
            "281\n",
            " 51% 282/548 [06:04<05:44,  1.30s/it]282\n",
            "282\n",
            " 52% 283/548 [06:06<05:49,  1.32s/it]283\n",
            "283\n",
            " 52% 284/548 [06:07<05:55,  1.35s/it]284\n",
            "284\n",
            " 52% 285/548 [06:08<05:57,  1.36s/it]285\n",
            "285\n",
            " 52% 286/548 [06:10<05:48,  1.33s/it]286\n",
            "286\n",
            " 52% 287/548 [06:12<06:42,  1.54s/it]287\n",
            "287\n",
            " 53% 288/548 [06:13<06:07,  1.41s/it]288\n",
            "288\n",
            " 53% 289/548 [06:14<05:51,  1.36s/it]289\n",
            "289\n",
            " 53% 290/548 [06:15<05:41,  1.32s/it]290\n",
            "290\n",
            " 53% 291/548 [06:17<05:41,  1.33s/it]291\n",
            "291\n",
            " 53% 292/548 [06:18<05:25,  1.27s/it]292\n",
            "292\n",
            " 53% 293/548 [06:19<05:25,  1.28s/it]293\n",
            "293\n",
            " 54% 294/548 [06:20<05:33,  1.31s/it]294\n",
            "294\n",
            " 54% 295/548 [06:22<05:23,  1.28s/it]295\n",
            "295\n",
            " 54% 296/548 [06:23<05:19,  1.27s/it]296\n",
            "296\n",
            " 54% 297/548 [06:24<05:14,  1.25s/it]297\n",
            "297\n",
            " 54% 298/548 [06:26<06:26,  1.54s/it]298\n",
            "298\n",
            " 55% 299/548 [06:28<06:09,  1.49s/it]299\n",
            "299\n",
            " 55% 300/548 [06:29<05:46,  1.40s/it]300\n",
            "300\n",
            " 55% 301/548 [06:30<05:39,  1.37s/it]301\n",
            "301\n",
            " 55% 302/548 [06:32<05:38,  1.38s/it]302\n",
            "302\n",
            " 55% 303/548 [06:33<05:33,  1.36s/it]303\n",
            "303\n",
            " 55% 304/548 [06:34<05:29,  1.35s/it]304\n",
            "304\n",
            " 56% 305/548 [06:36<05:25,  1.34s/it]305\n",
            "305\n",
            " 56% 306/548 [06:37<05:17,  1.31s/it]306\n",
            "306\n",
            " 56% 307/548 [06:39<06:12,  1.54s/it]307\n",
            "307\n",
            " 56% 308/548 [06:41<06:16,  1.57s/it]308\n",
            "308\n",
            " 56% 309/548 [06:42<05:47,  1.46s/it]309\n",
            "309\n",
            " 57% 310/548 [06:43<05:31,  1.39s/it]310\n",
            "310\n",
            " 57% 311/548 [06:44<05:13,  1.32s/it]311\n",
            "311\n",
            " 57% 312/548 [06:45<05:12,  1.32s/it]312\n",
            "312\n",
            " 57% 313/548 [06:47<05:15,  1.34s/it]313\n",
            "313\n",
            " 57% 314/548 [06:48<05:09,  1.32s/it]314\n",
            "314\n",
            " 57% 315/548 [06:50<05:52,  1.51s/it]315\n",
            "315\n",
            " 58% 316/548 [06:52<05:47,  1.50s/it]316\n",
            "316\n",
            " 58% 317/548 [06:53<05:34,  1.45s/it]317\n",
            "317\n",
            " 58% 318/548 [06:54<05:33,  1.45s/it]318\n",
            "318\n",
            " 58% 319/548 [06:56<05:17,  1.39s/it]319\n",
            "319\n",
            " 58% 320/548 [06:57<05:16,  1.39s/it]320\n",
            "320\n",
            " 59% 321/548 [06:59<05:28,  1.45s/it]321\n",
            "321\n",
            " 59% 322/548 [07:00<05:32,  1.47s/it]322\n",
            "322\n",
            " 59% 323/548 [07:02<06:24,  1.71s/it]323\n",
            "323\n",
            " 59% 324/548 [07:04<05:53,  1.58s/it]324\n",
            "324\n",
            " 59% 325/548 [07:05<05:40,  1.53s/it]325\n",
            "325\n",
            " 59% 326/548 [07:07<06:09,  1.67s/it]326\n",
            "326\n",
            " 60% 327/548 [07:08<05:50,  1.58s/it]327\n",
            "327\n",
            " 60% 328/548 [07:10<05:25,  1.48s/it]328\n",
            "328\n",
            " 60% 329/548 [07:11<05:23,  1.48s/it]329\n",
            "329\n",
            " 60% 330/548 [07:13<05:23,  1.48s/it]330\n",
            "330\n",
            " 60% 331/548 [07:14<05:12,  1.44s/it]331\n",
            "331\n",
            " 61% 332/548 [07:15<05:17,  1.47s/it]332\n",
            "332\n",
            " 61% 333/548 [07:17<05:20,  1.49s/it]333\n",
            "333\n",
            " 61% 334/548 [07:18<05:11,  1.45s/it]334\n",
            "334\n",
            " 61% 335/548 [07:20<04:57,  1.40s/it]335\n",
            "335\n",
            " 61% 336/548 [07:22<05:44,  1.62s/it]336\n",
            "336\n",
            " 61% 337/548 [07:23<05:36,  1.59s/it]337\n",
            "337\n",
            " 62% 338/548 [07:25<05:26,  1.56s/it]338\n",
            "338\n",
            " 62% 339/548 [07:26<05:12,  1.50s/it]339\n",
            "339\n",
            " 62% 340/548 [07:27<04:59,  1.44s/it]340\n",
            "340\n",
            " 62% 341/548 [07:30<05:46,  1.68s/it]341\n",
            "341\n",
            " 62% 342/548 [07:31<05:23,  1.57s/it]342\n",
            "342\n",
            " 63% 343/548 [07:32<04:57,  1.45s/it]343\n",
            "343\n",
            " 63% 344/548 [07:33<04:47,  1.41s/it]344\n",
            "344\n",
            " 63% 345/548 [07:35<04:36,  1.36s/it]345\n",
            "345\n",
            " 63% 346/548 [07:36<04:29,  1.33s/it]346\n",
            "346\n",
            " 63% 347/548 [07:37<04:26,  1.33s/it]347\n",
            "347\n",
            " 64% 348/548 [07:39<04:20,  1.30s/it]348\n",
            "348\n",
            " 64% 349/548 [07:41<05:08,  1.55s/it]349\n",
            "349\n",
            " 64% 350/548 [07:42<04:43,  1.43s/it]350\n",
            "350\n",
            " 64% 351/548 [07:44<05:17,  1.61s/it]351\n",
            "351\n",
            " 64% 352/548 [07:45<04:41,  1.44s/it]352\n",
            "352\n",
            " 64% 353/548 [07:46<04:24,  1.36s/it]353\n",
            "353\n",
            " 65% 354/548 [07:47<04:05,  1.27s/it]354\n",
            "354\n",
            " 65% 355/548 [07:48<03:58,  1.24s/it]355\n",
            "355\n",
            " 65% 356/548 [07:49<03:45,  1.18s/it]356\n",
            "356\n",
            " 65% 357/548 [07:51<03:52,  1.22s/it]357\n",
            "357\n",
            " 65% 358/548 [07:52<03:39,  1.16s/it]358\n",
            "358\n",
            " 66% 359/548 [07:53<03:43,  1.18s/it]359\n",
            "359\n",
            " 66% 360/548 [07:54<03:41,  1.18s/it]360\n",
            "360\n",
            " 66% 361/548 [07:56<04:19,  1.39s/it]361\n",
            "361\n",
            " 66% 362/548 [07:57<04:08,  1.33s/it]362\n",
            "362\n",
            " 66% 363/548 [07:58<04:06,  1.33s/it]363\n",
            "363\n",
            " 66% 364/548 [08:00<03:49,  1.25s/it]364\n",
            "364\n",
            " 67% 365/548 [08:01<03:46,  1.24s/it]365\n",
            "365\n",
            " 67% 366/548 [08:02<03:42,  1.22s/it]366\n",
            "366\n",
            " 67% 367/548 [08:05<04:58,  1.65s/it]367\n",
            "367\n",
            " 67% 368/548 [08:06<04:30,  1.50s/it]368\n",
            "368\n",
            " 67% 369/548 [08:07<04:08,  1.39s/it]369\n",
            "369\n",
            " 68% 370/548 [08:08<03:50,  1.30s/it]370\n",
            "370\n",
            " 68% 371/548 [08:09<03:42,  1.26s/it]371\n",
            "371\n",
            " 68% 372/548 [08:10<03:29,  1.19s/it]372\n",
            "372\n",
            " 68% 373/548 [08:11<03:18,  1.13s/it]373\n",
            "373\n",
            " 68% 374/548 [08:14<05:01,  1.73s/it]374\n",
            "374\n",
            " 68% 375/548 [08:16<04:34,  1.59s/it]375\n",
            "375\n",
            " 69% 376/548 [08:17<04:08,  1.44s/it]376\n",
            "376\n",
            " 69% 377/548 [08:18<03:59,  1.40s/it]377\n",
            "377\n",
            " 69% 378/548 [08:19<03:46,  1.33s/it]378\n",
            "378\n",
            " 69% 379/548 [08:20<03:37,  1.29s/it]379\n",
            "379\n",
            " 69% 380/548 [08:21<03:21,  1.20s/it]380\n",
            "380\n",
            " 70% 381/548 [08:22<03:08,  1.13s/it]381\n",
            "381\n",
            " 70% 382/548 [08:23<03:06,  1.12s/it]382\n",
            "382\n",
            " 70% 383/548 [08:24<03:03,  1.11s/it]383\n",
            "383\n",
            " 70% 384/548 [08:25<03:00,  1.10s/it]384\n",
            "384\n",
            " 70% 385/548 [08:27<03:03,  1.12s/it]385\n",
            "385\n",
            " 70% 386/548 [08:28<02:52,  1.06s/it]386\n",
            "386\n",
            " 71% 387/548 [08:29<02:56,  1.09s/it]387\n",
            "387\n",
            " 71% 388/548 [08:31<03:32,  1.33s/it]388\n",
            "388\n",
            " 71% 389/548 [08:32<03:24,  1.29s/it]389\n",
            "389\n",
            " 71% 390/548 [08:33<03:22,  1.28s/it]390\n",
            "390\n",
            " 71% 391/548 [08:34<03:10,  1.21s/it]391\n",
            "391\n",
            " 72% 392/548 [08:35<03:04,  1.18s/it]392\n",
            "392\n",
            " 72% 393/548 [08:36<03:04,  1.19s/it]393\n",
            "393\n",
            " 72% 394/548 [08:38<03:00,  1.17s/it]394\n",
            "394\n",
            " 72% 395/548 [08:39<02:58,  1.17s/it]395\n",
            "395\n",
            " 72% 396/548 [08:41<03:31,  1.39s/it]396\n",
            "396\n",
            " 72% 397/548 [08:42<03:17,  1.31s/it]397\n",
            "397\n",
            " 73% 398/548 [08:43<03:14,  1.30s/it]398\n",
            "398\n",
            " 73% 399/548 [08:44<03:10,  1.28s/it]399\n",
            "399\n",
            " 73% 400/548 [08:46<03:29,  1.42s/it]400\n",
            "400\n",
            " 73% 401/548 [08:48<03:54,  1.60s/it]401\n",
            "401\n",
            " 73% 402/548 [08:49<03:46,  1.55s/it]402\n",
            "402\n",
            " 74% 403/548 [08:51<03:29,  1.44s/it]403\n",
            "403\n",
            " 74% 404/548 [08:52<03:25,  1.42s/it]404\n",
            "404\n",
            " 74% 405/548 [08:53<03:12,  1.34s/it]405\n",
            "405\n",
            " 74% 406/548 [08:54<03:05,  1.30s/it]406\n",
            "406\n",
            " 74% 407/548 [08:56<02:58,  1.26s/it]407\n",
            "407\n",
            " 74% 408/548 [08:57<02:54,  1.25s/it]408\n",
            "408\n",
            " 75% 409/548 [08:59<03:43,  1.61s/it]409\n",
            "409\n",
            " 75% 410/548 [09:01<03:31,  1.53s/it]410\n",
            "410\n",
            " 75% 411/548 [09:02<03:22,  1.48s/it]411\n",
            "411\n",
            " 75% 412/548 [09:03<03:22,  1.49s/it]412\n",
            "412\n",
            " 75% 413/548 [09:05<03:07,  1.39s/it]413\n",
            "413\n",
            " 76% 414/548 [09:06<03:01,  1.35s/it]414\n",
            "414\n",
            " 76% 415/548 [09:07<02:53,  1.30s/it]415\n",
            "415\n",
            " 76% 416/548 [09:08<02:50,  1.29s/it]416\n",
            "416\n",
            " 76% 417/548 [09:10<02:45,  1.26s/it]417\n",
            "417\n",
            " 76% 418/548 [09:11<02:40,  1.24s/it]418\n",
            "418\n",
            " 76% 419/548 [09:13<03:04,  1.43s/it]419\n",
            "419\n",
            " 77% 420/548 [09:14<03:03,  1.44s/it]420\n",
            "420\n",
            " 77% 421/548 [09:15<02:56,  1.39s/it]421\n",
            "421\n",
            " 77% 422/548 [09:17<02:50,  1.35s/it]422\n",
            "422\n",
            " 77% 423/548 [09:18<02:48,  1.34s/it]423\n",
            "423\n",
            " 77% 424/548 [09:19<02:50,  1.38s/it]424\n",
            "424\n",
            " 78% 425/548 [09:21<02:45,  1.34s/it]425\n",
            "425\n",
            " 78% 426/548 [09:22<02:48,  1.38s/it]426\n",
            "426\n",
            " 78% 427/548 [09:24<02:49,  1.40s/it]427\n",
            "427\n",
            " 78% 428/548 [09:25<02:39,  1.33s/it]428\n",
            "428\n",
            " 78% 429/548 [09:27<03:08,  1.59s/it]429\n",
            "429\n",
            " 78% 430/548 [09:28<02:53,  1.47s/it]430\n",
            "430\n",
            " 79% 431/548 [09:30<02:51,  1.47s/it]431\n",
            "431\n",
            " 79% 432/548 [09:31<02:41,  1.39s/it]432\n",
            "432\n",
            " 79% 433/548 [09:32<02:39,  1.39s/it]433\n",
            "433\n",
            " 79% 434/548 [09:33<02:33,  1.35s/it]434\n",
            "434\n",
            " 79% 435/548 [09:35<02:36,  1.38s/it]435\n",
            "435\n",
            " 80% 436/548 [09:36<02:33,  1.37s/it]436\n",
            "436\n",
            " 80% 437/548 [09:37<02:27,  1.33s/it]437\n",
            "437\n",
            " 80% 438/548 [09:39<02:27,  1.34s/it]438\n",
            "438\n",
            " 80% 439/548 [09:41<02:45,  1.52s/it]439\n",
            "439\n",
            " 80% 440/548 [09:42<02:47,  1.55s/it]440\n",
            "440\n",
            " 80% 441/548 [09:44<02:33,  1.44s/it]441\n",
            "441\n",
            " 81% 442/548 [09:45<02:30,  1.42s/it]442\n",
            "442\n",
            " 81% 443/548 [09:46<02:24,  1.38s/it]443\n",
            "443\n",
            " 81% 444/548 [09:47<02:17,  1.33s/it]444\n",
            "444\n",
            " 81% 445/548 [09:49<02:14,  1.31s/it]445\n",
            "445\n",
            " 81% 446/548 [09:50<02:10,  1.28s/it]446\n",
            "446\n",
            " 82% 447/548 [09:51<02:17,  1.37s/it]447\n",
            "447\n",
            " 82% 448/548 [09:53<02:20,  1.40s/it]448\n",
            "448\n",
            " 82% 449/548 [09:54<02:17,  1.39s/it]449\n",
            "449\n",
            " 82% 450/548 [09:56<02:13,  1.36s/it]450\n",
            "450\n",
            " 82% 451/548 [09:57<02:09,  1.34s/it]451\n",
            "451\n",
            " 82% 452/548 [09:58<02:04,  1.29s/it]452\n",
            "452\n",
            " 83% 453/548 [10:00<02:07,  1.34s/it]453\n",
            "453\n",
            " 83% 454/548 [10:01<02:06,  1.35s/it]454\n",
            "454\n",
            " 83% 455/548 [10:02<02:08,  1.39s/it]455\n",
            "455\n",
            " 83% 456/548 [10:04<02:04,  1.35s/it]456\n",
            "456\n",
            " 83% 457/548 [10:05<02:03,  1.35s/it]457\n",
            "457\n",
            " 84% 458/548 [10:06<01:58,  1.32s/it]458\n",
            "458\n",
            " 84% 459/548 [10:08<01:58,  1.33s/it]459\n",
            "459\n",
            " 84% 460/548 [10:09<01:55,  1.31s/it]460\n",
            "460\n",
            " 84% 461/548 [10:10<01:57,  1.35s/it]461\n",
            "461\n",
            " 84% 462/548 [10:12<02:16,  1.59s/it]462\n",
            "462\n",
            " 84% 463/548 [10:14<02:09,  1.52s/it]463\n",
            "463\n",
            " 85% 464/548 [10:15<02:04,  1.49s/it]464\n",
            "464\n",
            " 85% 465/548 [10:17<01:59,  1.44s/it]465\n",
            "465\n",
            " 85% 466/548 [10:18<01:51,  1.36s/it]466\n",
            "466\n",
            " 85% 467/548 [10:20<02:09,  1.59s/it]467\n",
            "467\n",
            " 85% 468/548 [10:21<02:03,  1.54s/it]468\n",
            "468\n",
            " 86% 469/548 [10:23<01:58,  1.50s/it]469\n",
            "469\n",
            " 86% 470/548 [10:24<01:54,  1.47s/it]470\n",
            "470\n",
            " 86% 471/548 [10:26<02:08,  1.67s/it]471\n",
            "471\n",
            " 86% 472/548 [10:28<01:59,  1.57s/it]472\n",
            "472\n",
            " 86% 473/548 [10:29<01:53,  1.51s/it]473\n",
            "473\n",
            " 86% 474/548 [10:30<01:51,  1.50s/it]474\n",
            "474\n",
            " 87% 475/548 [10:32<01:44,  1.43s/it]475\n",
            "475\n",
            " 87% 476/548 [10:33<01:38,  1.37s/it]476\n",
            "476\n",
            " 87% 477/548 [10:34<01:36,  1.36s/it]477\n",
            "477\n",
            " 87% 478/548 [10:36<01:34,  1.35s/it]478\n",
            "478\n",
            " 87% 479/548 [10:38<02:01,  1.75s/it]479\n",
            "479\n",
            " 88% 480/548 [10:40<01:50,  1.62s/it]480\n",
            "480\n",
            " 88% 481/548 [10:42<02:01,  1.81s/it]481\n",
            "481\n",
            " 88% 482/548 [10:44<02:10,  1.98s/it]482\n",
            "482\n",
            " 88% 483/548 [10:45<01:54,  1.77s/it]483\n",
            "483\n",
            " 88% 484/548 [10:47<01:39,  1.56s/it]484\n",
            "484\n",
            " 89% 485/548 [10:48<01:30,  1.44s/it]485\n",
            "485\n",
            " 89% 486/548 [10:49<01:24,  1.37s/it]486\n",
            "486\n",
            " 89% 487/548 [10:50<01:21,  1.33s/it]487\n",
            "487\n",
            " 89% 488/548 [10:51<01:16,  1.28s/it]488\n",
            "488\n",
            " 89% 489/548 [10:52<01:11,  1.21s/it]489\n",
            "489\n",
            " 89% 490/548 [10:53<01:06,  1.15s/it]490\n",
            "490\n",
            " 90% 491/548 [10:55<01:08,  1.21s/it]491\n",
            "491\n",
            " 90% 492/548 [10:56<01:05,  1.18s/it]492\n",
            "492\n",
            " 90% 493/548 [10:57<01:05,  1.19s/it]493\n",
            "493\n",
            " 90% 494/548 [10:58<01:05,  1.21s/it]494\n",
            "494\n",
            " 90% 495/548 [10:59<01:03,  1.19s/it]495\n",
            "495\n",
            " 91% 496/548 [11:01<01:03,  1.23s/it]496\n",
            "496\n",
            " 91% 497/548 [11:02<00:59,  1.16s/it]497\n",
            "497\n",
            " 91% 498/548 [11:03<00:55,  1.10s/it]498\n",
            "498\n",
            " 91% 499/548 [11:04<00:54,  1.11s/it]499\n",
            "499\n",
            " 91% 500/548 [11:05<00:54,  1.13s/it]500\n",
            "500\n",
            " 91% 501/548 [11:06<00:51,  1.10s/it]501\n",
            "501\n",
            " 92% 502/548 [11:07<00:51,  1.11s/it]502\n",
            "502\n",
            " 92% 503/548 [11:08<00:52,  1.16s/it]503\n",
            "503\n",
            " 92% 504/548 [11:10<00:51,  1.16s/it]504\n",
            "504\n",
            " 92% 505/548 [11:11<00:47,  1.11s/it]505\n",
            "505\n",
            " 92% 506/548 [11:12<00:46,  1.11s/it]506\n",
            "506\n",
            " 93% 507/548 [11:13<00:44,  1.10s/it]507\n",
            "507\n",
            " 93% 508/548 [11:14<00:43,  1.09s/it]508\n",
            "508\n",
            " 93% 509/548 [11:15<00:46,  1.18s/it]509\n",
            "509\n",
            " 93% 510/548 [11:16<00:43,  1.14s/it]510\n",
            "510\n",
            " 93% 511/548 [11:18<00:42,  1.16s/it]511\n",
            "511\n",
            " 93% 512/548 [11:20<00:52,  1.45s/it]512\n",
            "512\n",
            " 94% 513/548 [11:21<00:46,  1.33s/it]513\n",
            "513\n",
            " 94% 514/548 [11:22<00:44,  1.31s/it]514\n",
            "514\n",
            " 94% 515/548 [11:23<00:40,  1.22s/it]515\n",
            "515\n",
            " 94% 516/548 [11:24<00:39,  1.23s/it]516\n",
            "516\n",
            " 94% 517/548 [11:27<00:48,  1.56s/it]517\n",
            "517\n",
            " 95% 518/548 [11:28<00:44,  1.47s/it]518\n",
            "518\n",
            " 95% 519/548 [11:29<00:39,  1.35s/it]519\n",
            "519\n",
            " 95% 520/548 [11:31<00:40,  1.46s/it]520\n",
            "520\n",
            " 95% 521/548 [11:32<00:37,  1.39s/it]521\n",
            "521\n",
            " 95% 522/548 [11:33<00:33,  1.29s/it]522\n",
            "522\n",
            " 95% 523/548 [11:34<00:30,  1.23s/it]523\n",
            "523\n",
            " 96% 524/548 [11:35<00:28,  1.19s/it]524\n",
            "524\n",
            " 96% 525/548 [11:36<00:27,  1.19s/it]525\n",
            "525\n",
            " 96% 526/548 [11:37<00:25,  1.14s/it]526\n",
            "526\n",
            " 96% 527/548 [11:38<00:23,  1.12s/it]527\n",
            "527\n",
            " 96% 528/548 [11:39<00:22,  1.13s/it]528\n",
            "528\n",
            " 97% 529/548 [11:41<00:22,  1.16s/it]529\n",
            "529\n",
            " 97% 530/548 [11:42<00:22,  1.23s/it]530\n",
            "530\n",
            " 97% 531/548 [11:44<00:23,  1.39s/it]531\n",
            "531\n",
            " 97% 532/548 [11:45<00:23,  1.44s/it]532\n",
            "532\n",
            " 97% 533/548 [11:47<00:21,  1.43s/it]533\n",
            "533\n",
            " 97% 534/548 [11:48<00:19,  1.38s/it]534\n",
            "534\n",
            " 98% 535/548 [11:49<00:17,  1.33s/it]535\n",
            "535\n",
            " 98% 536/548 [11:51<00:16,  1.36s/it]536\n",
            "536\n",
            " 98% 537/548 [11:52<00:14,  1.32s/it]537\n",
            "537\n",
            " 98% 538/548 [11:53<00:13,  1.36s/it]538\n",
            "538\n",
            " 98% 539/548 [11:55<00:12,  1.39s/it]539\n",
            "539\n",
            " 99% 540/548 [11:57<00:12,  1.59s/it]540\n",
            "540\n",
            " 99% 541/548 [11:58<00:10,  1.53s/it]541\n",
            "541\n",
            " 99% 542/548 [12:00<00:10,  1.71s/it]542\n",
            "542\n",
            " 99% 543/548 [12:02<00:07,  1.57s/it]543\n",
            "543\n",
            " 99% 544/548 [12:03<00:05,  1.48s/it]544\n",
            "544\n",
            " 99% 545/548 [12:04<00:04,  1.41s/it]545\n",
            "545\n",
            "100% 546/548 [12:06<00:02,  1.39s/it]546\n",
            "546\n",
            "100% 547/548 [12:07<00:01,  1.41s/it]547\n",
            "547\n",
            "100% 548/548 [12:09<00:00,  1.33s/it]\n",
            "data/visdrone/VisDrone2019-DET-val\n",
            ".visdrone_det_txt\n",
            "data/visdrone/VisDrone2019-DET-val/annotations\n",
            "data/visdrone/VisDrone2019-DET-val/images\n",
            "\n",
            "evaluating object category 1/10...\n",
            "evaluating object category 2/10...\n",
            "evaluating object category 3/10...\n",
            "evaluating object category 4/10...\n",
            "evaluating object category 5/10...\n",
            "evaluating object category 6/10...\n",
            "evaluating object category 7/10...\n",
            "evaluating object category 8/10...\n",
            "evaluating object category 9/10...\n",
            "evaluating object category 10/10...\n",
            "Evaluation completed. The performance of the detector is presented as follows.\n",
            "Average Precision  (AP) @[ IoU=0.50:0.95 | maxDets=500 ] = 0.12069211155176163%.\n",
            "Average Precision  (AP) @[ IoU=0.50      | maxDets=500 ] = 0.472805917263031%.\n",
            "Average Precision  (AP) @[ IoU=0.75      | maxDets=500 ] = 0.01638808473944664%.\n",
            "Average Recall     (AR) @[ IoU=0.50:0.95 | maxDets=  1 ] = 0.0474097840487957%.\n",
            "Average Recall     (AR) @[ IoU=0.50:0.95 | maxDets= 10 ] = 0.2876722514629364%.\n",
            "Average Recall     (AR) @[ IoU=0.50:0.95 | maxDets=100 ] = 1.0714521408081055%.\n",
            "Average Recall     (AR) @[ IoU=0.50:0.95 | maxDets=500 ] = 1.411355972290039%.\n",
            "Class 0 AP = 0.007569923996925354%\n",
            "Class 1 AP = 0.005899328738451004%\n",
            "Class 2 AP = 0.0%\n",
            "Class 3 AP = 0.8033015131950378%\n",
            "Class 4 AP = 0.003488138783723116%\n",
            "Class 5 AP = 0.0%\n",
            "Class 6 AP = 0.0%\n",
            "Class 7 AP = 0.0%\n",
            "Class 8 AP = 0.0%\n",
            "Class 9 AP = 0.0602029487490654%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"kok\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7zPf0H0P8AGn",
        "outputId": "fd7b0161-fc79-41f1-a0ac-155457ba8208"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "kok\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "torch.cuda.is_available()\n"
      ],
      "metadata": {
        "id": "pgwyBYxdBeRT",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "af549dfd-bcaf-4bbd-aa64-41a52e239fc1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "False"
            ]
          },
          "metadata": {},
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "torch.cuda.device_count()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LKC40U0xpIaJ",
        "outputId": "38f84c2e-8843-4103-dc9f-a413802a25f6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "os.chdir('/content/gdrive/MyDrive/QueryDet-PyTorch')"
      ],
      "metadata": {
        "id": "kYr9JqO1eTca",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 189
        },
        "outputId": "69dbcd3d-bd09-48be-e3ac-890af72fa4a7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "OSError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mOSError\u001b[0m                                   Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-9-629b04cee732>\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchdir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'/content/gdrive/MyDrive/QueryDet-PyTorch'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mOSError\u001b[0m: [Errno 107] Transport endpoint is not connected: '/content/gdrive/MyDrive/QueryDet-PyTorch'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!python infer_visdrone.py --config-file configs/visdrone/querydet_test.yaml --num-gpu 1 --eval-only MODEL.WEIGHTS work_dirs/visdrone_querydet/model_0005999.pth OUTPUT_DIR work_dirs/model_test-1"
      ],
      "metadata": {
        "id": "R1GsLXY5pJNK",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6e67eda1-33df-4896-b1ce-8e98235c2c93",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "** fvcore version of PathManager will be deprecated soon. **\n",
            "** Please migrate to the version in iopath repo. **\n",
            "https://github.com/facebookresearch/iopath \n",
            "\n",
            "Command Line Args: Namespace(config_file='configs/visdrone/querydet_test.yaml', dist_url='tcp://127.0.0.1:49152', eval_only=True, machine_rank=0, no_pretrain=False, num_gpus=1, num_machines=1, opts=['MODEL.WEIGHTS', 'work_dirs/visdrone_querydet/model_0005999.pth', 'OUTPUT_DIR', 'work_dirs/model_test-1'], resume=False)\n",
            "Loading config configs/visdrone/querydet_test.yaml with yaml.unsafe_load. Your machine may be at risk if the file contains malicious content.\n",
            "Loading config configs/visdrone/../BaseRetina.yaml with yaml.unsafe_load. Your machine may be at risk if the file contains malicious content.\n",
            "\u001b[32m[04/14 05:44:08 detectron2]: \u001b[0mRank of current process: 0. World size: 1\n",
            "\u001b[32m[04/14 05:44:11 detectron2]: \u001b[0mEnvironment info:\n",
            "----------------------  ---------------------------------------------------------------\n",
            "sys.platform            linux\n",
            "Python                  3.7.6 (default, Jan  8 2020, 19:59:22) [GCC 7.3.0]\n",
            "numpy                   1.18.5\n",
            "detectron2              0.4 @/usr/local/lib/python3.7/site-packages/detectron2\n",
            "Compiler                GCC 7.3\n",
            "CUDA compiler           CUDA 10.1\n",
            "detectron2 arch flags   3.7, 5.0, 5.2, 6.0, 6.1, 7.0, 7.5\n",
            "DETECTRON2_ENV_MODULE   <not set>\n",
            "PyTorch                 1.8.1+cu101 @/usr/local/lib/python3.7/site-packages/torch\n",
            "PyTorch debug build     False\n",
            "GPU available           True\n",
            "GPU 0                   Tesla T4 (arch=7.5)\n",
            "CUDA_HOME               /usr/local/cuda\n",
            "Pillow                  9.5.0\n",
            "torchvision             0.9.1+cu101 @/usr/local/lib/python3.7/site-packages/torchvision\n",
            "torchvision arch flags  3.5, 5.0, 6.0, 7.0, 7.5\n",
            "fvcore                  0.1.3.post20210317\n",
            "cv2                     4.2.0\n",
            "----------------------  ---------------------------------------------------------------\n",
            "PyTorch built with:\n",
            "  - GCC 7.3\n",
            "  - C++ Version: 201402\n",
            "  - Intel(R) Math Kernel Library Version 2020.0.0 Product Build 20191122 for Intel(R) 64 architecture applications\n",
            "  - Intel(R) MKL-DNN v1.7.0 (Git Hash 7aed236906b1f7a05c0917e5257a1af05e9ff683)\n",
            "  - OpenMP 201511 (a.k.a. OpenMP 4.5)\n",
            "  - NNPACK is enabled\n",
            "  - CPU capability usage: AVX2\n",
            "  - CUDA Runtime 10.1\n",
            "  - NVCC architecture flags: -gencode;arch=compute_37,code=sm_37;-gencode;arch=compute_50,code=sm_50;-gencode;arch=compute_60,code=sm_60;-gencode;arch=compute_70,code=sm_70\n",
            "  - CuDNN 7.6.3\n",
            "  - Magma 2.5.2\n",
            "  - Build settings: BLAS_INFO=mkl, BUILD_TYPE=Release, CUDA_VERSION=10.1, CUDNN_VERSION=7.6.3, CXX_COMPILER=/opt/rh/devtoolset-7/root/usr/bin/c++, CXX_FLAGS= -Wno-deprecated -fvisibility-inlines-hidden -DUSE_PTHREADPOOL -fopenmp -DNDEBUG -DUSE_KINETO -DUSE_FBGEMM -DUSE_QNNPACK -DUSE_PYTORCH_QNNPACK -DUSE_XNNPACK -O2 -fPIC -Wno-narrowing -Wall -Wextra -Werror=return-type -Wno-missing-field-initializers -Wno-type-limits -Wno-array-bounds -Wno-unknown-pragmas -Wno-sign-compare -Wno-unused-parameter -Wno-unused-variable -Wno-unused-function -Wno-unused-result -Wno-unused-local-typedefs -Wno-strict-overflow -Wno-strict-aliasing -Wno-error=deprecated-declarations -Wno-stringop-overflow -Wno-psabi -Wno-error=pedantic -Wno-error=redundant-decls -Wno-error=old-style-cast -fdiagnostics-color=always -faligned-new -Wno-unused-but-set-variable -Wno-maybe-uninitialized -fno-math-errno -fno-trapping-math -Werror=format -Wno-stringop-overflow, LAPACK_INFO=mkl, PERF_WITH_AVX=1, PERF_WITH_AVX2=1, PERF_WITH_AVX512=1, TORCH_VERSION=1.8.1, USE_CUDA=ON, USE_CUDNN=ON, USE_EXCEPTION_PTR=1, USE_GFLAGS=OFF, USE_GLOG=OFF, USE_MKL=ON, USE_MKLDNN=ON, USE_MPI=OFF, USE_NCCL=ON, USE_NNPACK=ON, USE_OPENMP=ON, \n",
            "\n",
            "\u001b[32m[04/14 05:44:11 detectron2]: \u001b[0mCommand line arguments: Namespace(config_file='configs/visdrone/querydet_test.yaml', dist_url='tcp://127.0.0.1:49152', eval_only=True, machine_rank=0, no_pretrain=False, num_gpus=1, num_machines=1, opts=['MODEL.WEIGHTS', 'work_dirs/visdrone_querydet/model_0005999.pth', 'OUTPUT_DIR', 'work_dirs/model_test-1'], resume=False)\n",
            "\u001b[32m[04/14 05:44:11 detectron2]: \u001b[0mContents of args.config_file=configs/visdrone/querydet_test.yaml:\n",
            "_BASE_: \"../BaseRetina.yaml\"\n",
            "OUTPUT_DIR: \"work_dirs/model_test\"\n",
            "\n",
            "MODEL:\n",
            "  META_ARCHITECTURE: \"RetinaNetQueryDet\"\n",
            "  WEIGHTS: \"detectron2://ImageNetPretrained/MSRA/R-50.pkl\"\n",
            "  \n",
            "  RESNETS:\n",
            "    DEPTH: 50\n",
            "  \n",
            "  ANCHOR_GENERATOR:\n",
            "    NAME: \"AnchorGeneratorWithCenter\"\n",
            "    SIZES: !!python/object/apply:eval [\"[[x, x * 2**(1.0/3), x * 2**(2.0/3)] for x in [16, 32, 64, 128, 256, 512]]\"]\n",
            "  \n",
            "  RETINANET:\n",
            "    IOU_THRESHOLDS: [0.4, 0.5]\n",
            "    IOU_LABELS: [0, -1, 1]\n",
            "    NUM_CLASSES: 10\n",
            "    IN_FEATURES: [\"p2\", \"p3\", \"p4\", \"p5\", \"p6\", \"p7\"]  \n",
            "    SCORE_THRESH_TEST: 0.0001\n",
            "    \n",
            "  RESNETS:\n",
            "    OUT_FEATURES: [\"res2\", \"res3\", \"res4\", \"res5\"]\n",
            "\n",
            "  FPN:\n",
            "    IN_FEATURES: [\"res2\", \"res3\", \"res4\", \"res5\"]\n",
            "  \n",
            "  QUERY:\n",
            "    FEATURES_WHOLE_TEST: [2, 3, 4, 5]\n",
            "    FEATURES_VALUE_TEST: [0, 1]\n",
            "    Q_FEATURE_TRAIN: [1, 2]\n",
            "    Q_FEATURE_TEST: [1, 2]\n",
            "    \n",
            "    ENCODE_CENTER_DIS_COEFF: [1., 1.]\n",
            "    ENCODE_SMALL_OBJ_SCALE: [[0, 32], [0, 64]]\n",
            "\n",
            "    THRESHOLD: 0.12\n",
            "    QUERY_INFER: False\n",
            "  \n",
            "  CUSTOM: \n",
            "    USE_SOFT_NMS: False\n",
            "    SOFT_NMS_METHOD: 'gaussian'\n",
            "    SOFT_NMS_SIGMA: 0.6\n",
            "    SOFT_NMS_THRESHOLD: 0.4\n",
            "    SOFT_NMS_PRUND: 0.0001\n",
            "\n",
            "VISDRONE:\n",
            "  TEST_LENGTH: 3999\n",
            "\n",
            "TEST:  \n",
            "  DETECTIONS_PER_IMAGE: 500\n",
            "\n",
            "META_INFO:\n",
            "  EVAL_GPU_TIME: True\n",
            "\n",
            "\u001b[32m[04/14 05:44:11 detectron2]: \u001b[0mRunning with full config:\n",
            "CUDNN_BENCHMARK: False\n",
            "DATALOADER:\n",
            "  ASPECT_RATIO_GROUPING: True\n",
            "  FILTER_EMPTY_ANNOTATIONS: True\n",
            "  NUM_WORKERS: 4\n",
            "  REPEAT_THRESHOLD: 0.0\n",
            "  SAMPLER_TRAIN: TrainingSampler\n",
            "DATASETS:\n",
            "  PRECOMPUTED_PROPOSAL_TOPK_TEST: 1000\n",
            "  PRECOMPUTED_PROPOSAL_TOPK_TRAIN: 2000\n",
            "  PROPOSAL_FILES_TEST: ()\n",
            "  PROPOSAL_FILES_TRAIN: ()\n",
            "  TEST: ('coco_2017_val',)\n",
            "  TRAIN: ('coco_2017_train',)\n",
            "GLOBAL:\n",
            "  HACK: 1.0\n",
            "INPUT:\n",
            "  CROP:\n",
            "    ENABLED: False\n",
            "    SIZE: [0.9, 0.9]\n",
            "    TYPE: relative_range\n",
            "  FORMAT: BGR\n",
            "  MASK_FORMAT: polygon\n",
            "  MAX_SIZE_TEST: 1333\n",
            "  MAX_SIZE_TRAIN: 1333\n",
            "  MIN_SIZE_TEST: 800\n",
            "  MIN_SIZE_TRAIN: (640, 672, 704, 736, 768, 800)\n",
            "  MIN_SIZE_TRAIN_SAMPLING: choice\n",
            "  RANDOM_FLIP: horizontal\n",
            "META_INFO:\n",
            "  EVAL_AP: True\n",
            "  EVAL_GPU_TIME: True\n",
            "  VIS_ROOT: \n",
            "MODEL:\n",
            "  ANCHOR_GENERATOR:\n",
            "    ANGLES: [[-90, 0, 90]]\n",
            "    ASPECT_RATIOS: [[0.5, 1.0, 2.0]]\n",
            "    NAME: AnchorGeneratorWithCenter\n",
            "    OFFSET: 0.0\n",
            "    SIZES: [[16, 20.15873679831797, 25.39841683149119], [32, 40.31747359663594, 50.79683366298238], [64, 80.63494719327188, 101.59366732596476], [128, 161.26989438654377, 203.18733465192952], [256, 322.53978877308754, 406.37466930385904], [512, 645.0795775461751, 812.7493386077181]]\n",
            "  BACKBONE:\n",
            "    FREEZE_AT: 2\n",
            "    NAME: build_retinanet_resnet_fpn_backbone\n",
            "  CUSTOM:\n",
            "    CLEAR_CUDA_CACHE: False\n",
            "    CLS_WEIGHTS: []\n",
            "    FOCAL_LOSS_ALPHAS: []\n",
            "    FOCAL_LOSS_GAMMAS: []\n",
            "    GIOU_LOSS: False\n",
            "    GRADIENT_CHECKPOINT: False\n",
            "    HEAD_BN: False\n",
            "    REG_WEIGHTS: []\n",
            "    SOFT_NMS_METHOD: gaussian\n",
            "    SOFT_NMS_PRUND: 0.0001\n",
            "    SOFT_NMS_SIGMA: 0.6\n",
            "    SOFT_NMS_THRESHOLD: 0.4\n",
            "    USE_LOOP_MATCHER: False\n",
            "    USE_SOFT_NMS: False\n",
            "  DEVICE: cuda\n",
            "  FPN:\n",
            "    FUSE_TYPE: sum\n",
            "    IN_FEATURES: ['res2', 'res3', 'res4', 'res5']\n",
            "    NORM: \n",
            "    OUT_CHANNELS: 256\n",
            "    TOP_LEVELS: 2\n",
            "  KEYPOINT_ON: False\n",
            "  LOAD_PROPOSALS: False\n",
            "  MASK_ON: False\n",
            "  META_ARCHITECTURE: RetinaNetQueryDet\n",
            "  PANOPTIC_FPN:\n",
            "    COMBINE:\n",
            "      ENABLED: True\n",
            "      INSTANCES_CONFIDENCE_THRESH: 0.5\n",
            "      OVERLAP_THRESH: 0.5\n",
            "      STUFF_AREA_LIMIT: 4096\n",
            "    INSTANCE_LOSS_WEIGHT: 1.0\n",
            "  PIXEL_MEAN: [103.53, 116.28, 123.675]\n",
            "  PIXEL_STD: [1.0, 1.0, 1.0]\n",
            "  PROPOSAL_GENERATOR:\n",
            "    MIN_SIZE: 0\n",
            "    NAME: RPN\n",
            "  QUERY:\n",
            "    CONTEXT: 2\n",
            "    ENCODE_CENTER_DIS_COEFF: [1.0, 1.0]\n",
            "    ENCODE_SMALL_OBJ_SCALE: [[0, 32], [0, 64]]\n",
            "    FEATURES_VALUE_TEST: [0, 1]\n",
            "    FEATURES_VALUE_TRAIN: [0, 1]\n",
            "    FEATURES_WHOLE_TEST: [2, 3, 4, 5]\n",
            "    FEATURES_WHOLE_TRAIN: [2, 3, 4, 5]\n",
            "    QUERY_INFER: False\n",
            "    QUERY_LOSS_GAMMA: []\n",
            "    QUERY_LOSS_WEIGHT: []\n",
            "    Q_FEATURE_TEST: [1, 2]\n",
            "    Q_FEATURE_TRAIN: [1, 2]\n",
            "    THRESHOLD: 0.12\n",
            "  RESNETS:\n",
            "    DEFORM_MODULATED: False\n",
            "    DEFORM_NUM_GROUPS: 1\n",
            "    DEFORM_ON_PER_STAGE: [False, False, False, False]\n",
            "    DEPTH: 50\n",
            "    NORM: FrozenBN\n",
            "    NUM_GROUPS: 1\n",
            "    OUT_FEATURES: ['res2', 'res3', 'res4', 'res5']\n",
            "    RES2_OUT_CHANNELS: 256\n",
            "    RES5_DILATION: 1\n",
            "    STEM_OUT_CHANNELS: 64\n",
            "    STRIDE_IN_1X1: True\n",
            "    WIDTH_PER_GROUP: 64\n",
            "  RETINANET:\n",
            "    BBOX_REG_LOSS_TYPE: smooth_l1\n",
            "    BBOX_REG_WEIGHTS: (1.0, 1.0, 1.0, 1.0)\n",
            "    FOCAL_LOSS_ALPHA: 0.25\n",
            "    FOCAL_LOSS_GAMMA: 2.0\n",
            "    IN_FEATURES: ['p2', 'p3', 'p4', 'p5', 'p6', 'p7']\n",
            "    IOU_LABELS: [0, -1, 1]\n",
            "    IOU_THRESHOLDS: [0.4, 0.5]\n",
            "    NMS_THRESH_TEST: 0.5\n",
            "    NORM: \n",
            "    NUM_CLASSES: 10\n",
            "    NUM_CONVS: 4\n",
            "    PRIOR_PROB: 0.01\n",
            "    SCORE_THRESH_TEST: 0.0001\n",
            "    SMOOTH_L1_LOSS_BETA: 0.1\n",
            "    TOPK_CANDIDATES_TEST: 1000\n",
            "  ROI_BOX_CASCADE_HEAD:\n",
            "    BBOX_REG_WEIGHTS: ((10.0, 10.0, 5.0, 5.0), (20.0, 20.0, 10.0, 10.0), (30.0, 30.0, 15.0, 15.0))\n",
            "    IOUS: (0.5, 0.6, 0.7)\n",
            "  ROI_BOX_HEAD:\n",
            "    BBOX_REG_LOSS_TYPE: smooth_l1\n",
            "    BBOX_REG_LOSS_WEIGHT: 1.0\n",
            "    BBOX_REG_WEIGHTS: (10.0, 10.0, 5.0, 5.0)\n",
            "    CLS_AGNOSTIC_BBOX_REG: False\n",
            "    CONV_DIM: 256\n",
            "    FC_DIM: 1024\n",
            "    NAME: \n",
            "    NORM: \n",
            "    NUM_CONV: 0\n",
            "    NUM_FC: 0\n",
            "    POOLER_RESOLUTION: 14\n",
            "    POOLER_SAMPLING_RATIO: 0\n",
            "    POOLER_TYPE: ROIAlignV2\n",
            "    SMOOTH_L1_BETA: 0.0\n",
            "    TRAIN_ON_PRED_BOXES: False\n",
            "  ROI_HEADS:\n",
            "    BATCH_SIZE_PER_IMAGE: 512\n",
            "    IN_FEATURES: ['res4']\n",
            "    IOU_LABELS: [0, 1]\n",
            "    IOU_THRESHOLDS: [0.5]\n",
            "    NAME: Res5ROIHeads\n",
            "    NMS_THRESH_TEST: 0.5\n",
            "    NUM_CLASSES: 80\n",
            "    POSITIVE_FRACTION: 0.25\n",
            "    PROPOSAL_APPEND_GT: True\n",
            "    SCORE_THRESH_TEST: 0.05\n",
            "  ROI_KEYPOINT_HEAD:\n",
            "    CONV_DIMS: (512, 512, 512, 512, 512, 512, 512, 512)\n",
            "    LOSS_WEIGHT: 1.0\n",
            "    MIN_KEYPOINTS_PER_IMAGE: 1\n",
            "    NAME: KRCNNConvDeconvUpsampleHead\n",
            "    NORMALIZE_LOSS_BY_VISIBLE_KEYPOINTS: True\n",
            "    NUM_KEYPOINTS: 17\n",
            "    POOLER_RESOLUTION: 14\n",
            "    POOLER_SAMPLING_RATIO: 0\n",
            "    POOLER_TYPE: ROIAlignV2\n",
            "  ROI_MASK_HEAD:\n",
            "    CLS_AGNOSTIC_MASK: False\n",
            "    CONV_DIM: 256\n",
            "    NAME: MaskRCNNConvUpsampleHead\n",
            "    NORM: \n",
            "    NUM_CONV: 0\n",
            "    POOLER_RESOLUTION: 14\n",
            "    POOLER_SAMPLING_RATIO: 0\n",
            "    POOLER_TYPE: ROIAlignV2\n",
            "  RPN:\n",
            "    BATCH_SIZE_PER_IMAGE: 256\n",
            "    BBOX_REG_LOSS_TYPE: smooth_l1\n",
            "    BBOX_REG_LOSS_WEIGHT: 1.0\n",
            "    BBOX_REG_WEIGHTS: (1.0, 1.0, 1.0, 1.0)\n",
            "    BOUNDARY_THRESH: -1\n",
            "    HEAD_NAME: StandardRPNHead\n",
            "    IN_FEATURES: ['res4']\n",
            "    IOU_LABELS: [0, -1, 1]\n",
            "    IOU_THRESHOLDS: [0.3, 0.7]\n",
            "    LOSS_WEIGHT: 1.0\n",
            "    NMS_THRESH: 0.7\n",
            "    POSITIVE_FRACTION: 0.5\n",
            "    POST_NMS_TOPK_TEST: 1000\n",
            "    POST_NMS_TOPK_TRAIN: 2000\n",
            "    PRE_NMS_TOPK_TEST: 6000\n",
            "    PRE_NMS_TOPK_TRAIN: 12000\n",
            "    SMOOTH_L1_BETA: 0.0\n",
            "  SEM_SEG_HEAD:\n",
            "    COMMON_STRIDE: 4\n",
            "    CONVS_DIM: 128\n",
            "    IGNORE_VALUE: 255\n",
            "    IN_FEATURES: ['p2', 'p3', 'p4', 'p5']\n",
            "    LOSS_WEIGHT: 1.0\n",
            "    NAME: SemSegFPNHead\n",
            "    NORM: GN\n",
            "    NUM_CLASSES: 54\n",
            "  WEIGHTS: work_dirs/visdrone_querydet/model_0005999.pth\n",
            "OUTPUT_DIR: work_dirs/model_test-1\n",
            "SEED: -1\n",
            "SOLVER:\n",
            "  AMP:\n",
            "    ENABLED: False\n",
            "  BASE_LR: 0.01\n",
            "  BIAS_LR_FACTOR: 1.0\n",
            "  CHECKPOINT_PERIOD: 5000\n",
            "  CLIP_GRADIENTS:\n",
            "    CLIP_TYPE: value\n",
            "    CLIP_VALUE: 1.0\n",
            "    ENABLED: False\n",
            "    NORM_TYPE: 2.0\n",
            "  GAMMA: 0.1\n",
            "  IMS_PER_BATCH: 16\n",
            "  LR_SCHEDULER_NAME: WarmupMultiStepLR\n",
            "  MAX_ITER: 90000\n",
            "  MOMENTUM: 0.9\n",
            "  NESTEROV: False\n",
            "  REFERENCE_WORLD_SIZE: 0\n",
            "  STEPS: (60000, 80000)\n",
            "  WARMUP_FACTOR: 0.001\n",
            "  WARMUP_ITERS: 1000\n",
            "  WARMUP_METHOD: linear\n",
            "  WEIGHT_DECAY: 0.0001\n",
            "  WEIGHT_DECAY_BIAS: 0.0001\n",
            "  WEIGHT_DECAY_NORM: 0.0\n",
            "TEST:\n",
            "  AUG:\n",
            "    ENABLED: False\n",
            "    FLIP: True\n",
            "    MAX_SIZE: 4000\n",
            "    MIN_SIZES: (400, 500, 600, 700, 800, 900, 1000, 1100, 1200)\n",
            "  DETECTIONS_PER_IMAGE: 500\n",
            "  EVAL_PERIOD: 0\n",
            "  EXPECTED_RESULTS: []\n",
            "  KEYPOINT_OKS_SIGMAS: []\n",
            "  PRECISE_BN:\n",
            "    ENABLED: False\n",
            "    NUM_ITER: 200\n",
            "VERSION: 2\n",
            "VISDRONE:\n",
            "  MAX_LENGTH: 1999\n",
            "  SHORT_LENGTH: [1200]\n",
            "  TEST_IMG_ROOT: data/visdrone/coco_format/val_images\n",
            "  TEST_JSON: data/visdrone/coco_format/annotations/val_label.json\n",
            "  TEST_LENGTH: 3999\n",
            "  TRAIN_JSON: data/visdrone/coco_format/annotations/train_label.json\n",
            "  TRING_IMG_ROOT: data//visdrone/coco_format/train_images\n",
            "VIS_PERIOD: 0\n",
            "\u001b[32m[04/14 05:44:12 detectron2]: \u001b[0mFull config saved to work_dirs/model_test-1/config.yaml\n",
            "\u001b[32m[04/14 05:44:12 d2.utils.env]: \u001b[0mUsing a generated random seed 13086147\n",
            "\u001b[32m[04/14 05:44:18 d2.engine.defaults]: \u001b[0mModel:\n",
            "RetinaNetQueryDet(\n",
            "  (backbone): FPN(\n",
            "    (fpn_lateral2): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1))\n",
            "    (fpn_output2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (fpn_lateral3): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1))\n",
            "    (fpn_output3): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (fpn_lateral4): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1))\n",
            "    (fpn_output4): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (fpn_lateral5): Conv2d(2048, 256, kernel_size=(1, 1), stride=(1, 1))\n",
            "    (fpn_output5): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (top_block): LastLevelP6P7(\n",
            "      (p6): Conv2d(2048, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
            "      (p7): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
            "    )\n",
            "    (bottom_up): ResNet(\n",
            "      (stem): BasicStem(\n",
            "        (conv1): Conv2d(\n",
            "          3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)\n",
            "        )\n",
            "      )\n",
            "      (res2): Sequential(\n",
            "        (0): BottleneckBlock(\n",
            "          (shortcut): Conv2d(\n",
            "            64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
            "          )\n",
            "          (conv1): Conv2d(\n",
            "            64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)\n",
            "          )\n",
            "          (conv2): Conv2d(\n",
            "            64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)\n",
            "          )\n",
            "          (conv3): Conv2d(\n",
            "            64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
            "          )\n",
            "        )\n",
            "        (1): BottleneckBlock(\n",
            "          (conv1): Conv2d(\n",
            "            256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)\n",
            "          )\n",
            "          (conv2): Conv2d(\n",
            "            64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)\n",
            "          )\n",
            "          (conv3): Conv2d(\n",
            "            64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
            "          )\n",
            "        )\n",
            "        (2): BottleneckBlock(\n",
            "          (conv1): Conv2d(\n",
            "            256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)\n",
            "          )\n",
            "          (conv2): Conv2d(\n",
            "            64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)\n",
            "          )\n",
            "          (conv3): Conv2d(\n",
            "            64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
            "          )\n",
            "        )\n",
            "      )\n",
            "      (res3): Sequential(\n",
            "        (0): BottleneckBlock(\n",
            "          (shortcut): Conv2d(\n",
            "            256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
            "          )\n",
            "          (conv1): Conv2d(\n",
            "            256, 128, kernel_size=(1, 1), stride=(2, 2), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)\n",
            "          )\n",
            "          (conv2): Conv2d(\n",
            "            128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)\n",
            "          )\n",
            "          (conv3): Conv2d(\n",
            "            128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
            "          )\n",
            "        )\n",
            "        (1): BottleneckBlock(\n",
            "          (conv1): Conv2d(\n",
            "            512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)\n",
            "          )\n",
            "          (conv2): Conv2d(\n",
            "            128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)\n",
            "          )\n",
            "          (conv3): Conv2d(\n",
            "            128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
            "          )\n",
            "        )\n",
            "        (2): BottleneckBlock(\n",
            "          (conv1): Conv2d(\n",
            "            512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)\n",
            "          )\n",
            "          (conv2): Conv2d(\n",
            "            128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)\n",
            "          )\n",
            "          (conv3): Conv2d(\n",
            "            128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
            "          )\n",
            "        )\n",
            "        (3): BottleneckBlock(\n",
            "          (conv1): Conv2d(\n",
            "            512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)\n",
            "          )\n",
            "          (conv2): Conv2d(\n",
            "            128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)\n",
            "          )\n",
            "          (conv3): Conv2d(\n",
            "            128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
            "          )\n",
            "        )\n",
            "      )\n",
            "      (res4): Sequential(\n",
            "        (0): BottleneckBlock(\n",
            "          (shortcut): Conv2d(\n",
            "            512, 1024, kernel_size=(1, 1), stride=(2, 2), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n",
            "          )\n",
            "          (conv1): Conv2d(\n",
            "            512, 256, kernel_size=(1, 1), stride=(2, 2), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
            "          )\n",
            "          (conv2): Conv2d(\n",
            "            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
            "          )\n",
            "          (conv3): Conv2d(\n",
            "            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n",
            "          )\n",
            "        )\n",
            "        (1): BottleneckBlock(\n",
            "          (conv1): Conv2d(\n",
            "            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
            "          )\n",
            "          (conv2): Conv2d(\n",
            "            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
            "          )\n",
            "          (conv3): Conv2d(\n",
            "            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n",
            "          )\n",
            "        )\n",
            "        (2): BottleneckBlock(\n",
            "          (conv1): Conv2d(\n",
            "            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
            "          )\n",
            "          (conv2): Conv2d(\n",
            "            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
            "          )\n",
            "          (conv3): Conv2d(\n",
            "            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n",
            "          )\n",
            "        )\n",
            "        (3): BottleneckBlock(\n",
            "          (conv1): Conv2d(\n",
            "            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
            "          )\n",
            "          (conv2): Conv2d(\n",
            "            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
            "          )\n",
            "          (conv3): Conv2d(\n",
            "            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n",
            "          )\n",
            "        )\n",
            "        (4): BottleneckBlock(\n",
            "          (conv1): Conv2d(\n",
            "            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
            "          )\n",
            "          (conv2): Conv2d(\n",
            "            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
            "          )\n",
            "          (conv3): Conv2d(\n",
            "            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n",
            "          )\n",
            "        )\n",
            "        (5): BottleneckBlock(\n",
            "          (conv1): Conv2d(\n",
            "            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
            "          )\n",
            "          (conv2): Conv2d(\n",
            "            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
            "          )\n",
            "          (conv3): Conv2d(\n",
            "            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n",
            "          )\n",
            "        )\n",
            "      )\n",
            "      (res5): Sequential(\n",
            "        (0): BottleneckBlock(\n",
            "          (shortcut): Conv2d(\n",
            "            1024, 2048, kernel_size=(1, 1), stride=(2, 2), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=2048, eps=1e-05)\n",
            "          )\n",
            "          (conv1): Conv2d(\n",
            "            1024, 512, kernel_size=(1, 1), stride=(2, 2), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
            "          )\n",
            "          (conv2): Conv2d(\n",
            "            512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
            "          )\n",
            "          (conv3): Conv2d(\n",
            "            512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=2048, eps=1e-05)\n",
            "          )\n",
            "        )\n",
            "        (1): BottleneckBlock(\n",
            "          (conv1): Conv2d(\n",
            "            2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
            "          )\n",
            "          (conv2): Conv2d(\n",
            "            512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
            "          )\n",
            "          (conv3): Conv2d(\n",
            "            512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=2048, eps=1e-05)\n",
            "          )\n",
            "        )\n",
            "        (2): BottleneckBlock(\n",
            "          (conv1): Conv2d(\n",
            "            2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
            "          )\n",
            "          (conv2): Conv2d(\n",
            "            512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
            "          )\n",
            "          (conv3): Conv2d(\n",
            "            512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=2048, eps=1e-05)\n",
            "          )\n",
            "        )\n",
            "      )\n",
            "    )\n",
            "  )\n",
            "  (det_head): RetinaNetHead_3x3(\n",
            "    (cls_layer_0): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (bbox_layer_0): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (cls_layer_1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (bbox_layer_1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (cls_layer_2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (bbox_layer_2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (cls_layer_3): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (bbox_layer_3): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (cls_score): Conv2d(256, 90, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (bbox_pred): Conv2d(256, 36, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "  )\n",
            "  (query_head): Head_3x3(\n",
            "    (layer_0): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (layer_1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (layer_2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (layer_3): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (pred_net): Conv2d(256, 1, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "  )\n",
            "  (anchor_generator): AnchorGeneratorWithCenter(\n",
            "    (cell_anchors): BufferList()\n",
            "  )\n",
            "  (query_anchor_generator): AnchorGeneratorWithCenter(\n",
            "    (cell_anchors): BufferList()\n",
            "  )\n",
            ")\n",
            "\u001b[32m[04/14 05:44:18 fvcore.common.checkpoint]: \u001b[0mLoading checkpoint from work_dirs/visdrone_querydet/model_0005999.pth\n",
            "\u001b[32m[04/14 05:44:22 d2.data.common]: \u001b[0mSerializing 548 elements to byte tensors and concatenating them all ...\n",
            "\u001b[32m[04/14 05:44:22 d2.data.common]: \u001b[0mSerialized dataset takes 0.08 MiB\n",
            "/usr/local/lib/python3.7/site-packages/torch/utils/data/dataloader.py:477: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  cpuset_checked))\n",
            "\u001b[32m[04/14 05:44:22 d2.evaluation.evaluator]: \u001b[0mStart inference on 548 images\n",
            "\u001b[32m[04/14 05:44:46 d2.evaluation.evaluator]: \u001b[0mInference done 11/548. 2.0823 s / img. ETA=0:18:41\n",
            "\u001b[32m[04/14 05:44:52 d2.evaluation.evaluator]: \u001b[0mInference done 14/548. 2.0906 s / img. ETA=0:18:40\n",
            "\u001b[32m[04/14 05:44:58 d2.evaluation.evaluator]: \u001b[0mInference done 17/548. 2.0951 s / img. ETA=0:18:36\n",
            "\u001b[32m[04/14 05:45:05 d2.evaluation.evaluator]: \u001b[0mInference done 20/548. 2.1016 s / img. ETA=0:18:33\n",
            "\u001b[32m[04/14 05:45:11 d2.evaluation.evaluator]: \u001b[0mInference done 23/548. 2.1098 s / img. ETA=0:18:31\n",
            "\u001b[32m[04/14 05:45:18 d2.evaluation.evaluator]: \u001b[0mInference done 26/548. 2.1149 s / img. ETA=0:18:28\n",
            "\u001b[32m[04/14 05:45:24 d2.evaluation.evaluator]: \u001b[0mInference done 29/548. 2.1199 s / img. ETA=0:18:24\n",
            "\u001b[32m[04/14 05:45:31 d2.evaluation.evaluator]: \u001b[0mInference done 32/548. 2.1278 s / img. ETA=0:18:22\n",
            "\u001b[32m[04/14 05:45:38 d2.evaluation.evaluator]: \u001b[0mInference done 35/548. 2.1352 s / img. ETA=0:18:19\n",
            "\u001b[32m[04/14 05:45:44 d2.evaluation.evaluator]: \u001b[0mInference done 38/548. 2.1430 s / img. ETA=0:18:17\n",
            "\u001b[32m[04/14 05:45:51 d2.evaluation.evaluator]: \u001b[0mInference done 41/548. 2.1508 s / img. ETA=0:18:14\n",
            "\u001b[32m[04/14 05:45:58 d2.evaluation.evaluator]: \u001b[0mInference done 44/548. 2.1596 s / img. ETA=0:18:12\n",
            "\u001b[32m[04/14 05:46:05 d2.evaluation.evaluator]: \u001b[0mInference done 47/548. 2.1686 s / img. ETA=0:18:10\n",
            "\u001b[32m[04/14 05:46:12 d2.evaluation.evaluator]: \u001b[0mInference done 50/548. 2.1780 s / img. ETA=0:18:08\n",
            "\u001b[32m[04/14 05:46:19 d2.evaluation.evaluator]: \u001b[0mInference done 53/548. 2.1885 s / img. ETA=0:18:07\n",
            "\u001b[32m[04/14 05:46:26 d2.evaluation.evaluator]: \u001b[0mInference done 56/548. 2.2000 s / img. ETA=0:18:06\n",
            "\u001b[32m[04/14 05:46:33 d2.evaluation.evaluator]: \u001b[0mInference done 59/548. 2.2141 s / img. ETA=0:18:06\n",
            "\u001b[32m[04/14 05:46:41 d2.evaluation.evaluator]: \u001b[0mInference done 62/548. 2.2251 s / img. ETA=0:18:05\n",
            "\u001b[32m[04/14 05:46:48 d2.evaluation.evaluator]: \u001b[0mInference done 65/548. 2.2324 s / img. ETA=0:18:02\n",
            "\u001b[32m[04/14 05:46:55 d2.evaluation.evaluator]: \u001b[0mInference done 68/548. 2.2379 s / img. ETA=0:17:58\n",
            "\u001b[32m[04/14 05:47:02 d2.evaluation.evaluator]: \u001b[0mInference done 71/548. 2.2413 s / img. ETA=0:17:53\n",
            "\u001b[32m[04/14 05:47:09 d2.evaluation.evaluator]: \u001b[0mInference done 74/548. 2.2441 s / img. ETA=0:17:47\n",
            "\u001b[32m[04/14 05:47:16 d2.evaluation.evaluator]: \u001b[0mInference done 77/548. 2.2465 s / img. ETA=0:17:42\n",
            "\u001b[32m[04/14 05:47:23 d2.evaluation.evaluator]: \u001b[0mInference done 80/548. 2.2489 s / img. ETA=0:17:36\n",
            "\u001b[32m[04/14 05:47:30 d2.evaluation.evaluator]: \u001b[0mInference done 83/548. 2.2515 s / img. ETA=0:17:31\n",
            "\u001b[32m[04/14 05:47:36 d2.evaluation.evaluator]: \u001b[0mInference done 86/548. 2.2538 s / img. ETA=0:17:25\n",
            "\u001b[32m[04/14 05:47:43 d2.evaluation.evaluator]: \u001b[0mInference done 89/548. 2.2563 s / img. ETA=0:17:19\n",
            "\u001b[32m[04/14 05:47:51 d2.evaluation.evaluator]: \u001b[0mInference done 92/548. 2.2593 s / img. ETA=0:17:14\n",
            "\u001b[32m[04/14 05:47:58 d2.evaluation.evaluator]: \u001b[0mInference done 95/548. 2.2623 s / img. ETA=0:17:08\n",
            "\u001b[32m[04/14 05:48:05 d2.evaluation.evaluator]: \u001b[0mInference done 98/548. 2.2649 s / img. ETA=0:17:03\n",
            "\u001b[32m[04/14 05:48:12 d2.evaluation.evaluator]: \u001b[0mInference done 101/548. 2.2678 s / img. ETA=0:16:57\n",
            "\u001b[32m[04/14 05:48:19 d2.evaluation.evaluator]: \u001b[0mInference done 104/548. 2.2699 s / img. ETA=0:16:51\n",
            "\u001b[32m[04/14 05:48:26 d2.evaluation.evaluator]: \u001b[0mInference done 107/548. 2.2713 s / img. ETA=0:16:45\n",
            "\u001b[32m[04/14 05:48:33 d2.evaluation.evaluator]: \u001b[0mInference done 110/548. 2.2730 s / img. ETA=0:16:39\n",
            "\u001b[32m[04/14 05:48:40 d2.evaluation.evaluator]: \u001b[0mInference done 113/548. 2.2742 s / img. ETA=0:16:33\n",
            "\u001b[32m[04/14 05:48:47 d2.evaluation.evaluator]: \u001b[0mInference done 116/548. 2.2758 s / img. ETA=0:16:27\n",
            "\u001b[32m[04/14 05:48:54 d2.evaluation.evaluator]: \u001b[0mInference done 119/548. 2.2772 s / img. ETA=0:16:20\n",
            "\u001b[32m[04/14 05:49:01 d2.evaluation.evaluator]: \u001b[0mInference done 122/548. 2.2781 s / img. ETA=0:16:14\n",
            "\u001b[32m[04/14 05:49:08 d2.evaluation.evaluator]: \u001b[0mInference done 125/548. 2.2796 s / img. ETA=0:16:08\n",
            "\u001b[32m[04/14 05:49:15 d2.evaluation.evaluator]: \u001b[0mInference done 128/548. 2.2811 s / img. ETA=0:16:02\n",
            "\u001b[32m[04/14 05:49:22 d2.evaluation.evaluator]: \u001b[0mInference done 131/548. 2.2824 s / img. ETA=0:15:55\n",
            "\u001b[32m[04/14 05:49:29 d2.evaluation.evaluator]: \u001b[0mInference done 134/548. 2.2835 s / img. ETA=0:15:49\n",
            "\u001b[32m[04/14 05:49:36 d2.evaluation.evaluator]: \u001b[0mInference done 137/548. 2.2848 s / img. ETA=0:15:43\n",
            "\u001b[32m[04/14 05:49:43 d2.evaluation.evaluator]: \u001b[0mInference done 140/548. 2.2858 s / img. ETA=0:15:36\n",
            "\u001b[32m[04/14 05:49:50 d2.evaluation.evaluator]: \u001b[0mInference done 143/548. 2.2873 s / img. ETA=0:15:30\n",
            "\u001b[32m[04/14 05:49:57 d2.evaluation.evaluator]: \u001b[0mInference done 146/548. 2.2884 s / img. ETA=0:15:23\n",
            "\u001b[32m[04/14 05:50:04 d2.evaluation.evaluator]: \u001b[0mInference done 149/548. 2.2893 s / img. ETA=0:15:17\n",
            "\u001b[32m[04/14 05:50:11 d2.evaluation.evaluator]: \u001b[0mInference done 152/548. 2.2905 s / img. ETA=0:15:10\n",
            "\u001b[32m[04/14 05:50:18 d2.evaluation.evaluator]: \u001b[0mInference done 155/548. 2.2914 s / img. ETA=0:15:04\n",
            "\u001b[32m[04/14 05:50:25 d2.evaluation.evaluator]: \u001b[0mInference done 158/548. 2.2921 s / img. ETA=0:14:57\n",
            "\u001b[32m[04/14 05:50:32 d2.evaluation.evaluator]: \u001b[0mInference done 161/548. 2.2932 s / img. ETA=0:14:51\n",
            "\u001b[32m[04/14 05:50:40 d2.evaluation.evaluator]: \u001b[0mInference done 164/548. 2.2941 s / img. ETA=0:14:44\n",
            "\u001b[32m[04/14 05:50:47 d2.evaluation.evaluator]: \u001b[0mInference done 167/548. 2.2952 s / img. ETA=0:14:38\n",
            "\u001b[32m[04/14 05:50:54 d2.evaluation.evaluator]: \u001b[0mInference done 170/548. 2.2961 s / img. ETA=0:14:31\n",
            "\u001b[32m[04/14 05:51:01 d2.evaluation.evaluator]: \u001b[0mInference done 173/548. 2.2969 s / img. ETA=0:14:25\n",
            "\u001b[32m[04/14 05:51:08 d2.evaluation.evaluator]: \u001b[0mInference done 176/548. 2.2975 s / img. ETA=0:14:18\n",
            "\u001b[32m[04/14 05:51:15 d2.evaluation.evaluator]: \u001b[0mInference done 179/548. 2.2983 s / img. ETA=0:14:11\n",
            "\u001b[32m[04/14 05:51:22 d2.evaluation.evaluator]: \u001b[0mInference done 182/548. 2.2988 s / img. ETA=0:14:04\n",
            "\u001b[32m[04/14 05:51:29 d2.evaluation.evaluator]: \u001b[0mInference done 185/548. 2.2995 s / img. ETA=0:13:58\n",
            "\u001b[32m[04/14 05:51:36 d2.evaluation.evaluator]: \u001b[0mInference done 188/548. 2.3000 s / img. ETA=0:13:51\n",
            "\u001b[32m[04/14 05:51:43 d2.evaluation.evaluator]: \u001b[0mInference done 191/548. 2.3006 s / img. ETA=0:13:44\n",
            "\u001b[32m[04/14 05:51:50 d2.evaluation.evaluator]: \u001b[0mInference done 194/548. 2.3008 s / img. ETA=0:13:38\n",
            "\u001b[32m[04/14 05:51:57 d2.evaluation.evaluator]: \u001b[0mInference done 197/548. 2.3016 s / img. ETA=0:13:31\n",
            "\u001b[32m[04/14 05:52:04 d2.evaluation.evaluator]: \u001b[0mInference done 200/548. 2.3017 s / img. ETA=0:13:24\n",
            "\u001b[32m[04/14 05:52:11 d2.evaluation.evaluator]: \u001b[0mInference done 203/548. 2.3020 s / img. ETA=0:13:18\n",
            "\u001b[32m[04/14 05:52:18 d2.evaluation.evaluator]: \u001b[0mInference done 206/548. 2.3022 s / img. ETA=0:13:11\n",
            "\u001b[32m[04/14 05:52:25 d2.evaluation.evaluator]: \u001b[0mInference done 209/548. 2.3026 s / img. ETA=0:13:04\n",
            "\u001b[32m[04/14 05:52:32 d2.evaluation.evaluator]: \u001b[0mInference done 212/548. 2.3029 s / img. ETA=0:12:57\n",
            "\u001b[32m[04/14 05:52:39 d2.evaluation.evaluator]: \u001b[0mInference done 215/548. 2.3035 s / img. ETA=0:12:50\n",
            "\u001b[32m[04/14 05:52:46 d2.evaluation.evaluator]: \u001b[0mInference done 218/548. 2.3040 s / img. ETA=0:12:43\n",
            "\u001b[32m[04/14 05:52:53 d2.evaluation.evaluator]: \u001b[0mInference done 221/548. 2.3044 s / img. ETA=0:12:37\n",
            "\u001b[32m[04/14 05:53:00 d2.evaluation.evaluator]: \u001b[0mInference done 224/548. 2.3047 s / img. ETA=0:12:30\n",
            "\u001b[32m[04/14 05:53:07 d2.evaluation.evaluator]: \u001b[0mInference done 227/548. 2.3050 s / img. ETA=0:12:23\n",
            "\u001b[32m[04/14 05:53:14 d2.evaluation.evaluator]: \u001b[0mInference done 230/548. 2.3055 s / img. ETA=0:12:16\n",
            "\u001b[32m[04/14 05:53:21 d2.evaluation.evaluator]: \u001b[0mInference done 233/548. 2.3059 s / img. ETA=0:12:09\n",
            "\u001b[32m[04/14 05:53:29 d2.evaluation.evaluator]: \u001b[0mInference done 236/548. 2.3063 s / img. ETA=0:12:03\n",
            "\u001b[32m[04/14 05:53:36 d2.evaluation.evaluator]: \u001b[0mInference done 239/548. 2.3066 s / img. ETA=0:11:56\n",
            "\u001b[32m[04/14 05:53:43 d2.evaluation.evaluator]: \u001b[0mInference done 242/548. 2.3073 s / img. ETA=0:11:49\n",
            "\u001b[32m[04/14 05:53:50 d2.evaluation.evaluator]: \u001b[0mInference done 245/548. 2.3076 s / img. ETA=0:11:42\n",
            "\u001b[32m[04/14 05:53:57 d2.evaluation.evaluator]: \u001b[0mInference done 248/548. 2.3083 s / img. ETA=0:11:35\n",
            "\u001b[32m[04/14 05:54:04 d2.evaluation.evaluator]: \u001b[0mInference done 251/548. 2.3086 s / img. ETA=0:11:28\n",
            "\u001b[32m[04/14 05:54:11 d2.evaluation.evaluator]: \u001b[0mInference done 254/548. 2.3091 s / img. ETA=0:11:22\n",
            "\u001b[32m[04/14 05:54:18 d2.evaluation.evaluator]: \u001b[0mInference done 257/548. 2.3094 s / img. ETA=0:11:15\n",
            "\u001b[32m[04/14 05:54:25 d2.evaluation.evaluator]: \u001b[0mInference done 260/548. 2.3098 s / img. ETA=0:11:08\n",
            "\u001b[32m[04/14 05:54:32 d2.evaluation.evaluator]: \u001b[0mInference done 263/548. 2.3102 s / img. ETA=0:11:01\n",
            "\u001b[32m[04/14 05:54:39 d2.evaluation.evaluator]: \u001b[0mInference done 266/548. 2.3106 s / img. ETA=0:10:54\n",
            "\u001b[32m[04/14 05:54:46 d2.evaluation.evaluator]: \u001b[0mInference done 269/548. 2.3110 s / img. ETA=0:10:47\n",
            "\u001b[32m[04/14 05:54:53 d2.evaluation.evaluator]: \u001b[0mInference done 272/548. 2.3112 s / img. ETA=0:10:40\n",
            "\u001b[32m[04/14 05:55:00 d2.evaluation.evaluator]: \u001b[0mInference done 275/548. 2.3116 s / img. ETA=0:10:34\n",
            "\u001b[32m[04/14 05:55:07 d2.evaluation.evaluator]: \u001b[0mInference done 278/548. 2.3118 s / img. ETA=0:10:27\n",
            "\u001b[32m[04/14 05:55:14 d2.evaluation.evaluator]: \u001b[0mInference done 281/548. 2.3120 s / img. ETA=0:10:20\n",
            "\u001b[32m[04/14 05:55:21 d2.evaluation.evaluator]: \u001b[0mInference done 284/548. 2.3122 s / img. ETA=0:10:13\n",
            "\u001b[32m[04/14 05:55:28 d2.evaluation.evaluator]: \u001b[0mInference done 287/548. 2.3123 s / img. ETA=0:10:06\n",
            "\u001b[32m[04/14 05:55:35 d2.evaluation.evaluator]: \u001b[0mInference done 290/548. 2.3126 s / img. ETA=0:09:59\n",
            "\u001b[32m[04/14 05:55:42 d2.evaluation.evaluator]: \u001b[0mInference done 293/548. 2.3127 s / img. ETA=0:09:52\n",
            "\u001b[32m[04/14 05:55:49 d2.evaluation.evaluator]: \u001b[0mInference done 296/548. 2.3129 s / img. ETA=0:09:45\n",
            "\u001b[32m[04/14 05:55:57 d2.evaluation.evaluator]: \u001b[0mInference done 299/548. 2.3133 s / img. ETA=0:09:38\n",
            "\u001b[32m[04/14 05:56:04 d2.evaluation.evaluator]: \u001b[0mInference done 302/548. 2.3137 s / img. ETA=0:09:31\n",
            "\u001b[32m[04/14 05:56:11 d2.evaluation.evaluator]: \u001b[0mInference done 305/548. 2.3140 s / img. ETA=0:09:24\n",
            "\u001b[32m[04/14 05:56:18 d2.evaluation.evaluator]: \u001b[0mInference done 308/548. 2.3142 s / img. ETA=0:09:18\n",
            "\u001b[32m[04/14 05:56:25 d2.evaluation.evaluator]: \u001b[0mInference done 311/548. 2.3147 s / img. ETA=0:09:11\n",
            "\u001b[32m[04/14 05:56:32 d2.evaluation.evaluator]: \u001b[0mInference done 314/548. 2.3149 s / img. ETA=0:09:04\n",
            "\u001b[32m[04/14 05:56:39 d2.evaluation.evaluator]: \u001b[0mInference done 317/548. 2.3151 s / img. ETA=0:08:57\n",
            "\u001b[32m[04/14 05:56:46 d2.evaluation.evaluator]: \u001b[0mInference done 320/548. 2.3154 s / img. ETA=0:08:50\n",
            "\u001b[32m[04/14 05:56:53 d2.evaluation.evaluator]: \u001b[0mInference done 323/548. 2.3154 s / img. ETA=0:08:43\n",
            "\u001b[32m[04/14 05:57:00 d2.evaluation.evaluator]: \u001b[0mInference done 326/548. 2.3157 s / img. ETA=0:08:36\n",
            "\u001b[32m[04/14 05:57:07 d2.evaluation.evaluator]: \u001b[0mInference done 329/548. 2.3158 s / img. ETA=0:08:29\n",
            "\u001b[32m[04/14 05:57:14 d2.evaluation.evaluator]: \u001b[0mInference done 332/548. 2.3161 s / img. ETA=0:08:22\n",
            "\u001b[32m[04/14 05:57:21 d2.evaluation.evaluator]: \u001b[0mInference done 335/548. 2.3163 s / img. ETA=0:08:15\n",
            "\u001b[32m[04/14 05:57:28 d2.evaluation.evaluator]: \u001b[0mInference done 338/548. 2.3163 s / img. ETA=0:08:08\n",
            "\u001b[32m[04/14 05:57:35 d2.evaluation.evaluator]: \u001b[0mInference done 341/548. 2.3165 s / img. ETA=0:08:01\n",
            "\u001b[32m[04/14 05:57:42 d2.evaluation.evaluator]: \u001b[0mInference done 344/548. 2.3167 s / img. ETA=0:07:54\n",
            "\u001b[32m[04/14 05:57:49 d2.evaluation.evaluator]: \u001b[0mInference done 347/548. 2.3169 s / img. ETA=0:07:47\n",
            "\u001b[32m[04/14 05:57:56 d2.evaluation.evaluator]: \u001b[0mInference done 350/548. 2.3169 s / img. ETA=0:07:40\n",
            "\u001b[32m[04/14 05:58:03 d2.evaluation.evaluator]: \u001b[0mInference done 353/548. 2.3171 s / img. ETA=0:07:33\n",
            "\u001b[32m[04/14 05:58:10 d2.evaluation.evaluator]: \u001b[0mInference done 356/548. 2.3172 s / img. ETA=0:07:26\n",
            "\u001b[32m[04/14 05:58:17 d2.evaluation.evaluator]: \u001b[0mInference done 359/548. 2.3173 s / img. ETA=0:07:20\n",
            "\u001b[32m[04/14 05:58:24 d2.evaluation.evaluator]: \u001b[0mInference done 362/548. 2.3174 s / img. ETA=0:07:13\n",
            "\u001b[32m[04/14 05:58:31 d2.evaluation.evaluator]: \u001b[0mInference done 365/548. 2.3175 s / img. ETA=0:07:06\n",
            "\u001b[32m[04/14 05:58:38 d2.evaluation.evaluator]: \u001b[0mInference done 368/548. 2.3177 s / img. ETA=0:06:59\n",
            "\u001b[32m[04/14 05:58:46 d2.evaluation.evaluator]: \u001b[0mInference done 371/548. 2.3179 s / img. ETA=0:06:52\n",
            "\u001b[32m[04/14 05:58:53 d2.evaluation.evaluator]: \u001b[0mInference done 374/548. 2.3179 s / img. ETA=0:06:45\n",
            "\u001b[32m[04/14 05:59:00 d2.evaluation.evaluator]: \u001b[0mInference done 377/548. 2.3180 s / img. ETA=0:06:38\n",
            "\u001b[32m[04/14 05:59:07 d2.evaluation.evaluator]: \u001b[0mInference done 380/548. 2.3181 s / img. ETA=0:06:31\n",
            "\u001b[32m[04/14 05:59:14 d2.evaluation.evaluator]: \u001b[0mInference done 383/548. 2.3182 s / img. ETA=0:06:24\n",
            "\u001b[32m[04/14 05:59:21 d2.evaluation.evaluator]: \u001b[0mInference done 386/548. 2.3184 s / img. ETA=0:06:17\n",
            "\u001b[32m[04/14 05:59:28 d2.evaluation.evaluator]: \u001b[0mInference done 389/548. 2.3185 s / img. ETA=0:06:10\n",
            "\u001b[32m[04/14 05:59:35 d2.evaluation.evaluator]: \u001b[0mInference done 392/548. 2.3186 s / img. ETA=0:06:03\n",
            "\u001b[32m[04/14 05:59:42 d2.evaluation.evaluator]: \u001b[0mInference done 395/548. 2.3187 s / img. ETA=0:05:56\n",
            "\u001b[32m[04/14 05:59:49 d2.evaluation.evaluator]: \u001b[0mInference done 398/548. 2.3188 s / img. ETA=0:05:49\n",
            "\u001b[32m[04/14 05:59:56 d2.evaluation.evaluator]: \u001b[0mInference done 401/548. 2.3191 s / img. ETA=0:05:42\n",
            "\u001b[32m[04/14 06:00:03 d2.evaluation.evaluator]: \u001b[0mInference done 404/548. 2.3194 s / img. ETA=0:05:35\n",
            "\u001b[32m[04/14 06:00:10 d2.evaluation.evaluator]: \u001b[0mInference done 407/548. 2.3195 s / img. ETA=0:05:28\n",
            "\u001b[32m[04/14 06:00:17 d2.evaluation.evaluator]: \u001b[0mInference done 410/548. 2.3195 s / img. ETA=0:05:21\n",
            "\u001b[32m[04/14 06:00:24 d2.evaluation.evaluator]: \u001b[0mInference done 413/548. 2.3196 s / img. ETA=0:05:14\n",
            "\u001b[32m[04/14 06:00:31 d2.evaluation.evaluator]: \u001b[0mInference done 416/548. 2.3198 s / img. ETA=0:05:07\n",
            "\u001b[32m[04/14 06:00:38 d2.evaluation.evaluator]: \u001b[0mInference done 419/548. 2.3200 s / img. ETA=0:05:00\n",
            "\u001b[32m[04/14 06:00:45 d2.evaluation.evaluator]: \u001b[0mInference done 422/548. 2.3201 s / img. ETA=0:04:53\n",
            "\u001b[32m[04/14 06:00:52 d2.evaluation.evaluator]: \u001b[0mInference done 425/548. 2.3202 s / img. ETA=0:04:46\n",
            "\u001b[32m[04/14 06:00:59 d2.evaluation.evaluator]: \u001b[0mInference done 428/548. 2.3202 s / img. ETA=0:04:39\n",
            "\u001b[32m[04/14 06:01:06 d2.evaluation.evaluator]: \u001b[0mInference done 431/548. 2.3203 s / img. ETA=0:04:32\n",
            "\u001b[32m[04/14 06:01:13 d2.evaluation.evaluator]: \u001b[0mInference done 434/548. 2.3204 s / img. ETA=0:04:25\n",
            "\u001b[32m[04/14 06:01:20 d2.evaluation.evaluator]: \u001b[0mInference done 437/548. 2.3205 s / img. ETA=0:04:18\n",
            "\u001b[32m[04/14 06:01:27 d2.evaluation.evaluator]: \u001b[0mInference done 440/548. 2.3206 s / img. ETA=0:04:11\n",
            "\u001b[32m[04/14 06:01:34 d2.evaluation.evaluator]: \u001b[0mInference done 443/548. 2.3207 s / img. ETA=0:04:04\n",
            "\u001b[32m[04/14 06:01:41 d2.evaluation.evaluator]: \u001b[0mInference done 446/548. 2.3208 s / img. ETA=0:03:57\n",
            "\u001b[32m[04/14 06:01:49 d2.evaluation.evaluator]: \u001b[0mInference done 449/548. 2.3208 s / img. ETA=0:03:50\n",
            "\u001b[32m[04/14 06:01:56 d2.evaluation.evaluator]: \u001b[0mInference done 452/548. 2.3209 s / img. ETA=0:03:43\n",
            "\u001b[32m[04/14 06:02:03 d2.evaluation.evaluator]: \u001b[0mInference done 455/548. 2.3209 s / img. ETA=0:03:36\n",
            "\u001b[32m[04/14 06:02:10 d2.evaluation.evaluator]: \u001b[0mInference done 458/548. 2.3210 s / img. ETA=0:03:29\n",
            "\u001b[32m[04/14 06:02:17 d2.evaluation.evaluator]: \u001b[0mInference done 461/548. 2.3211 s / img. ETA=0:03:22\n",
            "\u001b[32m[04/14 06:02:24 d2.evaluation.evaluator]: \u001b[0mInference done 464/548. 2.3213 s / img. ETA=0:03:15\n",
            "\u001b[32m[04/14 06:02:31 d2.evaluation.evaluator]: \u001b[0mInference done 467/548. 2.3214 s / img. ETA=0:03:08\n",
            "\u001b[32m[04/14 06:02:38 d2.evaluation.evaluator]: \u001b[0mInference done 470/548. 2.3214 s / img. ETA=0:03:01\n",
            "\u001b[32m[04/14 06:02:45 d2.evaluation.evaluator]: \u001b[0mInference done 473/548. 2.3215 s / img. ETA=0:02:54\n",
            "\u001b[32m[04/14 06:02:52 d2.evaluation.evaluator]: \u001b[0mInference done 476/548. 2.3216 s / img. ETA=0:02:47\n",
            "\u001b[32m[04/14 06:02:59 d2.evaluation.evaluator]: \u001b[0mInference done 479/548. 2.3218 s / img. ETA=0:02:40\n",
            "\u001b[32m[04/14 06:03:06 d2.evaluation.evaluator]: \u001b[0mInference done 482/548. 2.3219 s / img. ETA=0:02:33\n",
            "\u001b[32m[04/14 06:03:13 d2.evaluation.evaluator]: \u001b[0mInference done 485/548. 2.3219 s / img. ETA=0:02:26\n",
            "\u001b[32m[04/14 06:03:20 d2.evaluation.evaluator]: \u001b[0mInference done 488/548. 2.3221 s / img. ETA=0:02:19\n",
            "\u001b[32m[04/14 06:03:27 d2.evaluation.evaluator]: \u001b[0mInference done 491/548. 2.3222 s / img. ETA=0:02:12\n",
            "\u001b[32m[04/14 06:03:34 d2.evaluation.evaluator]: \u001b[0mInference done 494/548. 2.3223 s / img. ETA=0:02:05\n",
            "\u001b[32m[04/14 06:03:41 d2.evaluation.evaluator]: \u001b[0mInference done 497/548. 2.3223 s / img. ETA=0:01:58\n",
            "\u001b[32m[04/14 06:03:48 d2.evaluation.evaluator]: \u001b[0mInference done 500/548. 2.3225 s / img. ETA=0:01:51\n",
            "\u001b[32m[04/14 06:03:55 d2.evaluation.evaluator]: \u001b[0mInference done 503/548. 2.3226 s / img. ETA=0:01:45\n",
            "\u001b[32m[04/14 06:04:02 d2.evaluation.evaluator]: \u001b[0mInference done 506/548. 2.3227 s / img. ETA=0:01:38\n",
            "\u001b[32m[04/14 06:04:09 d2.evaluation.evaluator]: \u001b[0mInference done 509/548. 2.3227 s / img. ETA=0:01:31\n",
            "\u001b[32m[04/14 06:04:16 d2.evaluation.evaluator]: \u001b[0mInference done 512/548. 2.3228 s / img. ETA=0:01:24\n",
            "\u001b[32m[04/14 06:04:23 d2.evaluation.evaluator]: \u001b[0mInference done 515/548. 2.3229 s / img. ETA=0:01:17\n",
            "\u001b[32m[04/14 06:04:30 d2.evaluation.evaluator]: \u001b[0mInference done 518/548. 2.3230 s / img. ETA=0:01:10\n",
            "\u001b[32m[04/14 06:04:37 d2.evaluation.evaluator]: \u001b[0mInference done 521/548. 2.3230 s / img. ETA=0:01:03\n",
            "\u001b[32m[04/14 06:04:45 d2.evaluation.evaluator]: \u001b[0mInference done 524/548. 2.3232 s / img. ETA=0:00:56\n",
            "\u001b[32m[04/14 06:04:52 d2.evaluation.evaluator]: \u001b[0mInference done 527/548. 2.3233 s / img. ETA=0:00:49\n",
            "\u001b[32m[04/14 06:04:59 d2.evaluation.evaluator]: \u001b[0mInference done 530/548. 2.3233 s / img. ETA=0:00:42\n",
            "\u001b[32m[04/14 06:05:06 d2.evaluation.evaluator]: \u001b[0mInference done 533/548. 2.3234 s / img. ETA=0:00:35\n",
            "\u001b[32m[04/14 06:05:13 d2.evaluation.evaluator]: \u001b[0mInference done 536/548. 2.3235 s / img. ETA=0:00:28\n",
            "\u001b[32m[04/14 06:05:20 d2.evaluation.evaluator]: \u001b[0mInference done 539/548. 2.3236 s / img. ETA=0:00:21\n",
            "\u001b[32m[04/14 06:05:27 d2.evaluation.evaluator]: \u001b[0mInference done 542/548. 2.3237 s / img. ETA=0:00:14\n",
            "\u001b[32m[04/14 06:05:34 d2.evaluation.evaluator]: \u001b[0mInference done 545/548. 2.3237 s / img. ETA=0:00:07\n",
            "\u001b[32m[04/14 06:05:41 d2.evaluation.evaluator]: \u001b[0mInference done 548/548. 2.3237 s / img. ETA=0:00:00\n",
            "\u001b[32m[04/14 06:05:42 d2.evaluation.evaluator]: \u001b[0mTotal inference time: 0:21:09.169314 (2.337328 s / img per device, on 1 devices)\n",
            "\u001b[32m[04/14 06:05:42 d2.evaluation.evaluator]: \u001b[0mTotal inference pure compute time: 0:21:01 (2.323706 s / img per device, on 1 devices)\n",
            "\u001b[32m[04/14 06:05:50 d2.evaluation.testing]: \u001b[0mcopypaste: Task: GPU_Speed\n",
            "\u001b[32m[04/14 06:05:50 d2.evaluation.testing]: \u001b[0mcopypaste: Mean_FPS,Std_FPS,Max_FPS,Min_FPS,Mid_FPS\n",
            "\u001b[32m[04/14 06:05:50 d2.evaluation.testing]: \u001b[0mcopypaste: 0.4336,0.0116,0.4922,0.4072,0.4313\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!bash eval_visdrone.sh work_dirs/model_test-1/visdrone_infer.json"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lOhQu3oWm3A1",
        "outputId": "b2ff41f0-5b58-4dc0-dd2c-7028ce921b89"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Json to txt: .visdrone_det_txt\n",
            "  0% 0/548 [00:00<?, ?it/s]0\n",
            "0\n",
            "  0% 1/548 [00:01<14:56,  1.64s/it]1\n",
            "1\n",
            "  0% 2/548 [00:03<15:22,  1.69s/it]2\n",
            "2\n",
            "  1% 3/548 [00:05<15:34,  1.71s/it]3\n",
            "3\n",
            "  1% 4/548 [00:06<15:29,  1.71s/it]4\n",
            "4\n",
            "  1% 5/548 [00:08<16:08,  1.78s/it]5\n",
            "5\n",
            "  1% 6/548 [00:10<16:15,  1.80s/it]6\n",
            "6\n",
            "  1% 7/548 [00:13<17:44,  1.97s/it]7\n",
            "7\n",
            "  1% 8/548 [00:14<17:31,  1.95s/it]8\n",
            "8\n",
            "  2% 9/548 [00:16<16:39,  1.85s/it]9\n",
            "9\n",
            "  2% 10/548 [00:18<16:39,  1.86s/it]10\n",
            "10\n",
            "  2% 11/548 [00:20<15:52,  1.77s/it]11\n",
            "11\n",
            "  2% 12/548 [00:21<15:59,  1.79s/it]12\n",
            "12\n",
            "  2% 13/548 [00:23<15:30,  1.74s/it]13\n",
            "13\n",
            "  3% 14/548 [00:25<15:35,  1.75s/it]14\n",
            "14\n",
            "  3% 15/548 [00:26<15:16,  1.72s/it]15\n",
            "15\n",
            "  3% 16/548 [00:28<14:56,  1.69s/it]16\n",
            "16\n",
            "  3% 17/548 [00:30<14:54,  1.68s/it]17\n",
            "17\n",
            "  3% 18/548 [00:31<14:28,  1.64s/it]18\n",
            "18\n",
            "  3% 19/548 [00:33<14:55,  1.69s/it]19\n",
            "19\n",
            "  4% 20/548 [00:35<14:35,  1.66s/it]20\n",
            "20\n",
            "  4% 21/548 [00:36<14:39,  1.67s/it]21\n",
            "21\n",
            "  4% 22/548 [00:38<14:58,  1.71s/it]22\n",
            "22\n",
            "  4% 23/548 [00:40<14:48,  1.69s/it]23\n",
            "23\n",
            "  4% 24/548 [00:41<14:22,  1.65s/it]24\n",
            "24\n",
            "  5% 25/548 [00:43<14:29,  1.66s/it]25\n",
            "25\n",
            "  5% 26/548 [00:45<14:38,  1.68s/it]26\n",
            "26\n",
            "  5% 27/548 [00:46<14:26,  1.66s/it]27\n",
            "27\n",
            "  5% 28/548 [00:48<14:27,  1.67s/it]28\n",
            "28\n",
            "  5% 29/548 [00:50<14:31,  1.68s/it]29\n",
            "29\n",
            "  5% 30/548 [00:51<14:35,  1.69s/it]30\n",
            "30\n",
            "  6% 31/548 [00:53<14:56,  1.73s/it]31\n",
            "31\n",
            "  6% 32/548 [00:55<14:48,  1.72s/it]32\n",
            "32\n",
            "  6% 33/548 [00:57<15:01,  1.75s/it]33\n",
            "33\n",
            "  6% 34/548 [00:59<15:11,  1.77s/it]34\n",
            "34\n",
            "  6% 35/548 [01:00<14:39,  1.72s/it]35\n",
            "35\n",
            "  7% 36/548 [01:02<14:48,  1.74s/it]36\n",
            "36\n",
            "  7% 37/548 [01:04<15:05,  1.77s/it]37\n",
            "37\n",
            "  7% 38/548 [01:05<14:34,  1.71s/it]38\n",
            "38\n",
            "  7% 39/548 [01:07<13:59,  1.65s/it]39\n",
            "39\n",
            "  7% 40/548 [01:09<14:00,  1.65s/it]40\n",
            "40\n",
            "  7% 41/548 [01:10<14:06,  1.67s/it]41\n",
            "41\n",
            "  8% 42/548 [01:12<14:07,  1.67s/it]42\n",
            "42\n",
            "  8% 43/548 [01:14<14:26,  1.72s/it]43\n",
            "43\n",
            "  8% 44/548 [01:15<14:04,  1.67s/it]44\n",
            "44\n",
            "  8% 45/548 [01:17<14:17,  1.71s/it]45\n",
            "45\n",
            "  8% 46/548 [01:19<14:36,  1.75s/it]46\n",
            "46\n",
            "  9% 47/548 [01:21<14:45,  1.77s/it]47\n",
            "47\n",
            "  9% 48/548 [01:23<14:41,  1.76s/it]48\n",
            "48\n",
            "  9% 49/548 [01:24<14:30,  1.74s/it]49\n",
            "49\n",
            "  9% 50/548 [01:26<14:37,  1.76s/it]50\n",
            "50\n",
            "  9% 51/548 [01:28<14:41,  1.77s/it]51\n",
            "51\n",
            "  9% 52/548 [01:30<14:37,  1.77s/it]52\n",
            "52\n",
            " 10% 53/548 [01:32<15:02,  1.82s/it]53\n",
            "53\n",
            " 10% 54/548 [01:34<16:45,  2.03s/it]54\n",
            "54\n",
            " 10% 55/548 [01:36<16:10,  1.97s/it]55\n",
            "55\n",
            " 10% 56/548 [01:38<15:45,  1.92s/it]56\n",
            "56\n",
            " 10% 57/548 [01:40<15:22,  1.88s/it]57\n",
            "57\n",
            " 11% 58/548 [01:41<15:33,  1.90s/it]58\n",
            "58\n",
            " 11% 59/548 [01:43<15:00,  1.84s/it]59\n",
            "59\n",
            " 11% 60/548 [01:45<15:43,  1.93s/it]60\n",
            "60\n",
            " 11% 61/548 [01:47<15:09,  1.87s/it]61\n",
            "61\n",
            " 11% 62/548 [01:49<14:44,  1.82s/it]62\n",
            "62\n",
            " 11% 63/548 [01:51<14:32,  1.80s/it]63\n",
            "63\n",
            " 12% 64/548 [01:52<13:56,  1.73s/it]64\n",
            "64\n",
            " 12% 65/548 [01:54<13:44,  1.71s/it]65\n",
            "65\n",
            " 12% 66/548 [01:56<14:00,  1.74s/it]66\n",
            "66\n",
            " 12% 67/548 [01:58<15:18,  1.91s/it]67\n",
            "67\n",
            " 12% 68/548 [02:00<14:44,  1.84s/it]68\n",
            "68\n",
            " 13% 69/548 [02:01<14:39,  1.84s/it]69\n",
            "69\n",
            " 13% 70/548 [02:03<14:44,  1.85s/it]70\n",
            "70\n",
            " 13% 71/548 [02:05<14:49,  1.87s/it]71\n",
            "71\n",
            " 13% 72/548 [02:07<14:36,  1.84s/it]72\n",
            "72\n",
            " 13% 73/548 [02:09<14:00,  1.77s/it]73\n",
            "73\n",
            " 14% 74/548 [02:10<13:22,  1.69s/it]74\n",
            "74\n",
            " 14% 75/548 [02:12<13:25,  1.70s/it]75\n",
            "75\n",
            " 14% 76/548 [02:13<13:15,  1.69s/it]76\n",
            "76\n",
            " 14% 77/548 [02:16<15:44,  2.01s/it]77\n",
            "77\n",
            " 14% 78/548 [02:18<15:14,  1.95s/it]78\n",
            "78\n",
            " 14% 79/548 [02:20<14:17,  1.83s/it]79\n",
            "79\n",
            " 15% 80/548 [02:21<13:37,  1.75s/it]80\n",
            "80\n",
            " 15% 81/548 [02:23<13:00,  1.67s/it]81\n",
            "81\n",
            " 15% 82/548 [02:24<12:59,  1.67s/it]82\n",
            "82\n",
            " 15% 83/548 [02:26<13:18,  1.72s/it]83\n",
            "83\n",
            " 15% 84/548 [02:29<15:40,  2.03s/it]84\n",
            "84\n",
            " 16% 85/548 [02:31<15:25,  2.00s/it]85\n",
            "85\n",
            " 16% 86/548 [02:33<14:53,  1.93s/it]86\n",
            "86\n",
            " 16% 87/548 [02:34<14:14,  1.85s/it]87\n",
            "87\n",
            " 16% 88/548 [02:36<13:39,  1.78s/it]88\n",
            "88\n",
            " 16% 89/548 [02:38<13:37,  1.78s/it]89\n",
            "89\n",
            " 16% 90/548 [02:39<13:34,  1.78s/it]90\n",
            "90\n",
            " 17% 91/548 [02:41<13:02,  1.71s/it]91\n",
            "91\n",
            " 17% 92/548 [02:43<13:00,  1.71s/it]92\n",
            "92\n",
            " 17% 93/548 [02:44<13:03,  1.72s/it]93\n",
            "93\n",
            " 17% 94/548 [02:46<13:15,  1.75s/it]94\n",
            "94\n",
            " 17% 95/548 [02:48<13:52,  1.84s/it]95\n",
            "95\n",
            " 18% 96/548 [02:50<14:08,  1.88s/it]96\n",
            "96\n",
            " 18% 97/548 [02:52<13:49,  1.84s/it]97\n",
            "97\n",
            " 18% 98/548 [02:54<13:15,  1.77s/it]98\n",
            "98\n",
            " 18% 99/548 [02:55<12:54,  1.73s/it]99\n",
            "99\n",
            " 18% 100/548 [02:57<12:58,  1.74s/it]100\n",
            "100\n",
            " 18% 101/548 [02:59<13:35,  1.82s/it]101\n",
            "101\n",
            " 19% 102/548 [03:01<13:51,  1.86s/it]102\n",
            "102\n",
            " 19% 103/548 [03:03<13:44,  1.85s/it]103\n",
            "103\n",
            " 19% 104/548 [03:05<13:31,  1.83s/it]104\n",
            "104\n",
            " 19% 105/548 [03:06<13:33,  1.84s/it]105\n",
            "105\n",
            " 19% 106/548 [03:08<13:07,  1.78s/it]106\n",
            "106\n",
            " 20% 107/548 [03:10<12:49,  1.75s/it]107\n",
            "107\n",
            " 20% 108/548 [03:11<12:18,  1.68s/it]108\n",
            "108\n",
            " 20% 109/548 [03:13<12:44,  1.74s/it]109\n",
            "109\n",
            " 20% 110/548 [03:15<12:27,  1.71s/it]110\n",
            "110\n",
            " 20% 111/548 [03:17<12:33,  1.72s/it]111\n",
            "111\n",
            " 20% 112/548 [03:18<12:40,  1.74s/it]112\n",
            "112\n",
            " 21% 113/548 [03:20<12:42,  1.75s/it]113\n",
            "113\n",
            " 21% 114/548 [03:22<12:14,  1.69s/it]114\n",
            "114\n",
            " 21% 115/548 [03:23<12:01,  1.67s/it]115\n",
            "115\n",
            " 21% 116/548 [03:25<12:08,  1.69s/it]116\n",
            "116\n",
            " 21% 117/548 [03:27<11:49,  1.65s/it]117\n",
            "117\n",
            " 22% 118/548 [03:28<11:34,  1.61s/it]118\n",
            "118\n",
            " 22% 119/548 [03:30<11:40,  1.63s/it]119\n",
            "119\n",
            " 22% 120/548 [03:32<11:58,  1.68s/it]120\n",
            "120\n",
            " 22% 121/548 [03:33<11:58,  1.68s/it]121\n",
            "121\n",
            " 22% 122/548 [03:35<12:01,  1.69s/it]122\n",
            "122\n",
            " 22% 123/548 [03:37<11:45,  1.66s/it]123\n",
            "123\n",
            " 23% 124/548 [03:38<11:45,  1.66s/it]124\n",
            "124\n",
            " 23% 125/548 [03:40<11:29,  1.63s/it]125\n",
            "125\n",
            " 23% 126/548 [03:41<11:17,  1.61s/it]126\n",
            "126\n",
            " 23% 127/548 [03:43<11:16,  1.61s/it]127\n",
            "127\n",
            " 23% 128/548 [03:45<11:41,  1.67s/it]128\n",
            "128\n",
            " 24% 129/548 [03:46<11:32,  1.65s/it]129\n",
            "129\n",
            " 24% 130/548 [03:48<11:59,  1.72s/it]130\n",
            "130\n",
            " 24% 131/548 [03:50<12:11,  1.75s/it]131\n",
            "131\n",
            " 24% 132/548 [03:52<11:47,  1.70s/it]132\n",
            "132\n",
            " 24% 133/548 [03:53<11:34,  1.67s/it]133\n",
            "133\n",
            " 24% 134/548 [03:55<11:27,  1.66s/it]134\n",
            "134\n",
            " 25% 135/548 [03:56<11:15,  1.64s/it]135\n",
            "135\n",
            " 25% 136/548 [03:59<12:30,  1.82s/it]136\n",
            "136\n",
            " 25% 137/548 [04:02<16:13,  2.37s/it]137\n",
            "137\n",
            " 25% 138/548 [04:04<15:00,  2.20s/it]138\n",
            "138\n",
            " 25% 139/548 [04:06<13:55,  2.04s/it]139\n",
            "139\n",
            " 26% 140/548 [04:08<14:21,  2.11s/it]140\n",
            "140\n",
            " 26% 141/548 [04:11<15:50,  2.34s/it]141\n",
            "141\n",
            " 26% 142/548 [04:12<13:51,  2.05s/it]142\n",
            "142\n",
            " 26% 143/548 [04:14<12:44,  1.89s/it]143\n",
            "143\n",
            " 26% 144/548 [04:15<11:43,  1.74s/it]144\n",
            "144\n",
            " 26% 145/548 [04:17<11:21,  1.69s/it]145\n",
            "145\n",
            " 27% 146/548 [04:19<11:39,  1.74s/it]146\n",
            "146\n",
            " 27% 147/548 [04:20<11:40,  1.75s/it]147\n",
            "147\n",
            " 27% 148/548 [04:22<11:45,  1.76s/it]148\n",
            "148\n",
            " 27% 149/548 [04:24<12:03,  1.81s/it]149\n",
            "149\n",
            " 27% 150/548 [04:26<11:57,  1.80s/it]150\n",
            "150\n",
            " 28% 151/548 [04:28<11:40,  1.76s/it]151\n",
            "151\n",
            " 28% 152/548 [04:29<11:47,  1.79s/it]152\n",
            "152\n",
            " 28% 153/548 [04:31<11:35,  1.76s/it]153\n",
            "153\n",
            " 28% 154/548 [04:33<11:40,  1.78s/it]154\n",
            "154\n",
            " 28% 155/548 [04:35<11:26,  1.75s/it]155\n",
            "155\n",
            " 28% 156/548 [04:36<11:25,  1.75s/it]156\n",
            "156\n",
            " 29% 157/548 [04:38<11:53,  1.83s/it]157\n",
            "157\n",
            " 29% 158/548 [04:40<11:40,  1.80s/it]158\n",
            "158\n",
            " 29% 159/548 [04:42<11:19,  1.75s/it]159\n",
            "159\n",
            " 29% 160/548 [04:44<11:45,  1.82s/it]160\n",
            "160\n",
            " 29% 161/548 [04:45<11:31,  1.79s/it]161\n",
            "161\n",
            " 30% 162/548 [04:47<11:41,  1.82s/it]162\n",
            "162\n",
            " 30% 163/548 [04:49<11:21,  1.77s/it]163\n",
            "163\n",
            " 30% 164/548 [04:51<11:06,  1.74s/it]164\n",
            "164\n",
            " 30% 165/548 [04:52<11:07,  1.74s/it]165\n",
            "165\n",
            " 30% 166/548 [04:54<11:39,  1.83s/it]166\n",
            "166\n",
            " 30% 167/548 [04:56<11:13,  1.77s/it]167\n",
            "167\n",
            " 31% 168/548 [04:58<11:03,  1.75s/it]168\n",
            "168\n",
            " 31% 169/548 [05:00<11:27,  1.81s/it]169\n",
            "169\n",
            " 31% 170/548 [05:01<11:12,  1.78s/it]170\n",
            "170\n",
            " 31% 171/548 [05:03<10:53,  1.73s/it]171\n",
            "171\n",
            " 31% 172/548 [05:05<10:50,  1.73s/it]172\n",
            "172\n",
            " 32% 173/548 [05:07<11:16,  1.80s/it]173\n",
            "173\n",
            " 32% 174/548 [05:08<10:58,  1.76s/it]174\n",
            "174\n",
            " 32% 175/548 [05:11<12:35,  2.03s/it]175\n",
            "175\n",
            " 32% 176/548 [05:13<11:45,  1.90s/it]176\n",
            "176\n",
            " 32% 177/548 [05:14<11:19,  1.83s/it]177\n",
            "177\n",
            " 32% 178/548 [05:16<11:00,  1.79s/it]178\n",
            "178\n",
            " 33% 179/548 [05:18<10:58,  1.79s/it]179\n",
            "179\n",
            " 33% 180/548 [05:20<11:01,  1.80s/it]180\n",
            "180\n",
            " 33% 181/548 [05:21<10:37,  1.74s/it]181\n",
            "181\n",
            " 33% 182/548 [05:23<10:27,  1.71s/it]182\n",
            "182\n",
            " 33% 183/548 [05:24<10:10,  1.67s/it]183\n",
            "183\n",
            " 34% 184/548 [05:27<11:35,  1.91s/it]184\n",
            "184\n",
            " 34% 185/548 [05:29<11:14,  1.86s/it]185\n",
            "185\n",
            " 34% 186/548 [05:30<10:51,  1.80s/it]186\n",
            "186\n",
            " 34% 187/548 [05:32<10:38,  1.77s/it]187\n",
            "187\n",
            " 34% 188/548 [05:34<10:32,  1.76s/it]188\n",
            "188\n",
            " 34% 189/548 [05:36<10:33,  1.76s/it]189\n",
            "189\n",
            " 35% 190/548 [05:37<10:12,  1.71s/it]190\n",
            "190\n",
            " 35% 191/548 [05:39<09:47,  1.65s/it]191\n",
            "191\n",
            " 35% 192/548 [05:40<09:59,  1.69s/it]192\n",
            "192\n",
            " 35% 193/548 [05:42<09:37,  1.63s/it]193\n",
            "193\n",
            " 35% 194/548 [05:44<10:06,  1.71s/it]194\n",
            "194\n",
            " 36% 195/548 [05:46<10:06,  1.72s/it]195\n",
            "195\n",
            " 36% 196/548 [05:47<10:04,  1.72s/it]196\n",
            "196\n",
            " 36% 197/548 [05:49<10:06,  1.73s/it]197\n",
            "197\n",
            " 36% 198/548 [05:51<09:58,  1.71s/it]198\n",
            "198\n",
            " 36% 199/548 [05:52<09:40,  1.66s/it]199\n",
            "199\n",
            " 36% 200/548 [05:54<09:54,  1.71s/it]200\n",
            "200\n",
            " 37% 201/548 [05:56<09:58,  1.73s/it]201\n",
            "201\n",
            " 37% 202/548 [05:57<09:47,  1.70s/it]202\n",
            "202\n",
            " 37% 203/548 [05:59<09:50,  1.71s/it]203\n",
            "203\n",
            " 37% 204/548 [06:01<10:28,  1.83s/it]204\n",
            "204\n",
            " 37% 205/548 [06:03<10:29,  1.83s/it]205\n",
            "205\n",
            " 38% 206/548 [06:05<10:32,  1.85s/it]206\n",
            "206\n",
            " 38% 207/548 [06:07<10:08,  1.78s/it]207\n",
            "207\n",
            " 38% 208/548 [06:09<10:25,  1.84s/it]208\n",
            "208\n",
            " 38% 209/548 [06:10<09:57,  1.76s/it]209\n",
            "209\n",
            " 38% 210/548 [06:12<09:44,  1.73s/it]210\n",
            "210\n",
            " 39% 211/548 [06:13<09:32,  1.70s/it]211\n",
            "211\n",
            " 39% 212/548 [06:15<09:22,  1.67s/it]212\n",
            "212\n",
            " 39% 213/548 [06:17<09:19,  1.67s/it]213\n",
            "213\n",
            " 39% 214/548 [06:19<09:35,  1.72s/it]214\n",
            "214\n",
            " 39% 215/548 [06:21<09:56,  1.79s/it]215\n",
            "215\n",
            " 39% 216/548 [06:22<09:58,  1.80s/it]216\n",
            "216\n",
            " 40% 217/548 [06:24<09:25,  1.71s/it]217\n",
            "217\n",
            " 40% 218/548 [06:25<09:15,  1.68s/it]218\n",
            "218\n",
            " 40% 219/548 [06:27<09:22,  1.71s/it]219\n",
            "219\n",
            " 40% 220/548 [06:29<09:49,  1.80s/it]220\n",
            "220\n",
            " 40% 221/548 [06:31<09:54,  1.82s/it]221\n",
            "221\n",
            " 41% 222/548 [06:33<10:00,  1.84s/it]222\n",
            "222\n",
            " 41% 223/548 [06:35<10:20,  1.91s/it]223\n",
            "223\n",
            " 41% 224/548 [06:37<10:13,  1.89s/it]224\n",
            "224\n",
            " 41% 225/548 [06:39<09:42,  1.80s/it]225\n",
            "225\n",
            " 41% 226/548 [06:40<09:30,  1.77s/it]226\n",
            "226\n",
            " 41% 227/548 [06:42<09:36,  1.80s/it]227\n",
            "227\n",
            " 42% 228/548 [06:44<09:13,  1.73s/it]228\n",
            "228\n",
            " 42% 229/548 [06:45<08:59,  1.69s/it]229\n",
            "229\n",
            " 42% 230/548 [06:47<08:51,  1.67s/it]230\n",
            "230\n",
            " 42% 231/548 [06:48<08:23,  1.59s/it]231\n",
            "231\n",
            " 42% 232/548 [06:50<08:26,  1.60s/it]232\n",
            "232\n",
            " 43% 233/548 [06:52<08:44,  1.66s/it]233\n",
            "233\n",
            " 43% 234/548 [06:53<08:36,  1.64s/it]234\n",
            "234\n",
            " 43% 235/548 [06:55<08:53,  1.70s/it]235\n",
            "235\n",
            " 43% 236/548 [06:57<09:03,  1.74s/it]236\n",
            "236\n",
            " 43% 237/548 [06:59<09:16,  1.79s/it]237\n",
            "237\n",
            " 43% 238/548 [07:01<09:50,  1.91s/it]238\n",
            "238\n",
            " 44% 239/548 [07:03<09:47,  1.90s/it]239\n",
            "239\n",
            " 44% 240/548 [07:05<09:26,  1.84s/it]240\n",
            "240\n",
            " 44% 241/548 [07:07<09:26,  1.85s/it]241\n",
            "241\n",
            " 44% 242/548 [07:09<10:17,  2.02s/it]242\n",
            "242\n",
            " 44% 243/548 [07:11<09:58,  1.96s/it]243\n",
            "243\n",
            " 45% 244/548 [07:12<09:25,  1.86s/it]244\n",
            "244\n",
            " 45% 245/548 [07:14<09:04,  1.80s/it]245\n",
            "245\n",
            " 45% 246/548 [07:16<08:52,  1.76s/it]246\n",
            "246\n",
            " 45% 247/548 [07:17<08:47,  1.75s/it]247\n",
            "247\n",
            " 45% 248/548 [07:19<08:21,  1.67s/it]248\n",
            "248\n",
            " 45% 249/548 [07:21<08:48,  1.77s/it]249\n",
            "249\n",
            " 46% 250/548 [07:23<09:14,  1.86s/it]250\n",
            "250\n",
            " 46% 251/548 [07:25<08:59,  1.82s/it]251\n",
            "251\n",
            " 46% 252/548 [07:26<08:26,  1.71s/it]252\n",
            "252\n",
            " 46% 253/548 [07:28<08:41,  1.77s/it]253\n",
            "253\n",
            " 46% 254/548 [07:30<09:02,  1.84s/it]254\n",
            "254\n",
            " 47% 255/548 [07:32<08:45,  1.79s/it]255\n",
            "255\n",
            " 47% 256/548 [07:33<08:08,  1.67s/it]256\n",
            "256\n",
            " 47% 257/548 [07:35<08:01,  1.65s/it]257\n",
            "257\n",
            " 47% 258/548 [07:36<07:54,  1.64s/it]258\n",
            "258\n",
            " 47% 259/548 [07:38<07:29,  1.56s/it]259\n",
            "259\n",
            " 47% 260/548 [07:39<07:30,  1.56s/it]260\n",
            "260\n",
            " 48% 261/548 [07:41<07:19,  1.53s/it]261\n",
            "261\n",
            " 48% 262/548 [07:43<07:38,  1.60s/it]262\n",
            "262\n",
            " 48% 263/548 [07:44<07:46,  1.64s/it]263\n",
            "263\n",
            " 48% 264/548 [07:46<07:47,  1.65s/it]264\n",
            "264\n",
            " 48% 265/548 [07:48<08:13,  1.75s/it]265\n",
            "265\n",
            " 49% 266/548 [07:50<08:03,  1.72s/it]266\n",
            "266\n",
            " 49% 267/548 [07:51<07:56,  1.69s/it]267\n",
            "267\n",
            " 49% 268/548 [07:53<07:50,  1.68s/it]268\n",
            "268\n",
            " 49% 269/548 [07:55<07:47,  1.68s/it]269\n",
            "269\n",
            " 49% 270/548 [07:56<07:35,  1.64s/it]270\n",
            "270\n",
            " 49% 271/548 [07:58<07:30,  1.63s/it]271\n",
            "271\n",
            " 50% 272/548 [07:59<07:37,  1.66s/it]272\n",
            "272\n",
            " 50% 273/548 [08:02<08:13,  1.79s/it]273\n",
            "273\n",
            " 50% 274/548 [08:03<08:06,  1.77s/it]274\n",
            "274\n",
            " 50% 275/548 [08:05<08:06,  1.78s/it]275\n",
            "275\n",
            " 50% 276/548 [08:07<08:03,  1.78s/it]276\n",
            "276\n",
            " 51% 277/548 [08:08<07:46,  1.72s/it]277\n",
            "277\n",
            " 51% 278/548 [08:10<07:43,  1.72s/it]278\n",
            "278\n",
            " 51% 279/548 [08:12<07:48,  1.74s/it]279\n",
            "279\n",
            " 51% 280/548 [08:13<07:26,  1.67s/it]280\n",
            "280\n",
            " 51% 281/548 [08:15<07:16,  1.63s/it]281\n",
            "281\n",
            " 51% 282/548 [08:17<07:10,  1.62s/it]282\n",
            "282\n",
            " 52% 283/548 [08:18<07:29,  1.70s/it]283\n",
            "283\n",
            " 52% 284/548 [08:20<07:31,  1.71s/it]284\n",
            "284\n",
            " 52% 285/548 [08:22<07:49,  1.79s/it]285\n",
            "285\n",
            " 52% 286/548 [08:24<07:35,  1.74s/it]286\n",
            "286\n",
            " 52% 287/548 [08:25<07:21,  1.69s/it]287\n",
            "287\n",
            " 53% 288/548 [08:27<07:40,  1.77s/it]288\n",
            "288\n",
            " 53% 289/548 [08:29<07:33,  1.75s/it]289\n",
            "289\n",
            " 53% 290/548 [08:31<07:19,  1.70s/it]290\n",
            "290\n",
            " 53% 291/548 [08:32<07:14,  1.69s/it]291\n",
            "291\n",
            " 53% 292/548 [08:34<07:12,  1.69s/it]292\n",
            "292\n",
            " 53% 293/548 [08:36<07:20,  1.73s/it]293\n",
            "293\n",
            " 54% 294/548 [08:38<07:21,  1.74s/it]294\n",
            "294\n",
            " 54% 295/548 [08:39<06:54,  1.64s/it]295\n",
            "295\n",
            " 54% 296/548 [08:40<06:43,  1.60s/it]296\n",
            "296\n",
            " 54% 297/548 [08:42<06:36,  1.58s/it]297\n",
            "297\n",
            " 54% 298/548 [08:45<07:46,  1.87s/it]298\n",
            "298\n",
            " 55% 299/548 [08:46<07:35,  1.83s/it]299\n",
            "299\n",
            " 55% 300/548 [08:48<07:22,  1.79s/it]300\n",
            "300\n",
            " 55% 301/548 [08:49<07:01,  1.71s/it]301\n",
            "301\n",
            " 55% 302/548 [08:51<07:13,  1.76s/it]302\n",
            "302\n",
            " 55% 303/548 [08:53<07:07,  1.75s/it]303\n",
            "303\n",
            " 55% 304/548 [08:55<06:55,  1.70s/it]304\n",
            "304\n",
            " 56% 305/548 [08:57<07:06,  1.76s/it]305\n",
            "305\n",
            " 56% 306/548 [08:58<06:59,  1.73s/it]306\n",
            "306\n",
            " 56% 307/548 [09:00<06:44,  1.68s/it]307\n",
            "307\n",
            " 56% 308/548 [09:02<07:25,  1.85s/it]308\n",
            "308\n",
            " 56% 309/548 [09:04<08:02,  2.02s/it]309\n",
            "309\n",
            " 57% 310/548 [09:06<07:54,  1.99s/it]310\n",
            "310\n",
            " 57% 311/548 [09:08<07:21,  1.86s/it]311\n",
            "311\n",
            " 57% 312/548 [09:10<07:07,  1.81s/it]312\n",
            "312\n",
            " 57% 313/548 [09:11<07:05,  1.81s/it]313\n",
            "313\n",
            " 57% 314/548 [09:13<07:04,  1.81s/it]314\n",
            "314\n",
            " 57% 315/548 [09:15<06:50,  1.76s/it]315\n",
            "315\n",
            " 58% 316/548 [09:16<06:37,  1.71s/it]316\n",
            "316\n",
            " 58% 317/548 [09:18<06:18,  1.64s/it]317\n",
            "317\n",
            " 58% 318/548 [09:20<06:23,  1.67s/it]318\n",
            "318\n",
            " 58% 319/548 [09:21<06:32,  1.71s/it]319\n",
            "319\n",
            " 58% 320/548 [09:23<06:34,  1.73s/it]320\n",
            "320\n",
            " 59% 321/548 [09:25<06:24,  1.69s/it]321\n",
            "321\n",
            " 59% 322/548 [09:27<06:26,  1.71s/it]322\n",
            "322\n",
            " 59% 323/548 [09:28<06:09,  1.64s/it]323\n",
            "323\n",
            " 59% 324/548 [09:30<06:08,  1.65s/it]324\n",
            "324\n",
            " 59% 325/548 [09:32<06:14,  1.68s/it]325\n",
            "325\n",
            " 59% 326/548 [09:33<06:08,  1.66s/it]326\n",
            "326\n",
            " 60% 327/548 [09:35<06:24,  1.74s/it]327\n",
            "327\n",
            " 60% 328/548 [09:37<06:18,  1.72s/it]328\n",
            "328\n",
            " 60% 329/548 [09:38<06:16,  1.72s/it]329\n",
            "329\n",
            " 60% 330/548 [09:40<06:09,  1.69s/it]330\n",
            "330\n",
            " 60% 331/548 [09:42<06:25,  1.78s/it]331\n",
            "331\n",
            " 61% 332/548 [09:44<06:13,  1.73s/it]332\n",
            "332\n",
            " 61% 333/548 [09:45<05:53,  1.64s/it]333\n",
            "333\n",
            " 61% 334/548 [09:47<05:52,  1.65s/it]334\n",
            "334\n",
            " 61% 335/548 [09:49<06:16,  1.77s/it]335\n",
            "335\n",
            " 61% 336/548 [09:51<06:21,  1.80s/it]336\n",
            "336\n",
            " 61% 337/548 [09:52<06:02,  1.72s/it]337\n",
            "337\n",
            " 62% 338/548 [09:54<05:46,  1.65s/it]338\n",
            "338\n",
            " 62% 339/548 [09:55<05:39,  1.62s/it]339\n",
            "339\n",
            " 62% 340/548 [09:57<05:33,  1.60s/it]340\n",
            "340\n",
            " 62% 341/548 [09:59<05:48,  1.68s/it]341\n",
            "341\n",
            " 62% 342/548 [10:00<05:48,  1.69s/it]342\n",
            "342\n",
            " 63% 343/548 [10:02<06:06,  1.79s/it]343\n",
            "343\n",
            " 63% 344/548 [10:04<06:09,  1.81s/it]344\n",
            "344\n",
            " 63% 345/548 [10:06<05:56,  1.76s/it]345\n",
            "345\n",
            " 63% 346/548 [10:07<05:37,  1.67s/it]346\n",
            "346\n",
            " 63% 347/548 [10:09<05:40,  1.69s/it]347\n",
            "347\n",
            " 64% 348/548 [10:11<05:46,  1.73s/it]348\n",
            "348\n",
            " 64% 349/548 [10:13<05:58,  1.80s/it]349\n",
            "349\n",
            " 64% 350/548 [10:15<05:54,  1.79s/it]350\n",
            "350\n",
            " 64% 351/548 [10:16<05:42,  1.74s/it]351\n",
            "351\n",
            " 64% 352/548 [10:18<05:34,  1.70s/it]352\n",
            "352\n",
            " 64% 353/548 [10:20<05:27,  1.68s/it]353\n",
            "353\n",
            " 65% 354/548 [10:22<06:34,  2.03s/it]354\n",
            "354\n",
            " 65% 355/548 [10:24<06:00,  1.87s/it]355\n",
            "355\n",
            " 65% 356/548 [10:26<05:49,  1.82s/it]356\n",
            "356\n",
            " 65% 357/548 [10:27<05:37,  1.77s/it]357\n",
            "357\n",
            " 65% 358/548 [10:29<05:35,  1.77s/it]358\n",
            "358\n",
            " 66% 359/548 [10:31<05:24,  1.72s/it]359\n",
            "359\n",
            " 66% 360/548 [10:33<05:38,  1.80s/it]360\n",
            "360\n",
            " 66% 361/548 [10:34<05:31,  1.77s/it]361\n",
            "361\n",
            " 66% 362/548 [10:36<05:23,  1.74s/it]362\n",
            "362\n",
            " 66% 363/548 [10:38<05:26,  1.77s/it]363\n",
            "363\n",
            " 66% 364/548 [10:40<05:21,  1.75s/it]364\n",
            "364\n",
            " 67% 365/548 [10:41<05:01,  1.65s/it]365\n",
            "365\n",
            " 67% 366/548 [10:42<04:46,  1.58s/it]366\n",
            "366\n",
            " 67% 367/548 [10:44<05:00,  1.66s/it]367\n",
            "367\n",
            " 67% 368/548 [10:46<04:59,  1.67s/it]368\n",
            "368\n",
            " 67% 369/548 [10:48<04:55,  1.65s/it]369\n",
            "369\n",
            " 68% 370/548 [10:49<04:48,  1.62s/it]370\n",
            "370\n",
            " 68% 371/548 [10:51<04:57,  1.68s/it]371\n",
            "371\n",
            " 68% 372/548 [10:53<04:53,  1.66s/it]372\n",
            "372\n",
            " 68% 373/548 [10:54<04:51,  1.67s/it]373\n",
            "373\n",
            " 68% 374/548 [10:56<05:03,  1.74s/it]374\n",
            "374\n",
            " 68% 375/548 [10:59<05:59,  2.08s/it]375\n",
            "375\n",
            " 69% 376/548 [11:01<05:39,  1.97s/it]376\n",
            "376\n",
            " 69% 377/548 [11:03<05:37,  1.97s/it]377\n",
            "377\n",
            " 69% 378/548 [11:04<05:20,  1.89s/it]378\n",
            "378\n",
            " 69% 379/548 [11:06<05:14,  1.86s/it]379\n",
            "379\n",
            " 69% 380/548 [11:08<05:01,  1.79s/it]380\n",
            "380\n",
            " 70% 381/548 [11:09<04:51,  1.75s/it]381\n",
            "381\n",
            " 70% 382/548 [11:11<04:57,  1.79s/it]382\n",
            "382\n",
            " 70% 383/548 [11:13<05:03,  1.84s/it]383\n",
            "383\n",
            " 70% 384/548 [11:15<05:04,  1.86s/it]384\n",
            "384\n",
            " 70% 385/548 [11:17<05:05,  1.88s/it]385\n",
            "385\n",
            " 70% 386/548 [11:19<04:56,  1.83s/it]386\n",
            "386\n",
            " 71% 387/548 [11:20<04:39,  1.74s/it]387\n",
            "387\n",
            " 71% 388/548 [11:22<04:38,  1.74s/it]388\n",
            "388\n",
            " 71% 389/548 [11:24<04:33,  1.72s/it]389\n",
            "389\n",
            " 71% 390/548 [11:25<04:24,  1.67s/it]390\n",
            "390\n",
            " 71% 391/548 [11:27<04:15,  1.63s/it]391\n",
            "391\n",
            " 72% 392/548 [11:28<04:13,  1.62s/it]392\n",
            "392\n",
            " 72% 393/548 [11:30<04:15,  1.65s/it]393\n",
            "393\n",
            " 72% 394/548 [11:32<04:11,  1.63s/it]394\n",
            "394\n",
            " 72% 395/548 [11:33<04:14,  1.66s/it]395\n",
            "395\n",
            " 72% 396/548 [11:35<04:19,  1.71s/it]396\n",
            "396\n",
            " 72% 397/548 [11:37<04:18,  1.71s/it]397\n",
            "397\n",
            " 73% 398/548 [11:39<04:18,  1.72s/it]398\n",
            "398\n",
            " 73% 399/548 [11:40<04:12,  1.70s/it]399\n",
            "399\n",
            " 73% 400/548 [11:42<04:11,  1.70s/it]400\n",
            "400\n",
            " 73% 401/548 [11:44<04:03,  1.66s/it]401\n",
            "401\n",
            " 73% 402/548 [11:45<03:58,  1.64s/it]402\n",
            "402\n",
            " 74% 403/548 [11:47<03:52,  1.60s/it]403\n",
            "403\n",
            " 74% 404/548 [11:49<03:56,  1.64s/it]404\n",
            "404\n",
            " 74% 405/548 [11:50<04:01,  1.69s/it]405\n",
            "405\n",
            " 74% 406/548 [11:52<04:01,  1.70s/it]406\n",
            "406\n",
            " 74% 407/548 [11:54<04:00,  1.71s/it]407\n",
            "407\n",
            " 74% 408/548 [11:55<03:55,  1.68s/it]408\n",
            "408\n",
            " 75% 409/548 [11:57<03:53,  1.68s/it]409\n",
            "409\n",
            " 75% 410/548 [11:59<03:50,  1.67s/it]410\n",
            "410\n",
            " 75% 411/548 [12:00<03:46,  1.65s/it]411\n",
            "411\n",
            " 75% 412/548 [12:02<03:40,  1.62s/it]412\n",
            "412\n",
            " 75% 413/548 [12:04<03:56,  1.75s/it]413\n",
            "413\n",
            " 76% 414/548 [12:06<03:57,  1.77s/it]414\n",
            "414\n",
            " 76% 415/548 [12:07<03:48,  1.72s/it]415\n",
            "415\n",
            " 76% 416/548 [12:09<03:42,  1.69s/it]416\n",
            "416\n",
            " 76% 417/548 [12:11<03:38,  1.67s/it]417\n",
            "417\n",
            " 76% 418/548 [12:12<03:38,  1.68s/it]418\n",
            "418\n",
            " 76% 419/548 [12:14<03:33,  1.65s/it]419\n",
            "419\n",
            " 77% 420/548 [12:15<03:30,  1.64s/it]420\n",
            "420\n",
            " 77% 421/548 [12:17<03:34,  1.69s/it]421\n",
            "421\n",
            " 77% 422/548 [12:19<03:33,  1.70s/it]422\n",
            "422\n",
            " 77% 423/548 [12:21<03:33,  1.71s/it]423\n",
            "423\n",
            " 77% 424/548 [12:23<03:36,  1.74s/it]424\n",
            "424\n",
            " 78% 425/548 [12:24<03:30,  1.71s/it]425\n",
            "425\n",
            " 78% 426/548 [12:26<03:31,  1.74s/it]426\n",
            "426\n",
            " 78% 427/548 [12:28<03:28,  1.73s/it]427\n",
            "427\n",
            " 78% 428/548 [12:29<03:29,  1.75s/it]428\n",
            "428\n",
            " 78% 429/548 [12:31<03:31,  1.78s/it]429\n",
            "429\n",
            " 78% 430/548 [12:33<03:33,  1.81s/it]430\n",
            "430\n",
            " 79% 431/548 [12:35<03:28,  1.78s/it]431\n",
            "431\n",
            " 79% 432/548 [12:37<03:29,  1.80s/it]432\n",
            "432\n",
            " 79% 433/548 [12:39<03:29,  1.82s/it]433\n",
            "433\n",
            " 79% 434/548 [12:40<03:20,  1.76s/it]434\n",
            "434\n",
            " 79% 435/548 [12:42<03:07,  1.66s/it]435\n",
            "435\n",
            " 80% 436/548 [12:43<03:09,  1.69s/it]436\n",
            "436\n",
            " 80% 437/548 [12:45<03:09,  1.71s/it]437\n",
            "437\n",
            " 80% 438/548 [12:47<03:03,  1.67s/it]438\n",
            "438\n",
            " 80% 439/548 [12:48<02:59,  1.65s/it]439\n",
            "439\n",
            " 80% 440/548 [12:50<03:03,  1.70s/it]440\n",
            "440\n",
            " 80% 441/548 [12:52<02:57,  1.66s/it]441\n",
            "441\n",
            " 81% 442/548 [12:53<02:49,  1.60s/it]442\n",
            "442\n",
            " 81% 443/548 [12:55<02:53,  1.65s/it]443\n",
            "443\n",
            " 81% 444/548 [12:57<02:52,  1.66s/it]444\n",
            "444\n",
            " 81% 445/548 [12:59<02:57,  1.73s/it]445\n",
            "445\n",
            " 81% 446/548 [13:00<02:56,  1.73s/it]446\n",
            "446\n",
            " 82% 447/548 [13:03<03:30,  2.08s/it]447\n",
            "447\n",
            " 82% 448/548 [13:05<03:21,  2.01s/it]448\n",
            "448\n",
            " 82% 449/548 [13:07<03:14,  1.97s/it]449\n",
            "449\n",
            " 82% 450/548 [13:09<03:10,  1.94s/it]450\n",
            "450\n",
            " 82% 451/548 [13:11<03:02,  1.89s/it]451\n",
            "451\n",
            " 82% 452/548 [13:12<02:55,  1.83s/it]452\n",
            "452\n",
            " 83% 453/548 [13:14<02:53,  1.82s/it]453\n",
            "453\n",
            " 83% 454/548 [13:16<02:49,  1.80s/it]454\n",
            "454\n",
            " 83% 455/548 [13:18<02:47,  1.80s/it]455\n",
            "455\n",
            " 83% 456/548 [13:19<02:45,  1.80s/it]456\n",
            "456\n",
            " 83% 457/548 [13:21<02:42,  1.79s/it]457\n",
            "457\n",
            " 84% 458/548 [13:23<02:41,  1.79s/it]458\n",
            "458\n",
            " 84% 459/548 [13:24<02:30,  1.69s/it]459\n",
            "459\n",
            " 84% 460/548 [13:26<02:25,  1.65s/it]460\n",
            "460\n",
            " 84% 461/548 [13:28<02:21,  1.62s/it]461\n",
            "461\n",
            " 84% 462/548 [13:29<02:19,  1.62s/it]462\n",
            "462\n",
            " 84% 463/548 [13:31<02:11,  1.55s/it]463\n",
            "463\n",
            " 85% 464/548 [13:32<02:13,  1.59s/it]464\n",
            "464\n",
            " 85% 465/548 [13:34<02:16,  1.64s/it]465\n",
            "465\n",
            " 85% 466/548 [13:36<02:15,  1.66s/it]466\n",
            "466\n",
            " 85% 467/548 [13:37<02:14,  1.66s/it]467\n",
            "467\n",
            " 85% 468/548 [13:39<02:17,  1.72s/it]468\n",
            "468\n",
            " 86% 469/548 [13:41<02:19,  1.77s/it]469\n",
            "469\n",
            " 86% 470/548 [13:43<02:13,  1.71s/it]470\n",
            "470\n",
            " 86% 471/548 [13:44<02:12,  1.73s/it]471\n",
            "471\n",
            " 86% 472/548 [13:46<02:07,  1.68s/it]472\n",
            "472\n",
            " 86% 473/548 [13:47<01:59,  1.59s/it]473\n",
            "473\n",
            " 86% 474/548 [13:49<01:57,  1.59s/it]474\n",
            "474\n",
            " 87% 475/548 [13:51<02:16,  1.87s/it]475\n",
            "475\n",
            " 87% 476/548 [13:53<02:08,  1.78s/it]476\n",
            "476\n",
            " 87% 477/548 [13:55<02:04,  1.75s/it]477\n",
            "477\n",
            " 87% 478/548 [13:56<02:01,  1.74s/it]478\n",
            "478\n",
            " 87% 479/548 [13:58<01:59,  1.73s/it]479\n",
            "479\n",
            " 88% 480/548 [14:00<01:57,  1.73s/it]480\n",
            "480\n",
            " 88% 481/548 [14:01<01:53,  1.69s/it]481\n",
            "481\n",
            " 88% 482/548 [14:03<01:53,  1.72s/it]482\n",
            "482\n",
            " 88% 483/548 [14:05<01:58,  1.82s/it]483\n",
            "483\n",
            " 88% 484/548 [14:07<01:52,  1.76s/it]484\n",
            "484\n",
            " 89% 485/548 [14:09<01:48,  1.72s/it]485\n",
            "485\n",
            " 89% 486/548 [14:10<01:43,  1.66s/it]486\n",
            "486\n",
            " 89% 487/548 [14:12<01:44,  1.71s/it]487\n",
            "487\n",
            " 89% 488/548 [14:13<01:36,  1.61s/it]488\n",
            "488\n",
            " 89% 489/548 [14:15<01:37,  1.66s/it]489\n",
            "489\n",
            " 89% 490/548 [14:17<01:36,  1.66s/it]490\n",
            "490\n",
            " 90% 491/548 [14:19<01:37,  1.72s/it]491\n",
            "491\n",
            " 90% 492/548 [14:20<01:35,  1.71s/it]492\n",
            "492\n",
            " 90% 493/548 [14:23<01:52,  2.04s/it]493\n",
            "493\n",
            " 90% 494/548 [14:25<01:43,  1.92s/it]494\n",
            "494\n",
            " 90% 495/548 [14:26<01:34,  1.79s/it]495\n",
            "495\n",
            " 91% 496/548 [14:28<01:32,  1.78s/it]496\n",
            "496\n",
            " 91% 497/548 [14:30<01:29,  1.76s/it]497\n",
            "497\n",
            " 91% 498/548 [14:32<01:29,  1.79s/it]498\n",
            "498\n",
            " 91% 499/548 [14:33<01:27,  1.78s/it]499\n",
            "499\n",
            " 91% 500/548 [14:35<01:23,  1.73s/it]500\n",
            "500\n",
            " 91% 501/548 [14:37<01:19,  1.70s/it]501\n",
            "501\n",
            " 92% 502/548 [14:38<01:16,  1.66s/it]502\n",
            "502\n",
            " 92% 503/548 [14:40<01:15,  1.67s/it]503\n",
            "503\n",
            " 92% 504/548 [14:41<01:10,  1.60s/it]504\n",
            "504\n",
            " 92% 505/548 [14:43<01:10,  1.63s/it]505\n",
            "505\n",
            " 92% 506/548 [14:44<01:06,  1.59s/it]506\n",
            "506\n",
            " 93% 507/548 [14:46<01:05,  1.60s/it]507\n",
            "507\n",
            " 93% 508/548 [14:48<01:03,  1.60s/it]508\n",
            "508\n",
            " 93% 509/548 [14:49<01:02,  1.59s/it]509\n",
            "509\n",
            " 93% 510/548 [14:51<01:00,  1.60s/it]510\n",
            "510\n",
            " 93% 511/548 [14:53<01:00,  1.64s/it]511\n",
            "511\n",
            " 93% 512/548 [14:54<01:01,  1.70s/it]512\n",
            "512\n",
            " 94% 513/548 [14:56<00:58,  1.68s/it]513\n",
            "513\n",
            " 94% 514/548 [14:58<00:58,  1.72s/it]514\n",
            "514\n",
            " 94% 515/548 [15:00<00:56,  1.72s/it]515\n",
            "515\n",
            " 94% 516/548 [15:02<01:05,  2.06s/it]516\n",
            "516\n",
            " 94% 517/548 [15:04<01:00,  1.97s/it]517\n",
            "517\n",
            " 95% 518/548 [15:07<01:03,  2.11s/it]518\n",
            "518\n",
            " 95% 519/548 [15:08<00:56,  1.94s/it]519\n",
            "519\n",
            " 95% 520/548 [15:10<00:51,  1.83s/it]520\n",
            "520\n",
            " 95% 521/548 [15:12<00:55,  2.07s/it]521\n",
            "521\n",
            " 95% 522/548 [15:14<00:49,  1.92s/it]522\n",
            "522\n",
            " 95% 523/548 [15:16<00:46,  1.86s/it]523\n",
            "523\n",
            " 96% 524/548 [15:17<00:42,  1.76s/it]524\n",
            "524\n",
            " 96% 525/548 [15:19<00:40,  1.77s/it]525\n",
            "525\n",
            " 96% 526/548 [15:21<00:39,  1.79s/it]526\n",
            "526\n",
            " 96% 527/548 [15:23<00:38,  1.81s/it]527\n",
            "527\n",
            " 96% 528/548 [15:24<00:35,  1.80s/it]528\n",
            "528\n",
            " 97% 529/548 [15:26<00:34,  1.82s/it]529\n",
            "529\n",
            " 97% 530/548 [15:28<00:32,  1.83s/it]530\n",
            "530\n",
            " 97% 531/548 [15:30<00:31,  1.88s/it]531\n",
            "531\n",
            " 97% 532/548 [15:32<00:29,  1.83s/it]532\n",
            "532\n",
            " 97% 533/548 [15:34<00:27,  1.86s/it]533\n",
            "533\n",
            " 97% 534/548 [15:36<00:25,  1.84s/it]534\n",
            "534\n",
            " 98% 535/548 [15:37<00:23,  1.82s/it]535\n",
            "535\n",
            " 98% 536/548 [15:39<00:22,  1.87s/it]536\n",
            "536\n",
            " 98% 537/548 [15:41<00:20,  1.83s/it]537\n",
            "537\n",
            " 98% 538/548 [15:43<00:17,  1.77s/it]538\n",
            "538\n",
            " 98% 539/548 [15:44<00:15,  1.71s/it]539\n",
            "539\n",
            " 99% 540/548 [15:46<00:13,  1.72s/it]540\n",
            "540\n",
            " 99% 541/548 [15:48<00:12,  1.72s/it]541\n",
            "541\n",
            " 99% 542/548 [15:49<00:10,  1.71s/it]542\n",
            "542\n",
            " 99% 543/548 [15:51<00:08,  1.70s/it]543\n",
            "543\n",
            " 99% 544/548 [15:53<00:06,  1.70s/it]544\n",
            "544\n",
            " 99% 545/548 [15:54<00:05,  1.68s/it]545\n",
            "545\n",
            "100% 546/548 [15:56<00:03,  1.59s/it]546\n",
            "546\n",
            "100% 547/548 [15:58<00:01,  1.63s/it]547\n",
            "547\n",
            "100% 548/548 [15:59<00:00,  1.75s/it]\n",
            "data/visdrone/VisDrone2019-DET-val\n",
            ".visdrone_det_txt\n",
            "data/visdrone/VisDrone2019-DET-val/annotations\n",
            "data/visdrone/VisDrone2019-DET-val/images\n",
            "\n",
            "evaluating object category 1/10...\n",
            "evaluating object category 2/10...\n",
            "evaluating object category 3/10...\n",
            "evaluating object category 4/10...\n",
            "evaluating object category 5/10...\n",
            "evaluating object category 6/10...\n",
            "evaluating object category 7/10...\n",
            "evaluating object category 8/10...\n",
            "evaluating object category 9/10...\n",
            "evaluating object category 10/10...\n",
            "Evaluation completed. The performance of the detector is presented as follows.\n",
            "Average Precision  (AP) @[ IoU=0.50:0.95 | maxDets=500 ] = 14.447844505310059%.\n",
            "Average Precision  (AP) @[ IoU=0.50      | maxDets=500 ] = 28.35469627380371%.\n",
            "Average Precision  (AP) @[ IoU=0.75      | maxDets=500 ] = 12.96959400177002%.\n",
            "Average Recall     (AR) @[ IoU=0.50:0.95 | maxDets=  1 ] = 0.4316035211086273%.\n",
            "Average Recall     (AR) @[ IoU=0.50:0.95 | maxDets= 10 ] = 3.800873279571533%.\n",
            "Average Recall     (AR) @[ IoU=0.50:0.95 | maxDets=100 ] = 20.897602081298828%.\n",
            "Average Recall     (AR) @[ IoU=0.50:0.95 | maxDets=500 ] = 27.309383392333984%.\n",
            "Class 0 AP = 20.277584075927734%\n",
            "Class 1 AP = 9.495450973510742%\n",
            "Class 2 AP = 0.32357025146484375%\n",
            "Class 3 AP = 51.29943084716797%\n",
            "Class 4 AP = 10.810102462768555%\n",
            "Class 5 AP = 5.666044235229492%\n",
            "Class 6 AP = 0.9891419410705566%\n",
            "Class 7 AP = 0.8778271675109863%\n",
            "Class 8 AP = 7.134809970855713%\n",
            "Class 9 AP = 9.98400592803955%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "0VExma-2dUs-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "e3N6a0rNdUr8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!python infer_visdrone.py --config-file configs/visdrone/querydet_test.yaml --num-gpu 1 --eval-only MODEL.WEIGHTS work_dirs/visdrone_querydet/model_0003999.pth OUTPUT_DIR work_dirs/model_test-2"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9i-pLRI6dUm9",
        "outputId": "4e0b5781-46e8-4e44-b3a3-41d2cf9f76f7",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "** fvcore version of PathManager will be deprecated soon. **\n",
            "** Please migrate to the version in iopath repo. **\n",
            "https://github.com/facebookresearch/iopath \n",
            "\n",
            "Command Line Args: Namespace(config_file='configs/visdrone/querydet_test.yaml', dist_url='tcp://127.0.0.1:49152', eval_only=True, machine_rank=0, no_pretrain=False, num_gpus=1, num_machines=1, opts=['MODEL.WEIGHTS', 'work_dirs/visdrone_querydet/model_0003999.pth', 'OUTPUT_DIR', 'work_dirs/model_test-2'], resume=False)\n",
            "Loading config configs/visdrone/querydet_test.yaml with yaml.unsafe_load. Your machine may be at risk if the file contains malicious content.\n",
            "Loading config configs/visdrone/../BaseRetina.yaml with yaml.unsafe_load. Your machine may be at risk if the file contains malicious content.\n",
            "\u001b[32m[04/14 11:13:29 detectron2]: \u001b[0mRank of current process: 0. World size: 1\n",
            "\u001b[32m[04/14 11:13:33 detectron2]: \u001b[0mEnvironment info:\n",
            "----------------------  ---------------------------------------------------------------\n",
            "sys.platform            linux\n",
            "Python                  3.7.6 (default, Jan  8 2020, 19:59:22) [GCC 7.3.0]\n",
            "numpy                   1.21.6\n",
            "detectron2              0.4 @/usr/local/lib/python3.7/site-packages/detectron2\n",
            "Compiler                GCC 7.3\n",
            "CUDA compiler           CUDA 10.1\n",
            "detectron2 arch flags   3.7, 5.0, 5.2, 6.0, 6.1, 7.0, 7.5\n",
            "DETECTRON2_ENV_MODULE   <not set>\n",
            "PyTorch                 1.8.1+cu101 @/usr/local/lib/python3.7/site-packages/torch\n",
            "PyTorch debug build     False\n",
            "GPU available           True\n",
            "GPU 0                   Tesla T4 (arch=7.5)\n",
            "CUDA_HOME               /usr/local/cuda\n",
            "Pillow                  9.5.0\n",
            "torchvision             0.9.1+cu101 @/usr/local/lib/python3.7/site-packages/torchvision\n",
            "torchvision arch flags  3.5, 5.0, 6.0, 7.0, 7.5\n",
            "fvcore                  0.1.3.post20210317\n",
            "cv2                     4.7.0\n",
            "----------------------  ---------------------------------------------------------------\n",
            "PyTorch built with:\n",
            "  - GCC 7.3\n",
            "  - C++ Version: 201402\n",
            "  - Intel(R) Math Kernel Library Version 2020.0.0 Product Build 20191122 for Intel(R) 64 architecture applications\n",
            "  - Intel(R) MKL-DNN v1.7.0 (Git Hash 7aed236906b1f7a05c0917e5257a1af05e9ff683)\n",
            "  - OpenMP 201511 (a.k.a. OpenMP 4.5)\n",
            "  - NNPACK is enabled\n",
            "  - CPU capability usage: AVX2\n",
            "  - CUDA Runtime 10.1\n",
            "  - NVCC architecture flags: -gencode;arch=compute_37,code=sm_37;-gencode;arch=compute_50,code=sm_50;-gencode;arch=compute_60,code=sm_60;-gencode;arch=compute_70,code=sm_70\n",
            "  - CuDNN 7.6.3\n",
            "  - Magma 2.5.2\n",
            "  - Build settings: BLAS_INFO=mkl, BUILD_TYPE=Release, CUDA_VERSION=10.1, CUDNN_VERSION=7.6.3, CXX_COMPILER=/opt/rh/devtoolset-7/root/usr/bin/c++, CXX_FLAGS= -Wno-deprecated -fvisibility-inlines-hidden -DUSE_PTHREADPOOL -fopenmp -DNDEBUG -DUSE_KINETO -DUSE_FBGEMM -DUSE_QNNPACK -DUSE_PYTORCH_QNNPACK -DUSE_XNNPACK -O2 -fPIC -Wno-narrowing -Wall -Wextra -Werror=return-type -Wno-missing-field-initializers -Wno-type-limits -Wno-array-bounds -Wno-unknown-pragmas -Wno-sign-compare -Wno-unused-parameter -Wno-unused-variable -Wno-unused-function -Wno-unused-result -Wno-unused-local-typedefs -Wno-strict-overflow -Wno-strict-aliasing -Wno-error=deprecated-declarations -Wno-stringop-overflow -Wno-psabi -Wno-error=pedantic -Wno-error=redundant-decls -Wno-error=old-style-cast -fdiagnostics-color=always -faligned-new -Wno-unused-but-set-variable -Wno-maybe-uninitialized -fno-math-errno -fno-trapping-math -Werror=format -Wno-stringop-overflow, LAPACK_INFO=mkl, PERF_WITH_AVX=1, PERF_WITH_AVX2=1, PERF_WITH_AVX512=1, TORCH_VERSION=1.8.1, USE_CUDA=ON, USE_CUDNN=ON, USE_EXCEPTION_PTR=1, USE_GFLAGS=OFF, USE_GLOG=OFF, USE_MKL=ON, USE_MKLDNN=ON, USE_MPI=OFF, USE_NCCL=ON, USE_NNPACK=ON, USE_OPENMP=ON, \n",
            "\n",
            "\u001b[32m[04/14 11:13:33 detectron2]: \u001b[0mCommand line arguments: Namespace(config_file='configs/visdrone/querydet_test.yaml', dist_url='tcp://127.0.0.1:49152', eval_only=True, machine_rank=0, no_pretrain=False, num_gpus=1, num_machines=1, opts=['MODEL.WEIGHTS', 'work_dirs/visdrone_querydet/model_0003999.pth', 'OUTPUT_DIR', 'work_dirs/model_test-2'], resume=False)\n",
            "\u001b[32m[04/14 11:13:33 detectron2]: \u001b[0mContents of args.config_file=configs/visdrone/querydet_test.yaml:\n",
            "_BASE_: \"../BaseRetina.yaml\"\n",
            "OUTPUT_DIR: \"work_dirs/model_test\"\n",
            "\n",
            "MODEL:\n",
            "  META_ARCHITECTURE: \"RetinaNetQueryDet\"\n",
            "  WEIGHTS: \"detectron2://ImageNetPretrained/MSRA/R-50.pkl\"\n",
            "  \n",
            "  RESNETS:\n",
            "    DEPTH: 50\n",
            "  \n",
            "  ANCHOR_GENERATOR:\n",
            "    NAME: \"AnchorGeneratorWithCenter\"\n",
            "    SIZES: !!python/object/apply:eval [\"[[x, x * 2**(1.0/3), x * 2**(2.0/3)] for x in [16, 32, 64, 128, 256, 512]]\"]\n",
            "  \n",
            "  RETINANET:\n",
            "    IOU_THRESHOLDS: [0.4, 0.5]\n",
            "    IOU_LABELS: [0, -1, 1]\n",
            "    NUM_CLASSES: 10\n",
            "    IN_FEATURES: [\"p2\", \"p3\", \"p4\", \"p5\", \"p6\", \"p7\"]  \n",
            "    SCORE_THRESH_TEST: 0.0001\n",
            "    \n",
            "  RESNETS:\n",
            "    OUT_FEATURES: [\"res2\", \"res3\", \"res4\", \"res5\"]\n",
            "\n",
            "  FPN:\n",
            "    IN_FEATURES: [\"res2\", \"res3\", \"res4\", \"res5\"]\n",
            "  \n",
            "  QUERY:\n",
            "    FEATURES_WHOLE_TEST: [2, 3, 4, 5]\n",
            "    FEATURES_VALUE_TEST: [0, 1]\n",
            "    Q_FEATURE_TRAIN: [1, 2]\n",
            "    Q_FEATURE_TEST: [1, 2]\n",
            "    \n",
            "    ENCODE_CENTER_DIS_COEFF: [1., 1.]\n",
            "    ENCODE_SMALL_OBJ_SCALE: [[0, 32], [0, 64]]\n",
            "\n",
            "    THRESHOLD: 0.12\n",
            "    QUERY_INFER: False\n",
            "  \n",
            "  CUSTOM: \n",
            "    USE_SOFT_NMS: False\n",
            "    SOFT_NMS_METHOD: 'gaussian'\n",
            "    SOFT_NMS_SIGMA: 0.6\n",
            "    SOFT_NMS_THRESHOLD: 0.4\n",
            "    SOFT_NMS_PRUND: 0.0001\n",
            "\n",
            "VISDRONE:\n",
            "  TEST_LENGTH: 3999\n",
            "\n",
            "TEST:  \n",
            "  DETECTIONS_PER_IMAGE: 500\n",
            "\n",
            "META_INFO:\n",
            "  EVAL_GPU_TIME: True\n",
            "\n",
            "\u001b[32m[04/14 11:13:33 detectron2]: \u001b[0mRunning with full config:\n",
            "CUDNN_BENCHMARK: False\n",
            "DATALOADER:\n",
            "  ASPECT_RATIO_GROUPING: True\n",
            "  FILTER_EMPTY_ANNOTATIONS: True\n",
            "  NUM_WORKERS: 4\n",
            "  REPEAT_THRESHOLD: 0.0\n",
            "  SAMPLER_TRAIN: TrainingSampler\n",
            "DATASETS:\n",
            "  PRECOMPUTED_PROPOSAL_TOPK_TEST: 1000\n",
            "  PRECOMPUTED_PROPOSAL_TOPK_TRAIN: 2000\n",
            "  PROPOSAL_FILES_TEST: ()\n",
            "  PROPOSAL_FILES_TRAIN: ()\n",
            "  TEST: ('coco_2017_val',)\n",
            "  TRAIN: ('coco_2017_train',)\n",
            "GLOBAL:\n",
            "  HACK: 1.0\n",
            "INPUT:\n",
            "  CROP:\n",
            "    ENABLED: False\n",
            "    SIZE: [0.9, 0.9]\n",
            "    TYPE: relative_range\n",
            "  FORMAT: BGR\n",
            "  MASK_FORMAT: polygon\n",
            "  MAX_SIZE_TEST: 1333\n",
            "  MAX_SIZE_TRAIN: 1333\n",
            "  MIN_SIZE_TEST: 800\n",
            "  MIN_SIZE_TRAIN: (640, 672, 704, 736, 768, 800)\n",
            "  MIN_SIZE_TRAIN_SAMPLING: choice\n",
            "  RANDOM_FLIP: horizontal\n",
            "META_INFO:\n",
            "  EVAL_AP: True\n",
            "  EVAL_GPU_TIME: True\n",
            "  VIS_ROOT: \n",
            "MODEL:\n",
            "  ANCHOR_GENERATOR:\n",
            "    ANGLES: [[-90, 0, 90]]\n",
            "    ASPECT_RATIOS: [[0.5, 1.0, 2.0]]\n",
            "    NAME: AnchorGeneratorWithCenter\n",
            "    OFFSET: 0.0\n",
            "    SIZES: [[16, 20.15873679831797, 25.39841683149119], [32, 40.31747359663594, 50.79683366298238], [64, 80.63494719327188, 101.59366732596476], [128, 161.26989438654377, 203.18733465192952], [256, 322.53978877308754, 406.37466930385904], [512, 645.0795775461751, 812.7493386077181]]\n",
            "  BACKBONE:\n",
            "    FREEZE_AT: 2\n",
            "    NAME: build_retinanet_resnet_fpn_backbone\n",
            "  CUSTOM:\n",
            "    CLEAR_CUDA_CACHE: False\n",
            "    CLS_WEIGHTS: []\n",
            "    FOCAL_LOSS_ALPHAS: []\n",
            "    FOCAL_LOSS_GAMMAS: []\n",
            "    GIOU_LOSS: False\n",
            "    GRADIENT_CHECKPOINT: False\n",
            "    HEAD_BN: False\n",
            "    REG_WEIGHTS: []\n",
            "    SOFT_NMS_METHOD: gaussian\n",
            "    SOFT_NMS_PRUND: 0.0001\n",
            "    SOFT_NMS_SIGMA: 0.6\n",
            "    SOFT_NMS_THRESHOLD: 0.4\n",
            "    USE_LOOP_MATCHER: False\n",
            "    USE_SOFT_NMS: False\n",
            "  DEVICE: cuda\n",
            "  FPN:\n",
            "    FUSE_TYPE: sum\n",
            "    IN_FEATURES: ['res2', 'res3', 'res4', 'res5']\n",
            "    NORM: \n",
            "    OUT_CHANNELS: 256\n",
            "    TOP_LEVELS: 2\n",
            "  KEYPOINT_ON: False\n",
            "  LOAD_PROPOSALS: False\n",
            "  MASK_ON: False\n",
            "  META_ARCHITECTURE: RetinaNetQueryDet\n",
            "  PANOPTIC_FPN:\n",
            "    COMBINE:\n",
            "      ENABLED: True\n",
            "      INSTANCES_CONFIDENCE_THRESH: 0.5\n",
            "      OVERLAP_THRESH: 0.5\n",
            "      STUFF_AREA_LIMIT: 4096\n",
            "    INSTANCE_LOSS_WEIGHT: 1.0\n",
            "  PIXEL_MEAN: [103.53, 116.28, 123.675]\n",
            "  PIXEL_STD: [1.0, 1.0, 1.0]\n",
            "  PROPOSAL_GENERATOR:\n",
            "    MIN_SIZE: 0\n",
            "    NAME: RPN\n",
            "  QUERY:\n",
            "    CONTEXT: 2\n",
            "    ENCODE_CENTER_DIS_COEFF: [1.0, 1.0]\n",
            "    ENCODE_SMALL_OBJ_SCALE: [[0, 32], [0, 64]]\n",
            "    FEATURES_VALUE_TEST: [0, 1]\n",
            "    FEATURES_VALUE_TRAIN: [0, 1]\n",
            "    FEATURES_WHOLE_TEST: [2, 3, 4, 5]\n",
            "    FEATURES_WHOLE_TRAIN: [2, 3, 4, 5]\n",
            "    QUERY_INFER: False\n",
            "    QUERY_LOSS_GAMMA: []\n",
            "    QUERY_LOSS_WEIGHT: []\n",
            "    Q_FEATURE_TEST: [1, 2]\n",
            "    Q_FEATURE_TRAIN: [1, 2]\n",
            "    THRESHOLD: 0.12\n",
            "  RESNETS:\n",
            "    DEFORM_MODULATED: False\n",
            "    DEFORM_NUM_GROUPS: 1\n",
            "    DEFORM_ON_PER_STAGE: [False, False, False, False]\n",
            "    DEPTH: 50\n",
            "    NORM: FrozenBN\n",
            "    NUM_GROUPS: 1\n",
            "    OUT_FEATURES: ['res2', 'res3', 'res4', 'res5']\n",
            "    RES2_OUT_CHANNELS: 256\n",
            "    RES5_DILATION: 1\n",
            "    STEM_OUT_CHANNELS: 64\n",
            "    STRIDE_IN_1X1: True\n",
            "    WIDTH_PER_GROUP: 64\n",
            "  RETINANET:\n",
            "    BBOX_REG_LOSS_TYPE: smooth_l1\n",
            "    BBOX_REG_WEIGHTS: (1.0, 1.0, 1.0, 1.0)\n",
            "    FOCAL_LOSS_ALPHA: 0.25\n",
            "    FOCAL_LOSS_GAMMA: 2.0\n",
            "    IN_FEATURES: ['p2', 'p3', 'p4', 'p5', 'p6', 'p7']\n",
            "    IOU_LABELS: [0, -1, 1]\n",
            "    IOU_THRESHOLDS: [0.4, 0.5]\n",
            "    NMS_THRESH_TEST: 0.5\n",
            "    NORM: \n",
            "    NUM_CLASSES: 10\n",
            "    NUM_CONVS: 4\n",
            "    PRIOR_PROB: 0.01\n",
            "    SCORE_THRESH_TEST: 0.0001\n",
            "    SMOOTH_L1_LOSS_BETA: 0.1\n",
            "    TOPK_CANDIDATES_TEST: 1000\n",
            "  ROI_BOX_CASCADE_HEAD:\n",
            "    BBOX_REG_WEIGHTS: ((10.0, 10.0, 5.0, 5.0), (20.0, 20.0, 10.0, 10.0), (30.0, 30.0, 15.0, 15.0))\n",
            "    IOUS: (0.5, 0.6, 0.7)\n",
            "  ROI_BOX_HEAD:\n",
            "    BBOX_REG_LOSS_TYPE: smooth_l1\n",
            "    BBOX_REG_LOSS_WEIGHT: 1.0\n",
            "    BBOX_REG_WEIGHTS: (10.0, 10.0, 5.0, 5.0)\n",
            "    CLS_AGNOSTIC_BBOX_REG: False\n",
            "    CONV_DIM: 256\n",
            "    FC_DIM: 1024\n",
            "    NAME: \n",
            "    NORM: \n",
            "    NUM_CONV: 0\n",
            "    NUM_FC: 0\n",
            "    POOLER_RESOLUTION: 14\n",
            "    POOLER_SAMPLING_RATIO: 0\n",
            "    POOLER_TYPE: ROIAlignV2\n",
            "    SMOOTH_L1_BETA: 0.0\n",
            "    TRAIN_ON_PRED_BOXES: False\n",
            "  ROI_HEADS:\n",
            "    BATCH_SIZE_PER_IMAGE: 512\n",
            "    IN_FEATURES: ['res4']\n",
            "    IOU_LABELS: [0, 1]\n",
            "    IOU_THRESHOLDS: [0.5]\n",
            "    NAME: Res5ROIHeads\n",
            "    NMS_THRESH_TEST: 0.5\n",
            "    NUM_CLASSES: 80\n",
            "    POSITIVE_FRACTION: 0.25\n",
            "    PROPOSAL_APPEND_GT: True\n",
            "    SCORE_THRESH_TEST: 0.05\n",
            "  ROI_KEYPOINT_HEAD:\n",
            "    CONV_DIMS: (512, 512, 512, 512, 512, 512, 512, 512)\n",
            "    LOSS_WEIGHT: 1.0\n",
            "    MIN_KEYPOINTS_PER_IMAGE: 1\n",
            "    NAME: KRCNNConvDeconvUpsampleHead\n",
            "    NORMALIZE_LOSS_BY_VISIBLE_KEYPOINTS: True\n",
            "    NUM_KEYPOINTS: 17\n",
            "    POOLER_RESOLUTION: 14\n",
            "    POOLER_SAMPLING_RATIO: 0\n",
            "    POOLER_TYPE: ROIAlignV2\n",
            "  ROI_MASK_HEAD:\n",
            "    CLS_AGNOSTIC_MASK: False\n",
            "    CONV_DIM: 256\n",
            "    NAME: MaskRCNNConvUpsampleHead\n",
            "    NORM: \n",
            "    NUM_CONV: 0\n",
            "    POOLER_RESOLUTION: 14\n",
            "    POOLER_SAMPLING_RATIO: 0\n",
            "    POOLER_TYPE: ROIAlignV2\n",
            "  RPN:\n",
            "    BATCH_SIZE_PER_IMAGE: 256\n",
            "    BBOX_REG_LOSS_TYPE: smooth_l1\n",
            "    BBOX_REG_LOSS_WEIGHT: 1.0\n",
            "    BBOX_REG_WEIGHTS: (1.0, 1.0, 1.0, 1.0)\n",
            "    BOUNDARY_THRESH: -1\n",
            "    HEAD_NAME: StandardRPNHead\n",
            "    IN_FEATURES: ['res4']\n",
            "    IOU_LABELS: [0, -1, 1]\n",
            "    IOU_THRESHOLDS: [0.3, 0.7]\n",
            "    LOSS_WEIGHT: 1.0\n",
            "    NMS_THRESH: 0.7\n",
            "    POSITIVE_FRACTION: 0.5\n",
            "    POST_NMS_TOPK_TEST: 1000\n",
            "    POST_NMS_TOPK_TRAIN: 2000\n",
            "    PRE_NMS_TOPK_TEST: 6000\n",
            "    PRE_NMS_TOPK_TRAIN: 12000\n",
            "    SMOOTH_L1_BETA: 0.0\n",
            "  SEM_SEG_HEAD:\n",
            "    COMMON_STRIDE: 4\n",
            "    CONVS_DIM: 128\n",
            "    IGNORE_VALUE: 255\n",
            "    IN_FEATURES: ['p2', 'p3', 'p4', 'p5']\n",
            "    LOSS_WEIGHT: 1.0\n",
            "    NAME: SemSegFPNHead\n",
            "    NORM: GN\n",
            "    NUM_CLASSES: 54\n",
            "  WEIGHTS: work_dirs/visdrone_querydet/model_0003999.pth\n",
            "OUTPUT_DIR: work_dirs/model_test-2\n",
            "SEED: -1\n",
            "SOLVER:\n",
            "  AMP:\n",
            "    ENABLED: False\n",
            "  BASE_LR: 0.01\n",
            "  BIAS_LR_FACTOR: 1.0\n",
            "  CHECKPOINT_PERIOD: 5000\n",
            "  CLIP_GRADIENTS:\n",
            "    CLIP_TYPE: value\n",
            "    CLIP_VALUE: 1.0\n",
            "    ENABLED: False\n",
            "    NORM_TYPE: 2.0\n",
            "  GAMMA: 0.1\n",
            "  IMS_PER_BATCH: 16\n",
            "  LR_SCHEDULER_NAME: WarmupMultiStepLR\n",
            "  MAX_ITER: 90000\n",
            "  MOMENTUM: 0.9\n",
            "  NESTEROV: False\n",
            "  REFERENCE_WORLD_SIZE: 0\n",
            "  STEPS: (60000, 80000)\n",
            "  WARMUP_FACTOR: 0.001\n",
            "  WARMUP_ITERS: 1000\n",
            "  WARMUP_METHOD: linear\n",
            "  WEIGHT_DECAY: 0.0001\n",
            "  WEIGHT_DECAY_BIAS: 0.0001\n",
            "  WEIGHT_DECAY_NORM: 0.0\n",
            "TEST:\n",
            "  AUG:\n",
            "    ENABLED: False\n",
            "    FLIP: True\n",
            "    MAX_SIZE: 4000\n",
            "    MIN_SIZES: (400, 500, 600, 700, 800, 900, 1000, 1100, 1200)\n",
            "  DETECTIONS_PER_IMAGE: 500\n",
            "  EVAL_PERIOD: 0\n",
            "  EXPECTED_RESULTS: []\n",
            "  KEYPOINT_OKS_SIGMAS: []\n",
            "  PRECISE_BN:\n",
            "    ENABLED: False\n",
            "    NUM_ITER: 200\n",
            "VERSION: 2\n",
            "VISDRONE:\n",
            "  MAX_LENGTH: 1999\n",
            "  SHORT_LENGTH: [1200]\n",
            "  TEST_IMG_ROOT: data/visdrone/coco_format/val_images\n",
            "  TEST_JSON: data/visdrone/coco_format/annotations/val_label.json\n",
            "  TEST_LENGTH: 3999\n",
            "  TRAIN_JSON: data/visdrone/coco_format/annotations/train_label.json\n",
            "  TRING_IMG_ROOT: data//visdrone/coco_format/train_images\n",
            "VIS_PERIOD: 0\n",
            "\u001b[32m[04/14 11:13:36 detectron2]: \u001b[0mFull config saved to work_dirs/model_test-2/config.yaml\n",
            "\u001b[32m[04/14 11:13:36 d2.utils.env]: \u001b[0mUsing a generated random seed 37090969\n",
            "\u001b[32m[04/14 11:13:40 d2.engine.defaults]: \u001b[0mModel:\n",
            "RetinaNetQueryDet(\n",
            "  (backbone): FPN(\n",
            "    (fpn_lateral2): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1))\n",
            "    (fpn_output2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (fpn_lateral3): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1))\n",
            "    (fpn_output3): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (fpn_lateral4): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1))\n",
            "    (fpn_output4): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (fpn_lateral5): Conv2d(2048, 256, kernel_size=(1, 1), stride=(1, 1))\n",
            "    (fpn_output5): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (top_block): LastLevelP6P7(\n",
            "      (p6): Conv2d(2048, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
            "      (p7): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
            "    )\n",
            "    (bottom_up): ResNet(\n",
            "      (stem): BasicStem(\n",
            "        (conv1): Conv2d(\n",
            "          3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)\n",
            "        )\n",
            "      )\n",
            "      (res2): Sequential(\n",
            "        (0): BottleneckBlock(\n",
            "          (shortcut): Conv2d(\n",
            "            64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
            "          )\n",
            "          (conv1): Conv2d(\n",
            "            64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)\n",
            "          )\n",
            "          (conv2): Conv2d(\n",
            "            64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)\n",
            "          )\n",
            "          (conv3): Conv2d(\n",
            "            64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
            "          )\n",
            "        )\n",
            "        (1): BottleneckBlock(\n",
            "          (conv1): Conv2d(\n",
            "            256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)\n",
            "          )\n",
            "          (conv2): Conv2d(\n",
            "            64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)\n",
            "          )\n",
            "          (conv3): Conv2d(\n",
            "            64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
            "          )\n",
            "        )\n",
            "        (2): BottleneckBlock(\n",
            "          (conv1): Conv2d(\n",
            "            256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)\n",
            "          )\n",
            "          (conv2): Conv2d(\n",
            "            64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)\n",
            "          )\n",
            "          (conv3): Conv2d(\n",
            "            64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
            "          )\n",
            "        )\n",
            "      )\n",
            "      (res3): Sequential(\n",
            "        (0): BottleneckBlock(\n",
            "          (shortcut): Conv2d(\n",
            "            256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
            "          )\n",
            "          (conv1): Conv2d(\n",
            "            256, 128, kernel_size=(1, 1), stride=(2, 2), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)\n",
            "          )\n",
            "          (conv2): Conv2d(\n",
            "            128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)\n",
            "          )\n",
            "          (conv3): Conv2d(\n",
            "            128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
            "          )\n",
            "        )\n",
            "        (1): BottleneckBlock(\n",
            "          (conv1): Conv2d(\n",
            "            512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)\n",
            "          )\n",
            "          (conv2): Conv2d(\n",
            "            128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)\n",
            "          )\n",
            "          (conv3): Conv2d(\n",
            "            128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
            "          )\n",
            "        )\n",
            "        (2): BottleneckBlock(\n",
            "          (conv1): Conv2d(\n",
            "            512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)\n",
            "          )\n",
            "          (conv2): Conv2d(\n",
            "            128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)\n",
            "          )\n",
            "          (conv3): Conv2d(\n",
            "            128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
            "          )\n",
            "        )\n",
            "        (3): BottleneckBlock(\n",
            "          (conv1): Conv2d(\n",
            "            512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)\n",
            "          )\n",
            "          (conv2): Conv2d(\n",
            "            128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)\n",
            "          )\n",
            "          (conv3): Conv2d(\n",
            "            128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
            "          )\n",
            "        )\n",
            "      )\n",
            "      (res4): Sequential(\n",
            "        (0): BottleneckBlock(\n",
            "          (shortcut): Conv2d(\n",
            "            512, 1024, kernel_size=(1, 1), stride=(2, 2), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n",
            "          )\n",
            "          (conv1): Conv2d(\n",
            "            512, 256, kernel_size=(1, 1), stride=(2, 2), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
            "          )\n",
            "          (conv2): Conv2d(\n",
            "            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
            "          )\n",
            "          (conv3): Conv2d(\n",
            "            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n",
            "          )\n",
            "        )\n",
            "        (1): BottleneckBlock(\n",
            "          (conv1): Conv2d(\n",
            "            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
            "          )\n",
            "          (conv2): Conv2d(\n",
            "            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
            "          )\n",
            "          (conv3): Conv2d(\n",
            "            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n",
            "          )\n",
            "        )\n",
            "        (2): BottleneckBlock(\n",
            "          (conv1): Conv2d(\n",
            "            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
            "          )\n",
            "          (conv2): Conv2d(\n",
            "            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
            "          )\n",
            "          (conv3): Conv2d(\n",
            "            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n",
            "          )\n",
            "        )\n",
            "        (3): BottleneckBlock(\n",
            "          (conv1): Conv2d(\n",
            "            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
            "          )\n",
            "          (conv2): Conv2d(\n",
            "            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
            "          )\n",
            "          (conv3): Conv2d(\n",
            "            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n",
            "          )\n",
            "        )\n",
            "        (4): BottleneckBlock(\n",
            "          (conv1): Conv2d(\n",
            "            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
            "          )\n",
            "          (conv2): Conv2d(\n",
            "            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
            "          )\n",
            "          (conv3): Conv2d(\n",
            "            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n",
            "          )\n",
            "        )\n",
            "        (5): BottleneckBlock(\n",
            "          (conv1): Conv2d(\n",
            "            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
            "          )\n",
            "          (conv2): Conv2d(\n",
            "            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
            "          )\n",
            "          (conv3): Conv2d(\n",
            "            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n",
            "          )\n",
            "        )\n",
            "      )\n",
            "      (res5): Sequential(\n",
            "        (0): BottleneckBlock(\n",
            "          (shortcut): Conv2d(\n",
            "            1024, 2048, kernel_size=(1, 1), stride=(2, 2), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=2048, eps=1e-05)\n",
            "          )\n",
            "          (conv1): Conv2d(\n",
            "            1024, 512, kernel_size=(1, 1), stride=(2, 2), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
            "          )\n",
            "          (conv2): Conv2d(\n",
            "            512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
            "          )\n",
            "          (conv3): Conv2d(\n",
            "            512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=2048, eps=1e-05)\n",
            "          )\n",
            "        )\n",
            "        (1): BottleneckBlock(\n",
            "          (conv1): Conv2d(\n",
            "            2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
            "          )\n",
            "          (conv2): Conv2d(\n",
            "            512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
            "          )\n",
            "          (conv3): Conv2d(\n",
            "            512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=2048, eps=1e-05)\n",
            "          )\n",
            "        )\n",
            "        (2): BottleneckBlock(\n",
            "          (conv1): Conv2d(\n",
            "            2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
            "          )\n",
            "          (conv2): Conv2d(\n",
            "            512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
            "          )\n",
            "          (conv3): Conv2d(\n",
            "            512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=2048, eps=1e-05)\n",
            "          )\n",
            "        )\n",
            "      )\n",
            "    )\n",
            "  )\n",
            "  (det_head): RetinaNetHead_3x3(\n",
            "    (cls_layer_0): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (bbox_layer_0): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (cls_layer_1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (bbox_layer_1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (cls_layer_2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (bbox_layer_2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (cls_layer_3): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (bbox_layer_3): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (cls_score): Conv2d(256, 90, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (bbox_pred): Conv2d(256, 36, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "  )\n",
            "  (query_head): Head_3x3(\n",
            "    (layer_0): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (layer_1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (layer_2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (layer_3): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (pred_net): Conv2d(256, 1, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "  )\n",
            "  (anchor_generator): AnchorGeneratorWithCenter(\n",
            "    (cell_anchors): BufferList()\n",
            "  )\n",
            "  (query_anchor_generator): AnchorGeneratorWithCenter(\n",
            "    (cell_anchors): BufferList()\n",
            "  )\n",
            ")\n",
            "\u001b[32m[04/14 11:13:41 fvcore.common.checkpoint]: \u001b[0mLoading checkpoint from work_dirs/visdrone_querydet/model_0003999.pth\n",
            "\u001b[32m[04/14 11:13:49 d2.data.common]: \u001b[0mSerializing 548 elements to byte tensors and concatenating them all ...\n",
            "\u001b[32m[04/14 11:13:49 d2.data.common]: \u001b[0mSerialized dataset takes 0.08 MiB\n",
            "/usr/local/lib/python3.7/site-packages/torch/utils/data/dataloader.py:477: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  cpuset_checked))\n",
            "\u001b[32m[04/14 11:13:49 d2.evaluation.evaluator]: \u001b[0mStart inference on 548 images\n",
            "\u001b[32m[04/14 11:14:16 d2.evaluation.evaluator]: \u001b[0mInference done 11/548. 2.1323 s / img. ETA=0:19:08\n",
            "\u001b[32m[04/14 11:14:22 d2.evaluation.evaluator]: \u001b[0mInference done 14/548. 2.1396 s / img. ETA=0:19:07\n",
            "\u001b[32m[04/14 11:14:28 d2.evaluation.evaluator]: \u001b[0mInference done 17/548. 2.1416 s / img. ETA=0:19:02\n",
            "\u001b[32m[04/14 11:14:35 d2.evaluation.evaluator]: \u001b[0mInference done 20/548. 2.1461 s / img. ETA=0:18:57\n",
            "\u001b[32m[04/14 11:14:41 d2.evaluation.evaluator]: \u001b[0mInference done 22/548. 2.1519 s / img. ETA=0:19:56\n",
            "\u001b[32m[04/14 11:14:48 d2.evaluation.evaluator]: \u001b[0mInference done 25/548. 2.1583 s / img. ETA=0:19:45\n",
            "\u001b[32m[04/14 11:14:55 d2.evaluation.evaluator]: \u001b[0mInference done 28/548. 2.1621 s / img. ETA=0:19:34\n",
            "\u001b[32m[04/14 11:15:01 d2.evaluation.evaluator]: \u001b[0mInference done 31/548. 2.1659 s / img. ETA=0:19:24\n",
            "\u001b[32m[04/14 11:15:08 d2.evaluation.evaluator]: \u001b[0mInference done 34/548. 2.1697 s / img. ETA=0:19:15\n",
            "\u001b[32m[04/14 11:15:15 d2.evaluation.evaluator]: \u001b[0mInference done 37/548. 2.1729 s / img. ETA=0:19:07\n",
            "\u001b[32m[04/14 11:15:21 d2.evaluation.evaluator]: \u001b[0mInference done 40/548. 2.1775 s / img. ETA=0:19:00\n",
            "\u001b[32m[04/14 11:15:28 d2.evaluation.evaluator]: \u001b[0mInference done 43/548. 2.1814 s / img. ETA=0:18:53\n",
            "\u001b[32m[04/14 11:15:35 d2.evaluation.evaluator]: \u001b[0mInference done 46/548. 2.1859 s / img. ETA=0:18:46\n",
            "\u001b[32m[04/14 11:15:41 d2.evaluation.evaluator]: \u001b[0mInference done 49/548. 2.1895 s / img. ETA=0:18:40\n",
            "\u001b[32m[04/14 11:15:48 d2.evaluation.evaluator]: \u001b[0mInference done 52/548. 2.1934 s / img. ETA=0:18:34\n",
            "\u001b[32m[04/14 11:15:55 d2.evaluation.evaluator]: \u001b[0mInference done 55/548. 2.1967 s / img. ETA=0:18:27\n",
            "\u001b[32m[04/14 11:16:02 d2.evaluation.evaluator]: \u001b[0mInference done 58/548. 2.2012 s / img. ETA=0:18:21\n",
            "\u001b[32m[04/14 11:16:09 d2.evaluation.evaluator]: \u001b[0mInference done 61/548. 2.2070 s / img. ETA=0:18:17\n",
            "\u001b[32m[04/14 11:16:16 d2.evaluation.evaluator]: \u001b[0mInference done 64/548. 2.2118 s / img. ETA=0:18:11\n",
            "\u001b[32m[04/14 11:16:23 d2.evaluation.evaluator]: \u001b[0mInference done 67/548. 2.2168 s / img. ETA=0:18:06\n",
            "\u001b[32m[04/14 11:16:30 d2.evaluation.evaluator]: \u001b[0mInference done 70/548. 2.2230 s / img. ETA=0:18:02\n",
            "\u001b[32m[04/14 11:16:37 d2.evaluation.evaluator]: \u001b[0mInference done 73/548. 2.2285 s / img. ETA=0:17:57\n",
            "\u001b[32m[04/14 11:16:44 d2.evaluation.evaluator]: \u001b[0mInference done 75/548. 2.2322 s / img. ETA=0:18:07\n",
            "\u001b[32m[04/14 11:16:51 d2.evaluation.evaluator]: \u001b[0mInference done 78/548. 2.2372 s / img. ETA=0:18:02\n",
            "\u001b[32m[04/14 11:16:58 d2.evaluation.evaluator]: \u001b[0mInference done 81/548. 2.2413 s / img. ETA=0:17:56\n",
            "\u001b[32m[04/14 11:17:05 d2.evaluation.evaluator]: \u001b[0mInference done 84/548. 2.2447 s / img. ETA=0:17:49\n",
            "\u001b[32m[04/14 11:17:12 d2.evaluation.evaluator]: \u001b[0mInference done 87/548. 2.2471 s / img. ETA=0:17:43\n",
            "\u001b[32m[04/14 11:17:19 d2.evaluation.evaluator]: \u001b[0mInference done 90/548. 2.2492 s / img. ETA=0:17:36\n",
            "\u001b[32m[04/14 11:17:26 d2.evaluation.evaluator]: \u001b[0mInference done 93/548. 2.2518 s / img. ETA=0:17:29\n",
            "\u001b[32m[04/14 11:17:33 d2.evaluation.evaluator]: \u001b[0mInference done 96/548. 2.2539 s / img. ETA=0:17:23\n",
            "\u001b[32m[04/14 11:17:40 d2.evaluation.evaluator]: \u001b[0mInference done 99/548. 2.2562 s / img. ETA=0:17:16\n",
            "\u001b[32m[04/14 11:17:47 d2.evaluation.evaluator]: \u001b[0mInference done 102/548. 2.2583 s / img. ETA=0:17:10\n",
            "\u001b[32m[04/14 11:17:54 d2.evaluation.evaluator]: \u001b[0mInference done 105/548. 2.2604 s / img. ETA=0:17:03\n",
            "\u001b[32m[04/14 11:18:01 d2.evaluation.evaluator]: \u001b[0mInference done 108/548. 2.2626 s / img. ETA=0:16:57\n",
            "\u001b[32m[04/14 11:18:08 d2.evaluation.evaluator]: \u001b[0mInference done 111/548. 2.2643 s / img. ETA=0:16:50\n",
            "\u001b[32m[04/14 11:18:15 d2.evaluation.evaluator]: \u001b[0mInference done 114/548. 2.2666 s / img. ETA=0:16:44\n",
            "\u001b[32m[04/14 11:18:22 d2.evaluation.evaluator]: \u001b[0mInference done 117/548. 2.2680 s / img. ETA=0:16:37\n",
            "\u001b[32m[04/14 11:18:29 d2.evaluation.evaluator]: \u001b[0mInference done 120/548. 2.2696 s / img. ETA=0:16:31\n",
            "\u001b[32m[04/14 11:18:36 d2.evaluation.evaluator]: \u001b[0mInference done 123/548. 2.2711 s / img. ETA=0:16:24\n",
            "\u001b[32m[04/14 11:18:43 d2.evaluation.evaluator]: \u001b[0mInference done 126/548. 2.2728 s / img. ETA=0:16:17\n",
            "\u001b[32m[04/14 11:18:50 d2.evaluation.evaluator]: \u001b[0mInference done 129/548. 2.2743 s / img. ETA=0:16:11\n",
            "\u001b[32m[04/14 11:18:57 d2.evaluation.evaluator]: \u001b[0mInference done 132/548. 2.2753 s / img. ETA=0:16:04\n",
            "\u001b[32m[04/14 11:19:04 d2.evaluation.evaluator]: \u001b[0mInference done 135/548. 2.2764 s / img. ETA=0:15:57\n",
            "\u001b[32m[04/14 11:19:11 d2.evaluation.evaluator]: \u001b[0mInference done 138/548. 2.2775 s / img. ETA=0:15:50\n",
            "\u001b[32m[04/14 11:19:18 d2.evaluation.evaluator]: \u001b[0mInference done 141/548. 2.2784 s / img. ETA=0:15:43\n",
            "\u001b[32m[04/14 11:19:25 d2.evaluation.evaluator]: \u001b[0mInference done 144/548. 2.2798 s / img. ETA=0:15:37\n",
            "\u001b[32m[04/14 11:19:32 d2.evaluation.evaluator]: \u001b[0mInference done 147/548. 2.2807 s / img. ETA=0:15:30\n",
            "\u001b[32m[04/14 11:19:39 d2.evaluation.evaluator]: \u001b[0mInference done 150/548. 2.2817 s / img. ETA=0:15:23\n",
            "\u001b[32m[04/14 11:19:44 d2.evaluation.evaluator]: \u001b[0mInference done 152/548. 2.2824 s / img. ETA=0:15:20\n",
            "\u001b[32m[04/14 11:19:51 d2.evaluation.evaluator]: \u001b[0mInference done 155/548. 2.2833 s / img. ETA=0:15:13\n",
            "\u001b[32m[04/14 11:19:58 d2.evaluation.evaluator]: \u001b[0mInference done 158/548. 2.2840 s / img. ETA=0:15:06\n",
            "\u001b[32m[04/14 11:20:05 d2.evaluation.evaluator]: \u001b[0mInference done 161/548. 2.2848 s / img. ETA=0:14:59\n",
            "\u001b[32m[04/14 11:20:12 d2.evaluation.evaluator]: \u001b[0mInference done 164/548. 2.2859 s / img. ETA=0:14:53\n",
            "\u001b[32m[04/14 11:20:19 d2.evaluation.evaluator]: \u001b[0mInference done 167/548. 2.2867 s / img. ETA=0:14:46\n",
            "\u001b[32m[04/14 11:20:27 d2.evaluation.evaluator]: \u001b[0mInference done 170/548. 2.2877 s / img. ETA=0:14:39\n",
            "\u001b[32m[04/14 11:20:34 d2.evaluation.evaluator]: \u001b[0mInference done 173/548. 2.2887 s / img. ETA=0:14:32\n",
            "\u001b[32m[04/14 11:20:41 d2.evaluation.evaluator]: \u001b[0mInference done 176/548. 2.2893 s / img. ETA=0:14:25\n",
            "\u001b[32m[04/14 11:20:46 d2.evaluation.evaluator]: \u001b[0mInference done 178/548. 2.2899 s / img. ETA=0:14:22\n",
            "\u001b[32m[04/14 11:20:53 d2.evaluation.evaluator]: \u001b[0mInference done 181/548. 2.2907 s / img. ETA=0:14:15\n",
            "\u001b[32m[04/14 11:21:00 d2.evaluation.evaluator]: \u001b[0mInference done 184/548. 2.2912 s / img. ETA=0:14:08\n",
            "\u001b[32m[04/14 11:21:07 d2.evaluation.evaluator]: \u001b[0mInference done 187/548. 2.2919 s / img. ETA=0:14:01\n",
            "\u001b[32m[04/14 11:21:14 d2.evaluation.evaluator]: \u001b[0mInference done 190/548. 2.2926 s / img. ETA=0:13:54\n",
            "\u001b[32m[04/14 11:21:21 d2.evaluation.evaluator]: \u001b[0mInference done 193/548. 2.2930 s / img. ETA=0:13:47\n",
            "\u001b[32m[04/14 11:21:28 d2.evaluation.evaluator]: \u001b[0mInference done 196/548. 2.2934 s / img. ETA=0:13:40\n",
            "\u001b[32m[04/14 11:21:35 d2.evaluation.evaluator]: \u001b[0mInference done 199/548. 2.2939 s / img. ETA=0:13:33\n",
            "\u001b[32m[04/14 11:21:42 d2.evaluation.evaluator]: \u001b[0mInference done 202/548. 2.2942 s / img. ETA=0:13:26\n",
            "\u001b[32m[04/14 11:21:49 d2.evaluation.evaluator]: \u001b[0mInference done 205/548. 2.2946 s / img. ETA=0:13:19\n",
            "\u001b[32m[04/14 11:21:56 d2.evaluation.evaluator]: \u001b[0mInference done 208/548. 2.2950 s / img. ETA=0:13:12\n",
            "\u001b[32m[04/14 11:22:03 d2.evaluation.evaluator]: \u001b[0mInference done 211/548. 2.2954 s / img. ETA=0:13:05\n",
            "\u001b[32m[04/14 11:22:10 d2.evaluation.evaluator]: \u001b[0mInference done 214/548. 2.2958 s / img. ETA=0:12:58\n",
            "\u001b[32m[04/14 11:22:17 d2.evaluation.evaluator]: \u001b[0mInference done 217/548. 2.2963 s / img. ETA=0:12:51\n",
            "\u001b[32m[04/14 11:22:24 d2.evaluation.evaluator]: \u001b[0mInference done 220/548. 2.2967 s / img. ETA=0:12:44\n",
            "\u001b[32m[04/14 11:22:31 d2.evaluation.evaluator]: \u001b[0mInference done 223/548. 2.2971 s / img. ETA=0:12:37\n",
            "\u001b[32m[04/14 11:22:38 d2.evaluation.evaluator]: \u001b[0mInference done 226/548. 2.2974 s / img. ETA=0:12:30\n",
            "\u001b[32m[04/14 11:22:45 d2.evaluation.evaluator]: \u001b[0mInference done 229/548. 2.2978 s / img. ETA=0:12:23\n",
            "\u001b[32m[04/14 11:22:52 d2.evaluation.evaluator]: \u001b[0mInference done 232/548. 2.2982 s / img. ETA=0:12:16\n",
            "\u001b[32m[04/14 11:22:59 d2.evaluation.evaluator]: \u001b[0mInference done 235/548. 2.2988 s / img. ETA=0:12:09\n",
            "\u001b[32m[04/14 11:23:06 d2.evaluation.evaluator]: \u001b[0mInference done 238/548. 2.2991 s / img. ETA=0:12:03\n",
            "\u001b[32m[04/14 11:23:13 d2.evaluation.evaluator]: \u001b[0mInference done 241/548. 2.2997 s / img. ETA=0:11:56\n",
            "\u001b[32m[04/14 11:23:20 d2.evaluation.evaluator]: \u001b[0mInference done 244/548. 2.3000 s / img. ETA=0:11:49\n",
            "\u001b[32m[04/14 11:23:27 d2.evaluation.evaluator]: \u001b[0mInference done 247/548. 2.3006 s / img. ETA=0:11:42\n",
            "\u001b[32m[04/14 11:23:34 d2.evaluation.evaluator]: \u001b[0mInference done 250/548. 2.3010 s / img. ETA=0:11:35\n",
            "\u001b[32m[04/14 11:23:41 d2.evaluation.evaluator]: \u001b[0mInference done 253/548. 2.3013 s / img. ETA=0:11:28\n",
            "\u001b[32m[04/14 11:23:47 d2.evaluation.evaluator]: \u001b[0mInference done 255/548. 2.3015 s / img. ETA=0:11:25\n",
            "\u001b[32m[04/14 11:23:55 d2.evaluation.evaluator]: \u001b[0mInference done 258/548. 2.3019 s / img. ETA=0:11:18\n",
            "\u001b[32m[04/14 11:24:02 d2.evaluation.evaluator]: \u001b[0mInference done 261/548. 2.3022 s / img. ETA=0:11:11\n",
            "\u001b[32m[04/14 11:24:09 d2.evaluation.evaluator]: \u001b[0mInference done 264/548. 2.3025 s / img. ETA=0:11:04\n",
            "\u001b[32m[04/14 11:24:16 d2.evaluation.evaluator]: \u001b[0mInference done 267/548. 2.3029 s / img. ETA=0:10:57\n",
            "\u001b[32m[04/14 11:24:23 d2.evaluation.evaluator]: \u001b[0mInference done 270/548. 2.3033 s / img. ETA=0:10:50\n",
            "\u001b[32m[04/14 11:24:30 d2.evaluation.evaluator]: \u001b[0mInference done 273/548. 2.3035 s / img. ETA=0:10:43\n",
            "\u001b[32m[04/14 11:24:37 d2.evaluation.evaluator]: \u001b[0mInference done 276/548. 2.3037 s / img. ETA=0:10:36\n",
            "\u001b[32m[04/14 11:24:44 d2.evaluation.evaluator]: \u001b[0mInference done 279/548. 2.3041 s / img. ETA=0:10:29\n",
            "\u001b[32m[04/14 11:24:51 d2.evaluation.evaluator]: \u001b[0mInference done 282/548. 2.3044 s / img. ETA=0:10:22\n",
            "\u001b[32m[04/14 11:24:58 d2.evaluation.evaluator]: \u001b[0mInference done 285/548. 2.3046 s / img. ETA=0:10:15\n",
            "\u001b[32m[04/14 11:25:05 d2.evaluation.evaluator]: \u001b[0mInference done 288/548. 2.3048 s / img. ETA=0:10:08\n",
            "\u001b[32m[04/14 11:25:12 d2.evaluation.evaluator]: \u001b[0mInference done 291/548. 2.3051 s / img. ETA=0:10:01\n",
            "\u001b[32m[04/14 11:25:19 d2.evaluation.evaluator]: \u001b[0mInference done 294/548. 2.3053 s / img. ETA=0:09:54\n",
            "\u001b[32m[04/14 11:25:26 d2.evaluation.evaluator]: \u001b[0mInference done 297/548. 2.3055 s / img. ETA=0:09:47\n",
            "\u001b[32m[04/14 11:25:33 d2.evaluation.evaluator]: \u001b[0mInference done 300/548. 2.3057 s / img. ETA=0:09:40\n",
            "\u001b[32m[04/14 11:25:40 d2.evaluation.evaluator]: \u001b[0mInference done 303/548. 2.3060 s / img. ETA=0:09:33\n",
            "\u001b[32m[04/14 11:25:47 d2.evaluation.evaluator]: \u001b[0mInference done 306/548. 2.3061 s / img. ETA=0:09:26\n",
            "\u001b[32m[04/14 11:25:54 d2.evaluation.evaluator]: \u001b[0mInference done 309/548. 2.3063 s / img. ETA=0:09:19\n",
            "\u001b[32m[04/14 11:26:01 d2.evaluation.evaluator]: \u001b[0mInference done 312/548. 2.3065 s / img. ETA=0:09:12\n",
            "\u001b[32m[04/14 11:26:08 d2.evaluation.evaluator]: \u001b[0mInference done 315/548. 2.3067 s / img. ETA=0:09:05\n",
            "\u001b[32m[04/14 11:26:15 d2.evaluation.evaluator]: \u001b[0mInference done 318/548. 2.3068 s / img. ETA=0:08:57\n",
            "\u001b[32m[04/14 11:26:22 d2.evaluation.evaluator]: \u001b[0mInference done 321/548. 2.3070 s / img. ETA=0:08:50\n",
            "\u001b[32m[04/14 11:26:29 d2.evaluation.evaluator]: \u001b[0mInference done 324/548. 2.3071 s / img. ETA=0:08:43\n",
            "\u001b[32m[04/14 11:26:36 d2.evaluation.evaluator]: \u001b[0mInference done 327/548. 2.3075 s / img. ETA=0:08:36\n",
            "\u001b[32m[04/14 11:26:43 d2.evaluation.evaluator]: \u001b[0mInference done 330/548. 2.3077 s / img. ETA=0:08:29\n",
            "\u001b[32m[04/14 11:26:48 d2.evaluation.evaluator]: \u001b[0mInference done 331/548. 2.3079 s / img. ETA=0:08:29\n",
            "\u001b[32m[04/14 11:26:55 d2.evaluation.evaluator]: \u001b[0mInference done 334/548. 2.3080 s / img. ETA=0:08:22\n",
            "\u001b[32m[04/14 11:27:02 d2.evaluation.evaluator]: \u001b[0mInference done 337/548. 2.3082 s / img. ETA=0:08:15\n",
            "\u001b[32m[04/14 11:27:09 d2.evaluation.evaluator]: \u001b[0mInference done 340/548. 2.3085 s / img. ETA=0:08:08\n",
            "\u001b[32m[04/14 11:27:16 d2.evaluation.evaluator]: \u001b[0mInference done 343/548. 2.3088 s / img. ETA=0:08:01\n",
            "\u001b[32m[04/14 11:27:23 d2.evaluation.evaluator]: \u001b[0mInference done 346/548. 2.3090 s / img. ETA=0:07:54\n",
            "\u001b[32m[04/14 11:27:30 d2.evaluation.evaluator]: \u001b[0mInference done 349/548. 2.3091 s / img. ETA=0:07:47\n",
            "\u001b[32m[04/14 11:27:37 d2.evaluation.evaluator]: \u001b[0mInference done 352/548. 2.3094 s / img. ETA=0:07:40\n",
            "\u001b[32m[04/14 11:27:44 d2.evaluation.evaluator]: \u001b[0mInference done 355/548. 2.3095 s / img. ETA=0:07:33\n",
            "\u001b[32m[04/14 11:27:49 d2.evaluation.evaluator]: \u001b[0mInference done 356/548. 2.3095 s / img. ETA=0:07:32\n",
            "\u001b[32m[04/14 11:27:56 d2.evaluation.evaluator]: \u001b[0mInference done 359/548. 2.3096 s / img. ETA=0:07:25\n",
            "\u001b[32m[04/14 11:28:03 d2.evaluation.evaluator]: \u001b[0mInference done 362/548. 2.3098 s / img. ETA=0:07:18\n",
            "\u001b[32m[04/14 11:28:10 d2.evaluation.evaluator]: \u001b[0mInference done 365/548. 2.3100 s / img. ETA=0:07:10\n",
            "\u001b[32m[04/14 11:28:18 d2.evaluation.evaluator]: \u001b[0mInference done 368/548. 2.3102 s / img. ETA=0:07:03\n",
            "\u001b[32m[04/14 11:28:25 d2.evaluation.evaluator]: \u001b[0mInference done 371/548. 2.3106 s / img. ETA=0:06:56\n",
            "\u001b[32m[04/14 11:28:32 d2.evaluation.evaluator]: \u001b[0mInference done 374/548. 2.3109 s / img. ETA=0:06:49\n",
            "\u001b[32m[04/14 11:28:39 d2.evaluation.evaluator]: \u001b[0mInference done 377/548. 2.3111 s / img. ETA=0:06:42\n",
            "\u001b[32m[04/14 11:28:46 d2.evaluation.evaluator]: \u001b[0mInference done 380/548. 2.3113 s / img. ETA=0:06:35\n",
            "\u001b[32m[04/14 11:28:51 d2.evaluation.evaluator]: \u001b[0mInference done 381/548. 2.3113 s / img. ETA=0:06:34\n",
            "\u001b[32m[04/14 11:28:58 d2.evaluation.evaluator]: \u001b[0mInference done 384/548. 2.3115 s / img. ETA=0:06:27\n",
            "\u001b[32m[04/14 11:29:05 d2.evaluation.evaluator]: \u001b[0mInference done 387/548. 2.3117 s / img. ETA=0:06:20\n",
            "\u001b[32m[04/14 11:29:12 d2.evaluation.evaluator]: \u001b[0mInference done 390/548. 2.3120 s / img. ETA=0:06:13\n",
            "\u001b[32m[04/14 11:29:19 d2.evaluation.evaluator]: \u001b[0mInference done 393/548. 2.3121 s / img. ETA=0:06:06\n",
            "\u001b[32m[04/14 11:29:26 d2.evaluation.evaluator]: \u001b[0mInference done 396/548. 2.3123 s / img. ETA=0:05:58\n",
            "\u001b[32m[04/14 11:29:33 d2.evaluation.evaluator]: \u001b[0mInference done 399/548. 2.3125 s / img. ETA=0:05:51\n",
            "\u001b[32m[04/14 11:29:40 d2.evaluation.evaluator]: \u001b[0mInference done 402/548. 2.3127 s / img. ETA=0:05:44\n",
            "\u001b[32m[04/14 11:29:47 d2.evaluation.evaluator]: \u001b[0mInference done 405/548. 2.3129 s / img. ETA=0:05:37\n",
            "\u001b[32m[04/14 11:29:53 d2.evaluation.evaluator]: \u001b[0mInference done 407/548. 2.3129 s / img. ETA=0:05:33\n",
            "\u001b[32m[04/14 11:30:00 d2.evaluation.evaluator]: \u001b[0mInference done 410/548. 2.3130 s / img. ETA=0:05:26\n",
            "\u001b[32m[04/14 11:30:07 d2.evaluation.evaluator]: \u001b[0mInference done 413/548. 2.3131 s / img. ETA=0:05:18\n",
            "\u001b[32m[04/14 11:30:14 d2.evaluation.evaluator]: \u001b[0mInference done 416/548. 2.3133 s / img. ETA=0:05:11\n",
            "\u001b[32m[04/14 11:30:21 d2.evaluation.evaluator]: \u001b[0mInference done 419/548. 2.3134 s / img. ETA=0:05:04\n",
            "\u001b[32m[04/14 11:30:28 d2.evaluation.evaluator]: \u001b[0mInference done 422/548. 2.3136 s / img. ETA=0:04:57\n",
            "\u001b[32m[04/14 11:30:35 d2.evaluation.evaluator]: \u001b[0mInference done 425/548. 2.3138 s / img. ETA=0:04:50\n",
            "\u001b[32m[04/14 11:30:42 d2.evaluation.evaluator]: \u001b[0mInference done 428/548. 2.3138 s / img. ETA=0:04:43\n",
            "\u001b[32m[04/14 11:30:49 d2.evaluation.evaluator]: \u001b[0mInference done 431/548. 2.3139 s / img. ETA=0:04:36\n",
            "\u001b[32m[04/14 11:30:54 d2.evaluation.evaluator]: \u001b[0mInference done 433/548. 2.3140 s / img. ETA=0:04:31\n",
            "\u001b[32m[04/14 11:31:01 d2.evaluation.evaluator]: \u001b[0mInference done 436/548. 2.3141 s / img. ETA=0:04:24\n",
            "\u001b[32m[04/14 11:31:08 d2.evaluation.evaluator]: \u001b[0mInference done 439/548. 2.3144 s / img. ETA=0:04:17\n",
            "\u001b[32m[04/14 11:31:15 d2.evaluation.evaluator]: \u001b[0mInference done 442/548. 2.3147 s / img. ETA=0:04:10\n",
            "\u001b[32m[04/14 11:31:22 d2.evaluation.evaluator]: \u001b[0mInference done 445/548. 2.3147 s / img. ETA=0:04:03\n",
            "\u001b[32m[04/14 11:31:29 d2.evaluation.evaluator]: \u001b[0mInference done 448/548. 2.3148 s / img. ETA=0:03:56\n",
            "\u001b[32m[04/14 11:31:36 d2.evaluation.evaluator]: \u001b[0mInference done 451/548. 2.3150 s / img. ETA=0:03:49\n",
            "\u001b[32m[04/14 11:31:43 d2.evaluation.evaluator]: \u001b[0mInference done 454/548. 2.3150 s / img. ETA=0:03:42\n",
            "\u001b[32m[04/14 11:31:50 d2.evaluation.evaluator]: \u001b[0mInference done 457/548. 2.3151 s / img. ETA=0:03:34\n",
            "\u001b[32m[04/14 11:31:57 d2.evaluation.evaluator]: \u001b[0mInference done 460/548. 2.3153 s / img. ETA=0:03:27\n",
            "\u001b[32m[04/14 11:32:04 d2.evaluation.evaluator]: \u001b[0mInference done 463/548. 2.3153 s / img. ETA=0:03:20\n",
            "\u001b[32m[04/14 11:32:11 d2.evaluation.evaluator]: \u001b[0mInference done 466/548. 2.3154 s / img. ETA=0:03:13\n",
            "\u001b[32m[04/14 11:32:18 d2.evaluation.evaluator]: \u001b[0mInference done 469/548. 2.3154 s / img. ETA=0:03:06\n",
            "\u001b[32m[04/14 11:32:25 d2.evaluation.evaluator]: \u001b[0mInference done 472/548. 2.3155 s / img. ETA=0:02:59\n",
            "\u001b[32m[04/14 11:32:32 d2.evaluation.evaluator]: \u001b[0mInference done 475/548. 2.3156 s / img. ETA=0:02:52\n",
            "\u001b[32m[04/14 11:32:39 d2.evaluation.evaluator]: \u001b[0mInference done 478/548. 2.3157 s / img. ETA=0:02:45\n",
            "\u001b[32m[04/14 11:32:46 d2.evaluation.evaluator]: \u001b[0mInference done 481/548. 2.3157 s / img. ETA=0:02:38\n",
            "\u001b[32m[04/14 11:32:53 d2.evaluation.evaluator]: \u001b[0mInference done 484/548. 2.3158 s / img. ETA=0:02:31\n",
            "\u001b[32m[04/14 11:33:01 d2.evaluation.evaluator]: \u001b[0mInference done 487/548. 2.3158 s / img. ETA=0:02:23\n",
            "\u001b[32m[04/14 11:33:07 d2.evaluation.evaluator]: \u001b[0mInference done 490/548. 2.3158 s / img. ETA=0:02:16\n",
            "\u001b[32m[04/14 11:33:14 d2.evaluation.evaluator]: \u001b[0mInference done 493/548. 2.3159 s / img. ETA=0:02:09\n",
            "\u001b[32m[04/14 11:33:22 d2.evaluation.evaluator]: \u001b[0mInference done 496/548. 2.3160 s / img. ETA=0:02:02\n",
            "\u001b[32m[04/14 11:33:29 d2.evaluation.evaluator]: \u001b[0mInference done 499/548. 2.3160 s / img. ETA=0:01:55\n",
            "\u001b[32m[04/14 11:33:36 d2.evaluation.evaluator]: \u001b[0mInference done 502/548. 2.3162 s / img. ETA=0:01:48\n",
            "\u001b[32m[04/14 11:33:43 d2.evaluation.evaluator]: \u001b[0mInference done 505/548. 2.3162 s / img. ETA=0:01:41\n",
            "\u001b[32m[04/14 11:33:50 d2.evaluation.evaluator]: \u001b[0mInference done 508/548. 2.3164 s / img. ETA=0:01:34\n",
            "\u001b[32m[04/14 11:33:57 d2.evaluation.evaluator]: \u001b[0mInference done 511/548. 2.3166 s / img. ETA=0:01:27\n",
            "\u001b[32m[04/14 11:34:04 d2.evaluation.evaluator]: \u001b[0mInference done 514/548. 2.3167 s / img. ETA=0:01:20\n",
            "\u001b[32m[04/14 11:34:11 d2.evaluation.evaluator]: \u001b[0mInference done 517/548. 2.3169 s / img. ETA=0:01:13\n",
            "\u001b[32m[04/14 11:34:18 d2.evaluation.evaluator]: \u001b[0mInference done 520/548. 2.3169 s / img. ETA=0:01:06\n",
            "\u001b[32m[04/14 11:34:25 d2.evaluation.evaluator]: \u001b[0mInference done 523/548. 2.3171 s / img. ETA=0:00:58\n",
            "\u001b[32m[04/14 11:34:32 d2.evaluation.evaluator]: \u001b[0mInference done 526/548. 2.3171 s / img. ETA=0:00:51\n",
            "\u001b[32m[04/14 11:34:39 d2.evaluation.evaluator]: \u001b[0mInference done 529/548. 2.3172 s / img. ETA=0:00:44\n",
            "\u001b[32m[04/14 11:34:46 d2.evaluation.evaluator]: \u001b[0mInference done 532/548. 2.3173 s / img. ETA=0:00:37\n",
            "\u001b[32m[04/14 11:34:53 d2.evaluation.evaluator]: \u001b[0mInference done 535/548. 2.3174 s / img. ETA=0:00:30\n",
            "\u001b[32m[04/14 11:34:59 d2.evaluation.evaluator]: \u001b[0mInference done 537/548. 2.3175 s / img. ETA=0:00:25\n",
            "\u001b[32m[04/14 11:35:06 d2.evaluation.evaluator]: \u001b[0mInference done 540/548. 2.3176 s / img. ETA=0:00:18\n",
            "\u001b[32m[04/14 11:35:13 d2.evaluation.evaluator]: \u001b[0mInference done 543/548. 2.3177 s / img. ETA=0:00:11\n",
            "\u001b[32m[04/14 11:35:20 d2.evaluation.evaluator]: \u001b[0mInference done 546/548. 2.3177 s / img. ETA=0:00:04\n",
            "\u001b[32m[04/14 11:35:26 d2.evaluation.evaluator]: \u001b[0mTotal inference time: 0:21:23.527810 (2.363771 s / img per device, on 1 devices)\n",
            "\u001b[32m[04/14 11:35:26 d2.evaluation.evaluator]: \u001b[0mTotal inference pure compute time: 0:20:58 (2.317784 s / img per device, on 1 devices)\n",
            "\u001b[32m[04/14 11:35:33 d2.evaluation.testing]: \u001b[0mcopypaste: Task: GPU_Speed\n",
            "\u001b[32m[04/14 11:35:33 d2.evaluation.testing]: \u001b[0mcopypaste: Mean_FPS,Std_FPS,Max_FPS,Min_FPS,Mid_FPS\n",
            "\u001b[32m[04/14 11:35:33 d2.evaluation.testing]: \u001b[0mcopypaste: 0.4346,0.0092,0.4782,0.4228,0.4324\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!bash eval_visdrone.sh work_dirs/model_test-2/visdrone_infer.json"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "S8-5ZBRgdUlt",
        "outputId": "95d79cb1-daab-45fb-d89e-7515cc3a4439"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Json to txt: .visdrone_det_txt\n",
            "  0% 0/548 [00:00<?, ?it/s]0\n",
            "0\n",
            "  0% 1/548 [00:02<25:28,  2.79s/it]1\n",
            "1\n",
            "  0% 2/548 [00:05<25:49,  2.84s/it]2\n",
            "2\n",
            "  1% 3/548 [00:08<25:36,  2.82s/it]3\n",
            "3\n",
            "  1% 4/548 [00:13<31:04,  3.43s/it]4\n",
            "4\n",
            "  1% 5/548 [00:16<28:54,  3.19s/it]5\n",
            "5\n",
            "  1% 6/548 [00:19<30:22,  3.36s/it]6\n",
            "6\n",
            "  1% 7/548 [00:22<29:26,  3.27s/it]7\n",
            "7\n",
            "  1% 8/548 [00:25<27:56,  3.10s/it]8\n",
            "8\n",
            "  2% 9/548 [00:28<27:35,  3.07s/it]9\n",
            "9\n",
            "  2% 10/548 [00:31<27:29,  3.07s/it]10\n",
            "10\n",
            "  2% 11/548 [00:34<26:49,  3.00s/it]11\n",
            "11\n",
            "  2% 12/548 [00:36<25:03,  2.80s/it]12\n",
            "12\n",
            "  2% 13/548 [00:40<27:47,  3.12s/it]13\n",
            "13\n",
            "  3% 14/548 [00:44<29:03,  3.27s/it]14\n",
            "14\n",
            "  3% 15/548 [00:46<27:01,  3.04s/it]15\n",
            "15\n",
            "  3% 16/548 [00:49<25:25,  2.87s/it]16\n",
            "16\n",
            "  3% 17/548 [00:51<24:57,  2.82s/it]17\n",
            "17\n",
            "  3% 18/548 [00:54<24:11,  2.74s/it]18\n",
            "18\n",
            "  3% 19/548 [00:57<24:15,  2.75s/it]19\n",
            "19\n",
            "  4% 20/548 [01:00<24:49,  2.82s/it]20\n",
            "20\n",
            "  4% 21/548 [01:02<24:25,  2.78s/it]21\n",
            "21\n",
            "  4% 22/548 [01:05<23:12,  2.65s/it]22\n",
            "22\n",
            "  4% 23/548 [01:07<22:51,  2.61s/it]23\n",
            "23\n",
            "  4% 24/548 [01:10<22:50,  2.62s/it]24\n",
            "24\n",
            "  5% 25/548 [01:15<30:13,  3.47s/it]25\n",
            "25\n",
            "  5% 26/548 [01:20<32:41,  3.76s/it]26\n",
            "26\n",
            "  5% 27/548 [01:24<32:31,  3.74s/it]27\n",
            "27\n",
            "  5% 28/548 [01:26<29:16,  3.38s/it]28\n",
            "28\n",
            "  5% 29/548 [01:29<27:04,  3.13s/it]29\n",
            "29\n",
            "  5% 30/548 [01:32<28:21,  3.28s/it]30\n",
            "30\n",
            "  6% 31/548 [01:36<28:50,  3.35s/it]31\n",
            "31\n",
            "  6% 32/548 [01:38<26:43,  3.11s/it]32\n",
            "32\n",
            "  6% 33/548 [01:41<24:44,  2.88s/it]33\n",
            "33\n",
            "  6% 34/548 [01:43<23:52,  2.79s/it]34\n",
            "34\n",
            "  6% 35/548 [01:46<22:51,  2.67s/it]35\n",
            "35\n",
            "  7% 36/548 [01:48<22:26,  2.63s/it]36\n",
            "36\n",
            "  7% 37/548 [01:51<22:13,  2.61s/it]37\n",
            "37\n",
            "  7% 38/548 [01:53<21:25,  2.52s/it]38\n",
            "38\n",
            "  7% 39/548 [01:57<24:13,  2.86s/it]39\n",
            "39\n",
            "  7% 40/548 [02:00<25:49,  3.05s/it]40\n",
            "40\n",
            "  7% 41/548 [02:03<24:28,  2.90s/it]41\n",
            "41\n",
            "  8% 42/548 [02:06<24:28,  2.90s/it]42\n",
            "42\n",
            "  8% 43/548 [02:08<23:10,  2.75s/it]43\n",
            "43\n",
            "  8% 44/548 [02:12<27:29,  3.27s/it]44\n",
            "44\n",
            "  8% 45/548 [02:15<25:22,  3.03s/it]45\n",
            "45\n",
            "  8% 46/548 [02:18<24:22,  2.91s/it]46\n",
            "46\n",
            "  9% 47/548 [02:20<23:34,  2.82s/it]47\n",
            "47\n",
            "  9% 48/548 [02:23<22:48,  2.74s/it]48\n",
            "48\n",
            "  9% 49/548 [02:26<22:58,  2.76s/it]49\n",
            "49\n",
            "  9% 50/548 [02:28<22:42,  2.74s/it]50\n",
            "50\n",
            "  9% 51/548 [02:31<22:24,  2.70s/it]51\n",
            "51\n",
            "  9% 52/548 [02:33<22:04,  2.67s/it]52\n",
            "52\n",
            " 10% 53/548 [02:36<21:57,  2.66s/it]53\n",
            "53\n",
            " 10% 54/548 [02:39<23:02,  2.80s/it]54\n",
            "54\n",
            " 10% 55/548 [02:42<23:00,  2.80s/it]55\n",
            "55\n",
            " 10% 56/548 [02:45<23:15,  2.84s/it]56\n",
            "56\n",
            " 10% 57/548 [02:48<23:02,  2.82s/it]57\n",
            "57\n",
            " 11% 58/548 [02:51<24:46,  3.03s/it]58\n",
            "58\n",
            " 11% 59/548 [02:55<26:47,  3.29s/it]59\n",
            "59\n",
            " 11% 60/548 [02:59<27:07,  3.34s/it]60\n",
            "60\n",
            " 11% 61/548 [03:01<25:43,  3.17s/it]61\n",
            "61\n",
            " 11% 62/548 [03:04<24:40,  3.05s/it]62\n",
            "62\n",
            " 11% 63/548 [03:08<26:33,  3.29s/it]63\n",
            "63\n",
            " 12% 64/548 [03:13<31:02,  3.85s/it]64\n",
            "64\n",
            " 12% 65/548 [03:17<30:25,  3.78s/it]65\n",
            "65\n",
            " 12% 66/548 [03:20<29:21,  3.65s/it]66\n",
            "66\n",
            " 12% 67/548 [03:24<28:49,  3.60s/it]67\n",
            "67\n",
            " 12% 68/548 [03:26<26:21,  3.29s/it]68\n",
            "68\n",
            " 13% 69/548 [03:29<24:42,  3.09s/it]69\n",
            "69\n",
            " 13% 70/548 [03:32<23:52,  3.00s/it]70\n",
            "70\n",
            " 13% 71/548 [03:34<23:13,  2.92s/it]71\n",
            "71\n",
            " 13% 72/548 [03:37<22:00,  2.78s/it]72\n",
            "72\n",
            " 13% 73/548 [03:39<21:53,  2.76s/it]73\n",
            "73\n",
            " 14% 74/548 [03:42<21:39,  2.74s/it]74\n",
            "74\n",
            " 14% 75/548 [03:45<21:24,  2.72s/it]75\n",
            "75\n",
            " 14% 76/548 [03:48<21:19,  2.71s/it]76\n",
            "76\n",
            " 14% 77/548 [03:50<21:16,  2.71s/it]77\n",
            "77\n",
            " 14% 78/548 [03:53<20:54,  2.67s/it]78\n",
            "78\n",
            " 14% 79/548 [03:56<22:57,  2.94s/it]79\n",
            "79\n",
            " 15% 80/548 [03:59<22:41,  2.91s/it]80\n",
            "80\n",
            " 15% 81/548 [04:02<22:02,  2.83s/it]81\n",
            "81\n",
            " 15% 82/548 [04:05<21:45,  2.80s/it]82\n",
            "82\n",
            " 15% 83/548 [04:08<22:04,  2.85s/it]83\n",
            "83\n",
            " 15% 84/548 [04:10<20:38,  2.67s/it]84\n",
            "84\n",
            " 16% 85/548 [04:15<27:18,  3.54s/it]85\n",
            "85\n",
            " 16% 86/548 [04:18<25:17,  3.29s/it]86\n",
            "86\n",
            " 16% 87/548 [04:21<23:48,  3.10s/it]87\n",
            "87\n",
            " 16% 88/548 [04:23<23:02,  3.01s/it]88\n",
            "88\n",
            " 16% 89/548 [04:26<21:52,  2.86s/it]89\n",
            "89\n",
            " 16% 90/548 [04:28<20:53,  2.74s/it]90\n",
            "90\n",
            " 17% 91/548 [04:31<20:25,  2.68s/it]91\n",
            "91\n",
            " 17% 92/548 [04:34<20:31,  2.70s/it]92\n",
            "92\n",
            " 17% 93/548 [04:38<23:08,  3.05s/it]93\n",
            "93\n",
            " 17% 94/548 [04:42<25:30,  3.37s/it]94\n",
            "94\n",
            " 17% 95/548 [04:45<25:22,  3.36s/it]95\n",
            "95\n",
            " 18% 96/548 [04:48<25:03,  3.33s/it]96\n",
            "96\n",
            " 18% 97/548 [04:52<25:14,  3.36s/it]97\n",
            "97\n",
            " 18% 98/548 [04:55<25:59,  3.47s/it]98\n",
            "98\n",
            " 18% 99/548 [04:58<24:22,  3.26s/it]99\n",
            "99\n",
            " 18% 100/548 [05:01<22:56,  3.07s/it]100\n",
            "100\n",
            " 18% 101/548 [05:04<23:43,  3.18s/it]101\n",
            "101\n",
            " 19% 102/548 [05:07<21:51,  2.94s/it]102\n",
            "102\n",
            " 19% 103/548 [05:09<20:40,  2.79s/it]103\n",
            "103\n",
            " 19% 104/548 [05:12<20:08,  2.72s/it]104\n",
            "104\n",
            " 19% 105/548 [05:14<20:09,  2.73s/it]105\n",
            "105\n",
            " 19% 106/548 [05:19<23:08,  3.14s/it]106\n",
            "106\n",
            " 20% 107/548 [05:21<22:29,  3.06s/it]107\n",
            "107\n",
            " 20% 108/548 [05:24<21:11,  2.89s/it]108\n",
            "108\n",
            " 20% 109/548 [05:26<20:10,  2.76s/it]109\n",
            "109\n",
            " 20% 110/548 [05:29<19:24,  2.66s/it]110\n",
            "110\n",
            " 20% 111/548 [05:31<18:52,  2.59s/it]111\n",
            "111\n",
            " 20% 112/548 [05:34<18:30,  2.55s/it]112\n",
            "112\n",
            " 21% 113/548 [05:36<18:25,  2.54s/it]113\n",
            "113\n",
            " 21% 114/548 [05:39<19:06,  2.64s/it]114\n",
            "114\n",
            " 21% 115/548 [05:42<18:51,  2.61s/it]115\n",
            "115\n",
            " 21% 116/548 [05:45<21:23,  2.97s/it]116\n",
            "116\n",
            " 21% 117/548 [05:49<22:32,  3.14s/it]117\n",
            "117\n",
            " 22% 118/548 [05:51<20:50,  2.91s/it]118\n",
            "118\n",
            " 22% 119/548 [05:54<20:32,  2.87s/it]119\n",
            "119\n",
            " 22% 120/548 [05:57<19:46,  2.77s/it]120\n",
            "120\n",
            " 22% 121/548 [06:00<21:02,  2.96s/it]121\n",
            "121\n",
            " 22% 122/548 [06:03<20:16,  2.85s/it]122\n",
            "122\n",
            " 22% 123/548 [06:05<19:28,  2.75s/it]123\n",
            "123\n",
            " 23% 124/548 [06:08<19:10,  2.71s/it]124\n",
            "124\n",
            " 23% 125/548 [06:11<19:25,  2.76s/it]125\n",
            "125\n",
            " 23% 126/548 [06:15<23:36,  3.36s/it]126\n",
            "126\n",
            " 23% 127/548 [06:20<26:05,  3.72s/it]127\n",
            "127\n",
            " 23% 128/548 [06:22<23:23,  3.34s/it]128\n",
            "128\n",
            " 24% 129/548 [06:26<24:09,  3.46s/it]129\n",
            "129\n",
            " 24% 130/548 [06:29<21:59,  3.16s/it]130\n",
            "130\n",
            " 24% 131/548 [06:31<20:32,  2.96s/it]131\n",
            "131\n",
            " 24% 132/548 [06:34<19:48,  2.86s/it]132\n",
            "132\n",
            " 24% 133/548 [06:37<20:45,  3.00s/it]133\n",
            "133\n",
            " 24% 134/548 [06:40<21:18,  3.09s/it]134\n",
            "134\n",
            " 25% 135/548 [06:43<20:32,  2.98s/it]135\n",
            "135\n",
            " 25% 136/548 [06:46<19:39,  2.86s/it]136\n",
            "136\n",
            " 25% 137/548 [06:49<20:57,  3.06s/it]137\n",
            "137\n",
            " 25% 138/548 [06:52<20:05,  2.94s/it]138\n",
            "138\n",
            " 25% 139/548 [06:54<19:00,  2.79s/it]139\n",
            "139\n",
            " 26% 140/548 [06:57<18:29,  2.72s/it]140\n",
            "140\n",
            " 26% 141/548 [07:01<20:25,  3.01s/it]141\n",
            "141\n",
            " 26% 142/548 [07:03<20:07,  2.97s/it]142\n",
            "142\n",
            " 26% 143/548 [07:06<19:47,  2.93s/it]143\n",
            "143\n",
            " 26% 144/548 [07:09<18:49,  2.80s/it]144\n",
            "144\n",
            " 26% 145/548 [07:11<18:16,  2.72s/it]145\n",
            "145\n",
            " 27% 146/548 [07:14<18:16,  2.73s/it]146\n",
            "146\n",
            " 27% 147/548 [07:20<24:06,  3.61s/it]147\n",
            "147\n",
            " 27% 148/548 [07:22<22:15,  3.34s/it]148\n",
            "148\n",
            " 27% 149/548 [07:26<22:32,  3.39s/it]149\n",
            "149\n",
            " 27% 150/548 [07:28<20:39,  3.11s/it]150\n",
            "150\n",
            " 28% 151/548 [07:31<19:11,  2.90s/it]151\n",
            "151\n",
            " 28% 152/548 [07:35<20:51,  3.16s/it]152\n",
            "152\n",
            " 28% 153/548 [07:37<19:44,  3.00s/it]153\n",
            "153\n",
            " 28% 154/548 [07:40<19:14,  2.93s/it]154\n",
            "154\n",
            " 28% 155/548 [07:43<19:19,  2.95s/it]155\n",
            "155\n",
            " 28% 156/548 [07:45<18:10,  2.78s/it]156\n",
            "156\n",
            " 29% 157/548 [07:49<19:39,  3.02s/it]157\n",
            "157\n",
            " 29% 158/548 [07:52<18:56,  2.91s/it]158\n",
            "158\n",
            " 29% 159/548 [07:54<17:45,  2.74s/it]159\n",
            "159\n",
            " 29% 160/548 [07:58<19:29,  3.01s/it]160\n",
            "160\n",
            " 29% 161/548 [08:00<18:25,  2.86s/it]161\n",
            "161\n",
            " 30% 162/548 [08:03<18:09,  2.82s/it]162\n",
            "162\n",
            " 30% 163/548 [08:05<17:41,  2.76s/it]163\n",
            "163\n",
            " 30% 164/548 [08:08<17:02,  2.66s/it]164\n",
            "164\n",
            " 30% 165/548 [08:10<16:40,  2.61s/it]165\n",
            "165\n",
            " 30% 166/548 [08:13<16:33,  2.60s/it]166\n",
            "166\n",
            " 30% 167/548 [08:16<16:45,  2.64s/it]167\n",
            "167\n",
            " 31% 168/548 [08:20<20:03,  3.17s/it]168\n",
            "168\n",
            " 31% 169/548 [08:23<19:17,  3.05s/it]169\n",
            "169\n",
            " 31% 170/548 [08:26<19:04,  3.03s/it]170\n",
            "170\n",
            " 31% 171/548 [08:28<17:48,  2.83s/it]171\n",
            "171\n",
            " 31% 172/548 [08:31<17:25,  2.78s/it]172\n",
            "172\n",
            " 32% 173/548 [08:33<17:04,  2.73s/it]173\n",
            "173\n",
            " 32% 174/548 [08:36<16:57,  2.72s/it]174\n",
            "174\n",
            " 32% 175/548 [08:39<16:51,  2.71s/it]175\n",
            "175\n",
            " 32% 176/548 [08:42<18:25,  2.97s/it]176\n",
            "176\n",
            " 32% 177/548 [08:45<17:17,  2.80s/it]177\n",
            "177\n",
            " 32% 178/548 [08:47<16:49,  2.73s/it]178\n",
            "178\n",
            " 33% 179/548 [08:51<18:23,  2.99s/it]179\n",
            "179\n",
            " 33% 180/548 [08:54<19:17,  3.14s/it]180\n",
            "180\n",
            " 33% 181/548 [08:57<18:13,  2.98s/it]181\n",
            "181\n",
            " 33% 182/548 [08:59<17:01,  2.79s/it]182\n",
            "182\n",
            " 33% 183/548 [09:02<16:47,  2.76s/it]183\n",
            "183\n",
            " 34% 184/548 [09:05<16:09,  2.66s/it]184\n",
            "184\n",
            " 34% 185/548 [09:07<15:58,  2.64s/it]185\n",
            "185\n",
            " 34% 186/548 [09:11<17:37,  2.92s/it]186\n",
            "186\n",
            " 34% 187/548 [09:13<17:12,  2.86s/it]187\n",
            "187\n",
            " 34% 188/548 [09:17<18:25,  3.07s/it]188\n",
            "188\n",
            " 34% 189/548 [09:23<22:48,  3.81s/it]189\n",
            "189\n",
            " 35% 190/548 [09:25<21:06,  3.54s/it]190\n",
            "190\n",
            " 35% 191/548 [09:29<21:16,  3.58s/it]191\n",
            "191\n",
            " 35% 192/548 [09:33<21:46,  3.67s/it]192\n",
            "192\n",
            " 35% 193/548 [09:36<19:47,  3.35s/it]193\n",
            "193\n",
            " 35% 194/548 [09:38<18:48,  3.19s/it]194\n",
            "194\n",
            " 36% 195/548 [09:41<17:52,  3.04s/it]195\n",
            "195\n",
            " 36% 196/548 [09:44<17:33,  2.99s/it]196\n",
            "196\n",
            " 36% 197/548 [09:47<18:20,  3.14s/it]197\n",
            "197\n",
            " 36% 198/548 [09:51<19:08,  3.28s/it]198\n",
            "198\n",
            " 36% 199/548 [09:54<18:09,  3.12s/it]199\n",
            "199\n",
            " 36% 200/548 [09:57<18:55,  3.26s/it]200\n",
            "200\n",
            " 37% 201/548 [10:01<19:51,  3.43s/it]201\n",
            "201\n",
            " 37% 202/548 [10:04<18:15,  3.17s/it]202\n",
            "202\n",
            " 37% 203/548 [10:06<16:49,  2.93s/it]203\n",
            "203\n",
            " 37% 204/548 [10:09<16:27,  2.87s/it]204\n",
            "204\n",
            " 37% 205/548 [10:11<15:51,  2.77s/it]205\n",
            "205\n",
            " 38% 206/548 [10:14<15:31,  2.72s/it]206\n",
            "206\n",
            " 38% 207/548 [10:17<15:48,  2.78s/it]207\n",
            "207\n",
            " 38% 208/548 [10:20<15:59,  2.82s/it]208\n",
            "208\n",
            " 38% 209/548 [10:24<17:42,  3.13s/it]209\n",
            "209\n",
            " 38% 210/548 [10:28<19:27,  3.45s/it]210\n",
            "210\n",
            " 39% 211/548 [10:32<19:40,  3.50s/it]211\n",
            "211\n",
            " 39% 212/548 [10:34<17:52,  3.19s/it]212\n",
            "212\n",
            " 39% 213/548 [10:37<16:42,  2.99s/it]213\n",
            "213\n",
            " 39% 214/548 [10:39<16:12,  2.91s/it]214\n",
            "214\n",
            " 39% 215/548 [10:43<17:36,  3.17s/it]215\n",
            "215\n",
            " 39% 216/548 [10:46<17:19,  3.13s/it]216\n",
            "216\n",
            " 40% 217/548 [10:49<16:13,  2.94s/it]217\n",
            "217\n",
            " 40% 218/548 [10:52<16:55,  3.08s/it]218\n",
            "218\n",
            " 40% 219/548 [10:55<16:34,  3.02s/it]219\n",
            "219\n",
            " 40% 220/548 [10:59<17:50,  3.26s/it]220\n",
            "220\n",
            " 40% 221/548 [11:01<16:55,  3.11s/it]221\n",
            "221\n",
            " 41% 222/548 [11:05<18:00,  3.31s/it]222\n",
            "222\n",
            " 41% 223/548 [11:08<17:10,  3.17s/it]223\n",
            "223\n",
            " 41% 224/548 [11:11<16:08,  2.99s/it]224\n",
            "224\n",
            " 41% 225/548 [11:14<17:05,  3.18s/it]225\n",
            "225\n",
            " 41% 226/548 [11:17<16:25,  3.06s/it]226\n",
            "226\n",
            " 41% 227/548 [11:22<19:01,  3.55s/it]227\n",
            "227\n",
            " 42% 228/548 [11:24<17:38,  3.31s/it]228\n",
            "228\n",
            " 42% 229/548 [11:27<16:22,  3.08s/it]229\n",
            "229\n",
            " 42% 230/548 [11:30<15:37,  2.95s/it]230\n",
            "230\n",
            " 42% 231/548 [11:32<15:08,  2.87s/it]231\n",
            "231\n",
            " 42% 232/548 [11:36<16:08,  3.06s/it]232\n",
            "232\n",
            " 43% 233/548 [11:40<17:01,  3.24s/it]233\n",
            "233\n",
            " 43% 234/548 [11:42<16:04,  3.07s/it]234\n",
            "234\n",
            " 43% 235/548 [11:45<15:11,  2.91s/it]235\n",
            "235\n",
            " 43% 236/548 [11:48<16:10,  3.11s/it]236\n",
            "236\n",
            " 43% 237/548 [11:51<15:11,  2.93s/it]237\n",
            "237\n",
            " 43% 238/548 [11:53<14:27,  2.80s/it]238\n",
            "238\n",
            " 44% 239/548 [11:56<13:46,  2.67s/it]239\n",
            "239\n",
            " 44% 240/548 [11:58<13:27,  2.62s/it]240\n",
            "240\n",
            " 44% 241/548 [12:01<13:28,  2.63s/it]241\n",
            "241\n",
            " 44% 242/548 [12:03<13:14,  2.60s/it]242\n",
            "242\n",
            " 44% 243/548 [12:06<13:02,  2.56s/it]243\n",
            "243\n",
            " 45% 244/548 [12:08<13:03,  2.58s/it]244\n",
            "244\n",
            " 45% 245/548 [12:11<13:39,  2.70s/it]245\n",
            "245\n",
            " 45% 246/548 [12:14<13:41,  2.72s/it]246\n",
            "246\n",
            " 45% 247/548 [12:17<13:07,  2.62s/it]247\n",
            "247\n",
            " 45% 248/548 [12:22<17:52,  3.57s/it]248\n",
            "248\n",
            " 45% 249/548 [12:25<16:20,  3.28s/it]249\n",
            "249\n",
            " 46% 250/548 [12:28<15:24,  3.10s/it]250\n",
            "250\n",
            " 46% 251/548 [12:30<14:22,  2.90s/it]251\n",
            "251\n",
            " 46% 252/548 [12:33<13:40,  2.77s/it]252\n",
            "252\n",
            " 46% 253/548 [12:35<13:13,  2.69s/it]253\n",
            "253\n",
            " 46% 254/548 [12:38<13:10,  2.69s/it]254\n",
            "254\n",
            " 47% 255/548 [12:40<12:45,  2.61s/it]255\n",
            "255\n",
            " 47% 256/548 [12:43<12:35,  2.59s/it]256\n",
            "256\n",
            " 47% 257/548 [12:46<13:03,  2.69s/it]257\n",
            "257\n",
            " 47% 258/548 [12:49<13:13,  2.74s/it]258\n",
            "258\n",
            " 47% 259/548 [12:51<12:52,  2.67s/it]259\n",
            "259\n",
            " 47% 260/548 [12:53<12:28,  2.60s/it]260\n",
            "260\n",
            " 48% 261/548 [12:56<12:32,  2.62s/it]261\n",
            "261\n",
            " 48% 262/548 [12:59<12:26,  2.61s/it]262\n",
            "262\n",
            " 48% 263/548 [13:01<12:26,  2.62s/it]263\n",
            "263\n",
            " 48% 264/548 [13:05<13:28,  2.85s/it]264\n",
            "264\n",
            " 48% 265/548 [13:07<13:03,  2.77s/it]265\n",
            "265\n",
            " 49% 266/548 [13:10<12:52,  2.74s/it]266\n",
            "266\n",
            " 49% 267/548 [13:12<12:14,  2.61s/it]267\n",
            "267\n",
            " 49% 268/548 [13:15<11:50,  2.54s/it]268\n",
            "268\n",
            " 49% 269/548 [13:17<11:38,  2.50s/it]269\n",
            "269\n",
            " 49% 270/548 [13:20<11:47,  2.55s/it]270\n",
            "270\n",
            " 49% 271/548 [13:23<13:19,  2.89s/it]271\n",
            "271\n",
            " 50% 272/548 [13:26<13:03,  2.84s/it]272\n",
            "272\n",
            " 50% 273/548 [13:28<12:16,  2.68s/it]273\n",
            "273\n",
            " 50% 274/548 [13:31<12:07,  2.65s/it]274\n",
            "274\n",
            " 50% 275/548 [13:34<12:13,  2.69s/it]275\n",
            "275\n",
            " 50% 276/548 [13:36<11:57,  2.64s/it]276\n",
            "276\n",
            " 51% 277/548 [13:39<12:19,  2.73s/it]277\n",
            "277\n",
            " 51% 278/548 [13:42<11:53,  2.64s/it]278\n",
            "278\n",
            " 51% 279/548 [13:44<11:42,  2.61s/it]279\n",
            "279\n",
            " 51% 280/548 [13:48<13:06,  2.93s/it]280\n",
            "280\n",
            " 51% 281/548 [13:51<12:39,  2.84s/it]281\n",
            "281\n",
            " 51% 282/548 [13:53<11:58,  2.70s/it]282\n",
            "282\n",
            " 52% 283/548 [13:57<13:12,  2.99s/it]283\n",
            "283\n",
            " 52% 284/548 [14:00<13:10,  2.99s/it]284\n",
            "284\n",
            " 52% 285/548 [14:02<12:44,  2.91s/it]285\n",
            "285\n",
            " 52% 286/548 [14:06<13:32,  3.10s/it]286\n",
            "286\n",
            " 52% 287/548 [14:08<12:47,  2.94s/it]287\n",
            "287\n",
            " 53% 288/548 [14:12<13:37,  3.14s/it]288\n",
            "288\n",
            " 53% 289/548 [14:15<13:14,  3.07s/it]289\n",
            "289\n",
            " 53% 290/548 [14:17<12:18,  2.86s/it]290\n",
            "290\n",
            " 53% 291/548 [14:20<12:15,  2.86s/it]291\n",
            "291\n",
            " 53% 292/548 [14:24<14:00,  3.28s/it]292\n",
            "292\n",
            " 53% 293/548 [14:27<13:12,  3.11s/it]293\n",
            "293\n",
            " 54% 294/548 [14:30<12:21,  2.92s/it]294\n",
            "294\n",
            " 54% 295/548 [14:32<11:48,  2.80s/it]295\n",
            "295\n",
            " 54% 296/548 [14:35<11:20,  2.70s/it]296\n",
            "296\n",
            " 54% 297/548 [14:38<12:23,  2.96s/it]297\n",
            "297\n",
            " 54% 298/548 [14:41<11:54,  2.86s/it]298\n",
            "298\n",
            " 55% 299/548 [14:45<13:39,  3.29s/it]299\n",
            "299\n",
            " 55% 300/548 [14:49<14:15,  3.45s/it]300\n",
            "300\n",
            " 55% 301/548 [14:52<13:19,  3.24s/it]301\n",
            "301\n",
            " 55% 302/548 [14:54<12:28,  3.04s/it]302\n",
            "302\n",
            " 55% 303/548 [14:57<11:45,  2.88s/it]303\n",
            "303\n",
            " 55% 304/548 [14:59<11:19,  2.79s/it]304\n",
            "304\n",
            " 56% 305/548 [15:03<12:09,  3.00s/it]305\n",
            "305\n",
            " 56% 306/548 [15:06<11:57,  2.96s/it]306\n",
            "306\n",
            " 56% 307/548 [15:09<12:18,  3.06s/it]307\n",
            "307\n",
            " 56% 308/548 [15:12<11:53,  2.97s/it]308\n",
            "308\n",
            " 56% 309/548 [15:14<11:27,  2.88s/it]309\n",
            "309\n",
            " 57% 310/548 [15:18<12:15,  3.09s/it]310\n",
            "310\n",
            " 57% 311/548 [15:21<11:39,  2.95s/it]311\n",
            "311\n",
            " 57% 312/548 [15:23<11:13,  2.85s/it]312\n",
            "312\n",
            " 57% 313/548 [15:27<12:22,  3.16s/it]313\n",
            "313\n",
            " 57% 314/548 [15:31<12:48,  3.28s/it]314\n",
            "314\n",
            " 57% 315/548 [15:33<12:03,  3.10s/it]315\n",
            "315\n",
            " 58% 316/548 [15:36<11:19,  2.93s/it]316\n",
            "316\n",
            " 58% 317/548 [15:38<10:42,  2.78s/it]317\n",
            "317\n",
            " 58% 318/548 [15:41<10:21,  2.70s/it]318\n",
            "318\n",
            " 58% 319/548 [15:43<09:59,  2.62s/it]319\n",
            "319\n",
            " 58% 320/548 [15:46<09:50,  2.59s/it]320\n",
            "320\n",
            " 59% 321/548 [15:49<10:03,  2.66s/it]321\n",
            "321\n",
            " 59% 322/548 [15:52<10:25,  2.77s/it]322\n",
            "322\n",
            " 59% 323/548 [15:54<09:58,  2.66s/it]323\n",
            "323\n",
            " 59% 324/548 [15:57<09:53,  2.65s/it]324\n",
            "324\n",
            " 59% 325/548 [15:59<09:33,  2.57s/it]325\n",
            "325\n",
            " 59% 326/548 [16:02<09:28,  2.56s/it]326\n",
            "326\n",
            " 60% 327/548 [16:05<10:44,  2.91s/it]327\n",
            "327\n",
            " 60% 328/548 [16:08<10:34,  2.89s/it]328\n",
            "328\n",
            " 60% 329/548 [16:11<10:38,  2.91s/it]329\n",
            "329\n",
            " 60% 330/548 [16:14<10:19,  2.84s/it]330\n",
            "330\n",
            " 60% 331/548 [16:18<11:23,  3.15s/it]331\n",
            "331\n",
            " 61% 332/548 [16:20<10:37,  2.95s/it]332\n",
            "332\n",
            " 61% 333/548 [16:23<10:32,  2.94s/it]333\n",
            "333\n",
            " 61% 334/548 [16:27<11:41,  3.28s/it]334\n",
            "334\n",
            " 61% 335/548 [16:30<10:40,  3.01s/it]335\n",
            "335\n",
            " 61% 336/548 [16:32<10:13,  2.89s/it]336\n",
            "336\n",
            " 61% 337/548 [16:35<10:07,  2.88s/it]337\n",
            "337\n",
            " 62% 338/548 [16:39<10:56,  3.13s/it]338\n",
            "338\n",
            " 62% 339/548 [16:42<11:13,  3.22s/it]339\n",
            "339\n",
            " 62% 340/548 [16:45<10:35,  3.06s/it]340\n",
            "340\n",
            " 62% 341/548 [16:47<09:58,  2.89s/it]341\n",
            "341\n",
            " 62% 342/548 [16:50<09:41,  2.82s/it]342\n",
            "342\n",
            " 63% 343/548 [16:53<09:23,  2.75s/it]343\n",
            "343\n",
            " 63% 344/548 [16:55<08:58,  2.64s/it]344\n",
            "344\n",
            " 63% 345/548 [16:57<08:42,  2.57s/it]345\n",
            "345\n",
            " 63% 346/548 [17:00<08:32,  2.54s/it]346\n",
            "346\n",
            " 63% 347/548 [17:02<08:22,  2.50s/it]347\n",
            "347\n",
            " 64% 348/548 [17:05<08:21,  2.51s/it]348\n",
            "348\n",
            " 64% 349/548 [17:07<08:22,  2.53s/it]349\n",
            "349\n",
            " 64% 350/548 [17:10<08:27,  2.56s/it]350\n",
            "350\n",
            " 64% 351/548 [17:13<08:30,  2.59s/it]351\n",
            "351\n",
            " 64% 352/548 [17:16<09:19,  2.85s/it]352\n",
            "352\n",
            " 64% 353/548 [17:19<08:50,  2.72s/it]353\n",
            "353\n",
            " 65% 354/548 [17:21<08:50,  2.73s/it]354\n",
            "354\n",
            " 65% 355/548 [17:24<08:36,  2.68s/it]355\n",
            "355\n",
            " 65% 356/548 [17:28<09:54,  3.09s/it]356\n",
            "356\n",
            " 65% 357/548 [17:31<09:21,  2.94s/it]357\n",
            "357\n",
            " 65% 358/548 [17:33<08:55,  2.82s/it]358\n",
            "358\n",
            " 66% 359/548 [17:36<08:37,  2.74s/it]359\n",
            "359\n",
            " 66% 360/548 [17:38<08:16,  2.64s/it]360\n",
            "360\n",
            " 66% 361/548 [17:40<07:55,  2.54s/it]361\n",
            "361\n",
            " 66% 362/548 [17:43<08:04,  2.61s/it]362\n",
            "362\n",
            " 66% 363/548 [17:46<07:55,  2.57s/it]363\n",
            "363\n",
            " 66% 364/548 [17:48<07:54,  2.58s/it]364\n",
            "364\n",
            " 67% 365/548 [17:51<07:57,  2.61s/it]365\n",
            "365\n",
            " 67% 366/548 [17:53<07:52,  2.60s/it]366\n",
            "366\n",
            " 67% 367/548 [17:56<07:44,  2.57s/it]367\n",
            "367\n",
            " 67% 368/548 [17:58<07:41,  2.56s/it]368\n",
            "368\n",
            " 67% 369/548 [18:01<07:36,  2.55s/it]369\n",
            "369\n",
            " 68% 370/548 [18:03<07:22,  2.48s/it]370\n",
            "370\n",
            " 68% 371/548 [18:06<07:34,  2.57s/it]371\n",
            "371\n",
            " 68% 372/548 [18:09<07:24,  2.52s/it]372\n",
            "372\n",
            " 68% 373/548 [18:11<07:25,  2.55s/it]373\n",
            "373\n",
            " 68% 374/548 [18:14<07:18,  2.52s/it]374\n",
            "374\n",
            " 68% 375/548 [18:16<07:12,  2.50s/it]375\n",
            "375\n",
            " 69% 376/548 [18:20<08:46,  3.06s/it]376\n",
            "376\n",
            " 69% 377/548 [18:23<08:15,  2.90s/it]377\n",
            "377\n",
            " 69% 378/548 [18:25<07:51,  2.78s/it]378\n",
            "378\n",
            " 69% 379/548 [18:30<09:08,  3.25s/it]379\n",
            "379\n",
            " 69% 380/548 [18:32<08:33,  3.06s/it]380\n",
            "380\n",
            " 70% 381/548 [18:35<08:26,  3.03s/it]381\n",
            "381\n",
            " 70% 382/548 [18:38<08:28,  3.06s/it]382\n",
            "382\n",
            " 70% 383/548 [18:42<08:35,  3.12s/it]383\n",
            "383\n",
            " 70% 384/548 [18:44<08:11,  3.00s/it]384\n",
            "384\n",
            " 70% 385/548 [18:47<07:40,  2.83s/it]385\n",
            "385\n",
            " 70% 386/548 [18:49<07:22,  2.73s/it]386\n",
            "386\n",
            " 71% 387/548 [18:52<07:33,  2.82s/it]387\n",
            "387\n",
            " 71% 388/548 [18:56<08:04,  3.03s/it]388\n",
            "388\n",
            " 71% 389/548 [18:59<08:02,  3.04s/it]389\n",
            "389\n",
            " 71% 390/548 [19:02<07:38,  2.90s/it]390\n",
            "390\n",
            " 71% 391/548 [19:04<07:33,  2.89s/it]391\n",
            "391\n",
            " 72% 392/548 [19:07<07:21,  2.83s/it]392\n",
            "392\n",
            " 72% 393/548 [19:10<07:04,  2.74s/it]393\n",
            "393\n",
            " 72% 394/548 [19:13<07:36,  2.96s/it]394\n",
            "394\n",
            " 72% 395/548 [19:15<07:05,  2.78s/it]395\n",
            "395\n",
            " 72% 396/548 [19:18<06:53,  2.72s/it]396\n",
            "396\n",
            " 72% 397/548 [19:22<07:31,  2.99s/it]397\n",
            "397\n",
            " 73% 398/548 [19:24<07:05,  2.84s/it]398\n",
            "398\n",
            " 73% 399/548 [19:28<07:42,  3.10s/it]399\n",
            "399\n",
            " 73% 400/548 [19:30<07:16,  2.95s/it]400\n",
            "400\n",
            " 73% 401/548 [19:33<07:10,  2.93s/it]401\n",
            "401\n",
            " 73% 402/548 [19:36<07:02,  2.89s/it]402\n",
            "402\n",
            " 74% 403/548 [19:40<07:24,  3.06s/it]403\n",
            "403\n",
            " 74% 404/548 [19:42<06:55,  2.89s/it]404\n",
            "404\n",
            " 74% 405/548 [19:45<06:33,  2.75s/it]405\n",
            "405\n",
            " 74% 406/548 [19:47<06:14,  2.64s/it]406\n",
            "406\n",
            " 74% 407/548 [19:50<06:28,  2.75s/it]407\n",
            "407\n",
            " 74% 408/548 [19:52<06:16,  2.69s/it]408\n",
            "408\n",
            " 75% 409/548 [19:55<06:12,  2.68s/it]409\n",
            "409\n",
            " 75% 410/548 [19:58<06:12,  2.70s/it]410\n",
            "410\n",
            " 75% 411/548 [20:01<06:12,  2.72s/it]411\n",
            "411\n",
            " 75% 412/548 [20:03<05:57,  2.63s/it]412\n",
            "412\n",
            " 75% 413/548 [20:06<05:51,  2.60s/it]413\n",
            "413\n",
            " 76% 414/548 [20:08<05:40,  2.54s/it]414\n",
            "414\n",
            " 76% 415/548 [20:11<05:39,  2.55s/it]415\n",
            "415\n",
            " 76% 416/548 [20:13<05:41,  2.59s/it]416\n",
            "416\n",
            " 76% 417/548 [20:16<06:02,  2.77s/it]417\n",
            "417\n",
            " 76% 418/548 [20:19<05:55,  2.73s/it]418\n",
            "418\n",
            " 76% 419/548 [20:21<05:38,  2.62s/it]419\n",
            "419\n",
            " 77% 420/548 [20:24<05:34,  2.62s/it]420\n",
            "420\n",
            " 77% 421/548 [20:27<05:26,  2.57s/it]421\n",
            "421\n",
            " 77% 422/548 [20:32<07:29,  3.57s/it]422\n",
            "422\n",
            " 77% 423/548 [20:35<06:45,  3.24s/it]423\n",
            "423\n",
            " 77% 424/548 [20:37<06:17,  3.04s/it]424\n",
            "424\n",
            " 78% 425/548 [20:41<06:38,  3.24s/it]425\n",
            "425\n",
            " 78% 426/548 [20:44<06:13,  3.06s/it]426\n",
            "426\n",
            " 78% 427/548 [20:46<05:45,  2.86s/it]427\n",
            "427\n",
            " 78% 428/548 [20:49<05:29,  2.75s/it]428\n",
            "428\n",
            " 78% 429/548 [20:51<05:25,  2.73s/it]429\n",
            "429\n",
            " 78% 430/548 [20:54<05:14,  2.66s/it]430\n",
            "430\n",
            " 79% 431/548 [20:57<05:27,  2.80s/it]431\n",
            "431\n",
            " 79% 432/548 [21:00<05:25,  2.81s/it]432\n",
            "432\n",
            " 79% 433/548 [21:02<05:14,  2.74s/it]433\n",
            "433\n",
            " 79% 434/548 [21:05<05:00,  2.64s/it]434\n",
            "434\n",
            " 79% 435/548 [21:07<04:55,  2.62s/it]435\n",
            "435\n",
            " 80% 436/548 [21:11<05:29,  2.94s/it]436\n",
            "436\n",
            " 80% 437/548 [21:14<05:14,  2.83s/it]437\n",
            "437\n",
            " 80% 438/548 [21:16<05:04,  2.77s/it]438\n",
            "438\n",
            " 80% 439/548 [21:19<05:02,  2.78s/it]439\n",
            "439\n",
            " 80% 440/548 [21:22<05:06,  2.84s/it]440\n",
            "440\n",
            " 80% 441/548 [21:24<04:50,  2.71s/it]441\n",
            "441\n",
            " 81% 442/548 [21:27<04:48,  2.72s/it]442\n",
            "442\n",
            " 81% 443/548 [21:31<05:34,  3.18s/it]443\n",
            "443\n",
            " 81% 444/548 [21:34<05:14,  3.03s/it]444\n",
            "444\n",
            " 81% 445/548 [21:38<05:23,  3.14s/it]445\n",
            "445\n",
            " 81% 446/548 [21:40<05:00,  2.95s/it]446\n",
            "446\n",
            " 82% 447/548 [21:44<05:15,  3.12s/it]447\n",
            "447\n",
            " 82% 448/548 [21:46<04:54,  2.95s/it]448\n",
            "448\n",
            " 82% 449/548 [21:50<05:09,  3.13s/it]449\n",
            "449\n",
            " 82% 450/548 [21:53<05:20,  3.27s/it]450\n",
            "450\n",
            " 82% 451/548 [21:56<04:56,  3.06s/it]451\n",
            "451\n",
            " 82% 452/548 [22:00<05:15,  3.29s/it]452\n",
            "452\n",
            " 83% 453/548 [22:02<04:45,  3.01s/it]453\n",
            "453\n",
            " 83% 454/548 [22:05<04:34,  2.92s/it]454\n",
            "454\n",
            " 83% 455/548 [22:07<04:25,  2.86s/it]455\n",
            "455\n",
            " 83% 456/548 [22:10<04:12,  2.75s/it]456\n",
            "456\n",
            " 83% 457/548 [22:13<04:06,  2.71s/it]457\n",
            "457\n",
            " 84% 458/548 [22:15<04:05,  2.73s/it]458\n",
            "458\n",
            " 84% 459/548 [22:19<04:19,  2.91s/it]459\n",
            "459\n",
            " 84% 460/548 [22:21<04:02,  2.75s/it]460\n",
            "460\n",
            " 84% 461/548 [22:24<04:03,  2.80s/it]461\n",
            "461\n",
            " 84% 462/548 [22:27<04:18,  3.00s/it]462\n",
            "462\n",
            " 84% 463/548 [22:30<04:01,  2.84s/it]463\n",
            "463\n",
            " 85% 464/548 [22:34<04:32,  3.24s/it]464\n",
            "464\n",
            " 85% 465/548 [22:37<04:13,  3.05s/it]465\n",
            "465\n",
            " 85% 466/548 [22:39<03:53,  2.85s/it]466\n",
            "466\n",
            " 85% 467/548 [22:41<03:41,  2.73s/it]467\n",
            "467\n",
            " 85% 468/548 [22:45<03:58,  2.99s/it]468\n",
            "468\n",
            " 86% 469/548 [22:48<03:51,  2.93s/it]469\n",
            "469\n",
            " 86% 470/548 [22:50<03:39,  2.81s/it]470\n",
            "470\n",
            " 86% 471/548 [22:54<03:54,  3.05s/it]471\n",
            "471\n",
            " 86% 472/548 [22:56<03:36,  2.85s/it]472\n",
            "472\n",
            " 86% 473/548 [22:59<03:24,  2.73s/it]473\n",
            "473\n",
            " 86% 474/548 [23:01<03:13,  2.62s/it]474\n",
            "474\n",
            " 87% 475/548 [23:04<03:13,  2.65s/it]475\n",
            "475\n",
            " 87% 476/548 [23:08<03:34,  2.98s/it]476\n",
            "476\n",
            " 87% 477/548 [23:10<03:27,  2.92s/it]477\n",
            "477\n",
            " 87% 478/548 [23:13<03:24,  2.91s/it]478\n",
            "478\n",
            " 87% 479/548 [23:16<03:11,  2.77s/it]479\n",
            "479\n",
            " 88% 480/548 [23:18<03:05,  2.73s/it]480\n",
            "480\n",
            " 88% 481/548 [23:22<03:21,  3.00s/it]481\n",
            "481\n",
            " 88% 482/548 [23:25<03:09,  2.87s/it]482\n",
            "482\n",
            " 88% 483/548 [23:27<02:57,  2.73s/it]483\n",
            "483\n",
            " 88% 484/548 [23:30<02:52,  2.70s/it]484\n",
            "484\n",
            " 89% 485/548 [23:35<03:36,  3.43s/it]485\n",
            "485\n",
            " 89% 486/548 [23:39<03:39,  3.54s/it]486\n",
            "486\n",
            " 89% 487/548 [23:42<03:34,  3.51s/it]487\n",
            "487\n",
            " 89% 488/548 [23:46<03:30,  3.51s/it]488\n",
            "488\n",
            " 89% 489/548 [23:48<03:14,  3.30s/it]489\n",
            "489\n",
            " 89% 490/548 [23:52<03:16,  3.39s/it]490\n",
            "490\n",
            " 90% 491/548 [23:54<02:55,  3.08s/it]491\n",
            "491\n",
            " 90% 492/548 [23:58<02:59,  3.20s/it]492\n",
            "492\n",
            " 90% 493/548 [24:01<03:01,  3.30s/it]493\n",
            "493\n",
            " 90% 494/548 [24:04<02:52,  3.20s/it]494\n",
            "494\n",
            " 90% 495/548 [24:07<02:35,  2.94s/it]495\n",
            "495\n",
            " 91% 496/548 [24:09<02:29,  2.88s/it]496\n",
            "496\n",
            " 91% 497/548 [24:12<02:20,  2.75s/it]497\n",
            "497\n",
            " 91% 498/548 [24:15<02:29,  3.00s/it]498\n",
            "498\n",
            " 91% 499/548 [24:18<02:19,  2.84s/it]499\n",
            "499\n",
            " 91% 500/548 [24:21<02:16,  2.85s/it]500\n",
            "500\n",
            " 91% 501/548 [24:24<02:24,  3.07s/it]501\n",
            "501\n",
            " 92% 502/548 [24:27<02:16,  2.96s/it]502\n",
            "502\n",
            " 92% 503/548 [24:31<02:26,  3.25s/it]503\n",
            "503\n",
            " 92% 504/548 [24:36<02:41,  3.68s/it]504\n",
            "504\n",
            " 92% 505/548 [24:38<02:25,  3.38s/it]505\n",
            "505\n",
            " 92% 506/548 [24:41<02:16,  3.26s/it]506\n",
            "506\n",
            " 93% 507/548 [24:45<02:18,  3.37s/it]507\n",
            "507\n",
            " 93% 508/548 [24:48<02:06,  3.17s/it]508\n",
            "508\n",
            " 93% 509/548 [24:50<01:57,  3.01s/it]509\n",
            "509\n",
            " 93% 510/548 [24:53<01:50,  2.92s/it]510\n",
            "510\n",
            " 93% 511/548 [24:55<01:44,  2.81s/it]511\n",
            "511\n",
            " 93% 512/548 [24:58<01:39,  2.77s/it]512\n",
            "512\n",
            " 94% 513/548 [25:01<01:36,  2.75s/it]513\n",
            "513\n",
            " 94% 514/548 [25:04<01:33,  2.74s/it]514\n",
            "514\n",
            " 94% 515/548 [25:06<01:28,  2.70s/it]515\n",
            "515\n",
            " 94% 516/548 [25:09<01:26,  2.71s/it]516\n",
            "516\n",
            " 94% 517/548 [25:13<01:33,  3.01s/it]517\n",
            "517\n",
            " 95% 518/548 [25:15<01:27,  2.90s/it]518\n",
            "518\n",
            " 95% 519/548 [25:18<01:21,  2.82s/it]519\n",
            "519\n",
            " 95% 520/548 [25:21<01:17,  2.76s/it]520\n",
            "520\n",
            " 95% 521/548 [25:23<01:15,  2.78s/it]521\n",
            "521\n",
            " 95% 522/548 [25:27<01:16,  2.96s/it]522\n",
            "522\n",
            " 95% 523/548 [25:30<01:19,  3.20s/it]523\n",
            "523\n",
            " 96% 524/548 [25:35<01:23,  3.46s/it]524\n",
            "524\n",
            " 96% 525/548 [25:37<01:13,  3.22s/it]525\n",
            "525\n",
            " 96% 526/548 [25:40<01:07,  3.06s/it]526\n",
            "526\n",
            " 96% 527/548 [25:43<01:03,  3.01s/it]527\n",
            "527\n",
            " 96% 528/548 [25:45<00:57,  2.90s/it]528\n",
            "528\n",
            " 97% 529/548 [25:48<00:53,  2.84s/it]529\n",
            "529\n",
            " 97% 530/548 [25:52<00:55,  3.09s/it]530\n",
            "530\n",
            " 97% 531/548 [25:55<00:55,  3.25s/it]531\n",
            "531\n",
            " 97% 532/548 [25:59<00:54,  3.41s/it]532\n",
            "532\n",
            " 97% 533/548 [26:03<00:53,  3.54s/it]533\n",
            "533\n",
            " 97% 534/548 [26:06<00:46,  3.31s/it]534\n",
            "534\n",
            " 98% 535/548 [26:09<00:41,  3.17s/it]535\n",
            "535\n",
            " 98% 536/548 [26:12<00:37,  3.09s/it]536\n",
            "536\n",
            " 98% 537/548 [26:14<00:32,  2.97s/it]537\n",
            "537\n",
            " 98% 538/548 [26:17<00:30,  3.02s/it]538\n",
            "538\n",
            " 98% 539/548 [26:21<00:28,  3.12s/it]539\n",
            "539\n",
            " 99% 540/548 [26:24<00:25,  3.19s/it]540\n",
            "540\n",
            " 99% 541/548 [26:27<00:21,  3.07s/it]541\n",
            "541\n",
            " 99% 542/548 [26:30<00:17,  2.96s/it]542\n",
            "542\n",
            " 99% 543/548 [26:32<00:14,  2.87s/it]543\n",
            "543\n",
            " 99% 544/548 [26:38<00:14,  3.64s/it]544\n",
            "544\n",
            " 99% 545/548 [26:41<00:10,  3.39s/it]545\n",
            "545\n",
            "100% 546/548 [26:43<00:06,  3.24s/it]546\n",
            "546\n",
            "100% 547/548 [26:46<00:03,  3.12s/it]547\n",
            "547\n",
            "100% 548/548 [26:50<00:00,  2.94s/it]\n",
            "data/visdrone/VisDrone2019-DET-val\n",
            ".visdrone_det_txt\n",
            "data/visdrone/VisDrone2019-DET-val/annotations\n",
            "data/visdrone/VisDrone2019-DET-val/images\n",
            "\n",
            "evaluating object category 1/10...\n",
            "evaluating object category 2/10...\n",
            "evaluating object category 3/10...\n",
            "evaluating object category 4/10...\n",
            "evaluating object category 5/10...\n",
            "evaluating object category 6/10...\n",
            "evaluating object category 7/10...\n",
            "evaluating object category 8/10...\n",
            "evaluating object category 9/10...\n",
            "evaluating object category 10/10...\n",
            "Evaluation completed. The performance of the detector is presented as follows.\n",
            "Average Precision  (AP) @[ IoU=0.50:0.95 | maxDets=500 ] = 18.255563735961914%.\n",
            "Average Precision  (AP) @[ IoU=0.50      | maxDets=500 ] = 35.62028503417969%.\n",
            "Average Precision  (AP) @[ IoU=0.75      | maxDets=500 ] = 16.258344650268555%.\n",
            "Average Recall     (AR) @[ IoU=0.50:0.95 | maxDets=  1 ] = 0.4383193850517273%.\n",
            "Average Recall     (AR) @[ IoU=0.50:0.95 | maxDets= 10 ] = 4.223095893859863%.\n",
            "Average Recall     (AR) @[ IoU=0.50:0.95 | maxDets=100 ] = 26.046525955200195%.\n",
            "Average Recall     (AR) @[ IoU=0.50:0.95 | maxDets=500 ] = 34.195960998535156%.\n",
            "Class 0 AP = 23.394113540649414%\n",
            "Class 1 AP = 12.756424903869629%\n",
            "Class 2 AP = 4.102192401885986%\n",
            "Class 3 AP = 52.45000457763672%\n",
            "Class 4 AP = 19.047607421875%\n",
            "Class 5 AP = 6.844865322113037%\n",
            "Class 6 AP = 5.075650215148926%\n",
            "Class 7 AP = 2.5256712436676025%\n",
            "Class 8 AP = 6.912878513336182%\n",
            "Class 9 AP = 17.372325897216797%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "wPDjg8BadX06"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "NtEJYAr7dXvt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!python infer_visdrone.py --config-file configs/visdrone/querydet_test.yaml --num-gpu 1 --eval-only MODEL.WEIGHTS work_dirs/visdrone_querydet/model_0007999.pth OUTPUT_DIR work_dirs/model_test-3"
      ],
      "metadata": {
        "id": "ERyHy1bydXul",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "dfe7de1d-8403-479f-9a38-6e32e59c783b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "** fvcore version of PathManager will be deprecated soon. **\n",
            "** Please migrate to the version in iopath repo. **\n",
            "https://github.com/facebookresearch/iopath \n",
            "\n",
            "Command Line Args: Namespace(config_file='configs/visdrone/querydet_test.yaml', dist_url='tcp://127.0.0.1:49152', eval_only=True, machine_rank=0, no_pretrain=False, num_gpus=1, num_machines=1, opts=['MODEL.WEIGHTS', 'work_dirs/visdrone_querydet/model_0007999.pth', 'OUTPUT_DIR', 'work_dirs/model_test-3'], resume=False)\n",
            "Loading config configs/visdrone/querydet_test.yaml with yaml.unsafe_load. Your machine may be at risk if the file contains malicious content.\n",
            "Loading config configs/visdrone/../BaseRetina.yaml with yaml.unsafe_load. Your machine may be at risk if the file contains malicious content.\n",
            "\u001b[32m[04/15 01:35:50 detectron2]: \u001b[0mRank of current process: 0. World size: 1\n",
            "\u001b[32m[04/15 01:35:53 detectron2]: \u001b[0mEnvironment info:\n",
            "----------------------  ---------------------------------------------------------------\n",
            "sys.platform            linux\n",
            "Python                  3.7.6 (default, Jan  8 2020, 19:59:22) [GCC 7.3.0]\n",
            "numpy                   1.21.6\n",
            "detectron2              0.4 @/usr/local/lib/python3.7/site-packages/detectron2\n",
            "Compiler                GCC 7.3\n",
            "CUDA compiler           CUDA 10.1\n",
            "detectron2 arch flags   3.7, 5.0, 5.2, 6.0, 6.1, 7.0, 7.5\n",
            "DETECTRON2_ENV_MODULE   <not set>\n",
            "PyTorch                 1.8.1+cu101 @/usr/local/lib/python3.7/site-packages/torch\n",
            "PyTorch debug build     False\n",
            "GPU available           True\n",
            "GPU 0                   Tesla T4 (arch=7.5)\n",
            "CUDA_HOME               /usr/local/cuda\n",
            "Pillow                  9.5.0\n",
            "torchvision             0.9.1+cu101 @/usr/local/lib/python3.7/site-packages/torchvision\n",
            "torchvision arch flags  3.5, 5.0, 6.0, 7.0, 7.5\n",
            "fvcore                  0.1.3.post20210317\n",
            "cv2                     4.7.0\n",
            "----------------------  ---------------------------------------------------------------\n",
            "PyTorch built with:\n",
            "  - GCC 7.3\n",
            "  - C++ Version: 201402\n",
            "  - Intel(R) Math Kernel Library Version 2020.0.0 Product Build 20191122 for Intel(R) 64 architecture applications\n",
            "  - Intel(R) MKL-DNN v1.7.0 (Git Hash 7aed236906b1f7a05c0917e5257a1af05e9ff683)\n",
            "  - OpenMP 201511 (a.k.a. OpenMP 4.5)\n",
            "  - NNPACK is enabled\n",
            "  - CPU capability usage: AVX2\n",
            "  - CUDA Runtime 10.1\n",
            "  - NVCC architecture flags: -gencode;arch=compute_37,code=sm_37;-gencode;arch=compute_50,code=sm_50;-gencode;arch=compute_60,code=sm_60;-gencode;arch=compute_70,code=sm_70\n",
            "  - CuDNN 7.6.3\n",
            "  - Magma 2.5.2\n",
            "  - Build settings: BLAS_INFO=mkl, BUILD_TYPE=Release, CUDA_VERSION=10.1, CUDNN_VERSION=7.6.3, CXX_COMPILER=/opt/rh/devtoolset-7/root/usr/bin/c++, CXX_FLAGS= -Wno-deprecated -fvisibility-inlines-hidden -DUSE_PTHREADPOOL -fopenmp -DNDEBUG -DUSE_KINETO -DUSE_FBGEMM -DUSE_QNNPACK -DUSE_PYTORCH_QNNPACK -DUSE_XNNPACK -O2 -fPIC -Wno-narrowing -Wall -Wextra -Werror=return-type -Wno-missing-field-initializers -Wno-type-limits -Wno-array-bounds -Wno-unknown-pragmas -Wno-sign-compare -Wno-unused-parameter -Wno-unused-variable -Wno-unused-function -Wno-unused-result -Wno-unused-local-typedefs -Wno-strict-overflow -Wno-strict-aliasing -Wno-error=deprecated-declarations -Wno-stringop-overflow -Wno-psabi -Wno-error=pedantic -Wno-error=redundant-decls -Wno-error=old-style-cast -fdiagnostics-color=always -faligned-new -Wno-unused-but-set-variable -Wno-maybe-uninitialized -fno-math-errno -fno-trapping-math -Werror=format -Wno-stringop-overflow, LAPACK_INFO=mkl, PERF_WITH_AVX=1, PERF_WITH_AVX2=1, PERF_WITH_AVX512=1, TORCH_VERSION=1.8.1, USE_CUDA=ON, USE_CUDNN=ON, USE_EXCEPTION_PTR=1, USE_GFLAGS=OFF, USE_GLOG=OFF, USE_MKL=ON, USE_MKLDNN=ON, USE_MPI=OFF, USE_NCCL=ON, USE_NNPACK=ON, USE_OPENMP=ON, \n",
            "\n",
            "\u001b[32m[04/15 01:35:53 detectron2]: \u001b[0mCommand line arguments: Namespace(config_file='configs/visdrone/querydet_test.yaml', dist_url='tcp://127.0.0.1:49152', eval_only=True, machine_rank=0, no_pretrain=False, num_gpus=1, num_machines=1, opts=['MODEL.WEIGHTS', 'work_dirs/visdrone_querydet/model_0007999.pth', 'OUTPUT_DIR', 'work_dirs/model_test-3'], resume=False)\n",
            "\u001b[32m[04/15 01:35:53 detectron2]: \u001b[0mContents of args.config_file=configs/visdrone/querydet_test.yaml:\n",
            "_BASE_: \"../BaseRetina.yaml\"\n",
            "OUTPUT_DIR: \"work_dirs/model_test\"\n",
            "\n",
            "MODEL:\n",
            "  META_ARCHITECTURE: \"RetinaNetQueryDet\"\n",
            "  WEIGHTS: \"detectron2://ImageNetPretrained/MSRA/R-50.pkl\"\n",
            "  \n",
            "  RESNETS:\n",
            "    DEPTH: 50\n",
            "  \n",
            "  ANCHOR_GENERATOR:\n",
            "    NAME: \"AnchorGeneratorWithCenter\"\n",
            "    SIZES: !!python/object/apply:eval [\"[[x, x * 2**(1.0/3), x * 2**(2.0/3)] for x in [16, 32, 64, 128, 256, 512]]\"]\n",
            "  \n",
            "  RETINANET:\n",
            "    IOU_THRESHOLDS: [0.4, 0.5]\n",
            "    IOU_LABELS: [0, -1, 1]\n",
            "    NUM_CLASSES: 10\n",
            "    IN_FEATURES: [\"p2\", \"p3\", \"p4\", \"p5\", \"p6\", \"p7\"]  \n",
            "    SCORE_THRESH_TEST: 0.0001\n",
            "    \n",
            "  RESNETS:\n",
            "    OUT_FEATURES: [\"res2\", \"res3\", \"res4\", \"res5\"]\n",
            "\n",
            "  FPN:\n",
            "    IN_FEATURES: [\"res2\", \"res3\", \"res4\", \"res5\"]\n",
            "  \n",
            "  QUERY:\n",
            "    FEATURES_WHOLE_TEST: [2, 3, 4, 5]\n",
            "    FEATURES_VALUE_TEST: [0, 1]\n",
            "    Q_FEATURE_TRAIN: [1, 2]\n",
            "    Q_FEATURE_TEST: [1, 2]\n",
            "    \n",
            "    ENCODE_CENTER_DIS_COEFF: [1., 1.]\n",
            "    ENCODE_SMALL_OBJ_SCALE: [[0, 32], [0, 64]]\n",
            "\n",
            "    THRESHOLD: 0.12\n",
            "    QUERY_INFER: False\n",
            "  \n",
            "  CUSTOM: \n",
            "    USE_SOFT_NMS: False\n",
            "    SOFT_NMS_METHOD: 'gaussian'\n",
            "    SOFT_NMS_SIGMA: 0.6\n",
            "    SOFT_NMS_THRESHOLD: 0.4\n",
            "    SOFT_NMS_PRUND: 0.0001\n",
            "\n",
            "VISDRONE:\n",
            "  TEST_LENGTH: 3999\n",
            "\n",
            "TEST:  \n",
            "  DETECTIONS_PER_IMAGE: 500\n",
            "\n",
            "META_INFO:\n",
            "  EVAL_GPU_TIME: True\n",
            "\n",
            "\u001b[32m[04/15 01:35:53 detectron2]: \u001b[0mRunning with full config:\n",
            "CUDNN_BENCHMARK: False\n",
            "DATALOADER:\n",
            "  ASPECT_RATIO_GROUPING: True\n",
            "  FILTER_EMPTY_ANNOTATIONS: True\n",
            "  NUM_WORKERS: 4\n",
            "  REPEAT_THRESHOLD: 0.0\n",
            "  SAMPLER_TRAIN: TrainingSampler\n",
            "DATASETS:\n",
            "  PRECOMPUTED_PROPOSAL_TOPK_TEST: 1000\n",
            "  PRECOMPUTED_PROPOSAL_TOPK_TRAIN: 2000\n",
            "  PROPOSAL_FILES_TEST: ()\n",
            "  PROPOSAL_FILES_TRAIN: ()\n",
            "  TEST: ('coco_2017_val',)\n",
            "  TRAIN: ('coco_2017_train',)\n",
            "GLOBAL:\n",
            "  HACK: 1.0\n",
            "INPUT:\n",
            "  CROP:\n",
            "    ENABLED: False\n",
            "    SIZE: [0.9, 0.9]\n",
            "    TYPE: relative_range\n",
            "  FORMAT: BGR\n",
            "  MASK_FORMAT: polygon\n",
            "  MAX_SIZE_TEST: 1333\n",
            "  MAX_SIZE_TRAIN: 1333\n",
            "  MIN_SIZE_TEST: 800\n",
            "  MIN_SIZE_TRAIN: (640, 672, 704, 736, 768, 800)\n",
            "  MIN_SIZE_TRAIN_SAMPLING: choice\n",
            "  RANDOM_FLIP: horizontal\n",
            "META_INFO:\n",
            "  EVAL_AP: True\n",
            "  EVAL_GPU_TIME: True\n",
            "  VIS_ROOT: \n",
            "MODEL:\n",
            "  ANCHOR_GENERATOR:\n",
            "    ANGLES: [[-90, 0, 90]]\n",
            "    ASPECT_RATIOS: [[0.5, 1.0, 2.0]]\n",
            "    NAME: AnchorGeneratorWithCenter\n",
            "    OFFSET: 0.0\n",
            "    SIZES: [[16, 20.15873679831797, 25.39841683149119], [32, 40.31747359663594, 50.79683366298238], [64, 80.63494719327188, 101.59366732596476], [128, 161.26989438654377, 203.18733465192952], [256, 322.53978877308754, 406.37466930385904], [512, 645.0795775461751, 812.7493386077181]]\n",
            "  BACKBONE:\n",
            "    FREEZE_AT: 2\n",
            "    NAME: build_retinanet_resnet_fpn_backbone\n",
            "  CUSTOM:\n",
            "    CLEAR_CUDA_CACHE: False\n",
            "    CLS_WEIGHTS: []\n",
            "    FOCAL_LOSS_ALPHAS: []\n",
            "    FOCAL_LOSS_GAMMAS: []\n",
            "    GIOU_LOSS: False\n",
            "    GRADIENT_CHECKPOINT: False\n",
            "    HEAD_BN: False\n",
            "    REG_WEIGHTS: []\n",
            "    SOFT_NMS_METHOD: gaussian\n",
            "    SOFT_NMS_PRUND: 0.0001\n",
            "    SOFT_NMS_SIGMA: 0.6\n",
            "    SOFT_NMS_THRESHOLD: 0.4\n",
            "    USE_LOOP_MATCHER: False\n",
            "    USE_SOFT_NMS: False\n",
            "  DEVICE: cuda\n",
            "  FPN:\n",
            "    FUSE_TYPE: sum\n",
            "    IN_FEATURES: ['res2', 'res3', 'res4', 'res5']\n",
            "    NORM: \n",
            "    OUT_CHANNELS: 256\n",
            "    TOP_LEVELS: 2\n",
            "  KEYPOINT_ON: False\n",
            "  LOAD_PROPOSALS: False\n",
            "  MASK_ON: False\n",
            "  META_ARCHITECTURE: RetinaNetQueryDet\n",
            "  PANOPTIC_FPN:\n",
            "    COMBINE:\n",
            "      ENABLED: True\n",
            "      INSTANCES_CONFIDENCE_THRESH: 0.5\n",
            "      OVERLAP_THRESH: 0.5\n",
            "      STUFF_AREA_LIMIT: 4096\n",
            "    INSTANCE_LOSS_WEIGHT: 1.0\n",
            "  PIXEL_MEAN: [103.53, 116.28, 123.675]\n",
            "  PIXEL_STD: [1.0, 1.0, 1.0]\n",
            "  PROPOSAL_GENERATOR:\n",
            "    MIN_SIZE: 0\n",
            "    NAME: RPN\n",
            "  QUERY:\n",
            "    CONTEXT: 2\n",
            "    ENCODE_CENTER_DIS_COEFF: [1.0, 1.0]\n",
            "    ENCODE_SMALL_OBJ_SCALE: [[0, 32], [0, 64]]\n",
            "    FEATURES_VALUE_TEST: [0, 1]\n",
            "    FEATURES_VALUE_TRAIN: [0, 1]\n",
            "    FEATURES_WHOLE_TEST: [2, 3, 4, 5]\n",
            "    FEATURES_WHOLE_TRAIN: [2, 3, 4, 5]\n",
            "    QUERY_INFER: False\n",
            "    QUERY_LOSS_GAMMA: []\n",
            "    QUERY_LOSS_WEIGHT: []\n",
            "    Q_FEATURE_TEST: [1, 2]\n",
            "    Q_FEATURE_TRAIN: [1, 2]\n",
            "    THRESHOLD: 0.12\n",
            "  RESNETS:\n",
            "    DEFORM_MODULATED: False\n",
            "    DEFORM_NUM_GROUPS: 1\n",
            "    DEFORM_ON_PER_STAGE: [False, False, False, False]\n",
            "    DEPTH: 50\n",
            "    NORM: FrozenBN\n",
            "    NUM_GROUPS: 1\n",
            "    OUT_FEATURES: ['res2', 'res3', 'res4', 'res5']\n",
            "    RES2_OUT_CHANNELS: 256\n",
            "    RES5_DILATION: 1\n",
            "    STEM_OUT_CHANNELS: 64\n",
            "    STRIDE_IN_1X1: True\n",
            "    WIDTH_PER_GROUP: 64\n",
            "  RETINANET:\n",
            "    BBOX_REG_LOSS_TYPE: smooth_l1\n",
            "    BBOX_REG_WEIGHTS: (1.0, 1.0, 1.0, 1.0)\n",
            "    FOCAL_LOSS_ALPHA: 0.25\n",
            "    FOCAL_LOSS_GAMMA: 2.0\n",
            "    IN_FEATURES: ['p2', 'p3', 'p4', 'p5', 'p6', 'p7']\n",
            "    IOU_LABELS: [0, -1, 1]\n",
            "    IOU_THRESHOLDS: [0.4, 0.5]\n",
            "    NMS_THRESH_TEST: 0.5\n",
            "    NORM: \n",
            "    NUM_CLASSES: 10\n",
            "    NUM_CONVS: 4\n",
            "    PRIOR_PROB: 0.01\n",
            "    SCORE_THRESH_TEST: 0.0001\n",
            "    SMOOTH_L1_LOSS_BETA: 0.1\n",
            "    TOPK_CANDIDATES_TEST: 1000\n",
            "  ROI_BOX_CASCADE_HEAD:\n",
            "    BBOX_REG_WEIGHTS: ((10.0, 10.0, 5.0, 5.0), (20.0, 20.0, 10.0, 10.0), (30.0, 30.0, 15.0, 15.0))\n",
            "    IOUS: (0.5, 0.6, 0.7)\n",
            "  ROI_BOX_HEAD:\n",
            "    BBOX_REG_LOSS_TYPE: smooth_l1\n",
            "    BBOX_REG_LOSS_WEIGHT: 1.0\n",
            "    BBOX_REG_WEIGHTS: (10.0, 10.0, 5.0, 5.0)\n",
            "    CLS_AGNOSTIC_BBOX_REG: False\n",
            "    CONV_DIM: 256\n",
            "    FC_DIM: 1024\n",
            "    NAME: \n",
            "    NORM: \n",
            "    NUM_CONV: 0\n",
            "    NUM_FC: 0\n",
            "    POOLER_RESOLUTION: 14\n",
            "    POOLER_SAMPLING_RATIO: 0\n",
            "    POOLER_TYPE: ROIAlignV2\n",
            "    SMOOTH_L1_BETA: 0.0\n",
            "    TRAIN_ON_PRED_BOXES: False\n",
            "  ROI_HEADS:\n",
            "    BATCH_SIZE_PER_IMAGE: 512\n",
            "    IN_FEATURES: ['res4']\n",
            "    IOU_LABELS: [0, 1]\n",
            "    IOU_THRESHOLDS: [0.5]\n",
            "    NAME: Res5ROIHeads\n",
            "    NMS_THRESH_TEST: 0.5\n",
            "    NUM_CLASSES: 80\n",
            "    POSITIVE_FRACTION: 0.25\n",
            "    PROPOSAL_APPEND_GT: True\n",
            "    SCORE_THRESH_TEST: 0.05\n",
            "  ROI_KEYPOINT_HEAD:\n",
            "    CONV_DIMS: (512, 512, 512, 512, 512, 512, 512, 512)\n",
            "    LOSS_WEIGHT: 1.0\n",
            "    MIN_KEYPOINTS_PER_IMAGE: 1\n",
            "    NAME: KRCNNConvDeconvUpsampleHead\n",
            "    NORMALIZE_LOSS_BY_VISIBLE_KEYPOINTS: True\n",
            "    NUM_KEYPOINTS: 17\n",
            "    POOLER_RESOLUTION: 14\n",
            "    POOLER_SAMPLING_RATIO: 0\n",
            "    POOLER_TYPE: ROIAlignV2\n",
            "  ROI_MASK_HEAD:\n",
            "    CLS_AGNOSTIC_MASK: False\n",
            "    CONV_DIM: 256\n",
            "    NAME: MaskRCNNConvUpsampleHead\n",
            "    NORM: \n",
            "    NUM_CONV: 0\n",
            "    POOLER_RESOLUTION: 14\n",
            "    POOLER_SAMPLING_RATIO: 0\n",
            "    POOLER_TYPE: ROIAlignV2\n",
            "  RPN:\n",
            "    BATCH_SIZE_PER_IMAGE: 256\n",
            "    BBOX_REG_LOSS_TYPE: smooth_l1\n",
            "    BBOX_REG_LOSS_WEIGHT: 1.0\n",
            "    BBOX_REG_WEIGHTS: (1.0, 1.0, 1.0, 1.0)\n",
            "    BOUNDARY_THRESH: -1\n",
            "    HEAD_NAME: StandardRPNHead\n",
            "    IN_FEATURES: ['res4']\n",
            "    IOU_LABELS: [0, -1, 1]\n",
            "    IOU_THRESHOLDS: [0.3, 0.7]\n",
            "    LOSS_WEIGHT: 1.0\n",
            "    NMS_THRESH: 0.7\n",
            "    POSITIVE_FRACTION: 0.5\n",
            "    POST_NMS_TOPK_TEST: 1000\n",
            "    POST_NMS_TOPK_TRAIN: 2000\n",
            "    PRE_NMS_TOPK_TEST: 6000\n",
            "    PRE_NMS_TOPK_TRAIN: 12000\n",
            "    SMOOTH_L1_BETA: 0.0\n",
            "  SEM_SEG_HEAD:\n",
            "    COMMON_STRIDE: 4\n",
            "    CONVS_DIM: 128\n",
            "    IGNORE_VALUE: 255\n",
            "    IN_FEATURES: ['p2', 'p3', 'p4', 'p5']\n",
            "    LOSS_WEIGHT: 1.0\n",
            "    NAME: SemSegFPNHead\n",
            "    NORM: GN\n",
            "    NUM_CLASSES: 54\n",
            "  WEIGHTS: work_dirs/visdrone_querydet/model_0007999.pth\n",
            "OUTPUT_DIR: work_dirs/model_test-3\n",
            "SEED: -1\n",
            "SOLVER:\n",
            "  AMP:\n",
            "    ENABLED: False\n",
            "  BASE_LR: 0.01\n",
            "  BIAS_LR_FACTOR: 1.0\n",
            "  CHECKPOINT_PERIOD: 5000\n",
            "  CLIP_GRADIENTS:\n",
            "    CLIP_TYPE: value\n",
            "    CLIP_VALUE: 1.0\n",
            "    ENABLED: False\n",
            "    NORM_TYPE: 2.0\n",
            "  GAMMA: 0.1\n",
            "  IMS_PER_BATCH: 16\n",
            "  LR_SCHEDULER_NAME: WarmupMultiStepLR\n",
            "  MAX_ITER: 90000\n",
            "  MOMENTUM: 0.9\n",
            "  NESTEROV: False\n",
            "  REFERENCE_WORLD_SIZE: 0\n",
            "  STEPS: (60000, 80000)\n",
            "  WARMUP_FACTOR: 0.001\n",
            "  WARMUP_ITERS: 1000\n",
            "  WARMUP_METHOD: linear\n",
            "  WEIGHT_DECAY: 0.0001\n",
            "  WEIGHT_DECAY_BIAS: 0.0001\n",
            "  WEIGHT_DECAY_NORM: 0.0\n",
            "TEST:\n",
            "  AUG:\n",
            "    ENABLED: False\n",
            "    FLIP: True\n",
            "    MAX_SIZE: 4000\n",
            "    MIN_SIZES: (400, 500, 600, 700, 800, 900, 1000, 1100, 1200)\n",
            "  DETECTIONS_PER_IMAGE: 500\n",
            "  EVAL_PERIOD: 0\n",
            "  EXPECTED_RESULTS: []\n",
            "  KEYPOINT_OKS_SIGMAS: []\n",
            "  PRECISE_BN:\n",
            "    ENABLED: False\n",
            "    NUM_ITER: 200\n",
            "VERSION: 2\n",
            "VISDRONE:\n",
            "  MAX_LENGTH: 1999\n",
            "  SHORT_LENGTH: [1200]\n",
            "  TEST_IMG_ROOT: data/visdrone/coco_format/val_images\n",
            "  TEST_JSON: data/visdrone/coco_format/annotations/val_label.json\n",
            "  TEST_LENGTH: 3999\n",
            "  TRAIN_JSON: data/visdrone/coco_format/annotations/train_label.json\n",
            "  TRING_IMG_ROOT: data//visdrone/coco_format/train_images\n",
            "VIS_PERIOD: 0\n",
            "\u001b[32m[04/15 01:35:54 detectron2]: \u001b[0mFull config saved to work_dirs/model_test-3/config.yaml\n",
            "\u001b[32m[04/15 01:35:54 d2.utils.env]: \u001b[0mUsing a generated random seed 54734377\n",
            "\u001b[32m[04/15 01:35:58 d2.engine.defaults]: \u001b[0mModel:\n",
            "RetinaNetQueryDet(\n",
            "  (backbone): FPN(\n",
            "    (fpn_lateral2): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1))\n",
            "    (fpn_output2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (fpn_lateral3): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1))\n",
            "    (fpn_output3): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (fpn_lateral4): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1))\n",
            "    (fpn_output4): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (fpn_lateral5): Conv2d(2048, 256, kernel_size=(1, 1), stride=(1, 1))\n",
            "    (fpn_output5): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (top_block): LastLevelP6P7(\n",
            "      (p6): Conv2d(2048, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
            "      (p7): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
            "    )\n",
            "    (bottom_up): ResNet(\n",
            "      (stem): BasicStem(\n",
            "        (conv1): Conv2d(\n",
            "          3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)\n",
            "        )\n",
            "      )\n",
            "      (res2): Sequential(\n",
            "        (0): BottleneckBlock(\n",
            "          (shortcut): Conv2d(\n",
            "            64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
            "          )\n",
            "          (conv1): Conv2d(\n",
            "            64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)\n",
            "          )\n",
            "          (conv2): Conv2d(\n",
            "            64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)\n",
            "          )\n",
            "          (conv3): Conv2d(\n",
            "            64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
            "          )\n",
            "        )\n",
            "        (1): BottleneckBlock(\n",
            "          (conv1): Conv2d(\n",
            "            256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)\n",
            "          )\n",
            "          (conv2): Conv2d(\n",
            "            64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)\n",
            "          )\n",
            "          (conv3): Conv2d(\n",
            "            64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
            "          )\n",
            "        )\n",
            "        (2): BottleneckBlock(\n",
            "          (conv1): Conv2d(\n",
            "            256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)\n",
            "          )\n",
            "          (conv2): Conv2d(\n",
            "            64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)\n",
            "          )\n",
            "          (conv3): Conv2d(\n",
            "            64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
            "          )\n",
            "        )\n",
            "      )\n",
            "      (res3): Sequential(\n",
            "        (0): BottleneckBlock(\n",
            "          (shortcut): Conv2d(\n",
            "            256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
            "          )\n",
            "          (conv1): Conv2d(\n",
            "            256, 128, kernel_size=(1, 1), stride=(2, 2), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)\n",
            "          )\n",
            "          (conv2): Conv2d(\n",
            "            128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)\n",
            "          )\n",
            "          (conv3): Conv2d(\n",
            "            128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
            "          )\n",
            "        )\n",
            "        (1): BottleneckBlock(\n",
            "          (conv1): Conv2d(\n",
            "            512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)\n",
            "          )\n",
            "          (conv2): Conv2d(\n",
            "            128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)\n",
            "          )\n",
            "          (conv3): Conv2d(\n",
            "            128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
            "          )\n",
            "        )\n",
            "        (2): BottleneckBlock(\n",
            "          (conv1): Conv2d(\n",
            "            512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)\n",
            "          )\n",
            "          (conv2): Conv2d(\n",
            "            128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)\n",
            "          )\n",
            "          (conv3): Conv2d(\n",
            "            128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
            "          )\n",
            "        )\n",
            "        (3): BottleneckBlock(\n",
            "          (conv1): Conv2d(\n",
            "            512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)\n",
            "          )\n",
            "          (conv2): Conv2d(\n",
            "            128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)\n",
            "          )\n",
            "          (conv3): Conv2d(\n",
            "            128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
            "          )\n",
            "        )\n",
            "      )\n",
            "      (res4): Sequential(\n",
            "        (0): BottleneckBlock(\n",
            "          (shortcut): Conv2d(\n",
            "            512, 1024, kernel_size=(1, 1), stride=(2, 2), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n",
            "          )\n",
            "          (conv1): Conv2d(\n",
            "            512, 256, kernel_size=(1, 1), stride=(2, 2), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
            "          )\n",
            "          (conv2): Conv2d(\n",
            "            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
            "          )\n",
            "          (conv3): Conv2d(\n",
            "            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n",
            "          )\n",
            "        )\n",
            "        (1): BottleneckBlock(\n",
            "          (conv1): Conv2d(\n",
            "            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
            "          )\n",
            "          (conv2): Conv2d(\n",
            "            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
            "          )\n",
            "          (conv3): Conv2d(\n",
            "            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n",
            "          )\n",
            "        )\n",
            "        (2): BottleneckBlock(\n",
            "          (conv1): Conv2d(\n",
            "            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
            "          )\n",
            "          (conv2): Conv2d(\n",
            "            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
            "          )\n",
            "          (conv3): Conv2d(\n",
            "            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n",
            "          )\n",
            "        )\n",
            "        (3): BottleneckBlock(\n",
            "          (conv1): Conv2d(\n",
            "            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
            "          )\n",
            "          (conv2): Conv2d(\n",
            "            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
            "          )\n",
            "          (conv3): Conv2d(\n",
            "            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n",
            "          )\n",
            "        )\n",
            "        (4): BottleneckBlock(\n",
            "          (conv1): Conv2d(\n",
            "            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
            "          )\n",
            "          (conv2): Conv2d(\n",
            "            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
            "          )\n",
            "          (conv3): Conv2d(\n",
            "            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n",
            "          )\n",
            "        )\n",
            "        (5): BottleneckBlock(\n",
            "          (conv1): Conv2d(\n",
            "            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
            "          )\n",
            "          (conv2): Conv2d(\n",
            "            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
            "          )\n",
            "          (conv3): Conv2d(\n",
            "            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n",
            "          )\n",
            "        )\n",
            "      )\n",
            "      (res5): Sequential(\n",
            "        (0): BottleneckBlock(\n",
            "          (shortcut): Conv2d(\n",
            "            1024, 2048, kernel_size=(1, 1), stride=(2, 2), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=2048, eps=1e-05)\n",
            "          )\n",
            "          (conv1): Conv2d(\n",
            "            1024, 512, kernel_size=(1, 1), stride=(2, 2), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
            "          )\n",
            "          (conv2): Conv2d(\n",
            "            512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
            "          )\n",
            "          (conv3): Conv2d(\n",
            "            512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=2048, eps=1e-05)\n",
            "          )\n",
            "        )\n",
            "        (1): BottleneckBlock(\n",
            "          (conv1): Conv2d(\n",
            "            2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
            "          )\n",
            "          (conv2): Conv2d(\n",
            "            512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
            "          )\n",
            "          (conv3): Conv2d(\n",
            "            512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=2048, eps=1e-05)\n",
            "          )\n",
            "        )\n",
            "        (2): BottleneckBlock(\n",
            "          (conv1): Conv2d(\n",
            "            2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
            "          )\n",
            "          (conv2): Conv2d(\n",
            "            512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
            "          )\n",
            "          (conv3): Conv2d(\n",
            "            512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=2048, eps=1e-05)\n",
            "          )\n",
            "        )\n",
            "      )\n",
            "    )\n",
            "  )\n",
            "  (det_head): RetinaNetHead_3x3(\n",
            "    (cls_layer_0): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (bbox_layer_0): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (cls_layer_1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (bbox_layer_1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (cls_layer_2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (bbox_layer_2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (cls_layer_3): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (bbox_layer_3): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (cls_score): Conv2d(256, 90, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (bbox_pred): Conv2d(256, 36, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "  )\n",
            "  (query_head): Head_3x3(\n",
            "    (layer_0): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (layer_1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (layer_2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (layer_3): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (pred_net): Conv2d(256, 1, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "  )\n",
            "  (anchor_generator): AnchorGeneratorWithCenter(\n",
            "    (cell_anchors): BufferList()\n",
            "  )\n",
            "  (query_anchor_generator): AnchorGeneratorWithCenter(\n",
            "    (cell_anchors): BufferList()\n",
            "  )\n",
            ")\n",
            "\u001b[32m[04/15 01:35:58 fvcore.common.checkpoint]: \u001b[0mLoading checkpoint from work_dirs/visdrone_querydet/model_0007999.pth\n",
            "\u001b[32m[04/15 01:36:08 d2.data.common]: \u001b[0mSerializing 548 elements to byte tensors and concatenating them all ...\n",
            "\u001b[32m[04/15 01:36:08 d2.data.common]: \u001b[0mSerialized dataset takes 0.08 MiB\n",
            "/usr/local/lib/python3.7/site-packages/torch/utils/data/dataloader.py:477: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  cpuset_checked))\n",
            "\u001b[32m[04/15 01:36:08 d2.evaluation.evaluator]: \u001b[0mStart inference on 548 images\n",
            "\u001b[32m[04/15 01:36:34 d2.evaluation.evaluator]: \u001b[0mInference done 11/548. 2.2086 s / img. ETA=0:19:50\n",
            "\u001b[32m[04/15 01:36:41 d2.evaluation.evaluator]: \u001b[0mInference done 14/548. 2.2162 s / img. ETA=0:19:47\n",
            "\u001b[32m[04/15 01:36:47 d2.evaluation.evaluator]: \u001b[0mInference done 17/548. 2.2230 s / img. ETA=0:19:44\n",
            "\u001b[32m[04/15 01:36:54 d2.evaluation.evaluator]: \u001b[0mInference done 20/548. 2.2310 s / img. ETA=0:19:41\n",
            "\u001b[32m[04/15 01:37:01 d2.evaluation.evaluator]: \u001b[0mInference done 23/548. 2.2381 s / img. ETA=0:19:38\n",
            "\u001b[32m[04/15 01:37:08 d2.evaluation.evaluator]: \u001b[0mInference done 26/548. 2.2460 s / img. ETA=0:19:36\n",
            "\u001b[32m[04/15 01:37:15 d2.evaluation.evaluator]: \u001b[0mInference done 29/548. 2.2537 s / img. ETA=0:19:33\n",
            "\u001b[32m[04/15 01:37:22 d2.evaluation.evaluator]: \u001b[0mInference done 32/548. 2.2624 s / img. ETA=0:19:31\n",
            "\u001b[32m[04/15 01:37:29 d2.evaluation.evaluator]: \u001b[0mInference done 35/548. 2.2711 s / img. ETA=0:19:29\n",
            "\u001b[32m[04/15 01:37:36 d2.evaluation.evaluator]: \u001b[0mInference done 38/548. 2.2789 s / img. ETA=0:19:26\n",
            "\u001b[32m[04/15 01:37:43 d2.evaluation.evaluator]: \u001b[0mInference done 41/548. 2.2858 s / img. ETA=0:19:22\n",
            "\u001b[32m[04/15 01:37:50 d2.evaluation.evaluator]: \u001b[0mInference done 44/548. 2.2913 s / img. ETA=0:19:18\n",
            "\u001b[32m[04/15 01:37:57 d2.evaluation.evaluator]: \u001b[0mInference done 47/548. 2.2944 s / img. ETA=0:19:13\n",
            "\u001b[32m[04/15 01:38:04 d2.evaluation.evaluator]: \u001b[0mInference done 50/548. 2.2964 s / img. ETA=0:19:07\n",
            "\u001b[32m[04/15 01:38:11 d2.evaluation.evaluator]: \u001b[0mInference done 53/548. 2.2983 s / img. ETA=0:19:01\n",
            "\u001b[32m[04/15 01:38:18 d2.evaluation.evaluator]: \u001b[0mInference done 56/548. 2.2988 s / img. ETA=0:18:54\n",
            "\u001b[32m[04/15 01:38:25 d2.evaluation.evaluator]: \u001b[0mInference done 59/548. 2.2995 s / img. ETA=0:18:48\n",
            "\u001b[32m[04/15 01:38:32 d2.evaluation.evaluator]: \u001b[0mInference done 62/548. 2.3007 s / img. ETA=0:18:41\n",
            "\u001b[32m[04/15 01:38:39 d2.evaluation.evaluator]: \u001b[0mInference done 65/548. 2.3024 s / img. ETA=0:18:35\n",
            "\u001b[32m[04/15 01:38:46 d2.evaluation.evaluator]: \u001b[0mInference done 68/548. 2.3035 s / img. ETA=0:18:29\n",
            "\u001b[32m[04/15 01:38:53 d2.evaluation.evaluator]: \u001b[0mInference done 71/548. 2.3044 s / img. ETA=0:18:22\n",
            "\u001b[32m[04/15 01:39:00 d2.evaluation.evaluator]: \u001b[0mInference done 74/548. 2.3059 s / img. ETA=0:18:16\n",
            "\u001b[32m[04/15 01:39:07 d2.evaluation.evaluator]: \u001b[0mInference done 77/548. 2.3073 s / img. ETA=0:18:10\n",
            "\u001b[32m[04/15 01:39:14 d2.evaluation.evaluator]: \u001b[0mInference done 80/548. 2.3087 s / img. ETA=0:18:04\n",
            "\u001b[32m[04/15 01:39:21 d2.evaluation.evaluator]: \u001b[0mInference done 83/548. 2.3097 s / img. ETA=0:17:57\n",
            "\u001b[32m[04/15 01:39:28 d2.evaluation.evaluator]: \u001b[0mInference done 86/548. 2.3106 s / img. ETA=0:17:51\n",
            "\u001b[32m[04/15 01:39:35 d2.evaluation.evaluator]: \u001b[0mInference done 89/548. 2.3114 s / img. ETA=0:17:44\n",
            "\u001b[32m[04/15 01:39:42 d2.evaluation.evaluator]: \u001b[0mInference done 92/548. 2.3121 s / img. ETA=0:17:38\n",
            "\u001b[32m[04/15 01:39:49 d2.evaluation.evaluator]: \u001b[0mInference done 95/548. 2.3127 s / img. ETA=0:17:31\n",
            "\u001b[32m[04/15 01:39:56 d2.evaluation.evaluator]: \u001b[0mInference done 98/548. 2.3131 s / img. ETA=0:17:24\n",
            "\u001b[32m[04/15 01:40:03 d2.evaluation.evaluator]: \u001b[0mInference done 101/548. 2.3138 s / img. ETA=0:17:17\n",
            "\u001b[32m[04/15 01:40:10 d2.evaluation.evaluator]: \u001b[0mInference done 104/548. 2.3142 s / img. ETA=0:17:11\n",
            "\u001b[32m[04/15 01:40:17 d2.evaluation.evaluator]: \u001b[0mInference done 107/548. 2.3145 s / img. ETA=0:17:04\n",
            "\u001b[32m[04/15 01:40:24 d2.evaluation.evaluator]: \u001b[0mInference done 110/548. 2.3150 s / img. ETA=0:16:57\n",
            "\u001b[32m[04/15 01:40:31 d2.evaluation.evaluator]: \u001b[0mInference done 113/548. 2.3153 s / img. ETA=0:16:50\n",
            "\u001b[32m[04/15 01:40:39 d2.evaluation.evaluator]: \u001b[0mInference done 116/548. 2.3159 s / img. ETA=0:16:44\n",
            "\u001b[32m[04/15 01:40:46 d2.evaluation.evaluator]: \u001b[0mInference done 119/548. 2.3165 s / img. ETA=0:16:37\n",
            "\u001b[32m[04/15 01:40:53 d2.evaluation.evaluator]: \u001b[0mInference done 122/548. 2.3170 s / img. ETA=0:16:30\n",
            "\u001b[32m[04/15 01:41:00 d2.evaluation.evaluator]: \u001b[0mInference done 125/548. 2.3174 s / img. ETA=0:16:24\n",
            "\u001b[32m[04/15 01:41:07 d2.evaluation.evaluator]: \u001b[0mInference done 128/548. 2.3181 s / img. ETA=0:16:17\n",
            "\u001b[32m[04/15 01:41:14 d2.evaluation.evaluator]: \u001b[0mInference done 131/548. 2.3184 s / img. ETA=0:16:10\n",
            "\u001b[32m[04/15 01:41:21 d2.evaluation.evaluator]: \u001b[0mInference done 134/548. 2.3189 s / img. ETA=0:16:03\n",
            "\u001b[32m[04/15 01:41:28 d2.evaluation.evaluator]: \u001b[0mInference done 137/548. 2.3194 s / img. ETA=0:15:56\n",
            "\u001b[32m[04/15 01:41:35 d2.evaluation.evaluator]: \u001b[0mInference done 140/548. 2.3196 s / img. ETA=0:15:49\n",
            "\u001b[32m[04/15 01:41:42 d2.evaluation.evaluator]: \u001b[0mInference done 143/548. 2.3202 s / img. ETA=0:15:43\n",
            "\u001b[32m[04/15 01:41:49 d2.evaluation.evaluator]: \u001b[0mInference done 146/548. 2.3207 s / img. ETA=0:15:36\n",
            "\u001b[32m[04/15 01:41:56 d2.evaluation.evaluator]: \u001b[0mInference done 149/548. 2.3210 s / img. ETA=0:15:29\n",
            "\u001b[32m[04/15 01:42:03 d2.evaluation.evaluator]: \u001b[0mInference done 152/548. 2.3212 s / img. ETA=0:15:22\n",
            "\u001b[32m[04/15 01:42:10 d2.evaluation.evaluator]: \u001b[0mInference done 155/548. 2.3218 s / img. ETA=0:15:15\n",
            "\u001b[32m[04/15 01:42:17 d2.evaluation.evaluator]: \u001b[0mInference done 158/548. 2.3221 s / img. ETA=0:15:08\n",
            "\u001b[32m[04/15 01:42:24 d2.evaluation.evaluator]: \u001b[0mInference done 161/548. 2.3224 s / img. ETA=0:15:02\n",
            "\u001b[32m[04/15 01:42:31 d2.evaluation.evaluator]: \u001b[0mInference done 164/548. 2.3227 s / img. ETA=0:14:55\n",
            "\u001b[32m[04/15 01:42:38 d2.evaluation.evaluator]: \u001b[0mInference done 167/548. 2.3231 s / img. ETA=0:14:48\n",
            "\u001b[32m[04/15 01:42:45 d2.evaluation.evaluator]: \u001b[0mInference done 170/548. 2.3233 s / img. ETA=0:14:41\n",
            "\u001b[32m[04/15 01:42:52 d2.evaluation.evaluator]: \u001b[0mInference done 173/548. 2.3235 s / img. ETA=0:14:34\n",
            "\u001b[32m[04/15 01:42:59 d2.evaluation.evaluator]: \u001b[0mInference done 176/548. 2.3235 s / img. ETA=0:14:27\n",
            "\u001b[32m[04/15 01:43:06 d2.evaluation.evaluator]: \u001b[0mInference done 179/548. 2.3236 s / img. ETA=0:14:20\n",
            "\u001b[32m[04/15 01:43:13 d2.evaluation.evaluator]: \u001b[0mInference done 182/548. 2.3238 s / img. ETA=0:14:13\n",
            "\u001b[32m[04/15 01:43:20 d2.evaluation.evaluator]: \u001b[0mInference done 185/548. 2.3239 s / img. ETA=0:14:06\n",
            "\u001b[32m[04/15 01:43:27 d2.evaluation.evaluator]: \u001b[0mInference done 188/548. 2.3241 s / img. ETA=0:13:59\n",
            "\u001b[32m[04/15 01:43:34 d2.evaluation.evaluator]: \u001b[0mInference done 191/548. 2.3242 s / img. ETA=0:13:52\n",
            "\u001b[32m[04/15 01:43:41 d2.evaluation.evaluator]: \u001b[0mInference done 194/548. 2.3244 s / img. ETA=0:13:45\n",
            "\u001b[32m[04/15 01:43:49 d2.evaluation.evaluator]: \u001b[0mInference done 197/548. 2.3246 s / img. ETA=0:13:38\n",
            "\u001b[32m[04/15 01:43:55 d2.evaluation.evaluator]: \u001b[0mInference done 200/548. 2.3246 s / img. ETA=0:13:31\n",
            "\u001b[32m[04/15 01:44:03 d2.evaluation.evaluator]: \u001b[0mInference done 203/548. 2.3248 s / img. ETA=0:13:24\n",
            "\u001b[32m[04/15 01:44:10 d2.evaluation.evaluator]: \u001b[0mInference done 206/548. 2.3248 s / img. ETA=0:13:17\n",
            "\u001b[32m[04/15 01:44:17 d2.evaluation.evaluator]: \u001b[0mInference done 209/548. 2.3249 s / img. ETA=0:13:11\n",
            "\u001b[32m[04/15 01:44:24 d2.evaluation.evaluator]: \u001b[0mInference done 212/548. 2.3249 s / img. ETA=0:13:03\n",
            "\u001b[32m[04/15 01:44:31 d2.evaluation.evaluator]: \u001b[0mInference done 215/548. 2.3250 s / img. ETA=0:12:57\n",
            "\u001b[32m[04/15 01:44:38 d2.evaluation.evaluator]: \u001b[0mInference done 218/548. 2.3253 s / img. ETA=0:12:50\n",
            "\u001b[32m[04/15 01:44:45 d2.evaluation.evaluator]: \u001b[0mInference done 221/548. 2.3255 s / img. ETA=0:12:43\n",
            "\u001b[32m[04/15 01:44:52 d2.evaluation.evaluator]: \u001b[0mInference done 224/548. 2.3255 s / img. ETA=0:12:36\n",
            "\u001b[32m[04/15 01:44:59 d2.evaluation.evaluator]: \u001b[0mInference done 227/548. 2.3255 s / img. ETA=0:12:29\n",
            "\u001b[32m[04/15 01:45:06 d2.evaluation.evaluator]: \u001b[0mInference done 230/548. 2.3257 s / img. ETA=0:12:22\n",
            "\u001b[32m[04/15 01:45:13 d2.evaluation.evaluator]: \u001b[0mInference done 233/548. 2.3258 s / img. ETA=0:12:15\n",
            "\u001b[32m[04/15 01:45:20 d2.evaluation.evaluator]: \u001b[0mInference done 236/548. 2.3259 s / img. ETA=0:12:08\n",
            "\u001b[32m[04/15 01:45:27 d2.evaluation.evaluator]: \u001b[0mInference done 239/548. 2.3260 s / img. ETA=0:12:01\n",
            "\u001b[32m[04/15 01:45:34 d2.evaluation.evaluator]: \u001b[0mInference done 242/548. 2.3262 s / img. ETA=0:11:54\n",
            "\u001b[32m[04/15 01:45:41 d2.evaluation.evaluator]: \u001b[0mInference done 245/548. 2.3263 s / img. ETA=0:11:47\n",
            "\u001b[32m[04/15 01:45:48 d2.evaluation.evaluator]: \u001b[0mInference done 248/548. 2.3264 s / img. ETA=0:11:40\n",
            "\u001b[32m[04/15 01:45:55 d2.evaluation.evaluator]: \u001b[0mInference done 251/548. 2.3264 s / img. ETA=0:11:33\n",
            "\u001b[32m[04/15 01:46:02 d2.evaluation.evaluator]: \u001b[0mInference done 254/548. 2.3266 s / img. ETA=0:11:26\n",
            "\u001b[32m[04/15 01:46:09 d2.evaluation.evaluator]: \u001b[0mInference done 257/548. 2.3267 s / img. ETA=0:11:19\n",
            "\u001b[32m[04/15 01:46:16 d2.evaluation.evaluator]: \u001b[0mInference done 260/548. 2.3267 s / img. ETA=0:11:12\n",
            "\u001b[32m[04/15 01:46:23 d2.evaluation.evaluator]: \u001b[0mInference done 263/548. 2.3270 s / img. ETA=0:11:05\n",
            "\u001b[32m[04/15 01:46:30 d2.evaluation.evaluator]: \u001b[0mInference done 266/548. 2.3271 s / img. ETA=0:10:58\n",
            "\u001b[32m[04/15 01:46:37 d2.evaluation.evaluator]: \u001b[0mInference done 269/548. 2.3273 s / img. ETA=0:10:51\n",
            "\u001b[32m[04/15 01:46:44 d2.evaluation.evaluator]: \u001b[0mInference done 272/548. 2.3275 s / img. ETA=0:10:44\n",
            "\u001b[32m[04/15 01:46:51 d2.evaluation.evaluator]: \u001b[0mInference done 275/548. 2.3276 s / img. ETA=0:10:37\n",
            "\u001b[32m[04/15 01:46:58 d2.evaluation.evaluator]: \u001b[0mInference done 278/548. 2.3276 s / img. ETA=0:10:30\n",
            "\u001b[32m[04/15 01:47:05 d2.evaluation.evaluator]: \u001b[0mInference done 281/548. 2.3277 s / img. ETA=0:10:23\n",
            "\u001b[32m[04/15 01:47:12 d2.evaluation.evaluator]: \u001b[0mInference done 284/548. 2.3278 s / img. ETA=0:10:16\n",
            "\u001b[32m[04/15 01:47:19 d2.evaluation.evaluator]: \u001b[0mInference done 287/548. 2.3278 s / img. ETA=0:10:09\n",
            "\u001b[32m[04/15 01:47:26 d2.evaluation.evaluator]: \u001b[0mInference done 290/548. 2.3279 s / img. ETA=0:10:02\n",
            "\u001b[32m[04/15 01:47:33 d2.evaluation.evaluator]: \u001b[0mInference done 293/548. 2.3281 s / img. ETA=0:09:55\n",
            "\u001b[32m[04/15 01:47:41 d2.evaluation.evaluator]: \u001b[0mInference done 296/548. 2.3282 s / img. ETA=0:09:48\n",
            "\u001b[32m[04/15 01:47:48 d2.evaluation.evaluator]: \u001b[0mInference done 299/548. 2.3283 s / img. ETA=0:09:41\n",
            "\u001b[32m[04/15 01:47:55 d2.evaluation.evaluator]: \u001b[0mInference done 302/548. 2.3284 s / img. ETA=0:09:34\n",
            "\u001b[32m[04/15 01:48:02 d2.evaluation.evaluator]: \u001b[0mInference done 305/548. 2.3285 s / img. ETA=0:09:27\n",
            "\u001b[32m[04/15 01:48:09 d2.evaluation.evaluator]: \u001b[0mInference done 308/548. 2.3285 s / img. ETA=0:09:20\n",
            "\u001b[32m[04/15 01:48:16 d2.evaluation.evaluator]: \u001b[0mInference done 311/548. 2.3286 s / img. ETA=0:09:13\n",
            "\u001b[32m[04/15 01:48:23 d2.evaluation.evaluator]: \u001b[0mInference done 314/548. 2.3288 s / img. ETA=0:09:06\n",
            "\u001b[32m[04/15 01:48:30 d2.evaluation.evaluator]: \u001b[0mInference done 317/548. 2.3288 s / img. ETA=0:08:59\n",
            "\u001b[32m[04/15 01:48:37 d2.evaluation.evaluator]: \u001b[0mInference done 320/548. 2.3288 s / img. ETA=0:08:52\n",
            "\u001b[32m[04/15 01:48:44 d2.evaluation.evaluator]: \u001b[0mInference done 323/548. 2.3290 s / img. ETA=0:08:45\n",
            "\u001b[32m[04/15 01:48:51 d2.evaluation.evaluator]: \u001b[0mInference done 326/548. 2.3291 s / img. ETA=0:08:38\n",
            "\u001b[32m[04/15 01:48:58 d2.evaluation.evaluator]: \u001b[0mInference done 329/548. 2.3291 s / img. ETA=0:08:32\n",
            "\u001b[32m[04/15 01:49:05 d2.evaluation.evaluator]: \u001b[0mInference done 332/548. 2.3292 s / img. ETA=0:08:25\n",
            "\u001b[32m[04/15 01:49:12 d2.evaluation.evaluator]: \u001b[0mInference done 335/548. 2.3294 s / img. ETA=0:08:18\n",
            "\u001b[32m[04/15 01:49:19 d2.evaluation.evaluator]: \u001b[0mInference done 338/548. 2.3294 s / img. ETA=0:08:11\n",
            "\u001b[32m[04/15 01:49:26 d2.evaluation.evaluator]: \u001b[0mInference done 341/548. 2.3295 s / img. ETA=0:08:04\n",
            "\u001b[32m[04/15 01:49:33 d2.evaluation.evaluator]: \u001b[0mInference done 344/548. 2.3296 s / img. ETA=0:07:57\n",
            "\u001b[32m[04/15 01:49:40 d2.evaluation.evaluator]: \u001b[0mInference done 347/548. 2.3297 s / img. ETA=0:07:50\n",
            "\u001b[32m[04/15 01:49:47 d2.evaluation.evaluator]: \u001b[0mInference done 350/548. 2.3297 s / img. ETA=0:07:43\n",
            "\u001b[32m[04/15 01:49:54 d2.evaluation.evaluator]: \u001b[0mInference done 353/548. 2.3297 s / img. ETA=0:07:36\n",
            "\u001b[32m[04/15 01:50:01 d2.evaluation.evaluator]: \u001b[0mInference done 356/548. 2.3297 s / img. ETA=0:07:28\n",
            "\u001b[32m[04/15 01:50:08 d2.evaluation.evaluator]: \u001b[0mInference done 359/548. 2.3299 s / img. ETA=0:07:22\n",
            "\u001b[32m[04/15 01:50:15 d2.evaluation.evaluator]: \u001b[0mInference done 362/548. 2.3299 s / img. ETA=0:07:14\n",
            "\u001b[32m[04/15 01:50:22 d2.evaluation.evaluator]: \u001b[0mInference done 365/548. 2.3299 s / img. ETA=0:07:07\n",
            "\u001b[32m[04/15 01:50:30 d2.evaluation.evaluator]: \u001b[0mInference done 368/548. 2.3300 s / img. ETA=0:07:00\n",
            "\u001b[32m[04/15 01:50:37 d2.evaluation.evaluator]: \u001b[0mInference done 371/548. 2.3300 s / img. ETA=0:06:53\n",
            "\u001b[32m[04/15 01:50:44 d2.evaluation.evaluator]: \u001b[0mInference done 374/548. 2.3300 s / img. ETA=0:06:46\n",
            "\u001b[32m[04/15 01:50:51 d2.evaluation.evaluator]: \u001b[0mInference done 377/548. 2.3301 s / img. ETA=0:06:39\n",
            "\u001b[32m[04/15 01:50:58 d2.evaluation.evaluator]: \u001b[0mInference done 380/548. 2.3301 s / img. ETA=0:06:32\n",
            "\u001b[32m[04/15 01:51:05 d2.evaluation.evaluator]: \u001b[0mInference done 383/548. 2.3301 s / img. ETA=0:06:25\n",
            "\u001b[32m[04/15 01:51:12 d2.evaluation.evaluator]: \u001b[0mInference done 386/548. 2.3301 s / img. ETA=0:06:18\n",
            "\u001b[32m[04/15 01:51:19 d2.evaluation.evaluator]: \u001b[0mInference done 389/548. 2.3301 s / img. ETA=0:06:11\n",
            "\u001b[32m[04/15 01:51:26 d2.evaluation.evaluator]: \u001b[0mInference done 392/548. 2.3301 s / img. ETA=0:06:04\n",
            "\u001b[32m[04/15 01:51:33 d2.evaluation.evaluator]: \u001b[0mInference done 395/548. 2.3301 s / img. ETA=0:05:57\n",
            "\u001b[32m[04/15 01:51:40 d2.evaluation.evaluator]: \u001b[0mInference done 398/548. 2.3301 s / img. ETA=0:05:50\n",
            "\u001b[32m[04/15 01:51:47 d2.evaluation.evaluator]: \u001b[0mInference done 401/548. 2.3302 s / img. ETA=0:05:43\n",
            "\u001b[32m[04/15 01:51:54 d2.evaluation.evaluator]: \u001b[0mInference done 404/548. 2.3302 s / img. ETA=0:05:36\n",
            "\u001b[32m[04/15 01:52:01 d2.evaluation.evaluator]: \u001b[0mInference done 407/548. 2.3303 s / img. ETA=0:05:29\n",
            "\u001b[32m[04/15 01:52:08 d2.evaluation.evaluator]: \u001b[0mInference done 410/548. 2.3303 s / img. ETA=0:05:22\n",
            "\u001b[32m[04/15 01:52:15 d2.evaluation.evaluator]: \u001b[0mInference done 413/548. 2.3303 s / img. ETA=0:05:15\n",
            "\u001b[32m[04/15 01:52:22 d2.evaluation.evaluator]: \u001b[0mInference done 416/548. 2.3304 s / img. ETA=0:05:08\n",
            "\u001b[32m[04/15 01:52:29 d2.evaluation.evaluator]: \u001b[0mInference done 419/548. 2.3304 s / img. ETA=0:05:01\n",
            "\u001b[32m[04/15 01:52:36 d2.evaluation.evaluator]: \u001b[0mInference done 422/548. 2.3305 s / img. ETA=0:04:54\n",
            "\u001b[32m[04/15 01:52:43 d2.evaluation.evaluator]: \u001b[0mInference done 425/548. 2.3306 s / img. ETA=0:04:47\n",
            "\u001b[32m[04/15 01:52:50 d2.evaluation.evaluator]: \u001b[0mInference done 428/548. 2.3306 s / img. ETA=0:04:40\n",
            "\u001b[32m[04/15 01:52:57 d2.evaluation.evaluator]: \u001b[0mInference done 431/548. 2.3306 s / img. ETA=0:04:33\n",
            "\u001b[32m[04/15 01:53:04 d2.evaluation.evaluator]: \u001b[0mInference done 434/548. 2.3307 s / img. ETA=0:04:26\n",
            "\u001b[32m[04/15 01:53:11 d2.evaluation.evaluator]: \u001b[0mInference done 437/548. 2.3307 s / img. ETA=0:04:19\n",
            "\u001b[32m[04/15 01:53:18 d2.evaluation.evaluator]: \u001b[0mInference done 440/548. 2.3308 s / img. ETA=0:04:12\n",
            "\u001b[32m[04/15 01:53:25 d2.evaluation.evaluator]: \u001b[0mInference done 443/548. 2.3308 s / img. ETA=0:04:05\n",
            "\u001b[32m[04/15 01:53:32 d2.evaluation.evaluator]: \u001b[0mInference done 446/548. 2.3308 s / img. ETA=0:03:58\n",
            "\u001b[32m[04/15 01:53:39 d2.evaluation.evaluator]: \u001b[0mInference done 449/548. 2.3309 s / img. ETA=0:03:51\n",
            "\u001b[32m[04/15 01:53:46 d2.evaluation.evaluator]: \u001b[0mInference done 452/548. 2.3310 s / img. ETA=0:03:44\n",
            "\u001b[32m[04/15 01:53:54 d2.evaluation.evaluator]: \u001b[0mInference done 455/548. 2.3311 s / img. ETA=0:03:37\n",
            "\u001b[32m[04/15 01:54:01 d2.evaluation.evaluator]: \u001b[0mInference done 458/548. 2.3312 s / img. ETA=0:03:30\n",
            "\u001b[32m[04/15 01:54:08 d2.evaluation.evaluator]: \u001b[0mInference done 461/548. 2.3312 s / img. ETA=0:03:23\n",
            "\u001b[32m[04/15 01:54:15 d2.evaluation.evaluator]: \u001b[0mInference done 464/548. 2.3312 s / img. ETA=0:03:16\n",
            "\u001b[32m[04/15 01:54:22 d2.evaluation.evaluator]: \u001b[0mInference done 467/548. 2.3312 s / img. ETA=0:03:09\n",
            "\u001b[32m[04/15 01:54:29 d2.evaluation.evaluator]: \u001b[0mInference done 470/548. 2.3312 s / img. ETA=0:03:02\n",
            "\u001b[32m[04/15 01:54:36 d2.evaluation.evaluator]: \u001b[0mInference done 473/548. 2.3312 s / img. ETA=0:02:55\n",
            "\u001b[32m[04/15 01:54:43 d2.evaluation.evaluator]: \u001b[0mInference done 476/548. 2.3313 s / img. ETA=0:02:48\n",
            "\u001b[32m[04/15 01:54:50 d2.evaluation.evaluator]: \u001b[0mInference done 479/548. 2.3313 s / img. ETA=0:02:41\n",
            "\u001b[32m[04/15 01:54:57 d2.evaluation.evaluator]: \u001b[0mInference done 482/548. 2.3313 s / img. ETA=0:02:34\n",
            "\u001b[32m[04/15 01:55:04 d2.evaluation.evaluator]: \u001b[0mInference done 485/548. 2.3312 s / img. ETA=0:02:27\n",
            "\u001b[32m[04/15 01:55:11 d2.evaluation.evaluator]: \u001b[0mInference done 488/548. 2.3313 s / img. ETA=0:02:20\n",
            "\u001b[32m[04/15 01:55:18 d2.evaluation.evaluator]: \u001b[0mInference done 491/548. 2.3313 s / img. ETA=0:02:13\n",
            "\u001b[32m[04/15 01:55:25 d2.evaluation.evaluator]: \u001b[0mInference done 494/548. 2.3313 s / img. ETA=0:02:06\n",
            "\u001b[32m[04/15 01:55:32 d2.evaluation.evaluator]: \u001b[0mInference done 497/548. 2.3314 s / img. ETA=0:01:59\n",
            "\u001b[32m[04/15 01:55:39 d2.evaluation.evaluator]: \u001b[0mInference done 500/548. 2.3314 s / img. ETA=0:01:52\n",
            "\u001b[32m[04/15 01:55:46 d2.evaluation.evaluator]: \u001b[0mInference done 503/548. 2.3315 s / img. ETA=0:01:45\n",
            "\u001b[32m[04/15 01:55:53 d2.evaluation.evaluator]: \u001b[0mInference done 506/548. 2.3316 s / img. ETA=0:01:38\n",
            "\u001b[32m[04/15 01:56:00 d2.evaluation.evaluator]: \u001b[0mInference done 509/548. 2.3316 s / img. ETA=0:01:31\n",
            "\u001b[32m[04/15 01:56:07 d2.evaluation.evaluator]: \u001b[0mInference done 512/548. 2.3316 s / img. ETA=0:01:24\n",
            "\u001b[32m[04/15 01:56:14 d2.evaluation.evaluator]: \u001b[0mInference done 515/548. 2.3317 s / img. ETA=0:01:17\n",
            "\u001b[32m[04/15 01:56:21 d2.evaluation.evaluator]: \u001b[0mInference done 518/548. 2.3316 s / img. ETA=0:01:10\n",
            "\u001b[32m[04/15 01:56:28 d2.evaluation.evaluator]: \u001b[0mInference done 521/548. 2.3316 s / img. ETA=0:01:03\n",
            "\u001b[32m[04/15 01:56:35 d2.evaluation.evaluator]: \u001b[0mInference done 524/548. 2.3316 s / img. ETA=0:00:56\n",
            "\u001b[32m[04/15 01:56:42 d2.evaluation.evaluator]: \u001b[0mInference done 527/548. 2.3317 s / img. ETA=0:00:49\n",
            "\u001b[32m[04/15 01:56:49 d2.evaluation.evaluator]: \u001b[0mInference done 530/548. 2.3317 s / img. ETA=0:00:42\n",
            "\u001b[32m[04/15 01:56:56 d2.evaluation.evaluator]: \u001b[0mInference done 533/548. 2.3318 s / img. ETA=0:00:35\n",
            "\u001b[32m[04/15 01:57:03 d2.evaluation.evaluator]: \u001b[0mInference done 536/548. 2.3318 s / img. ETA=0:00:28\n",
            "\u001b[32m[04/15 01:57:11 d2.evaluation.evaluator]: \u001b[0mInference done 539/548. 2.3318 s / img. ETA=0:00:21\n",
            "\u001b[32m[04/15 01:57:18 d2.evaluation.evaluator]: \u001b[0mInference done 542/548. 2.3319 s / img. ETA=0:00:14\n",
            "\u001b[32m[04/15 01:57:25 d2.evaluation.evaluator]: \u001b[0mInference done 545/548. 2.3319 s / img. ETA=0:00:07\n",
            "\u001b[32m[04/15 01:57:32 d2.evaluation.evaluator]: \u001b[0mInference done 548/548. 2.3319 s / img. ETA=0:00:00\n",
            "\u001b[32m[04/15 01:57:34 d2.evaluation.evaluator]: \u001b[0mTotal inference time: 0:21:13.527881 (2.345355 s / img per device, on 1 devices)\n",
            "\u001b[32m[04/15 01:57:34 d2.evaluation.evaluator]: \u001b[0mTotal inference pure compute time: 0:21:06 (2.331910 s / img per device, on 1 devices)\n",
            "\u001b[32m[04/15 01:57:39 d2.evaluation.testing]: \u001b[0mcopypaste: Task: GPU_Speed\n",
            "\u001b[32m[04/15 01:57:39 d2.evaluation.testing]: \u001b[0mcopypaste: Mean_FPS,Std_FPS,Max_FPS,Min_FPS,Mid_FPS\n",
            "\u001b[32m[04/15 01:57:39 d2.evaluation.testing]: \u001b[0mcopypaste: 0.4316,0.0048,0.4609,0.4236,0.4309\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!bash eval_visdrone.sh work_dirs/model_test-3/visdrone_infer.json"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cwOAPy6HjNiB",
        "outputId": "888cf1ce-a028-447c-a280-3dc1a6b0c1bd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "bash: eval_visdrone.sh: No such file or directory\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "rrcvAIjujeE4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "gRffknWrjNe_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!python infer_visdrone.py --config-file configs/visdrone/querydet_test.yaml --num-gpu 1 --eval-only MODEL.WEIGHTS work_dirs/visdrone_querydet/model_0005999.pth OUTPUT_DIR work_dirs/model_test-4"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AJlIMR6jrzaY",
        "outputId": "d07a1c63-ab29-4604-e2ad-bf49b74a59d5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "** fvcore version of PathManager will be deprecated soon. **\n",
            "** Please migrate to the version in iopath repo. **\n",
            "https://github.com/facebookresearch/iopath \n",
            "\n",
            "Command Line Args: Namespace(config_file='configs/visdrone/querydet_test.yaml', dist_url='tcp://127.0.0.1:49152', eval_only=True, machine_rank=0, no_pretrain=False, num_gpus=1, num_machines=1, opts=['MODEL.WEIGHTS', 'work_dirs/visdrone_querydet/model_0005999.pth', 'OUTPUT_DIR', 'work_dirs/model_test-4'], resume=False)\n",
            "Loading config configs/visdrone/querydet_test.yaml with yaml.unsafe_load. Your machine may be at risk if the file contains malicious content.\n",
            "Loading config configs/visdrone/../BaseRetina.yaml with yaml.unsafe_load. Your machine may be at risk if the file contains malicious content.\n",
            "\u001b[32m[04/15 06:52:53 detectron2]: \u001b[0mRank of current process: 0. World size: 1\n",
            "\u001b[32m[04/15 06:52:55 detectron2]: \u001b[0mEnvironment info:\n",
            "----------------------  ---------------------------------------------------------------\n",
            "sys.platform            linux\n",
            "Python                  3.7.6 (default, Jan  8 2020, 19:59:22) [GCC 7.3.0]\n",
            "numpy                   1.21.6\n",
            "detectron2              0.4 @/usr/local/lib/python3.7/site-packages/detectron2\n",
            "Compiler                GCC 7.3\n",
            "CUDA compiler           CUDA 10.1\n",
            "detectron2 arch flags   3.7, 5.0, 5.2, 6.0, 6.1, 7.0, 7.5\n",
            "DETECTRON2_ENV_MODULE   <not set>\n",
            "PyTorch                 1.8.1+cu101 @/usr/local/lib/python3.7/site-packages/torch\n",
            "PyTorch debug build     False\n",
            "GPU available           True\n",
            "GPU 0                   Tesla T4 (arch=7.5)\n",
            "CUDA_HOME               /usr/local/cuda\n",
            "Pillow                  9.5.0\n",
            "torchvision             0.9.1+cu101 @/usr/local/lib/python3.7/site-packages/torchvision\n",
            "torchvision arch flags  3.5, 5.0, 6.0, 7.0, 7.5\n",
            "fvcore                  0.1.3.post20210317\n",
            "cv2                     4.7.0\n",
            "----------------------  ---------------------------------------------------------------\n",
            "PyTorch built with:\n",
            "  - GCC 7.3\n",
            "  - C++ Version: 201402\n",
            "  - Intel(R) Math Kernel Library Version 2020.0.0 Product Build 20191122 for Intel(R) 64 architecture applications\n",
            "  - Intel(R) MKL-DNN v1.7.0 (Git Hash 7aed236906b1f7a05c0917e5257a1af05e9ff683)\n",
            "  - OpenMP 201511 (a.k.a. OpenMP 4.5)\n",
            "  - NNPACK is enabled\n",
            "  - CPU capability usage: AVX2\n",
            "  - CUDA Runtime 10.1\n",
            "  - NVCC architecture flags: -gencode;arch=compute_37,code=sm_37;-gencode;arch=compute_50,code=sm_50;-gencode;arch=compute_60,code=sm_60;-gencode;arch=compute_70,code=sm_70\n",
            "  - CuDNN 7.6.3\n",
            "  - Magma 2.5.2\n",
            "  - Build settings: BLAS_INFO=mkl, BUILD_TYPE=Release, CUDA_VERSION=10.1, CUDNN_VERSION=7.6.3, CXX_COMPILER=/opt/rh/devtoolset-7/root/usr/bin/c++, CXX_FLAGS= -Wno-deprecated -fvisibility-inlines-hidden -DUSE_PTHREADPOOL -fopenmp -DNDEBUG -DUSE_KINETO -DUSE_FBGEMM -DUSE_QNNPACK -DUSE_PYTORCH_QNNPACK -DUSE_XNNPACK -O2 -fPIC -Wno-narrowing -Wall -Wextra -Werror=return-type -Wno-missing-field-initializers -Wno-type-limits -Wno-array-bounds -Wno-unknown-pragmas -Wno-sign-compare -Wno-unused-parameter -Wno-unused-variable -Wno-unused-function -Wno-unused-result -Wno-unused-local-typedefs -Wno-strict-overflow -Wno-strict-aliasing -Wno-error=deprecated-declarations -Wno-stringop-overflow -Wno-psabi -Wno-error=pedantic -Wno-error=redundant-decls -Wno-error=old-style-cast -fdiagnostics-color=always -faligned-new -Wno-unused-but-set-variable -Wno-maybe-uninitialized -fno-math-errno -fno-trapping-math -Werror=format -Wno-stringop-overflow, LAPACK_INFO=mkl, PERF_WITH_AVX=1, PERF_WITH_AVX2=1, PERF_WITH_AVX512=1, TORCH_VERSION=1.8.1, USE_CUDA=ON, USE_CUDNN=ON, USE_EXCEPTION_PTR=1, USE_GFLAGS=OFF, USE_GLOG=OFF, USE_MKL=ON, USE_MKLDNN=ON, USE_MPI=OFF, USE_NCCL=ON, USE_NNPACK=ON, USE_OPENMP=ON, \n",
            "\n",
            "\u001b[32m[04/15 06:52:55 detectron2]: \u001b[0mCommand line arguments: Namespace(config_file='configs/visdrone/querydet_test.yaml', dist_url='tcp://127.0.0.1:49152', eval_only=True, machine_rank=0, no_pretrain=False, num_gpus=1, num_machines=1, opts=['MODEL.WEIGHTS', 'work_dirs/visdrone_querydet/model_0005999.pth', 'OUTPUT_DIR', 'work_dirs/model_test-4'], resume=False)\n",
            "\u001b[32m[04/15 06:52:55 detectron2]: \u001b[0mContents of args.config_file=configs/visdrone/querydet_test.yaml:\n",
            "_BASE_: \"../BaseRetina.yaml\"\n",
            "OUTPUT_DIR: \"work_dirs/model_test\"\n",
            "\n",
            "MODEL:\n",
            "  META_ARCHITECTURE: \"RetinaNetQueryDet\"\n",
            "  WEIGHTS: \"detectron2://ImageNetPretrained/MSRA/R-50.pkl\"\n",
            "  \n",
            "  RESNETS:\n",
            "    DEPTH: 50\n",
            "  \n",
            "  ANCHOR_GENERATOR:\n",
            "    NAME: \"AnchorGeneratorWithCenter\"\n",
            "    SIZES: !!python/object/apply:eval [\"[[x, x * 2**(1.0/3), x * 2**(2.0/3)] for x in [16, 32, 64, 128, 256, 512]]\"]\n",
            "  \n",
            "  RETINANET:\n",
            "    IOU_THRESHOLDS: [0.4, 0.5]\n",
            "    IOU_LABELS: [0, -1, 1]\n",
            "    NUM_CLASSES: 10\n",
            "    IN_FEATURES: [\"p2\", \"p3\", \"p4\", \"p5\", \"p6\", \"p7\"]  \n",
            "    SCORE_THRESH_TEST: 0.0001\n",
            "    \n",
            "  RESNETS:\n",
            "    OUT_FEATURES: [\"res2\", \"res3\", \"res4\", \"res5\"]\n",
            "\n",
            "  FPN:\n",
            "    IN_FEATURES: [\"res2\", \"res3\", \"res4\", \"res5\"]\n",
            "  \n",
            "  QUERY:\n",
            "    FEATURES_WHOLE_TEST: [2, 3, 4, 5]\n",
            "    FEATURES_VALUE_TEST: [0, 1]\n",
            "    Q_FEATURE_TRAIN: [1, 2]\n",
            "    Q_FEATURE_TEST: [1, 2]\n",
            "    \n",
            "    ENCODE_CENTER_DIS_COEFF: [1., 1.]\n",
            "    ENCODE_SMALL_OBJ_SCALE: [[0, 32], [0, 64]]\n",
            "\n",
            "    THRESHOLD: 0.12\n",
            "    QUERY_INFER: False\n",
            "  \n",
            "  CUSTOM: \n",
            "    USE_SOFT_NMS: False\n",
            "    SOFT_NMS_METHOD: 'gaussian'\n",
            "    SOFT_NMS_SIGMA: 0.6\n",
            "    SOFT_NMS_THRESHOLD: 0.4\n",
            "    SOFT_NMS_PRUND: 0.0001\n",
            "\n",
            "VISDRONE:\n",
            "  TEST_LENGTH: 3999\n",
            "\n",
            "TEST:  \n",
            "  DETECTIONS_PER_IMAGE: 500\n",
            "\n",
            "META_INFO:\n",
            "  EVAL_GPU_TIME: True\n",
            "\n",
            "\u001b[32m[04/15 06:52:55 detectron2]: \u001b[0mRunning with full config:\n",
            "CUDNN_BENCHMARK: False\n",
            "DATALOADER:\n",
            "  ASPECT_RATIO_GROUPING: True\n",
            "  FILTER_EMPTY_ANNOTATIONS: True\n",
            "  NUM_WORKERS: 4\n",
            "  REPEAT_THRESHOLD: 0.0\n",
            "  SAMPLER_TRAIN: TrainingSampler\n",
            "DATASETS:\n",
            "  PRECOMPUTED_PROPOSAL_TOPK_TEST: 1000\n",
            "  PRECOMPUTED_PROPOSAL_TOPK_TRAIN: 2000\n",
            "  PROPOSAL_FILES_TEST: ()\n",
            "  PROPOSAL_FILES_TRAIN: ()\n",
            "  TEST: ('coco_2017_val',)\n",
            "  TRAIN: ('coco_2017_train',)\n",
            "GLOBAL:\n",
            "  HACK: 1.0\n",
            "INPUT:\n",
            "  CROP:\n",
            "    ENABLED: False\n",
            "    SIZE: [0.9, 0.9]\n",
            "    TYPE: relative_range\n",
            "  FORMAT: BGR\n",
            "  MASK_FORMAT: polygon\n",
            "  MAX_SIZE_TEST: 1333\n",
            "  MAX_SIZE_TRAIN: 1333\n",
            "  MIN_SIZE_TEST: 800\n",
            "  MIN_SIZE_TRAIN: (640, 672, 704, 736, 768, 800)\n",
            "  MIN_SIZE_TRAIN_SAMPLING: choice\n",
            "  RANDOM_FLIP: horizontal\n",
            "META_INFO:\n",
            "  EVAL_AP: True\n",
            "  EVAL_GPU_TIME: True\n",
            "  VIS_ROOT: \n",
            "MODEL:\n",
            "  ANCHOR_GENERATOR:\n",
            "    ANGLES: [[-90, 0, 90]]\n",
            "    ASPECT_RATIOS: [[0.5, 1.0, 2.0]]\n",
            "    NAME: AnchorGeneratorWithCenter\n",
            "    OFFSET: 0.0\n",
            "    SIZES: [[16, 20.15873679831797, 25.39841683149119], [32, 40.31747359663594, 50.79683366298238], [64, 80.63494719327188, 101.59366732596476], [128, 161.26989438654377, 203.18733465192952], [256, 322.53978877308754, 406.37466930385904], [512, 645.0795775461751, 812.7493386077181]]\n",
            "  BACKBONE:\n",
            "    FREEZE_AT: 2\n",
            "    NAME: build_retinanet_resnet_fpn_backbone\n",
            "  CUSTOM:\n",
            "    CLEAR_CUDA_CACHE: False\n",
            "    CLS_WEIGHTS: []\n",
            "    FOCAL_LOSS_ALPHAS: []\n",
            "    FOCAL_LOSS_GAMMAS: []\n",
            "    GIOU_LOSS: False\n",
            "    GRADIENT_CHECKPOINT: False\n",
            "    HEAD_BN: False\n",
            "    REG_WEIGHTS: []\n",
            "    SOFT_NMS_METHOD: gaussian\n",
            "    SOFT_NMS_PRUND: 0.0001\n",
            "    SOFT_NMS_SIGMA: 0.6\n",
            "    SOFT_NMS_THRESHOLD: 0.4\n",
            "    USE_LOOP_MATCHER: False\n",
            "    USE_SOFT_NMS: False\n",
            "  DEVICE: cuda\n",
            "  FPN:\n",
            "    FUSE_TYPE: sum\n",
            "    IN_FEATURES: ['res2', 'res3', 'res4', 'res5']\n",
            "    NORM: \n",
            "    OUT_CHANNELS: 256\n",
            "    TOP_LEVELS: 2\n",
            "  KEYPOINT_ON: False\n",
            "  LOAD_PROPOSALS: False\n",
            "  MASK_ON: False\n",
            "  META_ARCHITECTURE: RetinaNetQueryDet\n",
            "  PANOPTIC_FPN:\n",
            "    COMBINE:\n",
            "      ENABLED: True\n",
            "      INSTANCES_CONFIDENCE_THRESH: 0.5\n",
            "      OVERLAP_THRESH: 0.5\n",
            "      STUFF_AREA_LIMIT: 4096\n",
            "    INSTANCE_LOSS_WEIGHT: 1.0\n",
            "  PIXEL_MEAN: [103.53, 116.28, 123.675]\n",
            "  PIXEL_STD: [1.0, 1.0, 1.0]\n",
            "  PROPOSAL_GENERATOR:\n",
            "    MIN_SIZE: 0\n",
            "    NAME: RPN\n",
            "  QUERY:\n",
            "    CONTEXT: 2\n",
            "    ENCODE_CENTER_DIS_COEFF: [1.0, 1.0]\n",
            "    ENCODE_SMALL_OBJ_SCALE: [[0, 32], [0, 64]]\n",
            "    FEATURES_VALUE_TEST: [0, 1]\n",
            "    FEATURES_VALUE_TRAIN: [0, 1]\n",
            "    FEATURES_WHOLE_TEST: [2, 3, 4, 5]\n",
            "    FEATURES_WHOLE_TRAIN: [2, 3, 4, 5]\n",
            "    QUERY_INFER: False\n",
            "    QUERY_LOSS_GAMMA: []\n",
            "    QUERY_LOSS_WEIGHT: []\n",
            "    Q_FEATURE_TEST: [1, 2]\n",
            "    Q_FEATURE_TRAIN: [1, 2]\n",
            "    THRESHOLD: 0.12\n",
            "  RESNETS:\n",
            "    DEFORM_MODULATED: False\n",
            "    DEFORM_NUM_GROUPS: 1\n",
            "    DEFORM_ON_PER_STAGE: [False, False, False, False]\n",
            "    DEPTH: 50\n",
            "    NORM: FrozenBN\n",
            "    NUM_GROUPS: 1\n",
            "    OUT_FEATURES: ['res2', 'res3', 'res4', 'res5']\n",
            "    RES2_OUT_CHANNELS: 256\n",
            "    RES5_DILATION: 1\n",
            "    STEM_OUT_CHANNELS: 64\n",
            "    STRIDE_IN_1X1: True\n",
            "    WIDTH_PER_GROUP: 64\n",
            "  RETINANET:\n",
            "    BBOX_REG_LOSS_TYPE: smooth_l1\n",
            "    BBOX_REG_WEIGHTS: (1.0, 1.0, 1.0, 1.0)\n",
            "    FOCAL_LOSS_ALPHA: 0.25\n",
            "    FOCAL_LOSS_GAMMA: 2.0\n",
            "    IN_FEATURES: ['p2', 'p3', 'p4', 'p5', 'p6', 'p7']\n",
            "    IOU_LABELS: [0, -1, 1]\n",
            "    IOU_THRESHOLDS: [0.4, 0.5]\n",
            "    NMS_THRESH_TEST: 0.5\n",
            "    NORM: \n",
            "    NUM_CLASSES: 10\n",
            "    NUM_CONVS: 4\n",
            "    PRIOR_PROB: 0.01\n",
            "    SCORE_THRESH_TEST: 0.0001\n",
            "    SMOOTH_L1_LOSS_BETA: 0.1\n",
            "    TOPK_CANDIDATES_TEST: 1000\n",
            "  ROI_BOX_CASCADE_HEAD:\n",
            "    BBOX_REG_WEIGHTS: ((10.0, 10.0, 5.0, 5.0), (20.0, 20.0, 10.0, 10.0), (30.0, 30.0, 15.0, 15.0))\n",
            "    IOUS: (0.5, 0.6, 0.7)\n",
            "  ROI_BOX_HEAD:\n",
            "    BBOX_REG_LOSS_TYPE: smooth_l1\n",
            "    BBOX_REG_LOSS_WEIGHT: 1.0\n",
            "    BBOX_REG_WEIGHTS: (10.0, 10.0, 5.0, 5.0)\n",
            "    CLS_AGNOSTIC_BBOX_REG: False\n",
            "    CONV_DIM: 256\n",
            "    FC_DIM: 1024\n",
            "    NAME: \n",
            "    NORM: \n",
            "    NUM_CONV: 0\n",
            "    NUM_FC: 0\n",
            "    POOLER_RESOLUTION: 14\n",
            "    POOLER_SAMPLING_RATIO: 0\n",
            "    POOLER_TYPE: ROIAlignV2\n",
            "    SMOOTH_L1_BETA: 0.0\n",
            "    TRAIN_ON_PRED_BOXES: False\n",
            "  ROI_HEADS:\n",
            "    BATCH_SIZE_PER_IMAGE: 512\n",
            "    IN_FEATURES: ['res4']\n",
            "    IOU_LABELS: [0, 1]\n",
            "    IOU_THRESHOLDS: [0.5]\n",
            "    NAME: Res5ROIHeads\n",
            "    NMS_THRESH_TEST: 0.5\n",
            "    NUM_CLASSES: 80\n",
            "    POSITIVE_FRACTION: 0.25\n",
            "    PROPOSAL_APPEND_GT: True\n",
            "    SCORE_THRESH_TEST: 0.05\n",
            "  ROI_KEYPOINT_HEAD:\n",
            "    CONV_DIMS: (512, 512, 512, 512, 512, 512, 512, 512)\n",
            "    LOSS_WEIGHT: 1.0\n",
            "    MIN_KEYPOINTS_PER_IMAGE: 1\n",
            "    NAME: KRCNNConvDeconvUpsampleHead\n",
            "    NORMALIZE_LOSS_BY_VISIBLE_KEYPOINTS: True\n",
            "    NUM_KEYPOINTS: 17\n",
            "    POOLER_RESOLUTION: 14\n",
            "    POOLER_SAMPLING_RATIO: 0\n",
            "    POOLER_TYPE: ROIAlignV2\n",
            "  ROI_MASK_HEAD:\n",
            "    CLS_AGNOSTIC_MASK: False\n",
            "    CONV_DIM: 256\n",
            "    NAME: MaskRCNNConvUpsampleHead\n",
            "    NORM: \n",
            "    NUM_CONV: 0\n",
            "    POOLER_RESOLUTION: 14\n",
            "    POOLER_SAMPLING_RATIO: 0\n",
            "    POOLER_TYPE: ROIAlignV2\n",
            "  RPN:\n",
            "    BATCH_SIZE_PER_IMAGE: 256\n",
            "    BBOX_REG_LOSS_TYPE: smooth_l1\n",
            "    BBOX_REG_LOSS_WEIGHT: 1.0\n",
            "    BBOX_REG_WEIGHTS: (1.0, 1.0, 1.0, 1.0)\n",
            "    BOUNDARY_THRESH: -1\n",
            "    HEAD_NAME: StandardRPNHead\n",
            "    IN_FEATURES: ['res4']\n",
            "    IOU_LABELS: [0, -1, 1]\n",
            "    IOU_THRESHOLDS: [0.3, 0.7]\n",
            "    LOSS_WEIGHT: 1.0\n",
            "    NMS_THRESH: 0.7\n",
            "    POSITIVE_FRACTION: 0.5\n",
            "    POST_NMS_TOPK_TEST: 1000\n",
            "    POST_NMS_TOPK_TRAIN: 2000\n",
            "    PRE_NMS_TOPK_TEST: 6000\n",
            "    PRE_NMS_TOPK_TRAIN: 12000\n",
            "    SMOOTH_L1_BETA: 0.0\n",
            "  SEM_SEG_HEAD:\n",
            "    COMMON_STRIDE: 4\n",
            "    CONVS_DIM: 128\n",
            "    IGNORE_VALUE: 255\n",
            "    IN_FEATURES: ['p2', 'p3', 'p4', 'p5']\n",
            "    LOSS_WEIGHT: 1.0\n",
            "    NAME: SemSegFPNHead\n",
            "    NORM: GN\n",
            "    NUM_CLASSES: 54\n",
            "  WEIGHTS: work_dirs/visdrone_querydet/model_0005999.pth\n",
            "OUTPUT_DIR: work_dirs/model_test-4\n",
            "SEED: -1\n",
            "SOLVER:\n",
            "  AMP:\n",
            "    ENABLED: False\n",
            "  BASE_LR: 0.01\n",
            "  BIAS_LR_FACTOR: 1.0\n",
            "  CHECKPOINT_PERIOD: 5000\n",
            "  CLIP_GRADIENTS:\n",
            "    CLIP_TYPE: value\n",
            "    CLIP_VALUE: 1.0\n",
            "    ENABLED: False\n",
            "    NORM_TYPE: 2.0\n",
            "  GAMMA: 0.1\n",
            "  IMS_PER_BATCH: 16\n",
            "  LR_SCHEDULER_NAME: WarmupMultiStepLR\n",
            "  MAX_ITER: 90000\n",
            "  MOMENTUM: 0.9\n",
            "  NESTEROV: False\n",
            "  REFERENCE_WORLD_SIZE: 0\n",
            "  STEPS: (60000, 80000)\n",
            "  WARMUP_FACTOR: 0.001\n",
            "  WARMUP_ITERS: 1000\n",
            "  WARMUP_METHOD: linear\n",
            "  WEIGHT_DECAY: 0.0001\n",
            "  WEIGHT_DECAY_BIAS: 0.0001\n",
            "  WEIGHT_DECAY_NORM: 0.0\n",
            "TEST:\n",
            "  AUG:\n",
            "    ENABLED: False\n",
            "    FLIP: True\n",
            "    MAX_SIZE: 4000\n",
            "    MIN_SIZES: (400, 500, 600, 700, 800, 900, 1000, 1100, 1200)\n",
            "  DETECTIONS_PER_IMAGE: 500\n",
            "  EVAL_PERIOD: 0\n",
            "  EXPECTED_RESULTS: []\n",
            "  KEYPOINT_OKS_SIGMAS: []\n",
            "  PRECISE_BN:\n",
            "    ENABLED: False\n",
            "    NUM_ITER: 200\n",
            "VERSION: 2\n",
            "VISDRONE:\n",
            "  MAX_LENGTH: 1999\n",
            "  SHORT_LENGTH: [1200]\n",
            "  TEST_IMG_ROOT: data/visdrone/coco_format/val_images\n",
            "  TEST_JSON: data/visdrone/coco_format/annotations/val_label.json\n",
            "  TEST_LENGTH: 3999\n",
            "  TRAIN_JSON: data/visdrone/coco_format/annotations/train_label.json\n",
            "  TRING_IMG_ROOT: data//visdrone/coco_format/train_images\n",
            "VIS_PERIOD: 0\n",
            "\u001b[32m[04/15 06:52:57 detectron2]: \u001b[0mFull config saved to work_dirs/model_test-4/config.yaml\n",
            "\u001b[32m[04/15 06:52:57 d2.utils.env]: \u001b[0mUsing a generated random seed 57857988\n",
            "\u001b[32m[04/15 06:53:01 d2.engine.defaults]: \u001b[0mModel:\n",
            "RetinaNetQueryDet(\n",
            "  (backbone): FPN(\n",
            "    (fpn_lateral2): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1))\n",
            "    (fpn_output2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (fpn_lateral3): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1))\n",
            "    (fpn_output3): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (fpn_lateral4): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1))\n",
            "    (fpn_output4): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (fpn_lateral5): Conv2d(2048, 256, kernel_size=(1, 1), stride=(1, 1))\n",
            "    (fpn_output5): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (top_block): LastLevelP6P7(\n",
            "      (p6): Conv2d(2048, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
            "      (p7): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
            "    )\n",
            "    (bottom_up): ResNet(\n",
            "      (stem): BasicStem(\n",
            "        (conv1): Conv2d(\n",
            "          3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)\n",
            "        )\n",
            "      )\n",
            "      (res2): Sequential(\n",
            "        (0): BottleneckBlock(\n",
            "          (shortcut): Conv2d(\n",
            "            64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
            "          )\n",
            "          (conv1): Conv2d(\n",
            "            64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)\n",
            "          )\n",
            "          (conv2): Conv2d(\n",
            "            64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)\n",
            "          )\n",
            "          (conv3): Conv2d(\n",
            "            64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
            "          )\n",
            "        )\n",
            "        (1): BottleneckBlock(\n",
            "          (conv1): Conv2d(\n",
            "            256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)\n",
            "          )\n",
            "          (conv2): Conv2d(\n",
            "            64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)\n",
            "          )\n",
            "          (conv3): Conv2d(\n",
            "            64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
            "          )\n",
            "        )\n",
            "        (2): BottleneckBlock(\n",
            "          (conv1): Conv2d(\n",
            "            256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)\n",
            "          )\n",
            "          (conv2): Conv2d(\n",
            "            64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)\n",
            "          )\n",
            "          (conv3): Conv2d(\n",
            "            64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
            "          )\n",
            "        )\n",
            "      )\n",
            "      (res3): Sequential(\n",
            "        (0): BottleneckBlock(\n",
            "          (shortcut): Conv2d(\n",
            "            256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
            "          )\n",
            "          (conv1): Conv2d(\n",
            "            256, 128, kernel_size=(1, 1), stride=(2, 2), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)\n",
            "          )\n",
            "          (conv2): Conv2d(\n",
            "            128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)\n",
            "          )\n",
            "          (conv3): Conv2d(\n",
            "            128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
            "          )\n",
            "        )\n",
            "        (1): BottleneckBlock(\n",
            "          (conv1): Conv2d(\n",
            "            512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)\n",
            "          )\n",
            "          (conv2): Conv2d(\n",
            "            128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)\n",
            "          )\n",
            "          (conv3): Conv2d(\n",
            "            128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
            "          )\n",
            "        )\n",
            "        (2): BottleneckBlock(\n",
            "          (conv1): Conv2d(\n",
            "            512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)\n",
            "          )\n",
            "          (conv2): Conv2d(\n",
            "            128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)\n",
            "          )\n",
            "          (conv3): Conv2d(\n",
            "            128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
            "          )\n",
            "        )\n",
            "        (3): BottleneckBlock(\n",
            "          (conv1): Conv2d(\n",
            "            512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)\n",
            "          )\n",
            "          (conv2): Conv2d(\n",
            "            128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)\n",
            "          )\n",
            "          (conv3): Conv2d(\n",
            "            128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
            "          )\n",
            "        )\n",
            "      )\n",
            "      (res4): Sequential(\n",
            "        (0): BottleneckBlock(\n",
            "          (shortcut): Conv2d(\n",
            "            512, 1024, kernel_size=(1, 1), stride=(2, 2), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n",
            "          )\n",
            "          (conv1): Conv2d(\n",
            "            512, 256, kernel_size=(1, 1), stride=(2, 2), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
            "          )\n",
            "          (conv2): Conv2d(\n",
            "            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
            "          )\n",
            "          (conv3): Conv2d(\n",
            "            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n",
            "          )\n",
            "        )\n",
            "        (1): BottleneckBlock(\n",
            "          (conv1): Conv2d(\n",
            "            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
            "          )\n",
            "          (conv2): Conv2d(\n",
            "            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
            "          )\n",
            "          (conv3): Conv2d(\n",
            "            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n",
            "          )\n",
            "        )\n",
            "        (2): BottleneckBlock(\n",
            "          (conv1): Conv2d(\n",
            "            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
            "          )\n",
            "          (conv2): Conv2d(\n",
            "            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
            "          )\n",
            "          (conv3): Conv2d(\n",
            "            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n",
            "          )\n",
            "        )\n",
            "        (3): BottleneckBlock(\n",
            "          (conv1): Conv2d(\n",
            "            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
            "          )\n",
            "          (conv2): Conv2d(\n",
            "            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
            "          )\n",
            "          (conv3): Conv2d(\n",
            "            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n",
            "          )\n",
            "        )\n",
            "        (4): BottleneckBlock(\n",
            "          (conv1): Conv2d(\n",
            "            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
            "          )\n",
            "          (conv2): Conv2d(\n",
            "            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
            "          )\n",
            "          (conv3): Conv2d(\n",
            "            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n",
            "          )\n",
            "        )\n",
            "        (5): BottleneckBlock(\n",
            "          (conv1): Conv2d(\n",
            "            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
            "          )\n",
            "          (conv2): Conv2d(\n",
            "            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
            "          )\n",
            "          (conv3): Conv2d(\n",
            "            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n",
            "          )\n",
            "        )\n",
            "      )\n",
            "      (res5): Sequential(\n",
            "        (0): BottleneckBlock(\n",
            "          (shortcut): Conv2d(\n",
            "            1024, 2048, kernel_size=(1, 1), stride=(2, 2), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=2048, eps=1e-05)\n",
            "          )\n",
            "          (conv1): Conv2d(\n",
            "            1024, 512, kernel_size=(1, 1), stride=(2, 2), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
            "          )\n",
            "          (conv2): Conv2d(\n",
            "            512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
            "          )\n",
            "          (conv3): Conv2d(\n",
            "            512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=2048, eps=1e-05)\n",
            "          )\n",
            "        )\n",
            "        (1): BottleneckBlock(\n",
            "          (conv1): Conv2d(\n",
            "            2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
            "          )\n",
            "          (conv2): Conv2d(\n",
            "            512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
            "          )\n",
            "          (conv3): Conv2d(\n",
            "            512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=2048, eps=1e-05)\n",
            "          )\n",
            "        )\n",
            "        (2): BottleneckBlock(\n",
            "          (conv1): Conv2d(\n",
            "            2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
            "          )\n",
            "          (conv2): Conv2d(\n",
            "            512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
            "          )\n",
            "          (conv3): Conv2d(\n",
            "            512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=2048, eps=1e-05)\n",
            "          )\n",
            "        )\n",
            "      )\n",
            "    )\n",
            "  )\n",
            "  (det_head): RetinaNetHead_3x3(\n",
            "    (cls_layer_0): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (bbox_layer_0): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (cls_layer_1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (bbox_layer_1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (cls_layer_2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (bbox_layer_2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (cls_layer_3): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (bbox_layer_3): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (cls_score): Conv2d(256, 90, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (bbox_pred): Conv2d(256, 36, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "  )\n",
            "  (query_head): Head_3x3(\n",
            "    (layer_0): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (layer_1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (layer_2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (layer_3): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (pred_net): Conv2d(256, 1, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "  )\n",
            "  (anchor_generator): AnchorGeneratorWithCenter(\n",
            "    (cell_anchors): BufferList()\n",
            "  )\n",
            "  (query_anchor_generator): AnchorGeneratorWithCenter(\n",
            "    (cell_anchors): BufferList()\n",
            "  )\n",
            ")\n",
            "\u001b[32m[04/15 06:53:01 fvcore.common.checkpoint]: \u001b[0mLoading checkpoint from work_dirs/visdrone_querydet/model_0005999.pth\n",
            "\u001b[32m[04/15 06:53:19 d2.data.common]: \u001b[0mSerializing 548 elements to byte tensors and concatenating them all ...\n",
            "\u001b[32m[04/15 06:53:19 d2.data.common]: \u001b[0mSerialized dataset takes 0.08 MiB\n",
            "/usr/local/lib/python3.7/site-packages/torch/utils/data/dataloader.py:477: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  cpuset_checked))\n",
            "\u001b[32m[04/15 06:53:19 d2.evaluation.evaluator]: \u001b[0mStart inference on 548 images\n",
            "\u001b[32m[04/15 06:53:44 d2.evaluation.evaluator]: \u001b[0mInference done 11/548. 2.2035 s / img. ETA=0:19:47\n",
            "\u001b[32m[04/15 06:53:51 d2.evaluation.evaluator]: \u001b[0mInference done 14/548. 2.2139 s / img. ETA=0:19:48\n",
            "\u001b[32m[04/15 06:53:57 d2.evaluation.evaluator]: \u001b[0mInference done 17/548. 2.2187 s / img. ETA=0:19:44\n",
            "\u001b[32m[04/15 06:54:04 d2.evaluation.evaluator]: \u001b[0mInference done 20/548. 2.2235 s / img. ETA=0:19:39\n",
            "\u001b[32m[04/15 06:54:11 d2.evaluation.evaluator]: \u001b[0mInference done 23/548. 2.2306 s / img. ETA=0:19:36\n",
            "\u001b[32m[04/15 06:54:18 d2.evaluation.evaluator]: \u001b[0mInference done 26/548. 2.2376 s / img. ETA=0:19:34\n",
            "\u001b[32m[04/15 06:54:25 d2.evaluation.evaluator]: \u001b[0mInference done 29/548. 2.2437 s / img. ETA=0:19:30\n",
            "\u001b[32m[04/15 06:54:32 d2.evaluation.evaluator]: \u001b[0mInference done 32/548. 2.2510 s / img. ETA=0:19:27\n",
            "\u001b[32m[04/15 06:54:39 d2.evaluation.evaluator]: \u001b[0mInference done 35/548. 2.2578 s / img. ETA=0:19:23\n",
            "\u001b[32m[04/15 06:54:46 d2.evaluation.evaluator]: \u001b[0mInference done 38/548. 2.2641 s / img. ETA=0:19:20\n",
            "\u001b[32m[04/15 06:54:53 d2.evaluation.evaluator]: \u001b[0mInference done 41/548. 2.2718 s / img. ETA=0:19:17\n",
            "\u001b[32m[04/15 06:55:00 d2.evaluation.evaluator]: \u001b[0mInference done 44/548. 2.2784 s / img. ETA=0:19:13\n",
            "\u001b[32m[04/15 06:55:07 d2.evaluation.evaluator]: \u001b[0mInference done 47/548. 2.2838 s / img. ETA=0:19:09\n",
            "\u001b[32m[04/15 06:55:14 d2.evaluation.evaluator]: \u001b[0mInference done 50/548. 2.2878 s / img. ETA=0:19:04\n",
            "\u001b[32m[04/15 06:55:21 d2.evaluation.evaluator]: \u001b[0mInference done 53/548. 2.2906 s / img. ETA=0:18:59\n",
            "\u001b[32m[04/15 06:55:28 d2.evaluation.evaluator]: \u001b[0mInference done 56/548. 2.2923 s / img. ETA=0:18:53\n",
            "\u001b[32m[04/15 06:55:35 d2.evaluation.evaluator]: \u001b[0mInference done 59/548. 2.2934 s / img. ETA=0:18:46\n",
            "\u001b[32m[04/15 06:55:42 d2.evaluation.evaluator]: \u001b[0mInference done 62/548. 2.2945 s / img. ETA=0:18:40\n",
            "\u001b[32m[04/15 06:55:49 d2.evaluation.evaluator]: \u001b[0mInference done 65/548. 2.2959 s / img. ETA=0:18:34\n",
            "\u001b[32m[04/15 06:55:56 d2.evaluation.evaluator]: \u001b[0mInference done 68/548. 2.2969 s / img. ETA=0:18:27\n",
            "\u001b[32m[04/15 06:56:03 d2.evaluation.evaluator]: \u001b[0mInference done 71/548. 2.2985 s / img. ETA=0:18:21\n",
            "\u001b[32m[04/15 06:56:10 d2.evaluation.evaluator]: \u001b[0mInference done 74/548. 2.2996 s / img. ETA=0:18:17\n",
            "\u001b[32m[04/15 06:56:17 d2.evaluation.evaluator]: \u001b[0mInference done 77/548. 2.3011 s / img. ETA=0:18:11\n",
            "\u001b[32m[04/15 06:56:25 d2.evaluation.evaluator]: \u001b[0mInference done 80/548. 2.3033 s / img. ETA=0:18:05\n",
            "\u001b[32m[04/15 06:56:32 d2.evaluation.evaluator]: \u001b[0mInference done 83/548. 2.3050 s / img. ETA=0:17:58\n",
            "\u001b[32m[04/15 06:56:39 d2.evaluation.evaluator]: \u001b[0mInference done 86/548. 2.3058 s / img. ETA=0:17:52\n",
            "\u001b[32m[04/15 06:56:46 d2.evaluation.evaluator]: \u001b[0mInference done 89/548. 2.3070 s / img. ETA=0:17:45\n",
            "\u001b[32m[04/15 06:56:53 d2.evaluation.evaluator]: \u001b[0mInference done 92/548. 2.3083 s / img. ETA=0:17:39\n",
            "\u001b[32m[04/15 06:57:00 d2.evaluation.evaluator]: \u001b[0mInference done 95/548. 2.3091 s / img. ETA=0:17:32\n",
            "\u001b[32m[04/15 06:57:07 d2.evaluation.evaluator]: \u001b[0mInference done 98/548. 2.3096 s / img. ETA=0:17:25\n",
            "\u001b[32m[04/15 06:57:14 d2.evaluation.evaluator]: \u001b[0mInference done 101/548. 2.3105 s / img. ETA=0:17:19\n",
            "\u001b[32m[04/15 06:57:21 d2.evaluation.evaluator]: \u001b[0mInference done 104/548. 2.3112 s / img. ETA=0:17:12\n",
            "\u001b[32m[04/15 06:57:28 d2.evaluation.evaluator]: \u001b[0mInference done 107/548. 2.3119 s / img. ETA=0:17:05\n",
            "\u001b[32m[04/15 06:57:35 d2.evaluation.evaluator]: \u001b[0mInference done 110/548. 2.3125 s / img. ETA=0:16:59\n",
            "\u001b[32m[04/15 06:57:42 d2.evaluation.evaluator]: \u001b[0mInference done 113/548. 2.3133 s / img. ETA=0:16:52\n",
            "\u001b[32m[04/15 06:57:49 d2.evaluation.evaluator]: \u001b[0mInference done 116/548. 2.3140 s / img. ETA=0:16:46\n",
            "\u001b[32m[04/15 06:57:56 d2.evaluation.evaluator]: \u001b[0mInference done 119/548. 2.3144 s / img. ETA=0:16:39\n",
            "\u001b[32m[04/15 06:58:03 d2.evaluation.evaluator]: \u001b[0mInference done 122/548. 2.3148 s / img. ETA=0:16:32\n",
            "\u001b[32m[04/15 06:58:09 d2.evaluation.evaluator]: \u001b[0mInference done 124/548. 2.3152 s / img. ETA=0:16:30\n",
            "\u001b[32m[04/15 06:58:16 d2.evaluation.evaluator]: \u001b[0mInference done 127/548. 2.3157 s / img. ETA=0:16:23\n",
            "\u001b[32m[04/15 06:58:23 d2.evaluation.evaluator]: \u001b[0mInference done 130/548. 2.3162 s / img. ETA=0:16:16\n",
            "\u001b[32m[04/15 06:58:30 d2.evaluation.evaluator]: \u001b[0mInference done 133/548. 2.3169 s / img. ETA=0:16:09\n",
            "\u001b[32m[04/15 06:58:37 d2.evaluation.evaluator]: \u001b[0mInference done 136/548. 2.3171 s / img. ETA=0:16:02\n",
            "\u001b[32m[04/15 06:58:44 d2.evaluation.evaluator]: \u001b[0mInference done 139/548. 2.3175 s / img. ETA=0:15:55\n",
            "\u001b[32m[04/15 06:58:51 d2.evaluation.evaluator]: \u001b[0mInference done 142/548. 2.3178 s / img. ETA=0:15:48\n",
            "\u001b[32m[04/15 06:58:58 d2.evaluation.evaluator]: \u001b[0mInference done 145/548. 2.3181 s / img. ETA=0:15:41\n",
            "\u001b[32m[04/15 06:59:05 d2.evaluation.evaluator]: \u001b[0mInference done 148/548. 2.3185 s / img. ETA=0:15:34\n",
            "\u001b[32m[04/15 06:59:12 d2.evaluation.evaluator]: \u001b[0mInference done 151/548. 2.3188 s / img. ETA=0:15:27\n",
            "\u001b[32m[04/15 06:59:19 d2.evaluation.evaluator]: \u001b[0mInference done 154/548. 2.3194 s / img. ETA=0:15:21\n",
            "\u001b[32m[04/15 06:59:26 d2.evaluation.evaluator]: \u001b[0mInference done 157/548. 2.3197 s / img. ETA=0:15:14\n",
            "\u001b[32m[04/15 06:59:33 d2.evaluation.evaluator]: \u001b[0mInference done 160/548. 2.3200 s / img. ETA=0:15:07\n",
            "\u001b[32m[04/15 06:59:40 d2.evaluation.evaluator]: \u001b[0mInference done 163/548. 2.3204 s / img. ETA=0:15:00\n",
            "\u001b[32m[04/15 06:59:47 d2.evaluation.evaluator]: \u001b[0mInference done 166/548. 2.3207 s / img. ETA=0:14:53\n",
            "\u001b[32m[04/15 06:59:54 d2.evaluation.evaluator]: \u001b[0mInference done 169/548. 2.3210 s / img. ETA=0:14:46\n",
            "\u001b[32m[04/15 07:00:01 d2.evaluation.evaluator]: \u001b[0mInference done 172/548. 2.3215 s / img. ETA=0:14:39\n",
            "\u001b[32m[04/15 07:00:08 d2.evaluation.evaluator]: \u001b[0mInference done 175/548. 2.3219 s / img. ETA=0:14:32\n",
            "\u001b[32m[04/15 07:00:15 d2.evaluation.evaluator]: \u001b[0mInference done 178/548. 2.3222 s / img. ETA=0:14:25\n",
            "\u001b[32m[04/15 07:00:22 d2.evaluation.evaluator]: \u001b[0mInference done 181/548. 2.3225 s / img. ETA=0:14:18\n",
            "\u001b[32m[04/15 07:00:30 d2.evaluation.evaluator]: \u001b[0mInference done 184/548. 2.3228 s / img. ETA=0:14:11\n",
            "\u001b[32m[04/15 07:00:37 d2.evaluation.evaluator]: \u001b[0mInference done 187/548. 2.3232 s / img. ETA=0:14:04\n",
            "\u001b[32m[04/15 07:00:44 d2.evaluation.evaluator]: \u001b[0mInference done 190/548. 2.3233 s / img. ETA=0:13:57\n",
            "\u001b[32m[04/15 07:00:51 d2.evaluation.evaluator]: \u001b[0mInference done 193/548. 2.3234 s / img. ETA=0:13:50\n",
            "\u001b[32m[04/15 07:00:58 d2.evaluation.evaluator]: \u001b[0mInference done 196/548. 2.3236 s / img. ETA=0:13:43\n",
            "\u001b[32m[04/15 07:01:05 d2.evaluation.evaluator]: \u001b[0mInference done 199/548. 2.3238 s / img. ETA=0:13:36\n",
            "\u001b[32m[04/15 07:01:10 d2.evaluation.evaluator]: \u001b[0mInference done 201/548. 2.3238 s / img. ETA=0:13:33\n",
            "\u001b[32m[04/15 07:01:17 d2.evaluation.evaluator]: \u001b[0mInference done 204/548. 2.3239 s / img. ETA=0:13:26\n",
            "\u001b[32m[04/15 07:01:24 d2.evaluation.evaluator]: \u001b[0mInference done 207/548. 2.3242 s / img. ETA=0:13:19\n",
            "\u001b[32m[04/15 07:01:31 d2.evaluation.evaluator]: \u001b[0mInference done 210/548. 2.3243 s / img. ETA=0:13:12\n",
            "\u001b[32m[04/15 07:01:38 d2.evaluation.evaluator]: \u001b[0mInference done 213/548. 2.3248 s / img. ETA=0:13:05\n",
            "\u001b[32m[04/15 07:01:45 d2.evaluation.evaluator]: \u001b[0mInference done 216/548. 2.3250 s / img. ETA=0:12:58\n",
            "\u001b[32m[04/15 07:01:52 d2.evaluation.evaluator]: \u001b[0mInference done 219/548. 2.3251 s / img. ETA=0:12:51\n",
            "\u001b[32m[04/15 07:01:59 d2.evaluation.evaluator]: \u001b[0mInference done 222/548. 2.3250 s / img. ETA=0:12:44\n",
            "\u001b[32m[04/15 07:02:06 d2.evaluation.evaluator]: \u001b[0mInference done 225/548. 2.3251 s / img. ETA=0:12:37\n",
            "\u001b[32m[04/15 07:02:13 d2.evaluation.evaluator]: \u001b[0mInference done 228/548. 2.3252 s / img. ETA=0:12:30\n",
            "\u001b[32m[04/15 07:02:20 d2.evaluation.evaluator]: \u001b[0mInference done 231/548. 2.3253 s / img. ETA=0:12:23\n",
            "\u001b[32m[04/15 07:02:28 d2.evaluation.evaluator]: \u001b[0mInference done 234/548. 2.3255 s / img. ETA=0:12:16\n",
            "\u001b[32m[04/15 07:02:35 d2.evaluation.evaluator]: \u001b[0mInference done 237/548. 2.3255 s / img. ETA=0:12:09\n",
            "\u001b[32m[04/15 07:02:42 d2.evaluation.evaluator]: \u001b[0mInference done 240/548. 2.3257 s / img. ETA=0:12:02\n",
            "\u001b[32m[04/15 07:02:49 d2.evaluation.evaluator]: \u001b[0mInference done 243/548. 2.3259 s / img. ETA=0:11:55\n",
            "\u001b[32m[04/15 07:02:56 d2.evaluation.evaluator]: \u001b[0mInference done 246/548. 2.3260 s / img. ETA=0:11:48\n",
            "\u001b[32m[04/15 07:03:03 d2.evaluation.evaluator]: \u001b[0mInference done 249/548. 2.3262 s / img. ETA=0:11:41\n",
            "\u001b[32m[04/15 07:03:10 d2.evaluation.evaluator]: \u001b[0mInference done 252/548. 2.3263 s / img. ETA=0:11:34\n",
            "\u001b[32m[04/15 07:03:17 d2.evaluation.evaluator]: \u001b[0mInference done 255/548. 2.3266 s / img. ETA=0:11:27\n",
            "\u001b[32m[04/15 07:03:24 d2.evaluation.evaluator]: \u001b[0mInference done 258/548. 2.3267 s / img. ETA=0:11:20\n",
            "\u001b[32m[04/15 07:03:31 d2.evaluation.evaluator]: \u001b[0mInference done 261/548. 2.3267 s / img. ETA=0:11:13\n",
            "\u001b[32m[04/15 07:03:38 d2.evaluation.evaluator]: \u001b[0mInference done 264/548. 2.3269 s / img. ETA=0:11:05\n",
            "\u001b[32m[04/15 07:03:45 d2.evaluation.evaluator]: \u001b[0mInference done 267/548. 2.3268 s / img. ETA=0:10:58\n",
            "\u001b[32m[04/15 07:03:52 d2.evaluation.evaluator]: \u001b[0mInference done 270/548. 2.3270 s / img. ETA=0:10:51\n",
            "\u001b[32m[04/15 07:03:59 d2.evaluation.evaluator]: \u001b[0mInference done 273/548. 2.3272 s / img. ETA=0:10:44\n",
            "\u001b[32m[04/15 07:04:06 d2.evaluation.evaluator]: \u001b[0mInference done 276/548. 2.3272 s / img. ETA=0:10:37\n",
            "\u001b[32m[04/15 07:04:11 d2.evaluation.evaluator]: \u001b[0mInference done 278/548. 2.3273 s / img. ETA=0:10:33\n",
            "\u001b[32m[04/15 07:04:18 d2.evaluation.evaluator]: \u001b[0mInference done 281/548. 2.3274 s / img. ETA=0:10:26\n",
            "\u001b[32m[04/15 07:04:25 d2.evaluation.evaluator]: \u001b[0mInference done 284/548. 2.3275 s / img. ETA=0:10:19\n",
            "\u001b[32m[04/15 07:04:32 d2.evaluation.evaluator]: \u001b[0mInference done 287/548. 2.3276 s / img. ETA=0:10:12\n",
            "\u001b[32m[04/15 07:04:39 d2.evaluation.evaluator]: \u001b[0mInference done 290/548. 2.3277 s / img. ETA=0:10:05\n",
            "\u001b[32m[04/15 07:04:46 d2.evaluation.evaluator]: \u001b[0mInference done 293/548. 2.3278 s / img. ETA=0:09:58\n",
            "\u001b[32m[04/15 07:04:53 d2.evaluation.evaluator]: \u001b[0mInference done 296/548. 2.3279 s / img. ETA=0:09:51\n",
            "\u001b[32m[04/15 07:05:00 d2.evaluation.evaluator]: \u001b[0mInference done 299/548. 2.3279 s / img. ETA=0:09:44\n",
            "\u001b[32m[04/15 07:05:08 d2.evaluation.evaluator]: \u001b[0mInference done 302/548. 2.3281 s / img. ETA=0:09:37\n",
            "\u001b[32m[04/15 07:05:15 d2.evaluation.evaluator]: \u001b[0mInference done 305/548. 2.3283 s / img. ETA=0:09:30\n",
            "\u001b[32m[04/15 07:05:22 d2.evaluation.evaluator]: \u001b[0mInference done 308/548. 2.3285 s / img. ETA=0:09:23\n",
            "\u001b[32m[04/15 07:05:29 d2.evaluation.evaluator]: \u001b[0mInference done 311/548. 2.3286 s / img. ETA=0:09:16\n",
            "\u001b[32m[04/15 07:05:36 d2.evaluation.evaluator]: \u001b[0mInference done 314/548. 2.3288 s / img. ETA=0:09:09\n",
            "\u001b[32m[04/15 07:05:43 d2.evaluation.evaluator]: \u001b[0mInference done 317/548. 2.3288 s / img. ETA=0:09:02\n",
            "\u001b[32m[04/15 07:05:50 d2.evaluation.evaluator]: \u001b[0mInference done 320/548. 2.3289 s / img. ETA=0:08:55\n",
            "\u001b[32m[04/15 07:05:57 d2.evaluation.evaluator]: \u001b[0mInference done 323/548. 2.3290 s / img. ETA=0:08:48\n",
            "\u001b[32m[04/15 07:06:04 d2.evaluation.evaluator]: \u001b[0mInference done 326/548. 2.3292 s / img. ETA=0:08:41\n",
            "\u001b[32m[04/15 07:06:11 d2.evaluation.evaluator]: \u001b[0mInference done 329/548. 2.3293 s / img. ETA=0:08:34\n",
            "\u001b[32m[04/15 07:06:18 d2.evaluation.evaluator]: \u001b[0mInference done 332/548. 2.3294 s / img. ETA=0:08:27\n",
            "\u001b[32m[04/15 07:06:25 d2.evaluation.evaluator]: \u001b[0mInference done 335/548. 2.3294 s / img. ETA=0:08:20\n",
            "\u001b[32m[04/15 07:06:33 d2.evaluation.evaluator]: \u001b[0mInference done 338/548. 2.3295 s / img. ETA=0:08:13\n",
            "\u001b[32m[04/15 07:06:40 d2.evaluation.evaluator]: \u001b[0mInference done 341/548. 2.3296 s / img. ETA=0:08:06\n",
            "\u001b[32m[04/15 07:06:47 d2.evaluation.evaluator]: \u001b[0mInference done 344/548. 2.3297 s / img. ETA=0:07:59\n",
            "\u001b[32m[04/15 07:06:54 d2.evaluation.evaluator]: \u001b[0mInference done 347/548. 2.3298 s / img. ETA=0:07:51\n",
            "\u001b[32m[04/15 07:07:01 d2.evaluation.evaluator]: \u001b[0mInference done 350/548. 2.3298 s / img. ETA=0:07:44\n",
            "\u001b[32m[04/15 07:07:08 d2.evaluation.evaluator]: \u001b[0mInference done 353/548. 2.3298 s / img. ETA=0:07:37\n",
            "\u001b[32m[04/15 07:07:13 d2.evaluation.evaluator]: \u001b[0mInference done 355/548. 2.3298 s / img. ETA=0:07:33\n",
            "\u001b[32m[04/15 07:07:20 d2.evaluation.evaluator]: \u001b[0mInference done 358/548. 2.3298 s / img. ETA=0:07:26\n",
            "\u001b[32m[04/15 07:07:27 d2.evaluation.evaluator]: \u001b[0mInference done 361/548. 2.3299 s / img. ETA=0:07:19\n",
            "\u001b[32m[04/15 07:07:34 d2.evaluation.evaluator]: \u001b[0mInference done 364/548. 2.3299 s / img. ETA=0:07:12\n",
            "\u001b[32m[04/15 07:07:41 d2.evaluation.evaluator]: \u001b[0mInference done 367/548. 2.3299 s / img. ETA=0:07:05\n",
            "\u001b[32m[04/15 07:07:48 d2.evaluation.evaluator]: \u001b[0mInference done 370/548. 2.3299 s / img. ETA=0:06:58\n",
            "\u001b[32m[04/15 07:07:55 d2.evaluation.evaluator]: \u001b[0mInference done 373/548. 2.3299 s / img. ETA=0:06:51\n",
            "\u001b[32m[04/15 07:08:02 d2.evaluation.evaluator]: \u001b[0mInference done 376/548. 2.3300 s / img. ETA=0:06:44\n",
            "\u001b[32m[04/15 07:08:09 d2.evaluation.evaluator]: \u001b[0mInference done 379/548. 2.3301 s / img. ETA=0:06:37\n",
            "\u001b[32m[04/15 07:08:17 d2.evaluation.evaluator]: \u001b[0mInference done 382/548. 2.3301 s / img. ETA=0:06:30\n",
            "\u001b[32m[04/15 07:08:24 d2.evaluation.evaluator]: \u001b[0mInference done 385/548. 2.3302 s / img. ETA=0:06:23\n",
            "\u001b[32m[04/15 07:08:31 d2.evaluation.evaluator]: \u001b[0mInference done 388/548. 2.3303 s / img. ETA=0:06:15\n",
            "\u001b[32m[04/15 07:08:38 d2.evaluation.evaluator]: \u001b[0mInference done 391/548. 2.3304 s / img. ETA=0:06:08\n",
            "\u001b[32m[04/15 07:08:45 d2.evaluation.evaluator]: \u001b[0mInference done 394/548. 2.3305 s / img. ETA=0:06:01\n",
            "\u001b[32m[04/15 07:08:52 d2.evaluation.evaluator]: \u001b[0mInference done 397/548. 2.3305 s / img. ETA=0:05:54\n",
            "\u001b[32m[04/15 07:08:59 d2.evaluation.evaluator]: \u001b[0mInference done 400/548. 2.3306 s / img. ETA=0:05:47\n",
            "\u001b[32m[04/15 07:09:06 d2.evaluation.evaluator]: \u001b[0mInference done 403/548. 2.3306 s / img. ETA=0:05:40\n",
            "\u001b[32m[04/15 07:09:13 d2.evaluation.evaluator]: \u001b[0mInference done 406/548. 2.3308 s / img. ETA=0:05:33\n",
            "\u001b[32m[04/15 07:09:20 d2.evaluation.evaluator]: \u001b[0mInference done 409/548. 2.3308 s / img. ETA=0:05:26\n",
            "\u001b[32m[04/15 07:09:27 d2.evaluation.evaluator]: \u001b[0mInference done 412/548. 2.3309 s / img. ETA=0:05:19\n",
            "\u001b[32m[04/15 07:09:34 d2.evaluation.evaluator]: \u001b[0mInference done 415/548. 2.3310 s / img. ETA=0:05:12\n",
            "\u001b[32m[04/15 07:09:41 d2.evaluation.evaluator]: \u001b[0mInference done 418/548. 2.3310 s / img. ETA=0:05:05\n",
            "\u001b[32m[04/15 07:09:48 d2.evaluation.evaluator]: \u001b[0mInference done 421/548. 2.3311 s / img. ETA=0:04:58\n",
            "\u001b[32m[04/15 07:09:55 d2.evaluation.evaluator]: \u001b[0mInference done 424/548. 2.3311 s / img. ETA=0:04:51\n",
            "\u001b[32m[04/15 07:10:02 d2.evaluation.evaluator]: \u001b[0mInference done 427/548. 2.3311 s / img. ETA=0:04:44\n",
            "\u001b[32m[04/15 07:10:09 d2.evaluation.evaluator]: \u001b[0mInference done 430/548. 2.3311 s / img. ETA=0:04:37\n",
            "\u001b[32m[04/15 07:10:16 d2.evaluation.evaluator]: \u001b[0mInference done 433/548. 2.3312 s / img. ETA=0:04:30\n",
            "\u001b[32m[04/15 07:10:24 d2.evaluation.evaluator]: \u001b[0mInference done 436/548. 2.3313 s / img. ETA=0:04:23\n",
            "\u001b[32m[04/15 07:10:31 d2.evaluation.evaluator]: \u001b[0mInference done 439/548. 2.3313 s / img. ETA=0:04:16\n",
            "\u001b[32m[04/15 07:10:38 d2.evaluation.evaluator]: \u001b[0mInference done 442/548. 2.3314 s / img. ETA=0:04:09\n",
            "\u001b[32m[04/15 07:10:45 d2.evaluation.evaluator]: \u001b[0mInference done 445/548. 2.3315 s / img. ETA=0:04:02\n",
            "\u001b[32m[04/15 07:10:52 d2.evaluation.evaluator]: \u001b[0mInference done 448/548. 2.3315 s / img. ETA=0:03:55\n",
            "\u001b[32m[04/15 07:10:59 d2.evaluation.evaluator]: \u001b[0mInference done 451/548. 2.3316 s / img. ETA=0:03:47\n",
            "\u001b[32m[04/15 07:11:06 d2.evaluation.evaluator]: \u001b[0mInference done 454/548. 2.3317 s / img. ETA=0:03:40\n",
            "\u001b[32m[04/15 07:11:13 d2.evaluation.evaluator]: \u001b[0mInference done 457/548. 2.3317 s / img. ETA=0:03:33\n",
            "\u001b[32m[04/15 07:11:20 d2.evaluation.evaluator]: \u001b[0mInference done 460/548. 2.3317 s / img. ETA=0:03:26\n",
            "\u001b[32m[04/15 07:11:27 d2.evaluation.evaluator]: \u001b[0mInference done 463/548. 2.3317 s / img. ETA=0:03:19\n",
            "\u001b[32m[04/15 07:11:34 d2.evaluation.evaluator]: \u001b[0mInference done 466/548. 2.3317 s / img. ETA=0:03:12\n",
            "\u001b[32m[04/15 07:11:41 d2.evaluation.evaluator]: \u001b[0mInference done 469/548. 2.3318 s / img. ETA=0:03:05\n",
            "\u001b[32m[04/15 07:11:48 d2.evaluation.evaluator]: \u001b[0mInference done 472/548. 2.3318 s / img. ETA=0:02:58\n",
            "\u001b[32m[04/15 07:11:55 d2.evaluation.evaluator]: \u001b[0mInference done 475/548. 2.3319 s / img. ETA=0:02:51\n",
            "\u001b[32m[04/15 07:12:02 d2.evaluation.evaluator]: \u001b[0mInference done 478/548. 2.3319 s / img. ETA=0:02:44\n",
            "\u001b[32m[04/15 07:12:09 d2.evaluation.evaluator]: \u001b[0mInference done 481/548. 2.3320 s / img. ETA=0:02:37\n",
            "\u001b[32m[04/15 07:12:16 d2.evaluation.evaluator]: \u001b[0mInference done 484/548. 2.3320 s / img. ETA=0:02:30\n",
            "\u001b[32m[04/15 07:12:23 d2.evaluation.evaluator]: \u001b[0mInference done 487/548. 2.3320 s / img. ETA=0:02:23\n",
            "\u001b[32m[04/15 07:12:30 d2.evaluation.evaluator]: \u001b[0mInference done 490/548. 2.3321 s / img. ETA=0:02:16\n",
            "\u001b[32m[04/15 07:12:38 d2.evaluation.evaluator]: \u001b[0mInference done 493/548. 2.3321 s / img. ETA=0:02:09\n",
            "\u001b[32m[04/15 07:12:45 d2.evaluation.evaluator]: \u001b[0mInference done 496/548. 2.3322 s / img. ETA=0:02:02\n",
            "\u001b[32m[04/15 07:12:52 d2.evaluation.evaluator]: \u001b[0mInference done 499/548. 2.3322 s / img. ETA=0:01:55\n",
            "\u001b[32m[04/15 07:12:59 d2.evaluation.evaluator]: \u001b[0mInference done 502/548. 2.3323 s / img. ETA=0:01:48\n",
            "\u001b[32m[04/15 07:13:06 d2.evaluation.evaluator]: \u001b[0mInference done 505/548. 2.3323 s / img. ETA=0:01:41\n",
            "\u001b[32m[04/15 07:13:13 d2.evaluation.evaluator]: \u001b[0mInference done 508/548. 2.3324 s / img. ETA=0:01:34\n",
            "\u001b[32m[04/15 07:13:18 d2.evaluation.evaluator]: \u001b[0mInference done 510/548. 2.3324 s / img. ETA=0:01:29\n",
            "\u001b[32m[04/15 07:13:25 d2.evaluation.evaluator]: \u001b[0mInference done 513/548. 2.3324 s / img. ETA=0:01:22\n",
            "\u001b[32m[04/15 07:13:32 d2.evaluation.evaluator]: \u001b[0mInference done 516/548. 2.3325 s / img. ETA=0:01:15\n",
            "\u001b[32m[04/15 07:13:39 d2.evaluation.evaluator]: \u001b[0mInference done 519/548. 2.3325 s / img. ETA=0:01:08\n",
            "\u001b[32m[04/15 07:13:46 d2.evaluation.evaluator]: \u001b[0mInference done 522/548. 2.3325 s / img. ETA=0:01:01\n",
            "\u001b[32m[04/15 07:13:53 d2.evaluation.evaluator]: \u001b[0mInference done 525/548. 2.3325 s / img. ETA=0:00:54\n",
            "\u001b[32m[04/15 07:14:00 d2.evaluation.evaluator]: \u001b[0mInference done 528/548. 2.3324 s / img. ETA=0:00:47\n",
            "\u001b[32m[04/15 07:14:07 d2.evaluation.evaluator]: \u001b[0mInference done 531/548. 2.3325 s / img. ETA=0:00:39\n",
            "\u001b[32m[04/15 07:14:14 d2.evaluation.evaluator]: \u001b[0mInference done 534/548. 2.3325 s / img. ETA=0:00:32\n",
            "\u001b[32m[04/15 07:14:22 d2.evaluation.evaluator]: \u001b[0mInference done 537/548. 2.3325 s / img. ETA=0:00:25\n",
            "\u001b[32m[04/15 07:14:29 d2.evaluation.evaluator]: \u001b[0mInference done 540/548. 2.3326 s / img. ETA=0:00:18\n",
            "\u001b[32m[04/15 07:14:36 d2.evaluation.evaluator]: \u001b[0mInference done 543/548. 2.3326 s / img. ETA=0:00:11\n",
            "\u001b[32m[04/15 07:14:43 d2.evaluation.evaluator]: \u001b[0mInference done 546/548. 2.3326 s / img. ETA=0:00:04\n",
            "\u001b[32m[04/15 07:14:49 d2.evaluation.evaluator]: \u001b[0mTotal inference time: 0:21:18.096150 (2.353768 s / img per device, on 1 devices)\n",
            "\u001b[32m[04/15 07:14:49 d2.evaluation.evaluator]: \u001b[0mTotal inference pure compute time: 0:21:06 (2.332630 s / img per device, on 1 devices)\n",
            "\u001b[32m[04/15 07:14:54 d2.evaluation.testing]: \u001b[0mcopypaste: Task: GPU_Speed\n",
            "\u001b[32m[04/15 07:14:54 d2.evaluation.testing]: \u001b[0mcopypaste: Mean_FPS,Std_FPS,Max_FPS,Min_FPS,Mid_FPS\n",
            "\u001b[32m[04/15 07:14:54 d2.evaluation.testing]: \u001b[0mcopypaste: 0.4316,0.0051,0.4633,0.4215,0.4308\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!bash eval_visdrone.sh work_dirs/model_test-4/visdrone_infer.json"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "n5uMHir8jNcZ",
        "outputId": "3d95a276-851f-44a2-e8ec-c8faa8937bb0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Json to txt: .visdrone_det_txt\n",
            "  0% 0/548 [00:00<?, ?it/s]0\n",
            "0\n",
            "  0% 1/548 [00:01<10:11,  1.12s/it]1\n",
            "1\n",
            "  0% 2/548 [00:02<11:35,  1.27s/it]2\n",
            "2\n",
            "  1% 3/548 [00:04<12:26,  1.37s/it]3\n",
            "3\n",
            "  1% 4/548 [00:06<13:33,  1.50s/it]4\n",
            "4\n",
            "  1% 5/548 [00:07<13:53,  1.54s/it]5\n",
            "5\n",
            "  1% 6/548 [00:08<12:32,  1.39s/it]6\n",
            "6\n",
            "  1% 7/548 [00:11<14:50,  1.65s/it]7\n",
            "7\n",
            "  1% 8/548 [00:13<15:43,  1.75s/it]8\n",
            "8\n",
            "  2% 9/548 [00:14<14:05,  1.57s/it]9\n",
            "9\n",
            "  2% 10/548 [00:15<13:48,  1.54s/it]10\n",
            "10\n",
            "  2% 11/548 [00:17<14:24,  1.61s/it]11\n",
            "11\n",
            "  2% 12/548 [00:19<16:25,  1.84s/it]12\n",
            "12\n",
            "  2% 13/548 [00:21<15:52,  1.78s/it]13\n",
            "13\n",
            "  3% 14/548 [00:23<15:21,  1.73s/it]14\n",
            "14\n",
            "  3% 15/548 [00:24<13:46,  1.55s/it]15\n",
            "15\n",
            "  3% 16/548 [00:25<13:42,  1.55s/it]16\n",
            "16\n",
            "  3% 17/548 [00:27<15:34,  1.76s/it]17\n",
            "17\n",
            "  3% 18/548 [00:29<15:18,  1.73s/it]18\n",
            "18\n",
            "  3% 19/548 [00:31<14:54,  1.69s/it]19\n",
            "19\n",
            "  4% 20/548 [00:32<14:36,  1.66s/it]20\n",
            "20\n",
            "  4% 21/548 [00:34<14:40,  1.67s/it]21\n",
            "21\n",
            "  4% 22/548 [00:36<15:01,  1.71s/it]22\n",
            "22\n",
            "  4% 23/548 [00:38<15:14,  1.74s/it]23\n",
            "23\n",
            "  4% 24/548 [00:39<14:53,  1.70s/it]24\n",
            "24\n",
            "  5% 25/548 [00:41<14:46,  1.70s/it]25\n",
            "25\n",
            "  5% 26/548 [00:42<14:11,  1.63s/it]26\n",
            "26\n",
            "  5% 27/548 [00:44<14:24,  1.66s/it]27\n",
            "27\n",
            "  5% 28/548 [00:45<12:50,  1.48s/it]28\n",
            "28\n",
            "  5% 29/548 [00:47<13:16,  1.54s/it]29\n",
            "29\n",
            "  5% 30/548 [00:48<12:01,  1.39s/it]30\n",
            "30\n",
            "  6% 31/548 [00:50<14:33,  1.69s/it]31\n",
            "31\n",
            "  6% 32/548 [00:52<15:09,  1.76s/it]32\n",
            "32\n",
            "  6% 33/548 [00:54<14:39,  1.71s/it]33\n",
            "33\n",
            "  6% 34/548 [00:56<14:49,  1.73s/it]34\n",
            "34\n",
            "  6% 35/548 [00:57<14:16,  1.67s/it]35\n",
            "35\n",
            "  7% 36/548 [00:59<14:07,  1.66s/it]36\n",
            "36\n",
            "  7% 37/548 [01:00<14:04,  1.65s/it]37\n",
            "37\n",
            "  7% 38/548 [01:02<13:35,  1.60s/it]38\n",
            "38\n",
            "  7% 39/548 [01:04<15:26,  1.82s/it]39\n",
            "39\n",
            "  7% 40/548 [01:06<14:48,  1.75s/it]40\n",
            "40\n",
            "  7% 41/548 [01:07<14:29,  1.72s/it]41\n",
            "41\n",
            "  8% 42/548 [01:10<16:09,  1.92s/it]42\n",
            "42\n",
            "  8% 43/548 [01:11<15:22,  1.83s/it]43\n",
            "43\n",
            "  8% 44/548 [01:13<15:19,  1.82s/it]44\n",
            "44\n",
            "  8% 45/548 [01:15<14:46,  1.76s/it]45\n",
            "45\n",
            "  8% 46/548 [01:16<13:23,  1.60s/it]46\n",
            "46\n",
            "  9% 47/548 [01:18<14:44,  1.77s/it]47\n",
            "47\n",
            "  9% 48/548 [01:20<14:54,  1.79s/it]48\n",
            "48\n",
            "  9% 49/548 [01:22<14:19,  1.72s/it]49\n",
            "49\n",
            "  9% 50/548 [01:23<14:20,  1.73s/it]50\n",
            "50\n",
            "  9% 51/548 [01:25<14:02,  1.70s/it]51\n",
            "51\n",
            "  9% 52/548 [01:27<14:24,  1.74s/it]52\n",
            "52\n",
            " 10% 53/548 [01:29<14:10,  1.72s/it]53\n",
            "53\n",
            " 10% 54/548 [01:30<13:34,  1.65s/it]54\n",
            "54\n",
            " 10% 55/548 [01:32<13:17,  1.62s/it]55\n",
            "55\n",
            " 10% 56/548 [01:33<13:30,  1.65s/it]56\n",
            "56\n",
            " 10% 57/548 [01:36<14:55,  1.82s/it]57\n",
            "57\n",
            " 11% 58/548 [01:37<13:33,  1.66s/it]58\n",
            "58\n",
            " 11% 59/548 [01:39<14:01,  1.72s/it]59\n",
            "59\n",
            " 11% 60/548 [01:41<14:25,  1.77s/it]60\n",
            "60\n",
            " 11% 61/548 [01:42<14:16,  1.76s/it]61\n",
            "61\n",
            " 11% 62/548 [01:44<14:38,  1.81s/it]62\n",
            "62\n",
            " 11% 63/548 [01:46<14:54,  1.84s/it]63\n",
            "63\n",
            " 12% 64/548 [01:48<15:00,  1.86s/it]64\n",
            "64\n",
            " 12% 65/548 [01:50<14:36,  1.81s/it]65\n",
            "65\n",
            " 12% 66/548 [01:52<14:58,  1.86s/it]66\n",
            "66\n",
            " 12% 67/548 [01:53<13:39,  1.70s/it]67\n",
            "67\n",
            " 12% 68/548 [01:54<12:06,  1.51s/it]68\n",
            "68\n",
            " 13% 69/548 [01:56<12:16,  1.54s/it]69\n",
            "69\n",
            " 13% 70/548 [01:57<12:31,  1.57s/it]70\n",
            "70\n",
            " 13% 71/548 [01:59<12:47,  1.61s/it]71\n",
            "71\n",
            " 13% 72/548 [02:01<12:46,  1.61s/it]72\n",
            "72\n",
            " 13% 73/548 [02:02<13:13,  1.67s/it]73\n",
            "73\n",
            " 14% 74/548 [02:04<13:21,  1.69s/it]74\n",
            "74\n",
            " 14% 75/548 [02:06<13:21,  1.69s/it]75\n",
            "75\n",
            " 14% 76/548 [02:08<13:56,  1.77s/it]76\n",
            "76\n",
            " 14% 77/548 [02:09<13:26,  1.71s/it]77\n",
            "77\n",
            " 14% 78/548 [02:11<13:33,  1.73s/it]78\n",
            "78\n",
            " 14% 79/548 [02:13<13:44,  1.76s/it]79\n",
            "79\n",
            " 15% 80/548 [02:15<15:00,  1.92s/it]80\n",
            "80\n",
            " 15% 81/548 [02:17<14:20,  1.84s/it]81\n",
            "81\n",
            " 15% 82/548 [02:19<14:04,  1.81s/it]82\n",
            "82\n",
            " 15% 83/548 [02:21<15:12,  1.96s/it]83\n",
            "83\n",
            " 15% 84/548 [02:23<14:28,  1.87s/it]84\n",
            "84\n",
            " 16% 85/548 [02:25<14:21,  1.86s/it]85\n",
            "85\n",
            " 16% 86/548 [02:26<13:58,  1.82s/it]86\n",
            "86\n",
            " 16% 87/548 [02:29<15:08,  1.97s/it]87\n",
            "87\n",
            " 16% 88/548 [02:30<14:51,  1.94s/it]88\n",
            "88\n",
            " 16% 89/548 [02:33<15:12,  1.99s/it]89\n",
            "89\n",
            " 16% 90/548 [02:35<16:20,  2.14s/it]90\n",
            "90\n",
            " 17% 91/548 [02:37<15:27,  2.03s/it]91\n",
            "91\n",
            " 17% 92/548 [02:38<14:17,  1.88s/it]92\n",
            "92\n",
            " 17% 93/548 [02:40<13:31,  1.78s/it]93\n",
            "93\n",
            " 17% 94/548 [02:42<13:14,  1.75s/it]94\n",
            "94\n",
            " 17% 95/548 [02:43<11:40,  1.55s/it]95\n",
            "95\n",
            " 18% 96/548 [02:45<12:50,  1.70s/it]96\n",
            "96\n",
            " 18% 97/548 [02:46<12:34,  1.67s/it]97\n",
            "97\n",
            " 18% 98/548 [02:48<12:39,  1.69s/it]98\n",
            "98\n",
            " 18% 99/548 [02:50<12:30,  1.67s/it]99\n",
            "99\n",
            " 18% 100/548 [02:51<12:12,  1.64s/it]100\n",
            "100\n",
            " 18% 101/548 [02:53<11:36,  1.56s/it]101\n",
            "101\n",
            " 19% 102/548 [02:54<10:03,  1.35s/it]102\n",
            "102\n",
            " 19% 103/548 [02:56<12:45,  1.72s/it]103\n",
            "103\n",
            " 19% 104/548 [02:57<11:11,  1.51s/it]104\n",
            "104\n",
            " 19% 105/548 [02:59<11:51,  1.61s/it]105\n",
            "105\n",
            " 19% 106/548 [03:01<11:44,  1.59s/it]106\n",
            "106\n",
            " 20% 107/548 [03:02<10:34,  1.44s/it]107\n",
            "107\n",
            " 20% 108/548 [03:02<09:23,  1.28s/it]108\n",
            "108\n",
            " 20% 109/548 [03:04<10:31,  1.44s/it]109\n",
            "109\n",
            " 20% 110/548 [03:06<11:26,  1.57s/it]110\n",
            "110\n",
            " 20% 111/548 [03:08<11:27,  1.57s/it]111\n",
            "111\n",
            " 20% 112/548 [03:09<10:32,  1.45s/it]112\n",
            "112\n",
            " 21% 113/548 [03:11<12:16,  1.69s/it]113\n",
            "113\n",
            " 21% 114/548 [03:13<12:11,  1.69s/it]114\n",
            "114\n",
            " 21% 115/548 [03:15<12:14,  1.70s/it]115\n",
            "115\n",
            " 21% 116/548 [03:16<12:06,  1.68s/it]116\n",
            "116\n",
            " 21% 117/548 [03:18<11:56,  1.66s/it]117\n",
            "117\n",
            " 22% 118/548 [03:19<10:32,  1.47s/it]118\n",
            "118\n",
            " 22% 119/548 [03:21<11:32,  1.62s/it]119\n",
            "119\n",
            " 22% 120/548 [03:23<11:59,  1.68s/it]120\n",
            "120\n",
            " 22% 121/548 [03:25<12:29,  1.75s/it]121\n",
            "121\n",
            " 22% 122/548 [03:27<12:52,  1.81s/it]122\n",
            "122\n",
            " 22% 123/548 [03:28<12:40,  1.79s/it]123\n",
            "123\n",
            " 23% 124/548 [03:31<14:05,  2.00s/it]124\n",
            "124\n",
            " 23% 125/548 [03:32<13:35,  1.93s/it]125\n",
            "125\n",
            " 23% 126/548 [03:34<13:16,  1.89s/it]126\n",
            "126\n",
            " 23% 127/548 [03:36<13:05,  1.87s/it]127\n",
            "127\n",
            " 23% 128/548 [03:37<11:25,  1.63s/it]128\n",
            "128\n",
            " 24% 129/548 [03:39<11:35,  1.66s/it]129\n",
            "129\n",
            " 24% 130/548 [03:41<11:58,  1.72s/it]130\n",
            "130\n",
            " 24% 131/548 [03:43<12:10,  1.75s/it]131\n",
            "131\n",
            " 24% 132/548 [03:45<12:35,  1.82s/it]132\n",
            "132\n",
            " 24% 133/548 [03:46<10:51,  1.57s/it]133\n",
            "133\n",
            " 24% 134/548 [03:47<11:03,  1.60s/it]134\n",
            "134\n",
            " 25% 135/548 [03:50<12:30,  1.82s/it]135\n",
            "135\n",
            " 25% 136/548 [03:51<12:31,  1.82s/it]136\n",
            "136\n",
            " 25% 137/548 [03:54<13:42,  2.00s/it]137\n",
            "137\n",
            " 25% 138/548 [03:56<14:35,  2.14s/it]138\n",
            "138\n",
            " 25% 139/548 [03:58<13:55,  2.04s/it]139\n",
            "139\n",
            " 26% 140/548 [04:00<13:10,  1.94s/it]140\n",
            "140\n",
            " 26% 141/548 [04:01<12:24,  1.83s/it]141\n",
            "141\n",
            " 26% 142/548 [04:03<12:21,  1.83s/it]142\n",
            "142\n",
            " 26% 143/548 [04:04<11:11,  1.66s/it]143\n",
            "143\n",
            " 26% 144/548 [04:06<11:04,  1.65s/it]144\n",
            "144\n",
            " 26% 145/548 [04:08<10:46,  1.60s/it]145\n",
            "145\n",
            " 27% 146/548 [04:09<10:48,  1.61s/it]146\n",
            "146\n",
            " 27% 147/548 [04:11<10:43,  1.61s/it]147\n",
            "147\n",
            " 27% 148/548 [04:13<12:25,  1.86s/it]148\n",
            "148\n",
            " 27% 149/548 [04:14<10:53,  1.64s/it]149\n",
            "149\n",
            " 27% 150/548 [04:16<11:36,  1.75s/it]150\n",
            "150\n",
            " 28% 151/548 [04:18<11:10,  1.69s/it]151\n",
            "151\n",
            " 28% 152/548 [04:19<09:42,  1.47s/it]152\n",
            "152\n",
            " 28% 153/548 [04:21<10:26,  1.58s/it]153\n",
            "153\n",
            " 28% 154/548 [04:22<10:37,  1.62s/it]154\n",
            "154\n",
            " 28% 155/548 [04:25<11:58,  1.83s/it]155\n",
            "155\n",
            " 28% 156/548 [04:27<13:22,  2.05s/it]156\n",
            "156\n",
            " 29% 157/548 [04:29<12:59,  1.99s/it]157\n",
            "157\n",
            " 29% 158/548 [04:31<12:44,  1.96s/it]158\n",
            "158\n",
            " 29% 159/548 [04:33<13:16,  2.05s/it]159\n",
            "159\n",
            " 29% 160/548 [04:35<12:48,  1.98s/it]160\n",
            "160\n",
            " 29% 161/548 [04:37<11:58,  1.86s/it]161\n",
            "161\n",
            " 30% 162/548 [04:38<10:47,  1.68s/it]162\n",
            "162\n",
            " 30% 163/548 [04:40<10:36,  1.65s/it]163\n",
            "163\n",
            " 30% 164/548 [04:41<10:59,  1.72s/it]164\n",
            "164\n",
            " 30% 165/548 [04:42<09:21,  1.47s/it]165\n",
            "165\n",
            " 30% 166/548 [04:44<10:00,  1.57s/it]166\n",
            "166\n",
            " 30% 167/548 [04:47<11:47,  1.86s/it]167\n",
            "167\n",
            " 31% 168/548 [04:48<11:16,  1.78s/it]168\n",
            "168\n",
            " 31% 169/548 [04:50<10:39,  1.69s/it]169\n",
            "169\n",
            " 31% 170/548 [04:51<09:19,  1.48s/it]170\n",
            "170\n",
            " 31% 171/548 [04:54<11:48,  1.88s/it]171\n",
            "171\n",
            " 31% 172/548 [04:56<12:39,  2.02s/it]172\n",
            "172\n",
            " 32% 173/548 [04:57<10:41,  1.71s/it]173\n",
            "173\n",
            " 32% 174/548 [04:59<11:33,  1.85s/it]174\n",
            "174\n",
            " 32% 175/548 [05:01<11:03,  1.78s/it]175\n",
            "175\n",
            " 32% 176/548 [05:02<10:53,  1.76s/it]176\n",
            "176\n",
            " 32% 177/548 [05:04<10:38,  1.72s/it]177\n",
            "177\n",
            " 32% 178/548 [05:06<10:58,  1.78s/it]178\n",
            "178\n",
            " 33% 179/548 [05:08<10:58,  1.79s/it]179\n",
            "179\n",
            " 33% 180/548 [05:09<10:55,  1.78s/it]180\n",
            "180\n",
            " 33% 181/548 [05:10<09:20,  1.53s/it]181\n",
            "181\n",
            " 33% 182/548 [05:12<09:19,  1.53s/it]182\n",
            "182\n",
            " 33% 183/548 [05:14<09:26,  1.55s/it]183\n",
            "183\n",
            " 34% 184/548 [05:15<08:33,  1.41s/it]184\n",
            "184\n",
            " 34% 185/548 [05:16<08:46,  1.45s/it]185\n",
            "185\n",
            " 34% 186/548 [05:18<09:17,  1.54s/it]186\n",
            "186\n",
            " 34% 187/548 [05:19<08:40,  1.44s/it]187\n",
            "187\n",
            " 34% 188/548 [05:21<09:15,  1.54s/it]188\n",
            "188\n",
            " 34% 189/548 [05:23<10:30,  1.76s/it]189\n",
            "189\n",
            " 35% 190/548 [05:26<11:56,  2.00s/it]190\n",
            "190\n",
            " 35% 191/548 [05:28<12:05,  2.03s/it]191\n",
            "191\n",
            " 35% 192/548 [05:29<11:15,  1.90s/it]192\n",
            "192\n",
            " 35% 193/548 [05:31<10:49,  1.83s/it]193\n",
            "193\n",
            " 35% 194/548 [05:32<09:31,  1.61s/it]194\n",
            "194\n",
            " 36% 195/548 [05:34<10:10,  1.73s/it]195\n",
            "195\n",
            " 36% 196/548 [05:36<10:03,  1.71s/it]196\n",
            "196\n",
            " 36% 197/548 [05:38<10:28,  1.79s/it]197\n",
            "197\n",
            " 36% 198/548 [05:40<10:28,  1.80s/it]198\n",
            "198\n",
            " 36% 199/548 [05:41<10:22,  1.78s/it]199\n",
            "199\n",
            " 36% 200/548 [05:43<09:08,  1.58s/it]200\n",
            "200\n",
            " 37% 201/548 [05:43<08:01,  1.39s/it]201\n",
            "201\n",
            " 37% 202/548 [05:45<08:26,  1.47s/it]202\n",
            "202\n",
            " 37% 203/548 [05:47<08:40,  1.51s/it]203\n",
            "203\n",
            " 37% 204/548 [05:48<08:54,  1.55s/it]204\n",
            "204\n",
            " 37% 205/548 [05:50<09:11,  1.61s/it]205\n",
            "205\n",
            " 38% 206/548 [05:51<08:08,  1.43s/it]206\n",
            "206\n",
            " 38% 207/548 [05:53<08:41,  1.53s/it]207\n",
            "207\n",
            " 38% 208/548 [05:55<10:01,  1.77s/it]208\n",
            "208\n",
            " 38% 209/548 [05:57<10:16,  1.82s/it]209\n",
            "209\n",
            " 38% 210/548 [05:59<10:35,  1.88s/it]210\n",
            "210\n",
            " 39% 211/548 [06:01<10:16,  1.83s/it]211\n",
            "211\n",
            " 39% 212/548 [06:03<09:54,  1.77s/it]212\n",
            "212\n",
            " 39% 213/548 [06:03<08:30,  1.52s/it]213\n",
            "213\n",
            " 39% 214/548 [06:05<08:31,  1.53s/it]214\n",
            "214\n",
            " 39% 215/548 [06:06<07:31,  1.35s/it]215\n",
            "215\n",
            " 39% 216/548 [06:08<09:03,  1.64s/it]216\n",
            "216\n",
            " 40% 217/548 [06:10<09:06,  1.65s/it]217\n",
            "217\n",
            " 40% 218/548 [06:11<08:09,  1.48s/it]218\n",
            "218\n",
            " 40% 219/548 [06:13<09:29,  1.73s/it]219\n",
            "219\n",
            " 40% 220/548 [06:15<09:18,  1.70s/it]220\n",
            "220\n",
            " 40% 221/548 [06:17<09:23,  1.72s/it]221\n",
            "221\n",
            " 41% 222/548 [06:18<09:16,  1.71s/it]222\n",
            "222\n",
            " 41% 223/548 [06:20<09:35,  1.77s/it]223\n",
            "223\n",
            " 41% 224/548 [06:22<09:24,  1.74s/it]224\n",
            "224\n",
            " 41% 225/548 [06:23<08:42,  1.62s/it]225\n",
            "225\n",
            " 41% 226/548 [06:24<07:39,  1.43s/it]226\n",
            "226\n",
            " 41% 227/548 [06:26<08:21,  1.56s/it]227\n",
            "227\n",
            " 42% 228/548 [06:27<07:29,  1.40s/it]228\n",
            "228\n",
            " 42% 229/548 [06:28<07:04,  1.33s/it]229\n",
            "229\n",
            " 42% 230/548 [06:30<07:51,  1.48s/it]230\n",
            "230\n",
            " 42% 231/548 [06:32<08:02,  1.52s/it]231\n",
            "231\n",
            " 42% 232/548 [06:34<08:56,  1.70s/it]232\n",
            "232\n",
            " 43% 233/548 [06:36<09:18,  1.77s/it]233\n",
            "233\n",
            " 43% 234/548 [06:38<09:05,  1.74s/it]234\n",
            "234\n",
            " 43% 235/548 [06:39<07:48,  1.50s/it]235\n",
            "235\n",
            " 43% 236/548 [06:39<06:59,  1.35s/it]236\n",
            "236\n",
            " 43% 237/548 [06:41<07:35,  1.46s/it]237\n",
            "237\n",
            " 43% 238/548 [06:43<07:49,  1.51s/it]238\n",
            "238\n",
            " 44% 239/548 [06:44<07:03,  1.37s/it]239\n",
            "239\n",
            " 44% 240/548 [06:46<07:26,  1.45s/it]240\n",
            "240\n",
            " 44% 241/548 [06:47<07:37,  1.49s/it]241\n",
            "241\n",
            " 44% 242/548 [06:49<07:48,  1.53s/it]242\n",
            "242\n",
            " 44% 243/548 [06:50<07:48,  1.53s/it]243\n",
            "243\n",
            " 45% 244/548 [06:51<06:44,  1.33s/it]244\n",
            "244\n",
            " 45% 245/548 [06:53<08:00,  1.59s/it]245\n",
            "245\n",
            " 45% 246/548 [06:55<08:40,  1.72s/it]246\n",
            "246\n",
            " 45% 247/548 [06:57<08:28,  1.69s/it]247\n",
            "247\n",
            " 45% 248/548 [06:59<08:18,  1.66s/it]248\n",
            "248\n",
            " 45% 249/548 [07:00<07:25,  1.49s/it]249\n",
            "249\n",
            " 46% 250/548 [07:01<07:34,  1.52s/it]250\n",
            "250\n",
            " 46% 251/548 [07:02<06:53,  1.39s/it]251\n",
            "251\n",
            " 46% 252/548 [07:03<06:14,  1.26s/it]252\n",
            "252\n",
            " 46% 253/548 [07:05<06:38,  1.35s/it]253\n",
            "253\n",
            " 46% 254/548 [07:06<06:48,  1.39s/it]254\n",
            "254\n",
            " 47% 255/548 [07:09<08:05,  1.66s/it]255\n",
            "255\n",
            " 47% 256/548 [07:10<08:15,  1.70s/it]256\n",
            "256\n",
            " 47% 257/548 [07:12<08:20,  1.72s/it]257\n",
            "257\n",
            " 47% 258/548 [07:14<07:52,  1.63s/it]258\n",
            "258\n",
            " 47% 259/548 [07:15<07:17,  1.51s/it]259\n",
            "259\n",
            " 47% 260/548 [07:16<07:17,  1.52s/it]260\n",
            "260\n",
            " 48% 261/548 [07:17<06:26,  1.35s/it]261\n",
            "261\n",
            " 48% 262/548 [07:19<06:49,  1.43s/it]262\n",
            "262\n",
            " 48% 263/548 [07:20<06:34,  1.38s/it]263\n",
            "263\n",
            " 48% 264/548 [07:22<07:45,  1.64s/it]264\n",
            "264\n",
            " 48% 265/548 [07:25<08:29,  1.80s/it]265\n",
            "265\n",
            " 49% 266/548 [07:26<08:11,  1.74s/it]266\n",
            "266\n",
            " 49% 267/548 [07:27<07:05,  1.52s/it]267\n",
            "267\n",
            " 49% 268/548 [07:29<07:10,  1.54s/it]268\n",
            "268\n",
            " 49% 269/548 [07:31<07:27,  1.60s/it]269\n",
            "269\n",
            " 49% 270/548 [07:32<07:00,  1.51s/it]270\n",
            "270\n",
            " 49% 271/548 [07:34<07:22,  1.60s/it]271\n",
            "271\n",
            " 50% 272/548 [07:35<07:27,  1.62s/it]272\n",
            "272\n",
            " 50% 273/548 [07:37<07:30,  1.64s/it]273\n",
            "273\n",
            " 50% 274/548 [07:39<07:29,  1.64s/it]274\n",
            "274\n",
            " 50% 275/548 [07:41<08:17,  1.82s/it]275\n",
            "275\n",
            " 50% 276/548 [07:42<07:52,  1.74s/it]276\n",
            "276\n",
            " 51% 277/548 [07:44<08:00,  1.77s/it]277\n",
            "277\n",
            " 51% 278/548 [07:46<07:38,  1.70s/it]278\n",
            "278\n",
            " 51% 279/548 [07:47<07:29,  1.67s/it]279\n",
            "279\n",
            " 51% 280/548 [07:49<07:34,  1.69s/it]280\n",
            "280\n",
            " 51% 281/548 [07:51<07:21,  1.65s/it]281\n",
            "281\n",
            " 51% 282/548 [07:52<06:22,  1.44s/it]282\n",
            "282\n",
            " 52% 283/548 [07:53<06:18,  1.43s/it]283\n",
            "283\n",
            " 52% 284/548 [07:55<07:14,  1.65s/it]284\n",
            "284\n",
            " 52% 285/548 [07:57<06:52,  1.57s/it]285\n",
            "285\n",
            " 52% 286/548 [07:58<06:52,  1.58s/it]286\n",
            "286\n",
            " 52% 287/548 [07:59<05:58,  1.37s/it]287\n",
            "287\n",
            " 53% 288/548 [08:01<06:02,  1.39s/it]288\n",
            "288\n",
            " 53% 289/548 [08:01<05:21,  1.24s/it]289\n",
            "289\n",
            " 53% 290/548 [08:03<05:55,  1.38s/it]290\n",
            "290\n",
            " 53% 291/548 [08:05<06:12,  1.45s/it]291\n",
            "291\n",
            " 53% 292/548 [08:06<06:27,  1.51s/it]292\n",
            "292\n",
            " 53% 293/548 [08:09<07:21,  1.73s/it]293\n",
            "293\n",
            " 54% 294/548 [08:10<07:12,  1.70s/it]294\n",
            "294\n",
            " 54% 295/548 [08:12<06:54,  1.64s/it]295\n",
            "295\n",
            " 54% 296/548 [08:13<06:53,  1.64s/it]296\n",
            "296\n",
            " 54% 297/548 [08:16<07:46,  1.86s/it]297\n",
            "297\n",
            " 54% 298/548 [08:17<07:17,  1.75s/it]298\n",
            "298\n",
            " 55% 299/548 [08:19<07:08,  1.72s/it]299\n",
            "299\n",
            " 55% 300/548 [08:20<06:10,  1.50s/it]300\n",
            "300\n",
            " 55% 301/548 [08:21<05:28,  1.33s/it]301\n",
            "301\n",
            " 55% 302/548 [08:22<04:56,  1.20s/it]302\n",
            "302\n",
            " 55% 303/548 [08:23<05:23,  1.32s/it]303\n",
            "303\n",
            " 55% 304/548 [08:24<04:48,  1.18s/it]304\n",
            "304\n",
            " 56% 305/548 [08:26<05:25,  1.34s/it]305\n",
            "305\n",
            " 56% 306/548 [08:27<04:57,  1.23s/it]306\n",
            "306\n",
            " 56% 307/548 [08:28<05:20,  1.33s/it]307\n",
            "307\n",
            " 56% 308/548 [08:30<05:00,  1.25s/it]308\n",
            "308\n",
            " 56% 309/548 [08:31<05:33,  1.40s/it]309\n",
            "309\n",
            " 57% 310/548 [08:33<05:43,  1.44s/it]310\n",
            "310\n",
            " 57% 311/548 [08:34<04:59,  1.26s/it]311\n",
            "311\n",
            " 57% 312/548 [08:35<05:23,  1.37s/it]312\n",
            "312\n",
            " 57% 313/548 [08:37<05:25,  1.39s/it]313\n",
            "313\n",
            " 57% 314/548 [08:38<05:49,  1.49s/it]314\n",
            "314\n",
            " 57% 315/548 [08:40<05:49,  1.50s/it]315\n",
            "315\n",
            " 58% 316/548 [08:42<05:52,  1.52s/it]316\n",
            "316\n",
            " 58% 317/548 [08:43<06:02,  1.57s/it]317\n",
            "317\n",
            " 58% 318/548 [08:44<05:23,  1.41s/it]318\n",
            "318\n",
            " 58% 319/548 [08:45<04:49,  1.26s/it]319\n",
            "319\n",
            " 58% 320/548 [08:47<05:16,  1.39s/it]320\n",
            "320\n",
            " 59% 321/548 [08:49<05:48,  1.53s/it]321\n",
            "321\n",
            " 59% 322/548 [08:51<07:03,  1.87s/it]322\n",
            "322\n",
            " 59% 323/548 [08:53<07:07,  1.90s/it]323\n",
            "323\n",
            " 59% 324/548 [08:56<08:22,  2.24s/it]324\n",
            "324\n",
            " 59% 325/548 [08:58<07:52,  2.12s/it]325\n",
            "325\n",
            " 59% 326/548 [08:59<06:38,  1.80s/it]326\n",
            "326\n",
            " 60% 327/548 [09:01<06:39,  1.81s/it]327\n",
            "327\n",
            " 60% 328/548 [09:03<06:26,  1.76s/it]328\n",
            "328\n",
            " 60% 329/548 [09:04<06:11,  1.70s/it]329\n",
            "329\n",
            " 60% 330/548 [09:06<06:08,  1.69s/it]330\n",
            "330\n",
            " 60% 331/548 [09:07<05:23,  1.49s/it]331\n",
            "331\n",
            " 61% 332/548 [09:08<05:10,  1.44s/it]332\n",
            "332\n",
            " 61% 333/548 [09:09<04:34,  1.28s/it]333\n",
            "333\n",
            " 61% 334/548 [09:11<04:58,  1.40s/it]334\n",
            "334\n",
            " 61% 335/548 [09:13<05:18,  1.50s/it]335\n",
            "335\n",
            " 61% 336/548 [09:14<05:27,  1.54s/it]336\n",
            "336\n",
            " 61% 337/548 [09:16<05:34,  1.58s/it]337\n",
            "337\n",
            " 62% 338/548 [09:17<05:28,  1.56s/it]338\n",
            "338\n",
            " 62% 339/548 [09:19<05:37,  1.61s/it]339\n",
            "339\n",
            " 62% 340/548 [09:21<05:42,  1.65s/it]340\n",
            "340\n",
            " 62% 341/548 [09:22<05:32,  1.60s/it]341\n",
            "341\n",
            " 62% 342/548 [09:24<05:30,  1.60s/it]342\n",
            "342\n",
            " 63% 343/548 [09:26<05:28,  1.60s/it]343\n",
            "343\n",
            " 63% 344/548 [09:27<04:44,  1.40s/it]344\n",
            "344\n",
            " 63% 345/548 [09:28<05:05,  1.51s/it]345\n",
            "345\n",
            " 63% 346/548 [09:30<05:24,  1.60s/it]346\n",
            "346\n",
            " 63% 347/548 [09:32<05:19,  1.59s/it]347\n",
            "347\n",
            " 64% 348/548 [09:33<04:42,  1.41s/it]348\n",
            "348\n",
            " 64% 349/548 [09:34<04:44,  1.43s/it]349\n",
            "349\n",
            " 64% 350/548 [09:36<04:46,  1.45s/it]350\n",
            "350\n",
            " 64% 351/548 [09:37<05:03,  1.54s/it]351\n",
            "351\n",
            " 64% 352/548 [09:39<04:58,  1.52s/it]352\n",
            "352\n",
            " 64% 353/548 [09:41<05:41,  1.75s/it]353\n",
            "353\n",
            " 65% 354/548 [09:43<05:29,  1.70s/it]354\n",
            "354\n",
            " 65% 355/548 [09:45<05:43,  1.78s/it]355\n",
            "355\n",
            " 65% 356/548 [09:46<05:33,  1.74s/it]356\n",
            "356\n",
            " 65% 357/548 [09:48<05:36,  1.76s/it]357\n",
            "357\n",
            " 65% 358/548 [09:50<05:24,  1.71s/it]358\n",
            "358\n",
            " 66% 359/548 [09:51<04:40,  1.49s/it]359\n",
            "359\n",
            " 66% 360/548 [09:52<04:47,  1.53s/it]360\n",
            "360\n",
            " 66% 361/548 [09:54<04:44,  1.52s/it]361\n",
            "361\n",
            " 66% 362/548 [09:56<05:16,  1.70s/it]362\n",
            "362\n",
            " 66% 363/548 [09:58<05:15,  1.71s/it]363\n",
            "363\n",
            " 66% 364/548 [10:00<05:32,  1.81s/it]364\n",
            "364\n",
            " 67% 365/548 [10:01<05:20,  1.75s/it]365\n",
            "365\n",
            " 67% 366/548 [10:02<04:37,  1.52s/it]366\n",
            "366\n",
            " 67% 367/548 [10:04<04:44,  1.57s/it]367\n",
            "367\n",
            " 67% 368/548 [10:06<05:26,  1.81s/it]368\n",
            "368\n",
            " 67% 369/548 [10:07<04:36,  1.54s/it]369\n",
            "369\n",
            " 68% 370/548 [10:09<04:19,  1.46s/it]370\n",
            "370\n",
            " 68% 371/548 [10:10<04:37,  1.57s/it]371\n",
            "371\n",
            " 68% 372/548 [10:13<05:08,  1.75s/it]372\n",
            "372\n",
            " 68% 373/548 [10:14<04:57,  1.70s/it]373\n",
            "373\n",
            " 68% 374/548 [10:15<04:11,  1.45s/it]374\n",
            "374\n",
            " 68% 375/548 [10:17<04:27,  1.55s/it]375\n",
            "375\n",
            " 69% 376/548 [10:18<03:58,  1.38s/it]376\n",
            "376\n",
            " 69% 377/548 [10:19<04:09,  1.46s/it]377\n",
            "377\n",
            " 69% 378/548 [10:22<04:53,  1.73s/it]378\n",
            "378\n",
            " 69% 379/548 [10:23<04:46,  1.70s/it]379\n",
            "379\n",
            " 69% 380/548 [10:25<04:35,  1.64s/it]380\n",
            "380\n",
            " 70% 381/548 [10:27<04:31,  1.62s/it]381\n",
            "381\n",
            " 70% 382/548 [10:27<03:52,  1.40s/it]382\n",
            "382\n",
            " 70% 383/548 [10:29<04:09,  1.51s/it]383\n",
            "383\n",
            " 70% 384/548 [10:31<04:14,  1.55s/it]384\n",
            "384\n",
            " 70% 385/548 [10:32<03:54,  1.44s/it]385\n",
            "385\n",
            " 70% 386/548 [10:34<04:10,  1.54s/it]386\n",
            "386\n",
            " 71% 387/548 [10:36<04:16,  1.59s/it]387\n",
            "387\n",
            " 71% 388/548 [10:37<04:13,  1.58s/it]388\n",
            "388\n",
            " 71% 389/548 [10:39<04:10,  1.58s/it]389\n",
            "389\n",
            " 71% 390/548 [10:41<04:37,  1.76s/it]390\n",
            "390\n",
            " 71% 391/548 [10:42<04:24,  1.68s/it]391\n",
            "391\n",
            " 72% 392/548 [10:43<03:46,  1.45s/it]392\n",
            "392\n",
            " 72% 393/548 [10:45<03:48,  1.47s/it]393\n",
            "393\n",
            " 72% 394/548 [10:46<03:47,  1.48s/it]394\n",
            "394\n",
            " 72% 395/548 [10:48<03:50,  1.51s/it]395\n",
            "395\n",
            " 72% 396/548 [10:49<03:53,  1.54s/it]396\n",
            "396\n",
            " 72% 397/548 [10:51<03:38,  1.45s/it]397\n",
            "397\n",
            " 73% 398/548 [10:52<03:21,  1.35s/it]398\n",
            "398\n",
            " 73% 399/548 [10:54<04:08,  1.67s/it]399\n",
            "399\n",
            " 73% 400/548 [10:56<04:25,  1.79s/it]400\n",
            "400\n",
            " 73% 401/548 [10:58<04:13,  1.72s/it]401\n",
            "401\n",
            " 73% 402/548 [10:59<03:51,  1.58s/it]402\n",
            "402\n",
            " 74% 403/548 [11:01<03:46,  1.56s/it]403\n",
            "403\n",
            " 74% 404/548 [11:02<03:50,  1.60s/it]404\n",
            "404\n",
            " 74% 405/548 [11:05<04:16,  1.79s/it]405\n",
            "405\n",
            " 74% 406/548 [11:06<04:07,  1.74s/it]406\n",
            "406\n",
            " 74% 407/548 [11:08<04:01,  1.72s/it]407\n",
            "407\n",
            " 74% 408/548 [11:10<04:02,  1.73s/it]408\n",
            "408\n",
            " 75% 409/548 [11:11<03:55,  1.69s/it]409\n",
            "409\n",
            " 75% 410/548 [11:13<03:49,  1.66s/it]410\n",
            "410\n",
            " 75% 411/548 [11:15<04:06,  1.80s/it]411\n",
            "411\n",
            " 75% 412/548 [11:17<04:29,  1.98s/it]412\n",
            "412\n",
            " 75% 413/548 [11:19<04:10,  1.85s/it]413\n",
            "413\n",
            " 76% 414/548 [11:21<04:03,  1.81s/it]414\n",
            "414\n",
            " 76% 415/548 [11:23<04:20,  1.96s/it]415\n",
            "415\n",
            " 76% 416/548 [11:25<04:22,  1.99s/it]416\n",
            "416\n",
            " 76% 417/548 [11:27<04:19,  1.98s/it]417\n",
            "417\n",
            " 76% 418/548 [11:28<03:34,  1.65s/it]418\n",
            "418\n",
            " 76% 419/548 [11:30<03:39,  1.70s/it]419\n",
            "419\n",
            " 77% 420/548 [11:31<03:31,  1.65s/it]420\n",
            "420\n",
            " 77% 421/548 [11:33<03:26,  1.62s/it]421\n",
            "421\n",
            " 77% 422/548 [11:34<03:22,  1.60s/it]422\n",
            "422\n",
            " 77% 423/548 [11:35<02:59,  1.43s/it]423\n",
            "423\n",
            " 77% 424/548 [11:37<03:24,  1.65s/it]424\n",
            "424\n",
            " 78% 425/548 [11:39<03:22,  1.65s/it]425\n",
            "425\n",
            " 78% 426/548 [11:41<03:24,  1.68s/it]426\n",
            "426\n",
            " 78% 427/548 [11:42<03:20,  1.65s/it]427\n",
            "427\n",
            " 78% 428/548 [11:44<03:16,  1.64s/it]428\n",
            "428\n",
            " 78% 429/548 [11:46<03:20,  1.68s/it]429\n",
            "429\n",
            " 78% 430/548 [11:47<02:51,  1.45s/it]430\n",
            "430\n",
            " 79% 431/548 [11:48<02:58,  1.53s/it]431\n",
            "431\n",
            " 79% 432/548 [11:51<03:27,  1.79s/it]432\n",
            "432\n",
            " 79% 433/548 [11:53<03:24,  1.78s/it]433\n",
            "433\n",
            " 79% 434/548 [11:54<03:16,  1.73s/it]434\n",
            "434\n",
            " 79% 435/548 [11:56<03:30,  1.87s/it]435\n",
            "435\n",
            " 80% 436/548 [11:58<03:06,  1.66s/it]436\n",
            "436\n",
            " 80% 437/548 [11:59<02:58,  1.61s/it]437\n",
            "437\n",
            " 80% 438/548 [12:01<03:14,  1.77s/it]438\n",
            "438\n",
            " 80% 439/548 [12:02<02:42,  1.49s/it]439\n",
            "439\n",
            " 80% 440/548 [12:04<02:44,  1.52s/it]440\n",
            "440\n",
            " 80% 441/548 [12:06<03:07,  1.75s/it]441\n",
            "441\n",
            " 81% 442/548 [12:08<03:06,  1.76s/it]442\n",
            "442\n",
            " 81% 443/548 [12:09<03:01,  1.73s/it]443\n",
            "443\n",
            " 81% 444/548 [12:10<02:39,  1.54s/it]444\n",
            "444\n",
            " 81% 445/548 [12:13<03:10,  1.85s/it]445\n",
            "445\n",
            " 81% 446/548 [12:14<02:52,  1.69s/it]446\n",
            "446\n",
            " 82% 447/548 [12:16<02:53,  1.72s/it]447\n",
            "447\n",
            " 82% 448/548 [12:18<02:46,  1.66s/it]448\n",
            "448\n",
            " 82% 449/548 [12:20<03:04,  1.87s/it]449\n",
            "449\n",
            " 82% 450/548 [12:22<02:55,  1.79s/it]450\n",
            "450\n",
            " 82% 451/548 [12:23<02:50,  1.76s/it]451\n",
            "451\n",
            " 82% 452/548 [12:25<02:43,  1.71s/it]452\n",
            "452\n",
            " 83% 453/548 [12:27<02:38,  1.67s/it]453\n",
            "453\n",
            " 83% 454/548 [12:28<02:22,  1.51s/it]454\n",
            "454\n",
            " 83% 455/548 [12:29<02:07,  1.37s/it]455\n",
            "455\n",
            " 83% 456/548 [12:30<02:03,  1.34s/it]456\n",
            "456\n",
            " 83% 457/548 [12:31<01:55,  1.27s/it]457\n",
            "457\n",
            " 84% 458/548 [12:33<02:12,  1.47s/it]458\n",
            "458\n",
            " 84% 459/548 [12:35<02:31,  1.71s/it]459\n",
            "459\n",
            " 84% 460/548 [12:38<02:48,  1.91s/it]460\n",
            "460\n",
            " 84% 461/548 [12:39<02:35,  1.79s/it]461\n",
            "461\n",
            " 84% 462/548 [12:41<02:37,  1.83s/it]462\n",
            "462\n",
            " 84% 463/548 [12:43<02:39,  1.88s/it]463\n",
            "463\n",
            " 85% 464/548 [12:45<02:32,  1.81s/it]464\n",
            "464\n",
            " 85% 465/548 [12:46<02:27,  1.78s/it]465\n",
            "465\n",
            " 85% 466/548 [12:48<02:20,  1.72s/it]466\n",
            "466\n",
            " 85% 467/548 [12:49<02:08,  1.58s/it]467\n",
            "467\n",
            " 85% 468/548 [12:51<02:07,  1.59s/it]468\n",
            "468\n",
            " 86% 469/548 [12:52<02:02,  1.55s/it]469\n",
            "469\n",
            " 86% 470/548 [12:54<02:02,  1.58s/it]470\n",
            "470\n",
            " 86% 471/548 [12:55<01:47,  1.40s/it]471\n",
            "471\n",
            " 86% 472/548 [12:57<02:05,  1.64s/it]472\n",
            "472\n",
            " 86% 473/548 [12:58<01:46,  1.42s/it]473\n",
            "473\n",
            " 86% 474/548 [13:00<01:45,  1.43s/it]474\n",
            "474\n",
            " 87% 475/548 [13:00<01:33,  1.27s/it]475\n",
            "475\n",
            " 87% 476/548 [13:02<01:39,  1.39s/it]476\n",
            "476\n",
            " 87% 477/548 [13:03<01:28,  1.25s/it]477\n",
            "477\n",
            " 87% 478/548 [13:04<01:22,  1.18s/it]478\n",
            "478\n",
            " 87% 479/548 [13:06<01:29,  1.29s/it]479\n",
            "479\n",
            " 88% 480/548 [13:06<01:19,  1.18s/it]480\n",
            "480\n",
            " 88% 481/548 [13:08<01:25,  1.27s/it]481\n",
            "481\n",
            " 88% 482/548 [13:10<01:41,  1.54s/it]482\n",
            "482\n",
            " 88% 483/548 [13:12<01:43,  1.60s/it]483\n",
            "483\n",
            " 88% 484/548 [13:13<01:29,  1.40s/it]484\n",
            "484\n",
            " 89% 485/548 [13:15<01:45,  1.67s/it]485\n",
            "485\n",
            " 89% 486/548 [13:17<01:44,  1.68s/it]486\n",
            "486\n",
            " 89% 487/548 [13:18<01:40,  1.65s/it]487\n",
            "487\n",
            " 89% 488/548 [13:20<01:38,  1.64s/it]488\n",
            "488\n",
            " 89% 489/548 [13:22<01:39,  1.68s/it]489\n",
            "489\n",
            " 89% 490/548 [13:23<01:24,  1.46s/it]490\n",
            "490\n",
            " 90% 491/548 [13:24<01:27,  1.53s/it]491\n",
            "491\n",
            " 90% 492/548 [13:26<01:30,  1.61s/it]492\n",
            "492\n",
            " 90% 493/548 [13:27<01:21,  1.49s/it]493\n",
            "493\n",
            " 90% 494/548 [13:29<01:25,  1.58s/it]494\n",
            "494\n",
            " 90% 495/548 [13:31<01:24,  1.59s/it]495\n",
            "495\n",
            " 91% 496/548 [13:32<01:16,  1.48s/it]496\n",
            "496\n",
            " 91% 497/548 [13:34<01:18,  1.53s/it]497\n",
            "497\n",
            " 91% 498/548 [13:35<01:16,  1.54s/it]498\n",
            "498\n",
            " 91% 499/548 [13:37<01:16,  1.56s/it]499\n",
            "499\n",
            " 91% 500/548 [13:38<01:14,  1.55s/it]500\n",
            "500\n",
            " 91% 501/548 [13:39<01:04,  1.37s/it]501\n",
            "501\n",
            " 92% 502/548 [13:40<00:58,  1.28s/it]502\n",
            "502\n",
            " 92% 503/548 [13:42<01:02,  1.38s/it]503\n",
            "503\n",
            " 92% 504/548 [13:44<01:03,  1.44s/it]504\n",
            "504\n",
            " 92% 505/548 [13:45<00:55,  1.30s/it]505\n",
            "505\n",
            " 92% 506/548 [13:46<00:56,  1.35s/it]506\n",
            "506\n",
            " 93% 507/548 [13:47<00:51,  1.26s/it]507\n",
            "507\n",
            " 93% 508/548 [13:48<00:49,  1.23s/it]508\n",
            "508\n",
            " 93% 509/548 [13:50<00:53,  1.37s/it]509\n",
            "509\n",
            " 93% 510/548 [13:51<00:46,  1.22s/it]510\n",
            "510\n",
            " 93% 511/548 [13:52<00:49,  1.34s/it]511\n",
            "511\n",
            " 93% 512/548 [13:54<00:52,  1.45s/it]512\n",
            "512\n",
            " 94% 513/548 [13:57<01:04,  1.85s/it]513\n",
            "513\n",
            " 94% 514/548 [13:58<00:54,  1.61s/it]514\n",
            "514\n",
            " 94% 515/548 [13:59<00:47,  1.45s/it]515\n",
            "515\n",
            " 94% 516/548 [14:00<00:41,  1.29s/it]516\n",
            "516\n",
            " 94% 517/548 [14:02<00:43,  1.41s/it]517\n",
            "517\n",
            " 95% 518/548 [14:03<00:39,  1.31s/it]518\n",
            "518\n",
            " 95% 519/548 [14:04<00:34,  1.19s/it]519\n",
            "519\n",
            " 95% 520/548 [14:05<00:38,  1.37s/it]520\n",
            "520\n",
            " 95% 521/548 [14:07<00:37,  1.40s/it]521\n",
            "521\n",
            " 95% 522/548 [14:09<00:45,  1.75s/it]522\n",
            "522\n",
            " 95% 523/548 [14:11<00:41,  1.64s/it]523\n",
            "523\n",
            " 96% 524/548 [14:12<00:37,  1.55s/it]524\n",
            "524\n",
            " 96% 525/548 [14:13<00:31,  1.37s/it]525\n",
            "525\n",
            " 96% 526/548 [14:15<00:31,  1.45s/it]526\n",
            "526\n",
            " 96% 527/548 [14:16<00:28,  1.37s/it]527\n",
            "527\n",
            " 96% 528/548 [14:18<00:28,  1.44s/it]528\n",
            "528\n",
            " 97% 529/548 [14:19<00:28,  1.51s/it]529\n",
            "529\n",
            " 97% 530/548 [14:20<00:24,  1.36s/it]530\n",
            "530\n",
            " 97% 531/548 [14:22<00:23,  1.41s/it]531\n",
            "531\n",
            " 97% 532/548 [14:24<00:26,  1.66s/it]532\n",
            "532\n",
            " 97% 533/548 [14:26<00:24,  1.64s/it]533\n",
            "533\n",
            " 97% 534/548 [14:27<00:22,  1.62s/it]534\n",
            "534\n",
            " 98% 535/548 [14:29<00:21,  1.64s/it]535\n",
            "535\n",
            " 98% 536/548 [14:30<00:18,  1.55s/it]536\n",
            "536\n",
            " 98% 537/548 [14:31<00:15,  1.43s/it]537\n",
            "537\n",
            " 98% 538/548 [14:33<00:15,  1.54s/it]538\n",
            "538\n",
            " 98% 539/548 [14:34<00:12,  1.37s/it]539\n",
            "539\n",
            " 99% 540/548 [14:36<00:11,  1.49s/it]540\n",
            "540\n",
            " 99% 541/548 [14:38<00:11,  1.69s/it]541\n",
            "541\n",
            " 99% 542/548 [14:40<00:09,  1.66s/it]542\n",
            "542\n",
            " 99% 543/548 [14:41<00:08,  1.68s/it]543\n",
            "543\n",
            " 99% 544/548 [14:43<00:06,  1.66s/it]544\n",
            "544\n",
            " 99% 545/548 [14:44<00:04,  1.44s/it]545\n",
            "545\n",
            "100% 546/548 [14:45<00:02,  1.29s/it]546\n",
            "546\n",
            "100% 547/548 [14:47<00:01,  1.40s/it]547\n",
            "547\n",
            "100% 548/548 [14:48<00:00,  1.62s/it]\n",
            "data/visdrone/VisDrone2019-DET-val\n",
            ".visdrone_det_txt\n",
            "data/visdrone/VisDrone2019-DET-val/annotations\n",
            "data/visdrone/VisDrone2019-DET-val/images\n",
            "\n",
            "evaluating object category 1/10...\n",
            "evaluating object category 2/10...\n",
            "evaluating object category 3/10...\n",
            "evaluating object category 4/10...\n",
            "evaluating object category 5/10...\n",
            "evaluating object category 6/10...\n",
            "evaluating object category 7/10...\n",
            "evaluating object category 8/10...\n",
            "evaluating object category 9/10...\n",
            "evaluating object category 10/10...\n",
            "Evaluation completed. The performance of the detector is presented as follows.\n",
            "Average Precision  (AP) @[ IoU=0.50:0.95 | maxDets=500 ] = 26.08071517944336%.\n",
            "Average Precision  (AP) @[ IoU=0.50      | maxDets=500 ] = 47.4874267578125%.\n",
            "Average Precision  (AP) @[ IoU=0.75      | maxDets=500 ] = 24.97474479675293%.\n",
            "Average Recall     (AR) @[ IoU=0.50:0.95 | maxDets=  1 ] = 0.4551522731781006%.\n",
            "Average Recall     (AR) @[ IoU=0.50:0.95 | maxDets= 10 ] = 4.702436923980713%.\n",
            "Average Recall     (AR) @[ IoU=0.50:0.95 | maxDets=100 ] = 33.33198547363281%.\n",
            "Average Recall     (AR) @[ IoU=0.50:0.95 | maxDets=500 ] = 39.559383392333984%.\n",
            "Class 0 AP = 28.422632217407227%\n",
            "Class 1 AP = 18.919710159301758%\n",
            "Class 2 AP = 10.726886749267578%\n",
            "Class 3 AP = 58.101158142089844%\n",
            "Class 4 AP = 31.40835189819336%\n",
            "Class 5 AP = 16.938243865966797%\n",
            "Class 6 AP = 13.833226203918457%\n",
            "Class 7 AP = 6.0149431228637695%\n",
            "Class 8 AP = 32.82426071166992%\n",
            "Class 9 AP = 24.342653274536133%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "rmReWcpwrt_M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "D-02OBbMrt9o"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!python infer_visdrone.py --config-file configs/visdrone/querydet_test.yaml --num-gpu 1 --eval-only MODEL.WEIGHTS work_dirs/visdrone_querydet/model_0005999.pth OUTPUT_DIR work_dirs/model_test-5"
      ],
      "metadata": {
        "id": "R-dtTZd3rt5L",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8c1b8bd4-884c-4575-f7d8-dd5cf1b1bbe6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "** fvcore version of PathManager will be deprecated soon. **\n",
            "** Please migrate to the version in iopath repo. **\n",
            "https://github.com/facebookresearch/iopath \n",
            "\n",
            "Command Line Args: Namespace(config_file='configs/visdrone/querydet_test.yaml', dist_url='tcp://127.0.0.1:49152', eval_only=True, machine_rank=0, no_pretrain=False, num_gpus=1, num_machines=1, opts=['MODEL.WEIGHTS', 'work_dirs/visdrone_querydet/model_0005999.pth', 'OUTPUT_DIR', 'work_dirs/model_test-5'], resume=False)\n",
            "Loading config configs/visdrone/querydet_test.yaml with yaml.unsafe_load. Your machine may be at risk if the file contains malicious content.\n",
            "Loading config configs/visdrone/../BaseRetina.yaml with yaml.unsafe_load. Your machine may be at risk if the file contains malicious content.\n",
            "\u001b[32m[04/15 13:28:33 detectron2]: \u001b[0mRank of current process: 0. World size: 1\n",
            "\u001b[32m[04/15 13:28:35 detectron2]: \u001b[0mEnvironment info:\n",
            "----------------------  ---------------------------------------------------------------\n",
            "sys.platform            linux\n",
            "Python                  3.7.6 (default, Jan  8 2020, 19:59:22) [GCC 7.3.0]\n",
            "numpy                   1.21.6\n",
            "detectron2              0.4 @/usr/local/lib/python3.7/site-packages/detectron2\n",
            "Compiler                GCC 7.3\n",
            "CUDA compiler           CUDA 10.1\n",
            "detectron2 arch flags   3.7, 5.0, 5.2, 6.0, 6.1, 7.0, 7.5\n",
            "DETECTRON2_ENV_MODULE   <not set>\n",
            "PyTorch                 1.8.1+cu101 @/usr/local/lib/python3.7/site-packages/torch\n",
            "PyTorch debug build     False\n",
            "GPU available           True\n",
            "GPU 0                   Tesla T4 (arch=7.5)\n",
            "CUDA_HOME               /usr/local/cuda\n",
            "Pillow                  9.5.0\n",
            "torchvision             0.9.1+cu101 @/usr/local/lib/python3.7/site-packages/torchvision\n",
            "torchvision arch flags  3.5, 5.0, 6.0, 7.0, 7.5\n",
            "fvcore                  0.1.3.post20210317\n",
            "cv2                     4.7.0\n",
            "----------------------  ---------------------------------------------------------------\n",
            "PyTorch built with:\n",
            "  - GCC 7.3\n",
            "  - C++ Version: 201402\n",
            "  - Intel(R) Math Kernel Library Version 2020.0.0 Product Build 20191122 for Intel(R) 64 architecture applications\n",
            "  - Intel(R) MKL-DNN v1.7.0 (Git Hash 7aed236906b1f7a05c0917e5257a1af05e9ff683)\n",
            "  - OpenMP 201511 (a.k.a. OpenMP 4.5)\n",
            "  - NNPACK is enabled\n",
            "  - CPU capability usage: AVX2\n",
            "  - CUDA Runtime 10.1\n",
            "  - NVCC architecture flags: -gencode;arch=compute_37,code=sm_37;-gencode;arch=compute_50,code=sm_50;-gencode;arch=compute_60,code=sm_60;-gencode;arch=compute_70,code=sm_70\n",
            "  - CuDNN 7.6.3\n",
            "  - Magma 2.5.2\n",
            "  - Build settings: BLAS_INFO=mkl, BUILD_TYPE=Release, CUDA_VERSION=10.1, CUDNN_VERSION=7.6.3, CXX_COMPILER=/opt/rh/devtoolset-7/root/usr/bin/c++, CXX_FLAGS= -Wno-deprecated -fvisibility-inlines-hidden -DUSE_PTHREADPOOL -fopenmp -DNDEBUG -DUSE_KINETO -DUSE_FBGEMM -DUSE_QNNPACK -DUSE_PYTORCH_QNNPACK -DUSE_XNNPACK -O2 -fPIC -Wno-narrowing -Wall -Wextra -Werror=return-type -Wno-missing-field-initializers -Wno-type-limits -Wno-array-bounds -Wno-unknown-pragmas -Wno-sign-compare -Wno-unused-parameter -Wno-unused-variable -Wno-unused-function -Wno-unused-result -Wno-unused-local-typedefs -Wno-strict-overflow -Wno-strict-aliasing -Wno-error=deprecated-declarations -Wno-stringop-overflow -Wno-psabi -Wno-error=pedantic -Wno-error=redundant-decls -Wno-error=old-style-cast -fdiagnostics-color=always -faligned-new -Wno-unused-but-set-variable -Wno-maybe-uninitialized -fno-math-errno -fno-trapping-math -Werror=format -Wno-stringop-overflow, LAPACK_INFO=mkl, PERF_WITH_AVX=1, PERF_WITH_AVX2=1, PERF_WITH_AVX512=1, TORCH_VERSION=1.8.1, USE_CUDA=ON, USE_CUDNN=ON, USE_EXCEPTION_PTR=1, USE_GFLAGS=OFF, USE_GLOG=OFF, USE_MKL=ON, USE_MKLDNN=ON, USE_MPI=OFF, USE_NCCL=ON, USE_NNPACK=ON, USE_OPENMP=ON, \n",
            "\n",
            "\u001b[32m[04/15 13:28:35 detectron2]: \u001b[0mCommand line arguments: Namespace(config_file='configs/visdrone/querydet_test.yaml', dist_url='tcp://127.0.0.1:49152', eval_only=True, machine_rank=0, no_pretrain=False, num_gpus=1, num_machines=1, opts=['MODEL.WEIGHTS', 'work_dirs/visdrone_querydet/model_0005999.pth', 'OUTPUT_DIR', 'work_dirs/model_test-5'], resume=False)\n",
            "\u001b[32m[04/15 13:28:35 detectron2]: \u001b[0mContents of args.config_file=configs/visdrone/querydet_test.yaml:\n",
            "_BASE_: \"../BaseRetina.yaml\"\n",
            "OUTPUT_DIR: \"work_dirs/model_test\"\n",
            "\n",
            "MODEL:\n",
            "  META_ARCHITECTURE: \"RetinaNetQueryDet\"\n",
            "  WEIGHTS: \"detectron2://ImageNetPretrained/MSRA/R-50.pkl\"\n",
            "  \n",
            "  RESNETS:\n",
            "    DEPTH: 50\n",
            "  \n",
            "  ANCHOR_GENERATOR:\n",
            "    NAME: \"AnchorGeneratorWithCenter\"\n",
            "    SIZES: !!python/object/apply:eval [\"[[x, x * 2**(1.0/3), x * 2**(2.0/3)] for x in [16, 32, 64, 128, 256, 512]]\"]\n",
            "  \n",
            "  RETINANET:\n",
            "    IOU_THRESHOLDS: [0.4, 0.5]\n",
            "    IOU_LABELS: [0, -1, 1]\n",
            "    NUM_CLASSES: 10\n",
            "    IN_FEATURES: [\"p2\", \"p3\", \"p4\", \"p5\", \"p6\", \"p7\"]  \n",
            "    SCORE_THRESH_TEST: 0.0001\n",
            "    \n",
            "  RESNETS:\n",
            "    OUT_FEATURES: [\"res2\", \"res3\", \"res4\", \"res5\"]\n",
            "\n",
            "  FPN:\n",
            "    IN_FEATURES: [\"res2\", \"res3\", \"res4\", \"res5\"]\n",
            "  \n",
            "  QUERY:\n",
            "    FEATURES_WHOLE_TEST: [2, 3, 4, 5]\n",
            "    FEATURES_VALUE_TEST: [0, 1]\n",
            "    Q_FEATURE_TRAIN: [1, 2]\n",
            "    Q_FEATURE_TEST: [1, 2]\n",
            "    \n",
            "    ENCODE_CENTER_DIS_COEFF: [1., 1.]\n",
            "    ENCODE_SMALL_OBJ_SCALE: [[0, 32], [0, 64]]\n",
            "\n",
            "    THRESHOLD: 0.12\n",
            "    QUERY_INFER: False\n",
            "  \n",
            "  CUSTOM: \n",
            "    USE_SOFT_NMS: False\n",
            "    SOFT_NMS_METHOD: 'gaussian'\n",
            "    SOFT_NMS_SIGMA: 0.6\n",
            "    SOFT_NMS_THRESHOLD: 0.4\n",
            "    SOFT_NMS_PRUND: 0.0001\n",
            "\n",
            "VISDRONE:\n",
            "  TEST_LENGTH: 3999\n",
            "\n",
            "TEST:  \n",
            "  DETECTIONS_PER_IMAGE: 500\n",
            "\n",
            "META_INFO:\n",
            "  EVAL_GPU_TIME: True\n",
            "\n",
            "\u001b[32m[04/15 13:28:35 detectron2]: \u001b[0mRunning with full config:\n",
            "CUDNN_BENCHMARK: False\n",
            "DATALOADER:\n",
            "  ASPECT_RATIO_GROUPING: True\n",
            "  FILTER_EMPTY_ANNOTATIONS: True\n",
            "  NUM_WORKERS: 4\n",
            "  REPEAT_THRESHOLD: 0.0\n",
            "  SAMPLER_TRAIN: TrainingSampler\n",
            "DATASETS:\n",
            "  PRECOMPUTED_PROPOSAL_TOPK_TEST: 1000\n",
            "  PRECOMPUTED_PROPOSAL_TOPK_TRAIN: 2000\n",
            "  PROPOSAL_FILES_TEST: ()\n",
            "  PROPOSAL_FILES_TRAIN: ()\n",
            "  TEST: ('coco_2017_val',)\n",
            "  TRAIN: ('coco_2017_train',)\n",
            "GLOBAL:\n",
            "  HACK: 1.0\n",
            "INPUT:\n",
            "  CROP:\n",
            "    ENABLED: False\n",
            "    SIZE: [0.9, 0.9]\n",
            "    TYPE: relative_range\n",
            "  FORMAT: BGR\n",
            "  MASK_FORMAT: polygon\n",
            "  MAX_SIZE_TEST: 1333\n",
            "  MAX_SIZE_TRAIN: 1333\n",
            "  MIN_SIZE_TEST: 800\n",
            "  MIN_SIZE_TRAIN: (640, 672, 704, 736, 768, 800)\n",
            "  MIN_SIZE_TRAIN_SAMPLING: choice\n",
            "  RANDOM_FLIP: horizontal\n",
            "META_INFO:\n",
            "  EVAL_AP: True\n",
            "  EVAL_GPU_TIME: True\n",
            "  VIS_ROOT: \n",
            "MODEL:\n",
            "  ANCHOR_GENERATOR:\n",
            "    ANGLES: [[-90, 0, 90]]\n",
            "    ASPECT_RATIOS: [[0.5, 1.0, 2.0]]\n",
            "    NAME: AnchorGeneratorWithCenter\n",
            "    OFFSET: 0.0\n",
            "    SIZES: [[16, 20.15873679831797, 25.39841683149119], [32, 40.31747359663594, 50.79683366298238], [64, 80.63494719327188, 101.59366732596476], [128, 161.26989438654377, 203.18733465192952], [256, 322.53978877308754, 406.37466930385904], [512, 645.0795775461751, 812.7493386077181]]\n",
            "  BACKBONE:\n",
            "    FREEZE_AT: 2\n",
            "    NAME: build_retinanet_resnet_fpn_backbone\n",
            "  CUSTOM:\n",
            "    CLEAR_CUDA_CACHE: False\n",
            "    CLS_WEIGHTS: []\n",
            "    FOCAL_LOSS_ALPHAS: []\n",
            "    FOCAL_LOSS_GAMMAS: []\n",
            "    GIOU_LOSS: False\n",
            "    GRADIENT_CHECKPOINT: False\n",
            "    HEAD_BN: False\n",
            "    REG_WEIGHTS: []\n",
            "    SOFT_NMS_METHOD: gaussian\n",
            "    SOFT_NMS_PRUND: 0.0001\n",
            "    SOFT_NMS_SIGMA: 0.6\n",
            "    SOFT_NMS_THRESHOLD: 0.4\n",
            "    USE_LOOP_MATCHER: False\n",
            "    USE_SOFT_NMS: False\n",
            "  DEVICE: cuda\n",
            "  FPN:\n",
            "    FUSE_TYPE: sum\n",
            "    IN_FEATURES: ['res2', 'res3', 'res4', 'res5']\n",
            "    NORM: \n",
            "    OUT_CHANNELS: 256\n",
            "    TOP_LEVELS: 2\n",
            "  KEYPOINT_ON: False\n",
            "  LOAD_PROPOSALS: False\n",
            "  MASK_ON: False\n",
            "  META_ARCHITECTURE: RetinaNetQueryDet\n",
            "  PANOPTIC_FPN:\n",
            "    COMBINE:\n",
            "      ENABLED: True\n",
            "      INSTANCES_CONFIDENCE_THRESH: 0.5\n",
            "      OVERLAP_THRESH: 0.5\n",
            "      STUFF_AREA_LIMIT: 4096\n",
            "    INSTANCE_LOSS_WEIGHT: 1.0\n",
            "  PIXEL_MEAN: [103.53, 116.28, 123.675]\n",
            "  PIXEL_STD: [1.0, 1.0, 1.0]\n",
            "  PROPOSAL_GENERATOR:\n",
            "    MIN_SIZE: 0\n",
            "    NAME: RPN\n",
            "  QUERY:\n",
            "    CONTEXT: 2\n",
            "    ENCODE_CENTER_DIS_COEFF: [1.0, 1.0]\n",
            "    ENCODE_SMALL_OBJ_SCALE: [[0, 32], [0, 64]]\n",
            "    FEATURES_VALUE_TEST: [0, 1]\n",
            "    FEATURES_VALUE_TRAIN: [0, 1]\n",
            "    FEATURES_WHOLE_TEST: [2, 3, 4, 5]\n",
            "    FEATURES_WHOLE_TRAIN: [2, 3, 4, 5]\n",
            "    QUERY_INFER: False\n",
            "    QUERY_LOSS_GAMMA: []\n",
            "    QUERY_LOSS_WEIGHT: []\n",
            "    Q_FEATURE_TEST: [1, 2]\n",
            "    Q_FEATURE_TRAIN: [1, 2]\n",
            "    THRESHOLD: 0.12\n",
            "  RESNETS:\n",
            "    DEFORM_MODULATED: False\n",
            "    DEFORM_NUM_GROUPS: 1\n",
            "    DEFORM_ON_PER_STAGE: [False, False, False, False]\n",
            "    DEPTH: 50\n",
            "    NORM: FrozenBN\n",
            "    NUM_GROUPS: 1\n",
            "    OUT_FEATURES: ['res2', 'res3', 'res4', 'res5']\n",
            "    RES2_OUT_CHANNELS: 256\n",
            "    RES5_DILATION: 1\n",
            "    STEM_OUT_CHANNELS: 64\n",
            "    STRIDE_IN_1X1: True\n",
            "    WIDTH_PER_GROUP: 64\n",
            "  RETINANET:\n",
            "    BBOX_REG_LOSS_TYPE: smooth_l1\n",
            "    BBOX_REG_WEIGHTS: (1.0, 1.0, 1.0, 1.0)\n",
            "    FOCAL_LOSS_ALPHA: 0.25\n",
            "    FOCAL_LOSS_GAMMA: 2.0\n",
            "    IN_FEATURES: ['p2', 'p3', 'p4', 'p5', 'p6', 'p7']\n",
            "    IOU_LABELS: [0, -1, 1]\n",
            "    IOU_THRESHOLDS: [0.4, 0.5]\n",
            "    NMS_THRESH_TEST: 0.5\n",
            "    NORM: \n",
            "    NUM_CLASSES: 10\n",
            "    NUM_CONVS: 4\n",
            "    PRIOR_PROB: 0.01\n",
            "    SCORE_THRESH_TEST: 0.0001\n",
            "    SMOOTH_L1_LOSS_BETA: 0.1\n",
            "    TOPK_CANDIDATES_TEST: 1000\n",
            "  ROI_BOX_CASCADE_HEAD:\n",
            "    BBOX_REG_WEIGHTS: ((10.0, 10.0, 5.0, 5.0), (20.0, 20.0, 10.0, 10.0), (30.0, 30.0, 15.0, 15.0))\n",
            "    IOUS: (0.5, 0.6, 0.7)\n",
            "  ROI_BOX_HEAD:\n",
            "    BBOX_REG_LOSS_TYPE: smooth_l1\n",
            "    BBOX_REG_LOSS_WEIGHT: 1.0\n",
            "    BBOX_REG_WEIGHTS: (10.0, 10.0, 5.0, 5.0)\n",
            "    CLS_AGNOSTIC_BBOX_REG: False\n",
            "    CONV_DIM: 256\n",
            "    FC_DIM: 1024\n",
            "    NAME: \n",
            "    NORM: \n",
            "    NUM_CONV: 0\n",
            "    NUM_FC: 0\n",
            "    POOLER_RESOLUTION: 14\n",
            "    POOLER_SAMPLING_RATIO: 0\n",
            "    POOLER_TYPE: ROIAlignV2\n",
            "    SMOOTH_L1_BETA: 0.0\n",
            "    TRAIN_ON_PRED_BOXES: False\n",
            "  ROI_HEADS:\n",
            "    BATCH_SIZE_PER_IMAGE: 512\n",
            "    IN_FEATURES: ['res4']\n",
            "    IOU_LABELS: [0, 1]\n",
            "    IOU_THRESHOLDS: [0.5]\n",
            "    NAME: Res5ROIHeads\n",
            "    NMS_THRESH_TEST: 0.5\n",
            "    NUM_CLASSES: 80\n",
            "    POSITIVE_FRACTION: 0.25\n",
            "    PROPOSAL_APPEND_GT: True\n",
            "    SCORE_THRESH_TEST: 0.05\n",
            "  ROI_KEYPOINT_HEAD:\n",
            "    CONV_DIMS: (512, 512, 512, 512, 512, 512, 512, 512)\n",
            "    LOSS_WEIGHT: 1.0\n",
            "    MIN_KEYPOINTS_PER_IMAGE: 1\n",
            "    NAME: KRCNNConvDeconvUpsampleHead\n",
            "    NORMALIZE_LOSS_BY_VISIBLE_KEYPOINTS: True\n",
            "    NUM_KEYPOINTS: 17\n",
            "    POOLER_RESOLUTION: 14\n",
            "    POOLER_SAMPLING_RATIO: 0\n",
            "    POOLER_TYPE: ROIAlignV2\n",
            "  ROI_MASK_HEAD:\n",
            "    CLS_AGNOSTIC_MASK: False\n",
            "    CONV_DIM: 256\n",
            "    NAME: MaskRCNNConvUpsampleHead\n",
            "    NORM: \n",
            "    NUM_CONV: 0\n",
            "    POOLER_RESOLUTION: 14\n",
            "    POOLER_SAMPLING_RATIO: 0\n",
            "    POOLER_TYPE: ROIAlignV2\n",
            "  RPN:\n",
            "    BATCH_SIZE_PER_IMAGE: 256\n",
            "    BBOX_REG_LOSS_TYPE: smooth_l1\n",
            "    BBOX_REG_LOSS_WEIGHT: 1.0\n",
            "    BBOX_REG_WEIGHTS: (1.0, 1.0, 1.0, 1.0)\n",
            "    BOUNDARY_THRESH: -1\n",
            "    HEAD_NAME: StandardRPNHead\n",
            "    IN_FEATURES: ['res4']\n",
            "    IOU_LABELS: [0, -1, 1]\n",
            "    IOU_THRESHOLDS: [0.3, 0.7]\n",
            "    LOSS_WEIGHT: 1.0\n",
            "    NMS_THRESH: 0.7\n",
            "    POSITIVE_FRACTION: 0.5\n",
            "    POST_NMS_TOPK_TEST: 1000\n",
            "    POST_NMS_TOPK_TRAIN: 2000\n",
            "    PRE_NMS_TOPK_TEST: 6000\n",
            "    PRE_NMS_TOPK_TRAIN: 12000\n",
            "    SMOOTH_L1_BETA: 0.0\n",
            "  SEM_SEG_HEAD:\n",
            "    COMMON_STRIDE: 4\n",
            "    CONVS_DIM: 128\n",
            "    IGNORE_VALUE: 255\n",
            "    IN_FEATURES: ['p2', 'p3', 'p4', 'p5']\n",
            "    LOSS_WEIGHT: 1.0\n",
            "    NAME: SemSegFPNHead\n",
            "    NORM: GN\n",
            "    NUM_CLASSES: 54\n",
            "  WEIGHTS: work_dirs/visdrone_querydet/model_0005999.pth\n",
            "OUTPUT_DIR: work_dirs/model_test-5\n",
            "SEED: -1\n",
            "SOLVER:\n",
            "  AMP:\n",
            "    ENABLED: False\n",
            "  BASE_LR: 0.01\n",
            "  BIAS_LR_FACTOR: 1.0\n",
            "  CHECKPOINT_PERIOD: 5000\n",
            "  CLIP_GRADIENTS:\n",
            "    CLIP_TYPE: value\n",
            "    CLIP_VALUE: 1.0\n",
            "    ENABLED: False\n",
            "    NORM_TYPE: 2.0\n",
            "  GAMMA: 0.1\n",
            "  IMS_PER_BATCH: 16\n",
            "  LR_SCHEDULER_NAME: WarmupMultiStepLR\n",
            "  MAX_ITER: 90000\n",
            "  MOMENTUM: 0.9\n",
            "  NESTEROV: False\n",
            "  REFERENCE_WORLD_SIZE: 0\n",
            "  STEPS: (60000, 80000)\n",
            "  WARMUP_FACTOR: 0.001\n",
            "  WARMUP_ITERS: 1000\n",
            "  WARMUP_METHOD: linear\n",
            "  WEIGHT_DECAY: 0.0001\n",
            "  WEIGHT_DECAY_BIAS: 0.0001\n",
            "  WEIGHT_DECAY_NORM: 0.0\n",
            "TEST:\n",
            "  AUG:\n",
            "    ENABLED: False\n",
            "    FLIP: True\n",
            "    MAX_SIZE: 4000\n",
            "    MIN_SIZES: (400, 500, 600, 700, 800, 900, 1000, 1100, 1200)\n",
            "  DETECTIONS_PER_IMAGE: 500\n",
            "  EVAL_PERIOD: 0\n",
            "  EXPECTED_RESULTS: []\n",
            "  KEYPOINT_OKS_SIGMAS: []\n",
            "  PRECISE_BN:\n",
            "    ENABLED: False\n",
            "    NUM_ITER: 200\n",
            "VERSION: 2\n",
            "VISDRONE:\n",
            "  MAX_LENGTH: 1999\n",
            "  SHORT_LENGTH: [1200]\n",
            "  TEST_IMG_ROOT: data/visdrone/coco_format/val_images\n",
            "  TEST_JSON: data/visdrone/coco_format/annotations/val_label.json\n",
            "  TEST_LENGTH: 3999\n",
            "  TRAIN_JSON: data/visdrone/coco_format/annotations/train_label.json\n",
            "  TRING_IMG_ROOT: data//visdrone/coco_format/train_images\n",
            "VIS_PERIOD: 0\n",
            "\u001b[32m[04/15 13:28:36 detectron2]: \u001b[0mFull config saved to work_dirs/model_test-5/config.yaml\n",
            "\u001b[32m[04/15 13:28:36 d2.utils.env]: \u001b[0mUsing a generated random seed 36745523\n",
            "\u001b[32m[04/15 13:28:41 d2.engine.defaults]: \u001b[0mModel:\n",
            "RetinaNetQueryDet(\n",
            "  (backbone): FPN(\n",
            "    (fpn_lateral2): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1))\n",
            "    (fpn_output2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (fpn_lateral3): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1))\n",
            "    (fpn_output3): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (fpn_lateral4): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1))\n",
            "    (fpn_output4): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (fpn_lateral5): Conv2d(2048, 256, kernel_size=(1, 1), stride=(1, 1))\n",
            "    (fpn_output5): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (top_block): LastLevelP6P7(\n",
            "      (p6): Conv2d(2048, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
            "      (p7): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
            "    )\n",
            "    (bottom_up): ResNet(\n",
            "      (stem): BasicStem(\n",
            "        (conv1): Conv2d(\n",
            "          3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False\n",
            "          (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)\n",
            "        )\n",
            "      )\n",
            "      (res2): Sequential(\n",
            "        (0): BottleneckBlock(\n",
            "          (shortcut): Conv2d(\n",
            "            64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
            "          )\n",
            "          (conv1): Conv2d(\n",
            "            64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)\n",
            "          )\n",
            "          (conv2): Conv2d(\n",
            "            64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)\n",
            "          )\n",
            "          (conv3): Conv2d(\n",
            "            64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
            "          )\n",
            "        )\n",
            "        (1): BottleneckBlock(\n",
            "          (conv1): Conv2d(\n",
            "            256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)\n",
            "          )\n",
            "          (conv2): Conv2d(\n",
            "            64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)\n",
            "          )\n",
            "          (conv3): Conv2d(\n",
            "            64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
            "          )\n",
            "        )\n",
            "        (2): BottleneckBlock(\n",
            "          (conv1): Conv2d(\n",
            "            256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)\n",
            "          )\n",
            "          (conv2): Conv2d(\n",
            "            64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)\n",
            "          )\n",
            "          (conv3): Conv2d(\n",
            "            64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
            "          )\n",
            "        )\n",
            "      )\n",
            "      (res3): Sequential(\n",
            "        (0): BottleneckBlock(\n",
            "          (shortcut): Conv2d(\n",
            "            256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
            "          )\n",
            "          (conv1): Conv2d(\n",
            "            256, 128, kernel_size=(1, 1), stride=(2, 2), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)\n",
            "          )\n",
            "          (conv2): Conv2d(\n",
            "            128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)\n",
            "          )\n",
            "          (conv3): Conv2d(\n",
            "            128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
            "          )\n",
            "        )\n",
            "        (1): BottleneckBlock(\n",
            "          (conv1): Conv2d(\n",
            "            512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)\n",
            "          )\n",
            "          (conv2): Conv2d(\n",
            "            128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)\n",
            "          )\n",
            "          (conv3): Conv2d(\n",
            "            128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
            "          )\n",
            "        )\n",
            "        (2): BottleneckBlock(\n",
            "          (conv1): Conv2d(\n",
            "            512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)\n",
            "          )\n",
            "          (conv2): Conv2d(\n",
            "            128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)\n",
            "          )\n",
            "          (conv3): Conv2d(\n",
            "            128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
            "          )\n",
            "        )\n",
            "        (3): BottleneckBlock(\n",
            "          (conv1): Conv2d(\n",
            "            512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)\n",
            "          )\n",
            "          (conv2): Conv2d(\n",
            "            128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)\n",
            "          )\n",
            "          (conv3): Conv2d(\n",
            "            128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
            "          )\n",
            "        )\n",
            "      )\n",
            "      (res4): Sequential(\n",
            "        (0): BottleneckBlock(\n",
            "          (shortcut): Conv2d(\n",
            "            512, 1024, kernel_size=(1, 1), stride=(2, 2), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n",
            "          )\n",
            "          (conv1): Conv2d(\n",
            "            512, 256, kernel_size=(1, 1), stride=(2, 2), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
            "          )\n",
            "          (conv2): Conv2d(\n",
            "            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
            "          )\n",
            "          (conv3): Conv2d(\n",
            "            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n",
            "          )\n",
            "        )\n",
            "        (1): BottleneckBlock(\n",
            "          (conv1): Conv2d(\n",
            "            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
            "          )\n",
            "          (conv2): Conv2d(\n",
            "            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
            "          )\n",
            "          (conv3): Conv2d(\n",
            "            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n",
            "          )\n",
            "        )\n",
            "        (2): BottleneckBlock(\n",
            "          (conv1): Conv2d(\n",
            "            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
            "          )\n",
            "          (conv2): Conv2d(\n",
            "            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
            "          )\n",
            "          (conv3): Conv2d(\n",
            "            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n",
            "          )\n",
            "        )\n",
            "        (3): BottleneckBlock(\n",
            "          (conv1): Conv2d(\n",
            "            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
            "          )\n",
            "          (conv2): Conv2d(\n",
            "            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
            "          )\n",
            "          (conv3): Conv2d(\n",
            "            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n",
            "          )\n",
            "        )\n",
            "        (4): BottleneckBlock(\n",
            "          (conv1): Conv2d(\n",
            "            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
            "          )\n",
            "          (conv2): Conv2d(\n",
            "            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
            "          )\n",
            "          (conv3): Conv2d(\n",
            "            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n",
            "          )\n",
            "        )\n",
            "        (5): BottleneckBlock(\n",
            "          (conv1): Conv2d(\n",
            "            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
            "          )\n",
            "          (conv2): Conv2d(\n",
            "            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
            "          )\n",
            "          (conv3): Conv2d(\n",
            "            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n",
            "          )\n",
            "        )\n",
            "      )\n",
            "      (res5): Sequential(\n",
            "        (0): BottleneckBlock(\n",
            "          (shortcut): Conv2d(\n",
            "            1024, 2048, kernel_size=(1, 1), stride=(2, 2), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=2048, eps=1e-05)\n",
            "          )\n",
            "          (conv1): Conv2d(\n",
            "            1024, 512, kernel_size=(1, 1), stride=(2, 2), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
            "          )\n",
            "          (conv2): Conv2d(\n",
            "            512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
            "          )\n",
            "          (conv3): Conv2d(\n",
            "            512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=2048, eps=1e-05)\n",
            "          )\n",
            "        )\n",
            "        (1): BottleneckBlock(\n",
            "          (conv1): Conv2d(\n",
            "            2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
            "          )\n",
            "          (conv2): Conv2d(\n",
            "            512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
            "          )\n",
            "          (conv3): Conv2d(\n",
            "            512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=2048, eps=1e-05)\n",
            "          )\n",
            "        )\n",
            "        (2): BottleneckBlock(\n",
            "          (conv1): Conv2d(\n",
            "            2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
            "          )\n",
            "          (conv2): Conv2d(\n",
            "            512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
            "          )\n",
            "          (conv3): Conv2d(\n",
            "            512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "            (norm): FrozenBatchNorm2d(num_features=2048, eps=1e-05)\n",
            "          )\n",
            "        )\n",
            "      )\n",
            "    )\n",
            "  )\n",
            "  (det_head): RetinaNetHead_3x3(\n",
            "    (cls_layer_0): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (bbox_layer_0): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (cls_layer_1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (bbox_layer_1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (cls_layer_2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (bbox_layer_2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (cls_layer_3): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (bbox_layer_3): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (cls_score): Conv2d(256, 90, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (bbox_pred): Conv2d(256, 36, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "  )\n",
            "  (query_head): Head_3x3(\n",
            "    (layer_0): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (layer_1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (layer_2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (layer_3): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (pred_net): Conv2d(256, 1, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "  )\n",
            "  (anchor_generator): AnchorGeneratorWithCenter(\n",
            "    (cell_anchors): BufferList()\n",
            "  )\n",
            "  (query_anchor_generator): AnchorGeneratorWithCenter(\n",
            "    (cell_anchors): BufferList()\n",
            "  )\n",
            ")\n",
            "\u001b[32m[04/15 13:28:41 fvcore.common.checkpoint]: \u001b[0mLoading checkpoint from work_dirs/visdrone_querydet/model_0005999.pth\n",
            "\u001b[32m[04/15 13:29:05 d2.data.common]: \u001b[0mSerializing 548 elements to byte tensors and concatenating them all ...\n",
            "\u001b[32m[04/15 13:29:05 d2.data.common]: \u001b[0mSerialized dataset takes 0.08 MiB\n",
            "/usr/local/lib/python3.7/site-packages/torch/utils/data/dataloader.py:477: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  cpuset_checked))\n",
            "\u001b[32m[04/15 13:29:05 d2.evaluation.evaluator]: \u001b[0mStart inference on 548 images\n",
            "\u001b[32m[04/15 13:29:31 d2.evaluation.evaluator]: \u001b[0mInference done 11/548. 2.2236 s / img. ETA=0:19:56\n",
            "\u001b[32m[04/15 13:29:38 d2.evaluation.evaluator]: \u001b[0mInference done 14/548. 2.2398 s / img. ETA=0:19:59\n",
            "\u001b[32m[04/15 13:29:45 d2.evaluation.evaluator]: \u001b[0mInference done 17/548. 2.2493 s / img. ETA=0:19:57\n",
            "\u001b[32m[04/15 13:29:52 d2.evaluation.evaluator]: \u001b[0mInference done 20/548. 2.2589 s / img. ETA=0:19:56\n",
            "\u001b[32m[04/15 13:29:59 d2.evaluation.evaluator]: \u001b[0mInference done 23/548. 2.2717 s / img. ETA=0:19:56\n",
            "\u001b[32m[04/15 13:30:06 d2.evaluation.evaluator]: \u001b[0mInference done 26/548. 2.2834 s / img. ETA=0:19:55\n",
            "\u001b[32m[04/15 13:30:13 d2.evaluation.evaluator]: \u001b[0mInference done 29/548. 2.2954 s / img. ETA=0:19:55\n",
            "\u001b[32m[04/15 13:30:20 d2.evaluation.evaluator]: \u001b[0mInference done 32/548. 2.3036 s / img. ETA=0:19:52\n",
            "\u001b[32m[04/15 13:30:27 d2.evaluation.evaluator]: \u001b[0mInference done 35/548. 2.3091 s / img. ETA=0:19:48\n",
            "\u001b[32m[04/15 13:30:34 d2.evaluation.evaluator]: \u001b[0mInference done 38/548. 2.3125 s / img. ETA=0:19:43\n",
            "\u001b[32m[04/15 13:30:41 d2.evaluation.evaluator]: \u001b[0mInference done 41/548. 2.3135 s / img. ETA=0:19:36\n",
            "\u001b[32m[04/15 13:30:48 d2.evaluation.evaluator]: \u001b[0mInference done 44/548. 2.3134 s / img. ETA=0:19:29\n",
            "\u001b[32m[04/15 13:30:55 d2.evaluation.evaluator]: \u001b[0mInference done 47/548. 2.3128 s / img. ETA=0:19:22\n",
            "\u001b[32m[04/15 13:31:02 d2.evaluation.evaluator]: \u001b[0mInference done 50/548. 2.3123 s / img. ETA=0:19:15\n",
            "\u001b[32m[04/15 13:31:09 d2.evaluation.evaluator]: \u001b[0mInference done 53/548. 2.3125 s / img. ETA=0:19:08\n",
            "\u001b[32m[04/15 13:31:16 d2.evaluation.evaluator]: \u001b[0mInference done 56/548. 2.3130 s / img. ETA=0:19:01\n",
            "\u001b[32m[04/15 13:31:23 d2.evaluation.evaluator]: \u001b[0mInference done 59/548. 2.3142 s / img. ETA=0:18:55\n",
            "\u001b[32m[04/15 13:31:30 d2.evaluation.evaluator]: \u001b[0mInference done 62/548. 2.3157 s / img. ETA=0:18:50\n",
            "\u001b[32m[04/15 13:31:37 d2.evaluation.evaluator]: \u001b[0mInference done 65/548. 2.3164 s / img. ETA=0:18:43\n",
            "\u001b[32m[04/15 13:31:44 d2.evaluation.evaluator]: \u001b[0mInference done 68/548. 2.3181 s / img. ETA=0:18:37\n",
            "\u001b[32m[04/15 13:31:51 d2.evaluation.evaluator]: \u001b[0mInference done 71/548. 2.3188 s / img. ETA=0:18:30\n",
            "\u001b[32m[04/15 13:31:58 d2.evaluation.evaluator]: \u001b[0mInference done 74/548. 2.3193 s / img. ETA=0:18:24\n",
            "\u001b[32m[04/15 13:32:05 d2.evaluation.evaluator]: \u001b[0mInference done 77/548. 2.3195 s / img. ETA=0:18:17\n",
            "\u001b[32m[04/15 13:32:12 d2.evaluation.evaluator]: \u001b[0mInference done 80/548. 2.3198 s / img. ETA=0:18:10\n",
            "\u001b[32m[04/15 13:32:19 d2.evaluation.evaluator]: \u001b[0mInference done 83/548. 2.3199 s / img. ETA=0:18:03\n",
            "\u001b[32m[04/15 13:32:26 d2.evaluation.evaluator]: \u001b[0mInference done 86/548. 2.3197 s / img. ETA=0:17:56\n",
            "\u001b[32m[04/15 13:32:33 d2.evaluation.evaluator]: \u001b[0mInference done 89/548. 2.3203 s / img. ETA=0:17:49\n",
            "\u001b[32m[04/15 13:32:40 d2.evaluation.evaluator]: \u001b[0mInference done 92/548. 2.3214 s / img. ETA=0:17:42\n",
            "\u001b[32m[04/15 13:32:47 d2.evaluation.evaluator]: \u001b[0mInference done 95/548. 2.3219 s / img. ETA=0:17:36\n",
            "\u001b[32m[04/15 13:32:55 d2.evaluation.evaluator]: \u001b[0mInference done 98/548. 2.3224 s / img. ETA=0:17:29\n",
            "\u001b[32m[04/15 13:33:02 d2.evaluation.evaluator]: \u001b[0mInference done 101/548. 2.3227 s / img. ETA=0:17:22\n",
            "\u001b[32m[04/15 13:33:09 d2.evaluation.evaluator]: \u001b[0mInference done 104/548. 2.3227 s / img. ETA=0:17:15\n",
            "\u001b[32m[04/15 13:33:16 d2.evaluation.evaluator]: \u001b[0mInference done 107/548. 2.3229 s / img. ETA=0:17:08\n",
            "\u001b[32m[04/15 13:33:23 d2.evaluation.evaluator]: \u001b[0mInference done 110/548. 2.3230 s / img. ETA=0:17:01\n",
            "\u001b[32m[04/15 13:33:30 d2.evaluation.evaluator]: \u001b[0mInference done 113/548. 2.3230 s / img. ETA=0:16:54\n",
            "\u001b[32m[04/15 13:33:37 d2.evaluation.evaluator]: \u001b[0mInference done 116/548. 2.3234 s / img. ETA=0:16:47\n",
            "\u001b[32m[04/15 13:33:44 d2.evaluation.evaluator]: \u001b[0mInference done 119/548. 2.3234 s / img. ETA=0:16:41\n",
            "\u001b[32m[04/15 13:33:51 d2.evaluation.evaluator]: \u001b[0mInference done 122/548. 2.3234 s / img. ETA=0:16:33\n",
            "\u001b[32m[04/15 13:33:58 d2.evaluation.evaluator]: \u001b[0mInference done 125/548. 2.3236 s / img. ETA=0:16:27\n",
            "\u001b[32m[04/15 13:34:05 d2.evaluation.evaluator]: \u001b[0mInference done 128/548. 2.3239 s / img. ETA=0:16:20\n",
            "\u001b[32m[04/15 13:34:12 d2.evaluation.evaluator]: \u001b[0mInference done 131/548. 2.3242 s / img. ETA=0:16:13\n",
            "\u001b[32m[04/15 13:34:19 d2.evaluation.evaluator]: \u001b[0mInference done 134/548. 2.3244 s / img. ETA=0:16:06\n",
            "\u001b[32m[04/15 13:34:26 d2.evaluation.evaluator]: \u001b[0mInference done 137/548. 2.3244 s / img. ETA=0:15:59\n",
            "\u001b[32m[04/15 13:34:33 d2.evaluation.evaluator]: \u001b[0mInference done 140/548. 2.3245 s / img. ETA=0:15:52\n",
            "\u001b[32m[04/15 13:34:40 d2.evaluation.evaluator]: \u001b[0mInference done 143/548. 2.3245 s / img. ETA=0:15:45\n",
            "\u001b[32m[04/15 13:34:47 d2.evaluation.evaluator]: \u001b[0mInference done 146/548. 2.3247 s / img. ETA=0:15:38\n",
            "\u001b[32m[04/15 13:34:54 d2.evaluation.evaluator]: \u001b[0mInference done 149/548. 2.3250 s / img. ETA=0:15:31\n",
            "\u001b[32m[04/15 13:35:01 d2.evaluation.evaluator]: \u001b[0mInference done 152/548. 2.3250 s / img. ETA=0:15:24\n",
            "\u001b[32m[04/15 13:35:08 d2.evaluation.evaluator]: \u001b[0mInference done 155/548. 2.3251 s / img. ETA=0:15:17\n",
            "\u001b[32m[04/15 13:35:15 d2.evaluation.evaluator]: \u001b[0mInference done 158/548. 2.3252 s / img. ETA=0:15:10\n",
            "\u001b[32m[04/15 13:35:22 d2.evaluation.evaluator]: \u001b[0mInference done 161/548. 2.3253 s / img. ETA=0:15:03\n",
            "\u001b[32m[04/15 13:35:29 d2.evaluation.evaluator]: \u001b[0mInference done 164/548. 2.3255 s / img. ETA=0:14:56\n",
            "\u001b[32m[04/15 13:35:36 d2.evaluation.evaluator]: \u001b[0mInference done 167/548. 2.3257 s / img. ETA=0:14:49\n",
            "\u001b[32m[04/15 13:35:43 d2.evaluation.evaluator]: \u001b[0mInference done 170/548. 2.3258 s / img. ETA=0:14:42\n",
            "\u001b[32m[04/15 13:35:50 d2.evaluation.evaluator]: \u001b[0mInference done 173/548. 2.3258 s / img. ETA=0:14:35\n",
            "\u001b[32m[04/15 13:35:57 d2.evaluation.evaluator]: \u001b[0mInference done 176/548. 2.3262 s / img. ETA=0:14:28\n",
            "\u001b[32m[04/15 13:36:04 d2.evaluation.evaluator]: \u001b[0mInference done 179/548. 2.3262 s / img. ETA=0:14:21\n",
            "\u001b[32m[04/15 13:36:11 d2.evaluation.evaluator]: \u001b[0mInference done 182/548. 2.3263 s / img. ETA=0:14:14\n",
            "\u001b[32m[04/15 13:36:18 d2.evaluation.evaluator]: \u001b[0mInference done 185/548. 2.3265 s / img. ETA=0:14:07\n",
            "\u001b[32m[04/15 13:36:25 d2.evaluation.evaluator]: \u001b[0mInference done 188/548. 2.3267 s / img. ETA=0:14:00\n",
            "\u001b[32m[04/15 13:36:32 d2.evaluation.evaluator]: \u001b[0mInference done 191/548. 2.3269 s / img. ETA=0:13:54\n",
            "\u001b[32m[04/15 13:36:39 d2.evaluation.evaluator]: \u001b[0mInference done 194/548. 2.3271 s / img. ETA=0:13:47\n",
            "\u001b[32m[04/15 13:36:46 d2.evaluation.evaluator]: \u001b[0mInference done 197/548. 2.3275 s / img. ETA=0:13:40\n",
            "\u001b[32m[04/15 13:36:53 d2.evaluation.evaluator]: \u001b[0mInference done 200/548. 2.3274 s / img. ETA=0:13:33\n",
            "\u001b[32m[04/15 13:37:00 d2.evaluation.evaluator]: \u001b[0mInference done 203/548. 2.3276 s / img. ETA=0:13:26\n",
            "\u001b[32m[04/15 13:37:07 d2.evaluation.evaluator]: \u001b[0mInference done 206/548. 2.3278 s / img. ETA=0:13:19\n",
            "\u001b[32m[04/15 13:37:14 d2.evaluation.evaluator]: \u001b[0mInference done 209/548. 2.3280 s / img. ETA=0:13:12\n",
            "\u001b[32m[04/15 13:37:21 d2.evaluation.evaluator]: \u001b[0mInference done 212/548. 2.3279 s / img. ETA=0:13:05\n",
            "\u001b[32m[04/15 13:37:28 d2.evaluation.evaluator]: \u001b[0mInference done 215/548. 2.3280 s / img. ETA=0:12:58\n",
            "\u001b[32m[04/15 13:37:35 d2.evaluation.evaluator]: \u001b[0mInference done 218/548. 2.3281 s / img. ETA=0:12:51\n",
            "\u001b[32m[04/15 13:37:43 d2.evaluation.evaluator]: \u001b[0mInference done 221/548. 2.3282 s / img. ETA=0:12:44\n",
            "\u001b[32m[04/15 13:37:50 d2.evaluation.evaluator]: \u001b[0mInference done 224/548. 2.3281 s / img. ETA=0:12:37\n",
            "\u001b[32m[04/15 13:37:57 d2.evaluation.evaluator]: \u001b[0mInference done 227/548. 2.3281 s / img. ETA=0:12:30\n",
            "\u001b[32m[04/15 13:38:04 d2.evaluation.evaluator]: \u001b[0mInference done 230/548. 2.3283 s / img. ETA=0:12:23\n",
            "\u001b[32m[04/15 13:38:11 d2.evaluation.evaluator]: \u001b[0mInference done 233/548. 2.3284 s / img. ETA=0:12:16\n",
            "\u001b[32m[04/15 13:38:18 d2.evaluation.evaluator]: \u001b[0mInference done 236/548. 2.3284 s / img. ETA=0:12:09\n",
            "\u001b[32m[04/15 13:38:25 d2.evaluation.evaluator]: \u001b[0mInference done 239/548. 2.3283 s / img. ETA=0:12:02\n",
            "\u001b[32m[04/15 13:38:32 d2.evaluation.evaluator]: \u001b[0mInference done 242/548. 2.3284 s / img. ETA=0:11:55\n",
            "\u001b[32m[04/15 13:38:39 d2.evaluation.evaluator]: \u001b[0mInference done 245/548. 2.3283 s / img. ETA=0:11:48\n",
            "\u001b[32m[04/15 13:38:46 d2.evaluation.evaluator]: \u001b[0mInference done 248/548. 2.3283 s / img. ETA=0:11:41\n",
            "\u001b[32m[04/15 13:38:53 d2.evaluation.evaluator]: \u001b[0mInference done 251/548. 2.3284 s / img. ETA=0:11:34\n",
            "\u001b[32m[04/15 13:39:00 d2.evaluation.evaluator]: \u001b[0mInference done 254/548. 2.3284 s / img. ETA=0:11:27\n",
            "\u001b[32m[04/15 13:39:07 d2.evaluation.evaluator]: \u001b[0mInference done 257/548. 2.3285 s / img. ETA=0:11:20\n",
            "\u001b[32m[04/15 13:39:14 d2.evaluation.evaluator]: \u001b[0mInference done 260/548. 2.3284 s / img. ETA=0:11:13\n",
            "\u001b[32m[04/15 13:39:21 d2.evaluation.evaluator]: \u001b[0mInference done 263/548. 2.3284 s / img. ETA=0:11:06\n",
            "\u001b[32m[04/15 13:39:28 d2.evaluation.evaluator]: \u001b[0mInference done 266/548. 2.3283 s / img. ETA=0:10:59\n",
            "\u001b[32m[04/15 13:39:35 d2.evaluation.evaluator]: \u001b[0mInference done 269/548. 2.3283 s / img. ETA=0:10:52\n",
            "\u001b[32m[04/15 13:39:42 d2.evaluation.evaluator]: \u001b[0mInference done 272/548. 2.3283 s / img. ETA=0:10:45\n",
            "\u001b[32m[04/15 13:39:49 d2.evaluation.evaluator]: \u001b[0mInference done 275/548. 2.3283 s / img. ETA=0:10:38\n",
            "\u001b[32m[04/15 13:39:56 d2.evaluation.evaluator]: \u001b[0mInference done 278/548. 2.3282 s / img. ETA=0:10:31\n",
            "\u001b[32m[04/15 13:40:03 d2.evaluation.evaluator]: \u001b[0mInference done 281/548. 2.3283 s / img. ETA=0:10:24\n",
            "\u001b[32m[04/15 13:40:10 d2.evaluation.evaluator]: \u001b[0mInference done 284/548. 2.3284 s / img. ETA=0:10:17\n",
            "\u001b[32m[04/15 13:40:17 d2.evaluation.evaluator]: \u001b[0mInference done 287/548. 2.3284 s / img. ETA=0:10:10\n",
            "\u001b[32m[04/15 13:40:24 d2.evaluation.evaluator]: \u001b[0mInference done 290/548. 2.3283 s / img. ETA=0:10:03\n",
            "\u001b[32m[04/15 13:40:31 d2.evaluation.evaluator]: \u001b[0mInference done 293/548. 2.3283 s / img. ETA=0:09:56\n",
            "\u001b[32m[04/15 13:40:38 d2.evaluation.evaluator]: \u001b[0mInference done 296/548. 2.3283 s / img. ETA=0:09:49\n",
            "\u001b[32m[04/15 13:40:45 d2.evaluation.evaluator]: \u001b[0mInference done 299/548. 2.3284 s / img. ETA=0:09:42\n",
            "\u001b[32m[04/15 13:40:52 d2.evaluation.evaluator]: \u001b[0mInference done 302/548. 2.3284 s / img. ETA=0:09:35\n",
            "\u001b[32m[04/15 13:40:59 d2.evaluation.evaluator]: \u001b[0mInference done 305/548. 2.3284 s / img. ETA=0:09:27\n",
            "\u001b[32m[04/15 13:41:06 d2.evaluation.evaluator]: \u001b[0mInference done 308/548. 2.3285 s / img. ETA=0:09:21\n",
            "\u001b[32m[04/15 13:41:13 d2.evaluation.evaluator]: \u001b[0mInference done 311/548. 2.3285 s / img. ETA=0:09:14\n",
            "\u001b[32m[04/15 13:41:20 d2.evaluation.evaluator]: \u001b[0mInference done 314/548. 2.3284 s / img. ETA=0:09:06\n",
            "\u001b[32m[04/15 13:41:27 d2.evaluation.evaluator]: \u001b[0mInference done 317/548. 2.3285 s / img. ETA=0:08:59\n",
            "\u001b[32m[04/15 13:41:34 d2.evaluation.evaluator]: \u001b[0mInference done 320/548. 2.3286 s / img. ETA=0:08:52\n",
            "\u001b[32m[04/15 13:41:41 d2.evaluation.evaluator]: \u001b[0mInference done 323/548. 2.3286 s / img. ETA=0:08:45\n",
            "\u001b[32m[04/15 13:41:48 d2.evaluation.evaluator]: \u001b[0mInference done 326/548. 2.3286 s / img. ETA=0:08:38\n",
            "\u001b[32m[04/15 13:41:55 d2.evaluation.evaluator]: \u001b[0mInference done 329/548. 2.3286 s / img. ETA=0:08:31\n",
            "\u001b[32m[04/15 13:42:02 d2.evaluation.evaluator]: \u001b[0mInference done 332/548. 2.3287 s / img. ETA=0:08:24\n",
            "\u001b[32m[04/15 13:42:09 d2.evaluation.evaluator]: \u001b[0mInference done 335/548. 2.3286 s / img. ETA=0:08:17\n",
            "\u001b[32m[04/15 13:42:16 d2.evaluation.evaluator]: \u001b[0mInference done 338/548. 2.3287 s / img. ETA=0:08:10\n",
            "\u001b[32m[04/15 13:42:23 d2.evaluation.evaluator]: \u001b[0mInference done 341/548. 2.3287 s / img. ETA=0:08:03\n",
            "\u001b[32m[04/15 13:42:30 d2.evaluation.evaluator]: \u001b[0mInference done 344/548. 2.3287 s / img. ETA=0:07:56\n",
            "\u001b[32m[04/15 13:42:37 d2.evaluation.evaluator]: \u001b[0mInference done 347/548. 2.3287 s / img. ETA=0:07:50\n",
            "\u001b[32m[04/15 13:42:44 d2.evaluation.evaluator]: \u001b[0mInference done 350/548. 2.3286 s / img. ETA=0:07:43\n",
            "\u001b[32m[04/15 13:42:51 d2.evaluation.evaluator]: \u001b[0mInference done 353/548. 2.3286 s / img. ETA=0:07:35\n",
            "\u001b[32m[04/15 13:42:58 d2.evaluation.evaluator]: \u001b[0mInference done 356/548. 2.3286 s / img. ETA=0:07:28\n",
            "\u001b[32m[04/15 13:43:05 d2.evaluation.evaluator]: \u001b[0mInference done 359/548. 2.3285 s / img. ETA=0:07:21\n",
            "\u001b[32m[04/15 13:43:12 d2.evaluation.evaluator]: \u001b[0mInference done 362/548. 2.3286 s / img. ETA=0:07:14\n",
            "\u001b[32m[04/15 13:43:19 d2.evaluation.evaluator]: \u001b[0mInference done 365/548. 2.3287 s / img. ETA=0:07:07\n",
            "\u001b[32m[04/15 13:43:27 d2.evaluation.evaluator]: \u001b[0mInference done 368/548. 2.3287 s / img. ETA=0:07:00\n",
            "\u001b[32m[04/15 13:43:34 d2.evaluation.evaluator]: \u001b[0mInference done 371/548. 2.3289 s / img. ETA=0:06:53\n",
            "\u001b[32m[04/15 13:43:41 d2.evaluation.evaluator]: \u001b[0mInference done 374/548. 2.3289 s / img. ETA=0:06:46\n",
            "\u001b[32m[04/15 13:43:48 d2.evaluation.evaluator]: \u001b[0mInference done 377/548. 2.3289 s / img. ETA=0:06:39\n",
            "\u001b[32m[04/15 13:43:55 d2.evaluation.evaluator]: \u001b[0mInference done 380/548. 2.3290 s / img. ETA=0:06:32\n",
            "\u001b[32m[04/15 13:44:02 d2.evaluation.evaluator]: \u001b[0mInference done 383/548. 2.3290 s / img. ETA=0:06:25\n",
            "\u001b[32m[04/15 13:44:09 d2.evaluation.evaluator]: \u001b[0mInference done 386/548. 2.3290 s / img. ETA=0:06:18\n",
            "\u001b[32m[04/15 13:44:16 d2.evaluation.evaluator]: \u001b[0mInference done 389/548. 2.3291 s / img. ETA=0:06:11\n",
            "\u001b[32m[04/15 13:44:23 d2.evaluation.evaluator]: \u001b[0mInference done 392/548. 2.3292 s / img. ETA=0:06:04\n",
            "\u001b[32m[04/15 13:44:30 d2.evaluation.evaluator]: \u001b[0mInference done 395/548. 2.3293 s / img. ETA=0:05:57\n",
            "\u001b[32m[04/15 13:44:37 d2.evaluation.evaluator]: \u001b[0mInference done 398/548. 2.3293 s / img. ETA=0:05:50\n",
            "\u001b[32m[04/15 13:44:44 d2.evaluation.evaluator]: \u001b[0mInference done 401/548. 2.3294 s / img. ETA=0:05:43\n",
            "\u001b[32m[04/15 13:44:51 d2.evaluation.evaluator]: \u001b[0mInference done 404/548. 2.3295 s / img. ETA=0:05:36\n",
            "\u001b[32m[04/15 13:44:58 d2.evaluation.evaluator]: \u001b[0mInference done 407/548. 2.3295 s / img. ETA=0:05:29\n",
            "\u001b[32m[04/15 13:45:05 d2.evaluation.evaluator]: \u001b[0mInference done 410/548. 2.3295 s / img. ETA=0:05:22\n",
            "\u001b[32m[04/15 13:45:12 d2.evaluation.evaluator]: \u001b[0mInference done 413/548. 2.3294 s / img. ETA=0:05:15\n",
            "\u001b[32m[04/15 13:45:19 d2.evaluation.evaluator]: \u001b[0mInference done 416/548. 2.3294 s / img. ETA=0:05:08\n",
            "\u001b[32m[04/15 13:45:26 d2.evaluation.evaluator]: \u001b[0mInference done 419/548. 2.3294 s / img. ETA=0:05:01\n",
            "\u001b[32m[04/15 13:45:33 d2.evaluation.evaluator]: \u001b[0mInference done 422/548. 2.3294 s / img. ETA=0:04:54\n",
            "\u001b[32m[04/15 13:45:40 d2.evaluation.evaluator]: \u001b[0mInference done 425/548. 2.3294 s / img. ETA=0:04:47\n",
            "\u001b[32m[04/15 13:45:47 d2.evaluation.evaluator]: \u001b[0mInference done 428/548. 2.3293 s / img. ETA=0:04:40\n",
            "\u001b[32m[04/15 13:45:54 d2.evaluation.evaluator]: \u001b[0mInference done 431/548. 2.3292 s / img. ETA=0:04:33\n",
            "\u001b[32m[04/15 13:46:01 d2.evaluation.evaluator]: \u001b[0mInference done 434/548. 2.3294 s / img. ETA=0:04:26\n",
            "\u001b[32m[04/15 13:46:08 d2.evaluation.evaluator]: \u001b[0mInference done 437/548. 2.3293 s / img. ETA=0:04:19\n",
            "\u001b[32m[04/15 13:46:15 d2.evaluation.evaluator]: \u001b[0mInference done 440/548. 2.3293 s / img. ETA=0:04:12\n",
            "\u001b[32m[04/15 13:46:22 d2.evaluation.evaluator]: \u001b[0mInference done 443/548. 2.3293 s / img. ETA=0:04:05\n",
            "\u001b[32m[04/15 13:46:29 d2.evaluation.evaluator]: \u001b[0mInference done 446/548. 2.3293 s / img. ETA=0:03:58\n",
            "\u001b[32m[04/15 13:46:36 d2.evaluation.evaluator]: \u001b[0mInference done 449/548. 2.3292 s / img. ETA=0:03:51\n",
            "\u001b[32m[04/15 13:46:43 d2.evaluation.evaluator]: \u001b[0mInference done 452/548. 2.3293 s / img. ETA=0:03:44\n",
            "\u001b[32m[04/15 13:46:50 d2.evaluation.evaluator]: \u001b[0mInference done 455/548. 2.3293 s / img. ETA=0:03:37\n",
            "\u001b[32m[04/15 13:46:57 d2.evaluation.evaluator]: \u001b[0mInference done 458/548. 2.3292 s / img. ETA=0:03:30\n",
            "\u001b[32m[04/15 13:47:04 d2.evaluation.evaluator]: \u001b[0mInference done 461/548. 2.3293 s / img. ETA=0:03:23\n",
            "\u001b[32m[04/15 13:47:11 d2.evaluation.evaluator]: \u001b[0mInference done 464/548. 2.3293 s / img. ETA=0:03:16\n",
            "\u001b[32m[04/15 13:47:18 d2.evaluation.evaluator]: \u001b[0mInference done 467/548. 2.3294 s / img. ETA=0:03:09\n",
            "\u001b[32m[04/15 13:47:25 d2.evaluation.evaluator]: \u001b[0mInference done 470/548. 2.3294 s / img. ETA=0:03:02\n",
            "\u001b[32m[04/15 13:47:32 d2.evaluation.evaluator]: \u001b[0mInference done 473/548. 2.3295 s / img. ETA=0:02:55\n",
            "\u001b[32m[04/15 13:47:39 d2.evaluation.evaluator]: \u001b[0mInference done 476/548. 2.3295 s / img. ETA=0:02:48\n",
            "\u001b[32m[04/15 13:47:46 d2.evaluation.evaluator]: \u001b[0mInference done 479/548. 2.3295 s / img. ETA=0:02:41\n",
            "\u001b[32m[04/15 13:47:53 d2.evaluation.evaluator]: \u001b[0mInference done 482/548. 2.3295 s / img. ETA=0:02:34\n",
            "\u001b[32m[04/15 13:48:00 d2.evaluation.evaluator]: \u001b[0mInference done 485/548. 2.3295 s / img. ETA=0:02:27\n",
            "\u001b[32m[04/15 13:48:07 d2.evaluation.evaluator]: \u001b[0mInference done 488/548. 2.3295 s / img. ETA=0:02:20\n",
            "\u001b[32m[04/15 13:48:14 d2.evaluation.evaluator]: \u001b[0mInference done 491/548. 2.3295 s / img. ETA=0:02:13\n",
            "\u001b[32m[04/15 13:48:21 d2.evaluation.evaluator]: \u001b[0mInference done 494/548. 2.3296 s / img. ETA=0:02:06\n",
            "\u001b[32m[04/15 13:48:29 d2.evaluation.evaluator]: \u001b[0mInference done 497/548. 2.3297 s / img. ETA=0:01:59\n",
            "\u001b[32m[04/15 13:48:36 d2.evaluation.evaluator]: \u001b[0mInference done 500/548. 2.3298 s / img. ETA=0:01:52\n",
            "\u001b[32m[04/15 13:48:43 d2.evaluation.evaluator]: \u001b[0mInference done 503/548. 2.3298 s / img. ETA=0:01:45\n",
            "\u001b[32m[04/15 13:48:50 d2.evaluation.evaluator]: \u001b[0mInference done 506/548. 2.3298 s / img. ETA=0:01:38\n",
            "\u001b[32m[04/15 13:48:57 d2.evaluation.evaluator]: \u001b[0mInference done 509/548. 2.3298 s / img. ETA=0:01:31\n",
            "\u001b[32m[04/15 13:49:04 d2.evaluation.evaluator]: \u001b[0mInference done 512/548. 2.3298 s / img. ETA=0:01:24\n",
            "\u001b[32m[04/15 13:49:11 d2.evaluation.evaluator]: \u001b[0mInference done 515/548. 2.3299 s / img. ETA=0:01:17\n",
            "\u001b[32m[04/15 13:49:18 d2.evaluation.evaluator]: \u001b[0mInference done 518/548. 2.3300 s / img. ETA=0:01:10\n",
            "\u001b[32m[04/15 13:49:25 d2.evaluation.evaluator]: \u001b[0mInference done 521/548. 2.3300 s / img. ETA=0:01:03\n",
            "\u001b[32m[04/15 13:49:32 d2.evaluation.evaluator]: \u001b[0mInference done 524/548. 2.3300 s / img. ETA=0:00:56\n",
            "\u001b[32m[04/15 13:49:39 d2.evaluation.evaluator]: \u001b[0mInference done 527/548. 2.3300 s / img. ETA=0:00:49\n",
            "\u001b[32m[04/15 13:49:46 d2.evaluation.evaluator]: \u001b[0mInference done 530/548. 2.3300 s / img. ETA=0:00:42\n",
            "\u001b[32m[04/15 13:49:53 d2.evaluation.evaluator]: \u001b[0mInference done 533/548. 2.3300 s / img. ETA=0:00:35\n",
            "\u001b[32m[04/15 13:50:00 d2.evaluation.evaluator]: \u001b[0mInference done 536/548. 2.3299 s / img. ETA=0:00:28\n",
            "\u001b[32m[04/15 13:50:07 d2.evaluation.evaluator]: \u001b[0mInference done 539/548. 2.3299 s / img. ETA=0:00:21\n",
            "\u001b[32m[04/15 13:50:14 d2.evaluation.evaluator]: \u001b[0mInference done 542/548. 2.3299 s / img. ETA=0:00:14\n",
            "\u001b[32m[04/15 13:50:21 d2.evaluation.evaluator]: \u001b[0mInference done 545/548. 2.3299 s / img. ETA=0:00:07\n",
            "\u001b[32m[04/15 13:50:28 d2.evaluation.evaluator]: \u001b[0mInference done 548/548. 2.3299 s / img. ETA=0:00:00\n",
            "\u001b[32m[04/15 13:50:30 d2.evaluation.evaluator]: \u001b[0mTotal inference time: 0:21:12.792427 (2.344001 s / img per device, on 1 devices)\n",
            "\u001b[32m[04/15 13:50:30 d2.evaluation.evaluator]: \u001b[0mTotal inference pure compute time: 0:21:05 (2.329875 s / img per device, on 1 devices)\n",
            "\u001b[32m[04/15 13:50:35 d2.evaluation.testing]: \u001b[0mcopypaste: Task: GPU_Speed\n",
            "\u001b[32m[04/15 13:50:35 d2.evaluation.testing]: \u001b[0mcopypaste: Mean_FPS,Std_FPS,Max_FPS,Min_FPS,Mid_FPS\n",
            "\u001b[32m[04/15 13:50:35 d2.evaluation.testing]: \u001b[0mcopypaste: 0.4319,0.0042,0.4597,0.4197,0.4317\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!bash eval_visdrone.sh work_dirs/model_test-5/visdrone_infer.json"
      ],
      "metadata": {
        "id": "GFcH6yR-rt35",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "be35094d-eb6a-4f1f-c7ad-765fe72479b9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Json to txt: .visdrone_det_txt\n",
            "  0% 0/548 [00:00<?, ?it/s]0\n",
            "0\n",
            "  0% 1/548 [00:02<18:32,  2.03s/it]1\n",
            "1\n",
            "  0% 2/548 [00:04<20:38,  2.27s/it]2\n",
            "2\n",
            "  1% 3/548 [00:06<19:19,  2.13s/it]3\n",
            "3\n",
            "  1% 4/548 [00:09<20:53,  2.30s/it]4\n",
            "4\n",
            "  1% 5/548 [00:11<19:56,  2.20s/it]5\n",
            "5\n",
            "  1% 6/548 [00:14<21:24,  2.37s/it]6\n",
            "6\n",
            "  1% 7/548 [00:16<22:35,  2.50s/it]7\n",
            "7\n",
            "  1% 8/548 [00:19<21:40,  2.41s/it]8\n",
            "8\n",
            "  2% 9/548 [00:20<19:08,  2.13s/it]9\n",
            "9\n",
            "  2% 10/548 [00:23<20:45,  2.32s/it]10\n",
            "10\n",
            "  2% 11/548 [00:26<21:44,  2.43s/it]11\n",
            "11\n",
            "  2% 12/548 [00:27<19:55,  2.23s/it]12\n",
            "12\n",
            "  2% 13/548 [00:29<17:48,  2.00s/it]13\n",
            "13\n",
            "  3% 14/548 [00:32<20:07,  2.26s/it]14\n",
            "14\n",
            "  3% 15/548 [00:34<19:30,  2.20s/it]15\n",
            "15\n",
            "  3% 16/548 [00:36<20:44,  2.34s/it]16\n",
            "16\n",
            "  3% 17/548 [00:39<20:29,  2.32s/it]17\n",
            "17\n",
            "  3% 18/548 [00:40<19:11,  2.17s/it]18\n",
            "18\n",
            "  3% 19/548 [00:42<18:06,  2.05s/it]19\n",
            "19\n",
            "  4% 20/548 [00:44<17:13,  1.96s/it]20\n",
            "20\n",
            "  4% 21/548 [00:45<15:58,  1.82s/it]21\n",
            "21\n",
            "  4% 22/548 [00:47<14:40,  1.67s/it]22\n",
            "22\n",
            "  4% 23/548 [00:49<14:50,  1.70s/it]23\n",
            "23\n",
            "  4% 24/548 [00:51<15:38,  1.79s/it]24\n",
            "24\n",
            "  5% 25/548 [00:52<15:30,  1.78s/it]25\n",
            "25\n",
            "  5% 26/548 [00:54<15:49,  1.82s/it]26\n",
            "26\n",
            "  5% 27/548 [00:57<17:36,  2.03s/it]27\n",
            "27\n",
            "  5% 28/548 [00:58<15:41,  1.81s/it]28\n",
            "28\n",
            "  5% 29/548 [01:00<17:02,  1.97s/it]29\n",
            "29\n",
            "  5% 30/548 [01:02<17:12,  1.99s/it]30\n",
            "30\n",
            "  6% 31/548 [01:04<17:06,  1.99s/it]31\n",
            "31\n",
            "  6% 32/548 [01:06<16:45,  1.95s/it]32\n",
            "32\n",
            "  6% 33/548 [01:08<15:55,  1.86s/it]33\n",
            "33\n",
            "  6% 34/548 [01:10<15:58,  1.87s/it]34\n",
            "34\n",
            "  6% 35/548 [01:12<16:23,  1.92s/it]35\n",
            "35\n",
            "  7% 36/548 [01:13<14:29,  1.70s/it]36\n",
            "36\n",
            "  7% 37/548 [01:15<15:17,  1.80s/it]37\n",
            "37\n",
            "  7% 38/548 [01:17<15:35,  1.84s/it]38\n",
            "38\n",
            "  7% 39/548 [01:20<18:39,  2.20s/it]39\n",
            "39\n",
            "  7% 40/548 [01:21<16:38,  1.97s/it]40\n",
            "40\n",
            "  7% 41/548 [01:23<14:50,  1.76s/it]41\n",
            "41\n",
            "  8% 42/548 [01:24<14:45,  1.75s/it]42\n",
            "42\n",
            "  8% 43/548 [01:26<14:25,  1.71s/it]43\n",
            "43\n",
            "  8% 44/548 [01:27<13:33,  1.61s/it]44\n",
            "44\n",
            "  8% 45/548 [01:29<12:30,  1.49s/it]45\n",
            "45\n",
            "  8% 46/548 [01:31<14:49,  1.77s/it]46\n",
            "46\n",
            "  9% 47/548 [01:34<17:26,  2.09s/it]47\n",
            "47\n",
            "  9% 48/548 [01:36<16:25,  1.97s/it]48\n",
            "48\n",
            "  9% 49/548 [01:37<16:15,  1.96s/it]49\n",
            "49\n",
            "  9% 50/548 [01:39<16:18,  1.97s/it]50\n",
            "50\n",
            "  9% 51/548 [01:41<15:44,  1.90s/it]51\n",
            "51\n",
            "  9% 52/548 [01:43<15:43,  1.90s/it]52\n",
            "52\n",
            " 10% 53/548 [01:45<14:22,  1.74s/it]53\n",
            "53\n",
            " 10% 54/548 [01:46<14:21,  1.74s/it]54\n",
            "54\n",
            " 10% 55/548 [01:48<13:26,  1.64s/it]55\n",
            "55\n",
            " 10% 56/548 [01:49<12:37,  1.54s/it]56\n",
            "56\n",
            " 10% 57/548 [01:51<13:12,  1.61s/it]57\n",
            "57\n",
            " 11% 58/548 [01:53<13:41,  1.68s/it]58\n",
            "58\n",
            " 11% 59/548 [01:55<15:22,  1.89s/it]59\n",
            "59\n",
            " 11% 60/548 [01:56<14:24,  1.77s/it]60\n",
            "60\n",
            " 11% 61/548 [01:58<13:22,  1.65s/it]61\n",
            "61\n",
            " 11% 62/548 [02:00<14:21,  1.77s/it]62\n",
            "62\n",
            " 11% 63/548 [02:02<14:02,  1.74s/it]63\n",
            "63\n",
            " 12% 64/548 [02:03<13:51,  1.72s/it]64\n",
            "64\n",
            " 12% 65/548 [02:05<13:42,  1.70s/it]65\n",
            "65\n",
            " 12% 66/548 [02:06<12:48,  1.60s/it]66\n",
            "66\n",
            " 12% 67/548 [02:08<12:40,  1.58s/it]67\n",
            "67\n",
            " 12% 68/548 [02:10<13:23,  1.67s/it]68\n",
            "68\n",
            " 13% 69/548 [02:11<12:27,  1.56s/it]69\n",
            "69\n",
            " 13% 70/548 [02:13<12:43,  1.60s/it]70\n",
            "70\n",
            " 13% 71/548 [02:14<13:19,  1.68s/it]71\n",
            "71\n",
            " 13% 72/548 [02:16<13:15,  1.67s/it]72\n",
            "72\n",
            " 13% 73/548 [02:18<13:10,  1.66s/it]73\n",
            "73\n",
            " 14% 74/548 [02:20<13:30,  1.71s/it]74\n",
            "74\n",
            " 14% 75/548 [02:21<13:08,  1.67s/it]75\n",
            "75\n",
            " 14% 76/548 [02:23<13:28,  1.71s/it]76\n",
            "76\n",
            " 14% 77/548 [02:25<13:40,  1.74s/it]77\n",
            "77\n",
            " 14% 78/548 [02:26<12:24,  1.58s/it]78\n",
            "78\n",
            " 14% 79/548 [02:28<12:29,  1.60s/it]79\n",
            "79\n",
            " 15% 80/548 [02:29<12:43,  1.63s/it]80\n",
            "80\n",
            " 15% 81/548 [02:31<12:56,  1.66s/it]81\n",
            "81\n",
            " 15% 82/548 [02:32<12:00,  1.55s/it]82\n",
            "82\n",
            " 15% 83/548 [02:35<13:52,  1.79s/it]83\n",
            "83\n",
            " 15% 84/548 [02:37<14:00,  1.81s/it]84\n",
            "84\n",
            " 16% 85/548 [02:38<13:51,  1.80s/it]85\n",
            "85\n",
            " 16% 86/548 [02:40<13:31,  1.76s/it]86\n",
            "86\n",
            " 16% 87/548 [02:42<13:35,  1.77s/it]87\n",
            "87\n",
            " 16% 88/548 [02:43<13:15,  1.73s/it]88\n",
            "88\n",
            " 16% 89/548 [02:45<12:59,  1.70s/it]89\n",
            "89\n",
            " 16% 90/548 [02:47<13:29,  1.77s/it]90\n",
            "90\n",
            " 17% 91/548 [02:49<13:27,  1.77s/it]91\n",
            "91\n",
            " 17% 92/548 [02:51<13:28,  1.77s/it]92\n",
            "92\n",
            " 17% 93/548 [02:52<13:20,  1.76s/it]93\n",
            "93\n",
            " 17% 94/548 [02:54<12:44,  1.68s/it]94\n",
            "94\n",
            " 17% 95/548 [02:55<12:24,  1.64s/it]95\n",
            "95\n",
            " 18% 96/548 [02:57<12:02,  1.60s/it]96\n",
            "96\n",
            " 18% 97/548 [02:59<12:49,  1.71s/it]97\n",
            "97\n",
            " 18% 98/548 [03:01<14:03,  1.87s/it]98\n",
            "98\n",
            " 18% 99/548 [03:03<13:34,  1.81s/it]99\n",
            "99\n",
            " 18% 100/548 [03:05<14:41,  1.97s/it]100\n",
            "100\n",
            " 18% 101/548 [03:07<13:58,  1.87s/it]101\n",
            "101\n",
            " 19% 102/548 [03:08<13:30,  1.82s/it]102\n",
            "102\n",
            " 19% 103/548 [03:10<13:23,  1.81s/it]103\n",
            "103\n",
            " 19% 104/548 [03:12<13:01,  1.76s/it]104\n",
            "104\n",
            " 19% 105/548 [03:14<12:57,  1.76s/it]105\n",
            "105\n",
            " 19% 106/548 [03:15<12:56,  1.76s/it]106\n",
            "106\n",
            " 20% 107/548 [03:17<12:56,  1.76s/it]107\n",
            "107\n",
            " 20% 108/548 [03:19<12:32,  1.71s/it]108\n",
            "108\n",
            " 20% 109/548 [03:20<12:27,  1.70s/it]109\n",
            "109\n",
            " 20% 110/548 [03:22<12:32,  1.72s/it]110\n",
            "110\n",
            " 20% 111/548 [03:24<12:18,  1.69s/it]111\n",
            "111\n",
            " 20% 112/548 [03:26<13:11,  1.81s/it]112\n",
            "112\n",
            " 21% 113/548 [03:27<12:23,  1.71s/it]113\n",
            "113\n",
            " 21% 114/548 [03:29<12:28,  1.72s/it]114\n",
            "114\n",
            " 21% 115/548 [03:31<11:49,  1.64s/it]115\n",
            "115\n",
            " 21% 116/548 [03:32<11:59,  1.67s/it]116\n",
            "116\n",
            " 21% 117/548 [03:34<12:01,  1.67s/it]117\n",
            "117\n",
            " 22% 118/548 [03:36<11:50,  1.65s/it]118\n",
            "118\n",
            " 22% 119/548 [03:38<13:02,  1.82s/it]119\n",
            "119\n",
            " 22% 120/548 [03:39<12:21,  1.73s/it]120\n",
            "120\n",
            " 22% 121/548 [03:41<12:26,  1.75s/it]121\n",
            "121\n",
            " 22% 122/548 [03:44<14:03,  1.98s/it]122\n",
            "122\n",
            " 22% 123/548 [03:45<13:06,  1.85s/it]123\n",
            "123\n",
            " 23% 124/548 [03:47<12:37,  1.79s/it]124\n",
            "124\n",
            " 23% 125/548 [03:49<12:34,  1.78s/it]125\n",
            "125\n",
            " 23% 126/548 [03:50<11:46,  1.67s/it]126\n",
            "126\n",
            " 23% 127/548 [03:52<12:00,  1.71s/it]127\n",
            "127\n",
            " 23% 128/548 [03:53<10:51,  1.55s/it]128\n",
            "128\n",
            " 24% 129/548 [03:56<13:38,  1.95s/it]129\n",
            "129\n",
            " 24% 130/548 [03:58<13:47,  1.98s/it]130\n",
            "130\n",
            " 24% 131/548 [03:59<12:35,  1.81s/it]131\n",
            "131\n",
            " 24% 132/548 [04:01<13:13,  1.91s/it]132\n",
            "132\n",
            " 24% 133/548 [04:04<13:54,  2.01s/it]133\n",
            "133\n",
            " 24% 134/548 [04:05<13:22,  1.94s/it]134\n",
            "134\n",
            " 25% 135/548 [04:07<12:08,  1.76s/it]135\n",
            "135\n",
            " 25% 136/548 [04:08<11:42,  1.71s/it]136\n",
            "136\n",
            " 25% 137/548 [04:10<11:18,  1.65s/it]137\n",
            "137\n",
            " 25% 138/548 [04:12<11:21,  1.66s/it]138\n",
            "138\n",
            " 25% 139/548 [04:13<10:27,  1.53s/it]139\n",
            "139\n",
            " 26% 140/548 [04:15<10:54,  1.60s/it]140\n",
            "140\n",
            " 26% 141/548 [04:17<12:03,  1.78s/it]141\n",
            "141\n",
            " 26% 142/548 [04:18<11:00,  1.63s/it]142\n",
            "142\n",
            " 26% 143/548 [04:20<11:28,  1.70s/it]143\n",
            "143\n",
            " 26% 144/548 [04:22<11:25,  1.70s/it]144\n",
            "144\n",
            " 26% 145/548 [04:23<10:43,  1.60s/it]145\n",
            "145\n",
            " 27% 146/548 [04:25<10:45,  1.61s/it]146\n",
            "146\n",
            " 27% 147/548 [04:26<10:53,  1.63s/it]147\n",
            "147\n",
            " 27% 148/548 [04:28<10:07,  1.52s/it]148\n",
            "148\n",
            " 27% 149/548 [04:30<11:29,  1.73s/it]149\n",
            "149\n",
            " 27% 150/548 [04:31<11:04,  1.67s/it]150\n",
            "150\n",
            " 28% 151/548 [04:33<11:07,  1.68s/it]151\n",
            "151\n",
            " 28% 152/548 [04:35<10:45,  1.63s/it]152\n",
            "152\n",
            " 28% 153/548 [04:36<09:58,  1.52s/it]153\n",
            "153\n",
            " 28% 154/548 [04:38<12:09,  1.85s/it]154\n",
            "154\n",
            " 28% 155/548 [04:40<11:51,  1.81s/it]155\n",
            "155\n",
            " 28% 156/548 [04:42<11:19,  1.73s/it]156\n",
            "156\n",
            " 29% 157/548 [04:43<10:54,  1.67s/it]157\n",
            "157\n",
            " 29% 158/548 [04:45<11:18,  1.74s/it]158\n",
            "158\n",
            " 29% 159/548 [04:47<11:12,  1.73s/it]159\n",
            "159\n",
            " 29% 160/548 [04:49<11:15,  1.74s/it]160\n",
            "160\n",
            " 29% 161/548 [04:50<10:29,  1.63s/it]161\n",
            "161\n",
            " 30% 162/548 [04:52<10:24,  1.62s/it]162\n",
            "162\n",
            " 30% 163/548 [04:53<10:19,  1.61s/it]163\n",
            "163\n",
            " 30% 164/548 [04:55<10:12,  1.59s/it]164\n",
            "164\n",
            " 30% 165/548 [04:56<09:59,  1.56s/it]165\n",
            "165\n",
            " 30% 166/548 [04:58<10:08,  1.59s/it]166\n",
            "166\n",
            " 30% 167/548 [05:00<11:01,  1.74s/it]167\n",
            "167\n",
            " 31% 168/548 [05:02<10:44,  1.70s/it]168\n",
            "168\n",
            " 31% 169/548 [05:03<10:32,  1.67s/it]169\n",
            "169\n",
            " 31% 170/548 [05:05<10:28,  1.66s/it]170\n",
            "170\n",
            " 31% 171/548 [05:06<10:23,  1.65s/it]171\n",
            "171\n",
            " 31% 172/548 [05:08<09:54,  1.58s/it]172\n",
            "172\n",
            " 32% 173/548 [05:10<10:31,  1.68s/it]173\n",
            "173\n",
            " 32% 174/548 [05:11<09:39,  1.55s/it]174\n",
            "174\n",
            " 32% 175/548 [05:13<09:45,  1.57s/it]175\n",
            "175\n",
            " 32% 176/548 [05:14<09:15,  1.49s/it]176\n",
            "176\n",
            " 32% 177/548 [05:16<09:28,  1.53s/it]177\n",
            "177\n",
            " 32% 178/548 [05:17<09:36,  1.56s/it]178\n",
            "178\n",
            " 33% 179/548 [05:19<09:46,  1.59s/it]179\n",
            "179\n",
            " 33% 180/548 [05:20<09:44,  1.59s/it]180\n",
            "180\n",
            " 33% 181/548 [05:22<09:52,  1.61s/it]181\n",
            "181\n",
            " 33% 182/548 [05:24<09:46,  1.60s/it]182\n",
            "182\n",
            " 33% 183/548 [05:25<10:03,  1.65s/it]183\n",
            "183\n",
            " 34% 184/548 [05:27<09:20,  1.54s/it]184\n",
            "184\n",
            " 34% 185/548 [05:28<09:30,  1.57s/it]185\n",
            "185\n",
            " 34% 186/548 [05:30<09:19,  1.55s/it]186\n",
            "186\n",
            " 34% 187/548 [05:32<09:42,  1.61s/it]187\n",
            "187\n",
            " 34% 188/548 [05:33<09:32,  1.59s/it]188\n",
            "188\n",
            " 34% 189/548 [05:35<10:54,  1.82s/it]189\n",
            "189\n",
            " 35% 190/548 [05:37<09:59,  1.67s/it]190\n",
            "190\n",
            " 35% 191/548 [05:38<09:16,  1.56s/it]191\n",
            "191\n",
            " 35% 192/548 [05:40<09:15,  1.56s/it]192\n",
            "192\n",
            " 35% 193/548 [05:41<08:45,  1.48s/it]193\n",
            "193\n",
            " 35% 194/548 [05:43<09:04,  1.54s/it]194\n",
            "194\n",
            " 36% 195/548 [05:44<09:19,  1.59s/it]195\n",
            "195\n",
            " 36% 196/548 [05:46<09:15,  1.58s/it]196\n",
            "196\n",
            " 36% 197/548 [05:47<09:13,  1.58s/it]197\n",
            "197\n",
            " 36% 198/548 [05:49<09:30,  1.63s/it]198\n",
            "198\n",
            " 36% 199/548 [05:51<09:49,  1.69s/it]199\n",
            "199\n",
            " 36% 200/548 [05:53<09:51,  1.70s/it]200\n",
            "200\n",
            " 37% 201/548 [05:55<09:53,  1.71s/it]201\n",
            "201\n",
            " 37% 202/548 [05:56<09:53,  1.71s/it]202\n",
            "202\n",
            " 37% 203/548 [05:58<09:45,  1.70s/it]203\n",
            "203\n",
            " 37% 204/548 [05:59<09:15,  1.62s/it]204\n",
            "204\n",
            " 37% 205/548 [06:02<10:15,  1.79s/it]205\n",
            "205\n",
            " 38% 206/548 [06:03<10:06,  1.77s/it]206\n",
            "206\n",
            " 38% 207/548 [06:05<09:43,  1.71s/it]207\n",
            "207\n",
            " 38% 208/548 [06:07<09:45,  1.72s/it]208\n",
            "208\n",
            " 38% 209/548 [06:09<10:34,  1.87s/it]209\n",
            "209\n",
            " 38% 210/548 [06:10<10:14,  1.82s/it]210\n",
            "210\n",
            " 39% 211/548 [06:12<10:07,  1.80s/it]211\n",
            "211\n",
            " 39% 212/548 [06:14<09:45,  1.74s/it]212\n",
            "212\n",
            " 39% 213/548 [06:16<09:36,  1.72s/it]213\n",
            "213\n",
            " 39% 214/548 [06:17<09:14,  1.66s/it]214\n",
            "214\n",
            " 39% 215/548 [06:18<08:25,  1.52s/it]215\n",
            "215\n",
            " 39% 216/548 [06:19<07:51,  1.42s/it]216\n",
            "216\n",
            " 40% 217/548 [06:21<07:40,  1.39s/it]217\n",
            "217\n",
            " 40% 218/548 [06:22<08:09,  1.48s/it]218\n",
            "218\n",
            " 40% 219/548 [06:25<09:16,  1.69s/it]219\n",
            "219\n",
            " 40% 220/548 [06:26<08:44,  1.60s/it]220\n",
            "220\n",
            " 40% 221/548 [06:27<08:16,  1.52s/it]221\n",
            "221\n",
            " 41% 222/548 [06:29<08:33,  1.57s/it]222\n",
            "222\n",
            " 41% 223/548 [06:31<08:41,  1.60s/it]223\n",
            "223\n",
            " 41% 224/548 [06:32<08:51,  1.64s/it]224\n",
            "224\n",
            " 41% 225/548 [06:34<08:44,  1.62s/it]225\n",
            "225\n",
            " 41% 226/548 [06:36<08:49,  1.65s/it]226\n",
            "226\n",
            " 41% 227/548 [06:37<08:57,  1.68s/it]227\n",
            "227\n",
            " 42% 228/548 [06:39<09:25,  1.77s/it]228\n",
            "228\n",
            " 42% 229/548 [06:41<08:33,  1.61s/it]229\n",
            "229\n",
            " 42% 230/548 [06:42<08:27,  1.60s/it]230\n",
            "230\n",
            " 42% 231/548 [06:44<08:47,  1.66s/it]231\n",
            "231\n",
            " 42% 232/548 [06:46<08:28,  1.61s/it]232\n",
            "232\n",
            " 43% 233/548 [06:47<08:33,  1.63s/it]233\n",
            "233\n",
            " 43% 234/548 [06:49<08:56,  1.71s/it]234\n",
            "234\n",
            " 43% 235/548 [06:51<08:29,  1.63s/it]235\n",
            "235\n",
            " 43% 236/548 [06:52<08:01,  1.54s/it]236\n",
            "236\n",
            " 43% 237/548 [06:54<08:28,  1.63s/it]237\n",
            "237\n",
            " 43% 238/548 [06:55<08:29,  1.64s/it]238\n",
            "238\n",
            " 44% 239/548 [06:57<08:13,  1.60s/it]239\n",
            "239\n",
            " 44% 240/548 [06:58<08:03,  1.57s/it]240\n",
            "240\n",
            " 44% 241/548 [07:00<07:48,  1.53s/it]241\n",
            "241\n",
            " 44% 242/548 [07:02<08:39,  1.70s/it]242\n",
            "242\n",
            " 44% 243/548 [07:04<08:42,  1.71s/it]243\n",
            "243\n",
            " 45% 244/548 [07:05<08:27,  1.67s/it]244\n",
            "244\n",
            " 45% 245/548 [07:07<08:24,  1.66s/it]245\n",
            "245\n",
            " 45% 246/548 [07:09<08:31,  1.70s/it]246\n",
            "246\n",
            " 45% 247/548 [07:10<07:49,  1.56s/it]247\n",
            "247\n",
            " 45% 248/548 [07:12<08:16,  1.66s/it]248\n",
            "248\n",
            " 45% 249/548 [07:13<07:49,  1.57s/it]249\n",
            "249\n",
            " 46% 250/548 [07:15<08:12,  1.65s/it]250\n",
            "250\n",
            " 46% 251/548 [07:17<08:30,  1.72s/it]251\n",
            "251\n",
            " 46% 252/548 [07:19<08:29,  1.72s/it]252\n",
            "252\n",
            " 46% 253/548 [07:20<08:23,  1.71s/it]253\n",
            "253\n",
            " 46% 254/548 [07:22<08:30,  1.74s/it]254\n",
            "254\n",
            " 47% 255/548 [07:24<08:22,  1.71s/it]255\n",
            "255\n",
            " 47% 256/548 [07:25<07:59,  1.64s/it]256\n",
            "256\n",
            " 47% 257/548 [07:27<07:37,  1.57s/it]257\n",
            "257\n",
            " 47% 258/548 [07:28<07:54,  1.64s/it]258\n",
            "258\n",
            " 47% 259/548 [07:30<07:49,  1.63s/it]259\n",
            "259\n",
            " 47% 260/548 [07:32<07:51,  1.64s/it]260\n",
            "260\n",
            " 48% 261/548 [07:33<07:45,  1.62s/it]261\n",
            "261\n",
            " 48% 262/548 [07:36<08:43,  1.83s/it]262\n",
            "262\n",
            " 48% 263/548 [07:38<09:13,  1.94s/it]263\n",
            "263\n",
            " 48% 264/548 [07:40<09:26,  1.99s/it]264\n",
            "264\n",
            " 48% 265/548 [07:42<09:13,  1.96s/it]265\n",
            "265\n",
            " 49% 266/548 [07:43<08:13,  1.75s/it]266\n",
            "266\n",
            " 49% 267/548 [07:45<08:14,  1.76s/it]267\n",
            "267\n",
            " 49% 268/548 [07:46<07:28,  1.60s/it]268\n",
            "268\n",
            " 49% 269/548 [07:48<08:16,  1.78s/it]269\n",
            "269\n",
            " 49% 270/548 [07:50<08:53,  1.92s/it]270\n",
            "270\n",
            " 49% 271/548 [07:52<08:36,  1.86s/it]271\n",
            "271\n",
            " 50% 272/548 [07:54<08:41,  1.89s/it]272\n",
            "272\n",
            " 50% 273/548 [07:56<07:54,  1.73s/it]273\n",
            "273\n",
            " 50% 274/548 [07:57<07:50,  1.72s/it]274\n",
            "274\n",
            " 50% 275/548 [07:59<08:03,  1.77s/it]275\n",
            "275\n",
            " 50% 276/548 [08:01<07:54,  1.75s/it]276\n",
            "276\n",
            " 51% 277/548 [08:03<08:24,  1.86s/it]277\n",
            "277\n",
            " 51% 278/548 [08:05<08:06,  1.80s/it]278\n",
            "278\n",
            " 51% 279/548 [08:06<07:51,  1.75s/it]279\n",
            "279\n",
            " 51% 280/548 [08:08<07:13,  1.62s/it]280\n",
            "280\n",
            " 51% 281/548 [08:09<07:17,  1.64s/it]281\n",
            "281\n",
            " 51% 282/548 [08:11<07:15,  1.64s/it]282\n",
            "282\n",
            " 52% 283/548 [08:13<07:25,  1.68s/it]283\n",
            "283\n",
            " 52% 284/548 [08:14<07:24,  1.68s/it]284\n",
            "284\n",
            " 52% 285/548 [08:16<07:15,  1.66s/it]285\n",
            "285\n",
            " 52% 286/548 [08:18<07:13,  1.65s/it]286\n",
            "286\n",
            " 52% 287/548 [08:19<07:19,  1.68s/it]287\n",
            "287\n",
            " 53% 288/548 [08:21<07:06,  1.64s/it]288\n",
            "288\n",
            " 53% 289/548 [08:22<06:44,  1.56s/it]289\n",
            "289\n",
            " 53% 290/548 [08:25<07:41,  1.79s/it]290\n",
            "290\n",
            " 53% 291/548 [08:26<07:00,  1.63s/it]291\n",
            "291\n",
            " 53% 292/548 [08:27<06:35,  1.54s/it]292\n",
            "292\n",
            " 53% 293/548 [08:28<06:10,  1.45s/it]293\n",
            "293\n",
            " 54% 294/548 [08:30<06:10,  1.46s/it]294\n",
            "294\n",
            " 54% 295/548 [08:32<06:24,  1.52s/it]295\n",
            "295\n",
            " 54% 296/548 [08:33<06:36,  1.57s/it]296\n",
            "296\n",
            " 54% 297/548 [08:35<06:46,  1.62s/it]297\n",
            "297\n",
            " 54% 298/548 [08:36<06:32,  1.57s/it]298\n",
            "298\n",
            " 55% 299/548 [08:38<06:11,  1.49s/it]299\n",
            "299\n",
            " 55% 300/548 [08:39<06:22,  1.54s/it]300\n",
            "300\n",
            " 55% 301/548 [08:41<06:32,  1.59s/it]301\n",
            "301\n",
            " 55% 302/548 [08:42<06:06,  1.49s/it]302\n",
            "302\n",
            " 55% 303/548 [08:44<06:27,  1.58s/it]303\n",
            "303\n",
            " 55% 304/548 [08:46<06:29,  1.59s/it]304\n",
            "304\n",
            " 56% 305/548 [08:47<06:29,  1.60s/it]305\n",
            "305\n",
            " 56% 306/548 [08:49<06:13,  1.54s/it]306\n",
            "306\n",
            " 56% 307/548 [08:51<06:43,  1.67s/it]307\n",
            "307\n",
            " 56% 308/548 [08:52<06:19,  1.58s/it]308\n",
            "308\n",
            " 56% 309/548 [08:54<06:15,  1.57s/it]309\n",
            "309\n",
            " 57% 310/548 [08:56<07:21,  1.85s/it]310\n",
            "310\n",
            " 57% 311/548 [08:58<07:45,  1.96s/it]311\n",
            "311\n",
            " 57% 312/548 [09:00<07:27,  1.90s/it]312\n",
            "312\n",
            " 57% 313/548 [09:02<07:29,  1.91s/it]313\n",
            "313\n",
            " 57% 314/548 [09:04<07:21,  1.89s/it]314\n",
            "314\n",
            " 57% 315/548 [09:06<07:53,  2.03s/it]315\n",
            "315\n",
            " 58% 316/548 [09:08<07:16,  1.88s/it]316\n",
            "316\n",
            " 58% 317/548 [09:09<06:46,  1.76s/it]317\n",
            "317\n",
            " 58% 318/548 [09:11<06:42,  1.75s/it]318\n",
            "318\n",
            " 58% 319/548 [09:13<06:50,  1.79s/it]319\n",
            "319\n",
            " 58% 320/548 [09:14<06:25,  1.69s/it]320\n",
            "320\n",
            " 59% 321/548 [09:16<05:51,  1.55s/it]321\n",
            "321\n",
            " 59% 322/548 [09:17<05:44,  1.52s/it]322\n",
            "322\n",
            " 59% 323/548 [09:19<06:09,  1.64s/it]323\n",
            "323\n",
            " 59% 324/548 [09:21<06:25,  1.72s/it]324\n",
            "324\n",
            " 59% 325/548 [09:23<06:29,  1.75s/it]325\n",
            "325\n",
            " 59% 326/548 [09:24<06:27,  1.75s/it]326\n",
            "326\n",
            " 60% 327/548 [09:26<06:20,  1.72s/it]327\n",
            "327\n",
            " 60% 328/548 [09:28<06:15,  1.70s/it]328\n",
            "328\n",
            " 60% 329/548 [09:30<06:16,  1.72s/it]329\n",
            "329\n",
            " 60% 330/548 [09:31<05:53,  1.62s/it]330\n",
            "330\n",
            " 60% 331/548 [09:32<05:35,  1.55s/it]331\n",
            "331\n",
            " 61% 332/548 [09:34<06:03,  1.68s/it]332\n",
            "332\n",
            " 61% 333/548 [09:36<06:00,  1.68s/it]333\n",
            "333\n",
            " 61% 334/548 [09:38<05:50,  1.64s/it]334\n",
            "334\n",
            " 61% 335/548 [09:39<06:11,  1.74s/it]335\n",
            "335\n",
            " 61% 336/548 [09:41<05:56,  1.68s/it]336\n",
            "336\n",
            " 61% 337/548 [09:42<05:31,  1.57s/it]337\n",
            "337\n",
            " 62% 338/548 [09:44<05:19,  1.52s/it]338\n",
            "338\n",
            " 62% 339/548 [09:45<05:27,  1.56s/it]339\n",
            "339\n",
            " 62% 340/548 [09:47<05:08,  1.48s/it]340\n",
            "340\n",
            " 62% 341/548 [09:48<05:21,  1.56s/it]341\n",
            "341\n",
            " 62% 342/548 [09:50<05:26,  1.59s/it]342\n",
            "342\n",
            " 63% 343/548 [09:52<05:38,  1.65s/it]343\n",
            "343\n",
            " 63% 344/548 [09:54<05:54,  1.74s/it]344\n",
            "344\n",
            " 63% 345/548 [09:56<05:58,  1.76s/it]345\n",
            "345\n",
            " 63% 346/548 [09:57<05:57,  1.77s/it]346\n",
            "346\n",
            " 63% 347/548 [09:59<05:53,  1.76s/it]347\n",
            "347\n",
            " 64% 348/548 [10:01<05:26,  1.63s/it]348\n",
            "348\n",
            " 64% 349/548 [10:02<05:37,  1.70s/it]349\n",
            "349\n",
            " 64% 350/548 [10:04<05:37,  1.70s/it]350\n",
            "350\n",
            " 64% 351/548 [10:06<05:32,  1.69s/it]351\n",
            "351\n",
            " 64% 352/548 [10:07<05:26,  1.67s/it]352\n",
            "352\n",
            " 64% 353/548 [10:09<05:28,  1.68s/it]353\n",
            "353\n",
            " 65% 354/548 [10:10<05:11,  1.60s/it]354\n",
            "354\n",
            " 65% 355/548 [10:12<04:54,  1.53s/it]355\n",
            "355\n",
            " 65% 356/548 [10:14<05:16,  1.65s/it]356\n",
            "356\n",
            " 65% 357/548 [10:16<05:41,  1.79s/it]357\n",
            "357\n",
            " 65% 358/548 [10:17<05:27,  1.73s/it]358\n",
            "358\n",
            " 66% 359/548 [10:19<05:30,  1.75s/it]359\n",
            "359\n",
            " 66% 360/548 [10:22<06:08,  1.96s/it]360\n",
            "360\n",
            " 66% 361/548 [10:23<05:49,  1.87s/it]361\n",
            "361\n",
            " 66% 362/548 [10:25<05:34,  1.80s/it]362\n",
            "362\n",
            " 66% 363/548 [10:27<05:52,  1.90s/it]363\n",
            "363\n",
            " 66% 364/548 [10:29<05:46,  1.88s/it]364\n",
            "364\n",
            " 67% 365/548 [10:31<05:33,  1.82s/it]365\n",
            "365\n",
            " 67% 366/548 [10:32<05:23,  1.78s/it]366\n",
            "366\n",
            " 67% 367/548 [10:34<05:19,  1.77s/it]367\n",
            "367\n",
            " 67% 368/548 [10:36<05:24,  1.80s/it]368\n",
            "368\n",
            " 67% 369/548 [10:38<05:11,  1.74s/it]369\n",
            "369\n",
            " 68% 370/548 [10:39<05:08,  1.73s/it]370\n",
            "370\n",
            " 68% 371/548 [10:41<05:02,  1.71s/it]371\n",
            "371\n",
            " 68% 372/548 [10:42<04:36,  1.57s/it]372\n",
            "372\n",
            " 68% 373/548 [10:44<04:43,  1.62s/it]373\n",
            "373\n",
            " 68% 374/548 [10:47<05:43,  1.97s/it]374\n",
            "374\n",
            " 68% 375/548 [10:49<05:31,  1.92s/it]375\n",
            "375\n",
            " 69% 376/548 [10:50<05:17,  1.85s/it]376\n",
            "376\n",
            " 69% 377/548 [10:52<05:13,  1.83s/it]377\n",
            "377\n",
            " 69% 378/548 [10:54<04:58,  1.76s/it]378\n",
            "378\n",
            " 69% 379/548 [10:55<04:33,  1.62s/it]379\n",
            "379\n",
            " 69% 380/548 [10:57<04:45,  1.70s/it]380\n",
            "380\n",
            " 70% 381/548 [10:58<04:38,  1.67s/it]381\n",
            "381\n",
            " 70% 382/548 [11:00<04:29,  1.62s/it]382\n",
            "382\n",
            " 70% 383/548 [11:02<04:36,  1.67s/it]383\n",
            "383\n",
            " 70% 384/548 [11:03<04:42,  1.72s/it]384\n",
            "384\n",
            " 70% 385/548 [11:06<05:05,  1.87s/it]385\n",
            "385\n",
            " 70% 386/548 [11:07<04:58,  1.84s/it]386\n",
            "386\n",
            " 71% 387/548 [11:09<04:55,  1.84s/it]387\n",
            "387\n",
            " 71% 388/548 [11:11<04:45,  1.79s/it]388\n",
            "388\n",
            " 71% 389/548 [11:12<04:21,  1.65s/it]389\n",
            "389\n",
            " 71% 390/548 [11:15<05:23,  2.05s/it]390\n",
            "390\n",
            " 71% 391/548 [11:17<05:11,  1.98s/it]391\n",
            "391\n",
            " 72% 392/548 [11:19<05:00,  1.92s/it]392\n",
            "392\n",
            " 72% 393/548 [11:21<04:50,  1.87s/it]393\n",
            "393\n",
            " 72% 394/548 [11:22<04:41,  1.82s/it]394\n",
            "394\n",
            " 72% 395/548 [11:24<04:08,  1.62s/it]395\n",
            "395\n",
            " 72% 396/548 [11:25<03:51,  1.52s/it]396\n",
            "396\n",
            " 72% 397/548 [11:26<03:56,  1.56s/it]397\n",
            "397\n",
            " 73% 398/548 [11:29<04:35,  1.84s/it]398\n",
            "398\n",
            " 73% 399/548 [11:31<05:02,  2.03s/it]399\n",
            "399\n",
            " 73% 400/548 [11:33<04:44,  1.92s/it]400\n",
            "400\n",
            " 73% 401/548 [11:35<04:42,  1.92s/it]401\n",
            "401\n",
            " 73% 402/548 [11:37<04:32,  1.87s/it]402\n",
            "402\n",
            " 74% 403/548 [11:38<04:20,  1.80s/it]403\n",
            "403\n",
            " 74% 404/548 [11:40<04:09,  1.73s/it]404\n",
            "404\n",
            " 74% 405/548 [11:42<04:14,  1.78s/it]405\n",
            "405\n",
            " 74% 406/548 [11:43<04:05,  1.73s/it]406\n",
            "406\n",
            " 74% 407/548 [11:45<03:56,  1.68s/it]407\n",
            "407\n",
            " 74% 408/548 [11:47<03:58,  1.71s/it]408\n",
            "408\n",
            " 75% 409/548 [11:48<03:49,  1.65s/it]409\n",
            "409\n",
            " 75% 410/548 [11:50<03:55,  1.71s/it]410\n",
            "410\n",
            " 75% 411/548 [11:52<03:54,  1.71s/it]411\n",
            "411\n",
            " 75% 412/548 [11:53<03:37,  1.60s/it]412\n",
            "412\n",
            " 75% 413/548 [11:55<03:26,  1.53s/it]413\n",
            "413\n",
            " 76% 414/548 [11:56<03:15,  1.46s/it]414\n",
            "414\n",
            " 76% 415/548 [11:58<03:21,  1.52s/it]415\n",
            "415\n",
            " 76% 416/548 [11:59<03:25,  1.56s/it]416\n",
            "416\n",
            " 76% 417/548 [12:01<03:23,  1.55s/it]417\n",
            "417\n",
            " 76% 418/548 [12:02<03:22,  1.55s/it]418\n",
            "418\n",
            " 76% 419/548 [12:04<03:23,  1.58s/it]419\n",
            "419\n",
            " 77% 420/548 [12:06<03:37,  1.70s/it]420\n",
            "420\n",
            " 77% 421/548 [12:08<03:40,  1.74s/it]421\n",
            "421\n",
            " 77% 422/548 [12:10<03:40,  1.75s/it]422\n",
            "422\n",
            " 77% 423/548 [12:11<03:35,  1.73s/it]423\n",
            "423\n",
            " 77% 424/548 [12:13<03:32,  1.71s/it]424\n",
            "424\n",
            " 78% 425/548 [12:15<03:33,  1.74s/it]425\n",
            "425\n",
            " 78% 426/548 [12:16<03:23,  1.67s/it]426\n",
            "426\n",
            " 78% 427/548 [12:18<03:41,  1.83s/it]427\n",
            "427\n",
            " 78% 428/548 [12:20<03:33,  1.78s/it]428\n",
            "428\n",
            " 78% 429/548 [12:22<03:51,  1.95s/it]429\n",
            "429\n",
            " 78% 430/548 [12:24<03:38,  1.85s/it]430\n",
            "430\n",
            " 79% 431/548 [12:26<03:28,  1.78s/it]431\n",
            "431\n",
            " 79% 432/548 [12:27<03:18,  1.72s/it]432\n",
            "432\n",
            " 79% 433/548 [12:29<03:20,  1.74s/it]433\n",
            "433\n",
            " 79% 434/548 [12:32<03:54,  2.06s/it]434\n",
            "434\n",
            " 79% 435/548 [12:33<03:37,  1.92s/it]435\n",
            "435\n",
            " 80% 436/548 [12:36<03:52,  2.07s/it]436\n",
            "436\n",
            " 80% 437/548 [12:38<03:54,  2.11s/it]437\n",
            "437\n",
            " 80% 438/548 [12:40<03:42,  2.02s/it]438\n",
            "438\n",
            " 80% 439/548 [12:42<03:41,  2.03s/it]439\n",
            "439\n",
            " 80% 440/548 [12:44<03:30,  1.95s/it]440\n",
            "440\n",
            " 80% 441/548 [12:45<03:15,  1.82s/it]441\n",
            "441\n",
            " 81% 442/548 [12:47<03:20,  1.89s/it]442\n",
            "442\n",
            " 81% 443/548 [12:49<03:31,  2.01s/it]443\n",
            "443\n",
            " 81% 444/548 [12:52<03:45,  2.17s/it]444\n",
            "444\n",
            " 81% 445/548 [12:54<03:26,  2.01s/it]445\n",
            "445\n",
            " 81% 446/548 [12:55<03:12,  1.89s/it]446\n",
            "446\n",
            " 82% 447/548 [12:57<03:01,  1.79s/it]447\n",
            "447\n",
            " 82% 448/548 [12:59<03:13,  1.94s/it]448\n",
            "448\n",
            " 82% 449/548 [13:01<03:03,  1.85s/it]449\n",
            "449\n",
            " 82% 450/548 [13:03<02:58,  1.82s/it]450\n",
            "450\n",
            " 82% 451/548 [13:04<02:51,  1.77s/it]451\n",
            "451\n",
            " 82% 452/548 [13:06<02:59,  1.87s/it]452\n",
            "452\n",
            " 83% 453/548 [13:08<02:53,  1.83s/it]453\n",
            "453\n",
            " 83% 454/548 [13:10<02:44,  1.75s/it]454\n",
            "454\n",
            " 83% 455/548 [13:11<02:36,  1.69s/it]455\n",
            "455\n",
            " 83% 456/548 [13:13<02:40,  1.74s/it]456\n",
            "456\n",
            " 83% 457/548 [13:15<02:39,  1.75s/it]457\n",
            "457\n",
            " 84% 458/548 [13:16<02:34,  1.72s/it]458\n",
            "458\n",
            " 84% 459/548 [13:19<02:47,  1.88s/it]459\n",
            "459\n",
            " 84% 460/548 [13:21<02:55,  2.00s/it]460\n",
            "460\n",
            " 84% 461/548 [13:22<02:41,  1.85s/it]461\n",
            "461\n",
            " 84% 462/548 [13:24<02:34,  1.80s/it]462\n",
            "462\n",
            " 84% 463/548 [13:26<02:27,  1.73s/it]463\n",
            "463\n",
            " 85% 464/548 [13:27<02:21,  1.68s/it]464\n",
            "464\n",
            " 85% 465/548 [13:29<02:16,  1.65s/it]465\n",
            "465\n",
            " 85% 466/548 [13:31<02:21,  1.72s/it]466\n",
            "466\n",
            " 85% 467/548 [13:33<02:21,  1.75s/it]467\n",
            "467\n",
            " 85% 468/548 [13:34<02:18,  1.73s/it]468\n",
            "468\n",
            " 86% 469/548 [13:36<02:19,  1.76s/it]469\n",
            "469\n",
            " 86% 470/548 [13:38<02:18,  1.77s/it]470\n",
            "470\n",
            " 86% 471/548 [13:40<02:14,  1.74s/it]471\n",
            "471\n",
            " 86% 472/548 [13:41<02:16,  1.80s/it]472\n",
            "472\n",
            " 86% 473/548 [13:43<02:13,  1.79s/it]473\n",
            "473\n",
            " 86% 474/548 [13:45<02:06,  1.71s/it]474\n",
            "474\n",
            " 87% 475/548 [13:47<02:07,  1.74s/it]475\n",
            "475\n",
            " 87% 476/548 [13:48<02:07,  1.77s/it]476\n",
            "476\n",
            " 87% 477/548 [13:50<02:03,  1.73s/it]477\n",
            "477\n",
            " 87% 478/548 [13:52<01:58,  1.69s/it]478\n",
            "478\n",
            " 87% 479/548 [13:53<01:54,  1.66s/it]479\n",
            "479\n",
            " 88% 480/548 [13:55<02:02,  1.80s/it]480\n",
            "480\n",
            " 88% 481/548 [13:57<02:04,  1.85s/it]481\n",
            "481\n",
            " 88% 482/548 [13:59<01:55,  1.76s/it]482\n",
            "482\n",
            " 88% 483/548 [14:01<02:00,  1.85s/it]483\n",
            "483\n",
            " 88% 484/548 [14:03<02:00,  1.89s/it]484\n",
            "484\n",
            " 89% 485/548 [14:05<02:05,  2.00s/it]485\n",
            "485\n",
            " 89% 486/548 [14:07<01:59,  1.93s/it]486\n",
            "486\n",
            " 89% 487/548 [14:09<01:52,  1.84s/it]487\n",
            "487\n",
            " 89% 488/548 [14:10<01:48,  1.80s/it]488\n",
            "488\n",
            " 89% 489/548 [14:12<01:47,  1.83s/it]489\n",
            "489\n",
            " 89% 490/548 [14:14<01:49,  1.89s/it]490\n",
            "490\n",
            " 90% 491/548 [14:16<01:44,  1.84s/it]491\n",
            "491\n",
            " 90% 492/548 [14:18<01:47,  1.92s/it]492\n",
            "492\n",
            " 90% 493/548 [14:20<01:42,  1.86s/it]493\n",
            "493\n",
            " 90% 494/548 [14:21<01:38,  1.83s/it]494\n",
            "494\n",
            " 90% 495/548 [14:23<01:38,  1.86s/it]495\n",
            "495\n",
            " 91% 496/548 [14:25<01:33,  1.81s/it]496\n",
            "496\n",
            " 91% 497/548 [14:27<01:32,  1.81s/it]497\n",
            "497\n",
            " 91% 498/548 [14:29<01:30,  1.81s/it]498\n",
            "498\n",
            " 91% 499/548 [14:30<01:24,  1.72s/it]499\n",
            "499\n",
            " 91% 500/548 [14:32<01:24,  1.77s/it]500\n",
            "500\n",
            " 91% 501/548 [14:35<01:33,  2.00s/it]501\n",
            "501\n",
            " 92% 502/548 [14:36<01:27,  1.90s/it]502\n",
            "502\n",
            " 92% 503/548 [14:38<01:23,  1.86s/it]503\n",
            "503\n",
            " 92% 504/548 [14:40<01:19,  1.82s/it]504\n",
            "504\n",
            " 92% 505/548 [14:42<01:18,  1.83s/it]505\n",
            "505\n",
            " 92% 506/548 [14:44<01:17,  1.86s/it]506\n",
            "506\n",
            " 93% 507/548 [14:45<01:16,  1.86s/it]507\n",
            "507\n",
            " 93% 508/548 [14:47<01:11,  1.78s/it]508\n",
            "508\n",
            " 93% 509/548 [14:50<01:17,  2.00s/it]509\n",
            "509\n",
            " 93% 510/548 [14:51<01:09,  1.84s/it]510\n",
            "510\n",
            " 93% 511/548 [14:53<01:06,  1.80s/it]511\n",
            "511\n",
            " 93% 512/548 [14:54<01:03,  1.77s/it]512\n",
            "512\n",
            " 94% 513/548 [14:56<01:02,  1.80s/it]513\n",
            "513\n",
            " 94% 514/548 [14:58<01:03,  1.86s/it]514\n",
            "514\n",
            " 94% 515/548 [15:00<01:00,  1.84s/it]515\n",
            "515\n",
            " 94% 516/548 [15:02<01:04,  2.00s/it]516\n",
            "516\n",
            " 94% 517/548 [15:05<01:08,  2.20s/it]517\n",
            "517\n",
            " 95% 518/548 [15:07<01:02,  2.07s/it]518\n",
            "518\n",
            " 95% 519/548 [15:09<00:57,  1.99s/it]519\n",
            "519\n",
            " 95% 520/548 [15:10<00:52,  1.89s/it]520\n",
            "520\n",
            " 95% 521/548 [15:12<00:50,  1.87s/it]521\n",
            "521\n",
            " 95% 522/548 [15:14<00:50,  1.93s/it]522\n",
            "522\n",
            " 95% 523/548 [15:16<00:48,  1.96s/it]523\n",
            "523\n",
            " 96% 524/548 [15:18<00:48,  2.00s/it]524\n",
            "524\n",
            " 96% 525/548 [15:20<00:44,  1.94s/it]525\n",
            "525\n",
            " 96% 526/548 [15:22<00:40,  1.86s/it]526\n",
            "526\n",
            " 96% 527/548 [15:24<00:42,  2.03s/it]527\n",
            "527\n",
            " 96% 528/548 [15:27<00:42,  2.14s/it]528\n",
            "528\n",
            " 97% 529/548 [15:28<00:37,  1.96s/it]529\n",
            "529\n",
            " 97% 530/548 [15:31<00:37,  2.07s/it]530\n",
            "530\n",
            " 97% 531/548 [15:32<00:33,  1.97s/it]531\n",
            "531\n",
            " 97% 532/548 [15:34<00:30,  1.90s/it]532\n",
            "532\n",
            " 97% 533/548 [15:36<00:30,  2.06s/it]533\n",
            "533\n",
            " 97% 534/548 [15:38<00:26,  1.88s/it]534\n",
            "534\n",
            " 98% 535/548 [15:40<00:23,  1.82s/it]535\n",
            "535\n",
            " 98% 536/548 [15:41<00:21,  1.76s/it]536\n",
            "536\n",
            " 98% 537/548 [15:43<00:19,  1.76s/it]537\n",
            "537\n",
            " 98% 538/548 [15:44<00:16,  1.70s/it]538\n",
            "538\n",
            " 98% 539/548 [15:46<00:15,  1.74s/it]539\n",
            "539\n",
            " 99% 540/548 [15:48<00:13,  1.73s/it]540\n",
            "540\n",
            " 99% 541/548 [15:50<00:12,  1.78s/it]541\n",
            "541\n",
            " 99% 542/548 [15:52<00:10,  1.78s/it]542\n",
            "542\n",
            " 99% 543/548 [15:54<00:09,  1.80s/it]543\n",
            "543\n",
            " 99% 544/548 [15:55<00:07,  1.83s/it]544\n",
            "544\n",
            " 99% 545/548 [15:57<00:05,  1.73s/it]545\n",
            "545\n",
            "100% 546/548 [15:59<00:03,  1.76s/it]546\n",
            "546\n",
            "100% 547/548 [16:01<00:01,  1.76s/it]547\n",
            "547\n",
            "100% 548/548 [16:02<00:00,  1.76s/it]\n",
            "data/visdrone/VisDrone2019-DET-val\n",
            ".visdrone_det_txt\n",
            "data/visdrone/VisDrone2019-DET-val/annotations\n",
            "data/visdrone/VisDrone2019-DET-val/images\n",
            "\n",
            "evaluating object category 1/10...\n",
            "evaluating object category 2/10...\n",
            "evaluating object category 3/10...\n",
            "evaluating object category 4/10...\n",
            "evaluating object category 5/10...\n",
            "evaluating object category 6/10...\n",
            "evaluating object category 7/10...\n",
            "evaluating object category 8/10...\n",
            "evaluating object category 9/10...\n",
            "evaluating object category 10/10...\n",
            "Evaluation completed. The performance of the detector is presented as follows.\n",
            "Average Precision  (AP) @[ IoU=0.50:0.95 | maxDets=500 ] = 20.26317596435547%.\n",
            "Average Precision  (AP) @[ IoU=0.50      | maxDets=500 ] = 37.50722122192383%.\n",
            "Average Precision  (AP) @[ IoU=0.75      | maxDets=500 ] = 19.193572998046875%.\n",
            "Average Recall     (AR) @[ IoU=0.50:0.95 | maxDets=  1 ] = 0.4710117280483246%.\n",
            "Average Recall     (AR) @[ IoU=0.50:0.95 | maxDets= 10 ] = 4.550312042236328%.\n",
            "Average Recall     (AR) @[ IoU=0.50:0.95 | maxDets=100 ] = 28.73590087890625%.\n",
            "Average Recall     (AR) @[ IoU=0.50:0.95 | maxDets=500 ] = 34.078739166259766%.\n",
            "Class 0 AP = 25.853282928466797%\n",
            "Class 1 AP = 16.864025115966797%\n",
            "Class 2 AP = 3.4577813148498535%\n",
            "Class 3 AP = 55.78565216064453%\n",
            "Class 4 AP = 18.38214111328125%\n",
            "Class 5 AP = 9.023462295532227%\n",
            "Class 6 AP = 9.059354782104492%\n",
            "Class 7 AP = 4.951039791107178%\n",
            "Class 8 AP = 7.312675476074219%\n",
            "Class 9 AP = 18.48517417907715%\n",
            "^C\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import detectron2_backbone"
      ],
      "metadata": {
        "id": "GpLlzJyVxmVB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%cd visdrone_eval\n",
        "!pip install -e ."
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zr-MzAECx98A",
        "outputId": "e8714146-9d3a-429f-e058-a8fccde7759e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/gdrive/MyDrive/.shared/QueryDet-PyTorch/visdrone_eval\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Obtaining file:///content/gdrive/MyDrive/.shared/QueryDet-PyTorch/visdrone_eval\n",
            "Collecting numpy~=1.18.5\n",
            "  Downloading numpy-1.18.5-cp37-cp37m-manylinux1_x86_64.whl (20.1 MB)\n",
            "\u001b[K     |████████████████████████████████| 20.1 MB 1.2 MB/s \n",
            "\u001b[?25hCollecting setuptools~=46.4.0\n",
            "  Downloading setuptools-46.4.0-py3-none-any.whl (583 kB)\n",
            "\u001b[K     |████████████████████████████████| 583 kB 65.7 MB/s \n",
            "\u001b[?25hCollecting opencv-python~=4.2.0.34\n",
            "  Downloading opencv_python-4.2.0.34-cp37-cp37m-manylinux1_x86_64.whl (28.2 MB)\n",
            "\u001b[K     |████████████████████████████████| 28.2 MB 1.3 MB/s \n",
            "\u001b[?25hInstalling collected packages: numpy, setuptools, opencv-python, visdrone-eval\n",
            "  Attempting uninstall: numpy\n",
            "    Found existing installation: numpy 1.21.6\n",
            "    Uninstalling numpy-1.21.6:\n",
            "      Successfully uninstalled numpy-1.21.6\n",
            "  Attempting uninstall: setuptools\n",
            "    Found existing installation: setuptools 45.2.0.post20200210\n",
            "    Uninstalling setuptools-45.2.0.post20200210:\n",
            "      Successfully uninstalled setuptools-45.2.0.post20200210\n",
            "  Attempting uninstall: opencv-python\n",
            "    Found existing installation: opencv-python 4.7.0.72\n",
            "    Uninstalling opencv-python-4.7.0.72:\n",
            "      Successfully uninstalled opencv-python-4.7.0.72\n",
            "  Running setup.py develop for visdrone-eval\n",
            "Successfully installed numpy-1.18.5 opencv-python-4.2.0.34 setuptools-46.4.0 visdrone-eval\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%cd detectron2_backbone\n",
        "!python setup.py build develop"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KmFbJiqTyjQ9",
        "outputId": "0db81a48-eefe-48ea-d0ab-c6ea07a951d7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/gdrive/MyDrive/.shared/QueryDet-PyTorch/detectron2_backbone\n",
            "running build\n",
            "running build_py\n",
            "running develop\n",
            "running egg_info\n",
            "writing detectron2_backbone.egg-info/PKG-INFO\n",
            "writing dependency_links to detectron2_backbone.egg-info/dependency_links.txt\n",
            "writing requirements to detectron2_backbone.egg-info/requires.txt\n",
            "writing top-level names to detectron2_backbone.egg-info/top_level.txt\n",
            "reading manifest file 'detectron2_backbone.egg-info/SOURCES.txt'\n",
            "writing manifest file 'detectron2_backbone.egg-info/SOURCES.txt'\n",
            "running build_ext\n",
            "Creating /usr/local/lib/python3.7/site-packages/detectron2-backbone.egg-link (link to .)\n",
            "Adding detectron2-backbone 0.0.1 to easy-install.pth file\n",
            "\n",
            "Installed /content/gdrive/MyDrive/.shared/QueryDet-PyTorch/detectron2_backbone\n",
            "Processing dependencies for detectron2-backbone==0.0.1\n",
            "Searching for addict\n",
            "Reading https://pypi.org/simple/addict/\n",
            "Downloading https://files.pythonhosted.org/packages/6a/00/b08f23b7d7e1e14ce01419a467b583edbb93c6cdb8654e54a9cc579cd61f/addict-2.4.0-py3-none-any.whl#sha256=249bb56bbfd3cdc2a004ea0ff4c2b6ddc84d53bc2194761636eb314d5cfa5dfc\n",
            "Best match: addict 2.4.0\n",
            "Processing addict-2.4.0-py3-none-any.whl\n",
            "Installing addict-2.4.0-py3-none-any.whl to /usr/local/lib/python3.7/site-packages\n",
            "Adding addict 2.4.0 to easy-install.pth file\n",
            "\n",
            "Installed /usr/local/lib/python3.7/site-packages/addict-2.4.0-py3.7.egg\n",
            "Searching for tensorboard==2.11.2\n",
            "Best match: tensorboard 2.11.2\n",
            "Adding tensorboard 2.11.2 to easy-install.pth file\n",
            "Installing tensorboard script to /usr/local/bin\n",
            "\n",
            "Using /usr/local/lib/python3.7/site-packages\n",
            "Searching for tqdm==4.42.1\n",
            "Best match: tqdm 4.42.1\n",
            "Adding tqdm 4.42.1 to easy-install.pth file\n",
            "Installing tqdm script to /usr/local/bin\n",
            "\n",
            "Using /usr/local/lib/python3.7/site-packages\n",
            "Searching for matplotlib==3.5.3\n",
            "Best match: matplotlib 3.5.3\n",
            "Adding matplotlib 3.5.3 to easy-install.pth file\n",
            "\n",
            "Using /usr/local/lib/python3.7/site-packages\n",
            "Searching for cloudpickle==2.2.1\n",
            "Best match: cloudpickle 2.2.1\n",
            "Adding cloudpickle 2.2.1 to easy-install.pth file\n",
            "\n",
            "Using /usr/local/lib/python3.7/site-packages\n",
            "Searching for tabulate==0.9.0\n",
            "Best match: tabulate 0.9.0\n",
            "Adding tabulate 0.9.0 to easy-install.pth file\n",
            "Installing tabulate script to /usr/local/bin\n",
            "\n",
            "Using /usr/local/lib/python3.7/site-packages\n",
            "Searching for PyYAML==6.0\n",
            "Best match: PyYAML 6.0\n",
            "Adding PyYAML 6.0 to easy-install.pth file\n",
            "\n",
            "Using /usr/local/lib/python3.7/site-packages\n",
            "Searching for yacs==0.1.8\n",
            "Best match: yacs 0.1.8\n",
            "Adding yacs 0.1.8 to easy-install.pth file\n",
            "\n",
            "Using /usr/local/lib/python3.7/site-packages\n",
            "Searching for Pillow==9.5.0\n",
            "Best match: Pillow 9.5.0\n",
            "Adding Pillow 9.5.0 to easy-install.pth file\n",
            "\n",
            "Using /usr/local/lib/python3.7/site-packages\n",
            "Searching for termcolor==2.2.0\n",
            "Best match: termcolor 2.2.0\n",
            "Adding termcolor 2.2.0 to easy-install.pth file\n",
            "\n",
            "Using /usr/local/lib/python3.7/site-packages\n",
            "Searching for numpy==1.21.6\n",
            "Best match: numpy 1.21.6\n",
            "Adding numpy 1.21.6 to easy-install.pth file\n",
            "Installing f2py script to /usr/local/bin\n",
            "Installing f2py3 script to /usr/local/bin\n",
            "Installing f2py3.7 script to /usr/local/bin\n",
            "\n",
            "Using /usr/local/lib/python3.7/site-packages\n",
            "Searching for grpcio==1.53.0\n",
            "Best match: grpcio 1.53.0\n",
            "Adding grpcio 1.53.0 to easy-install.pth file\n",
            "\n",
            "Using /usr/local/lib/python3.7/site-packages\n",
            "Searching for absl-py==1.4.0\n",
            "Best match: absl-py 1.4.0\n",
            "Adding absl-py 1.4.0 to easy-install.pth file\n",
            "\n",
            "Using /usr/local/lib/python3.7/site-packages\n",
            "Searching for protobuf==3.20.3\n",
            "Best match: protobuf 3.20.3\n",
            "Adding protobuf 3.20.3 to easy-install.pth file\n",
            "\n",
            "Using /usr/local/lib/python3.7/site-packages\n",
            "Searching for Werkzeug==2.2.3\n",
            "Best match: Werkzeug 2.2.3\n",
            "Adding Werkzeug 2.2.3 to easy-install.pth file\n",
            "\n",
            "Using /usr/local/lib/python3.7/site-packages\n",
            "Searching for requests==2.22.0\n",
            "Best match: requests 2.22.0\n",
            "Adding requests 2.22.0 to easy-install.pth file\n",
            "\n",
            "Using /usr/local/lib/python3.7/site-packages\n",
            "Searching for Markdown==3.4.3\n",
            "Best match: Markdown 3.4.3\n",
            "Adding Markdown 3.4.3 to easy-install.pth file\n",
            "Installing markdown_py script to /usr/local/bin\n",
            "\n",
            "Using /usr/local/lib/python3.7/site-packages\n",
            "Searching for setuptools==45.2.0.post20200210\n",
            "Best match: setuptools 45.2.0.post20200210\n",
            "Adding setuptools 45.2.0.post20200210 to easy-install.pth file\n",
            "Installing easy_install script to /usr/local/bin\n",
            "\n",
            "Using /usr/local/lib/python3.7/site-packages\n",
            "Searching for tensorboard-data-server==0.6.1\n",
            "Best match: tensorboard-data-server 0.6.1\n",
            "Adding tensorboard-data-server 0.6.1 to easy-install.pth file\n",
            "\n",
            "Using /usr/local/lib/python3.7/site-packages\n",
            "Searching for wheel==0.34.2\n",
            "Best match: wheel 0.34.2\n",
            "Adding wheel 0.34.2 to easy-install.pth file\n",
            "Installing wheel script to /usr/local/bin\n",
            "\n",
            "Using /usr/local/lib/python3.7/site-packages\n",
            "Searching for tensorboard-plugin-wit==1.8.1\n",
            "Best match: tensorboard-plugin-wit 1.8.1\n",
            "Adding tensorboard-plugin-wit 1.8.1 to easy-install.pth file\n",
            "\n",
            "Using /usr/local/lib/python3.7/site-packages\n",
            "Searching for google-auth==2.17.3\n",
            "Best match: google-auth 2.17.3\n",
            "Adding google-auth 2.17.3 to easy-install.pth file\n",
            "\n",
            "Using /usr/local/lib/python3.7/site-packages\n",
            "Searching for google-auth-oauthlib==0.4.6\n",
            "Best match: google-auth-oauthlib 0.4.6\n",
            "Adding google-auth-oauthlib 0.4.6 to easy-install.pth file\n",
            "Installing google-oauthlib-tool script to /usr/local/bin\n",
            "\n",
            "Using /usr/local/lib/python3.7/site-packages\n",
            "Searching for cycler==0.11.0\n",
            "Best match: cycler 0.11.0\n",
            "Adding cycler 0.11.0 to easy-install.pth file\n",
            "\n",
            "Using /usr/local/lib/python3.7/site-packages\n",
            "Searching for pyparsing==3.0.9\n",
            "Best match: pyparsing 3.0.9\n",
            "Adding pyparsing 3.0.9 to easy-install.pth file\n",
            "\n",
            "Using /usr/local/lib/python3.7/site-packages\n",
            "Searching for packaging==23.1\n",
            "Best match: packaging 23.1\n",
            "Adding packaging 23.1 to easy-install.pth file\n",
            "\n",
            "Using /usr/local/lib/python3.7/site-packages\n",
            "Searching for python-dateutil==2.8.2\n",
            "Best match: python-dateutil 2.8.2\n",
            "Adding python-dateutil 2.8.2 to easy-install.pth file\n",
            "\n",
            "Using /usr/local/lib/python3.7/site-packages\n",
            "Searching for kiwisolver==1.4.4\n",
            "Best match: kiwisolver 1.4.4\n",
            "Adding kiwisolver 1.4.4 to easy-install.pth file\n",
            "\n",
            "Using /usr/local/lib/python3.7/site-packages\n",
            "Searching for fonttools==4.38.0\n",
            "Best match: fonttools 4.38.0\n",
            "Adding fonttools 4.38.0 to easy-install.pth file\n",
            "Installing fonttools script to /usr/local/bin\n",
            "Installing pyftmerge script to /usr/local/bin\n",
            "Installing pyftsubset script to /usr/local/bin\n",
            "Installing ttx script to /usr/local/bin\n",
            "\n",
            "Using /usr/local/lib/python3.7/site-packages\n",
            "Searching for MarkupSafe==2.1.2\n",
            "Best match: MarkupSafe 2.1.2\n",
            "Adding MarkupSafe 2.1.2 to easy-install.pth file\n",
            "\n",
            "Using /usr/local/lib/python3.7/site-packages\n",
            "Searching for certifi==2019.11.28\n",
            "Best match: certifi 2019.11.28\n",
            "Adding certifi 2019.11.28 to easy-install.pth file\n",
            "\n",
            "Using /usr/local/lib/python3.7/site-packages\n",
            "Searching for idna==2.8\n",
            "Best match: idna 2.8\n",
            "Adding idna 2.8 to easy-install.pth file\n",
            "\n",
            "Using /usr/local/lib/python3.7/site-packages\n",
            "Searching for chardet==3.0.4\n",
            "Best match: chardet 3.0.4\n",
            "Adding chardet 3.0.4 to easy-install.pth file\n",
            "Installing chardetect script to /usr/local/bin\n",
            "\n",
            "Using /usr/local/lib/python3.7/site-packages\n",
            "Searching for urllib3==1.25.8\n",
            "Best match: urllib3 1.25.8\n",
            "Adding urllib3 1.25.8 to easy-install.pth file\n",
            "\n",
            "Using /usr/local/lib/python3.7/site-packages\n",
            "Searching for importlib-metadata==6.3.0\n",
            "Best match: importlib-metadata 6.3.0\n",
            "Adding importlib-metadata 6.3.0 to easy-install.pth file\n",
            "\n",
            "Using /usr/local/lib/python3.7/site-packages\n",
            "Searching for cachetools==5.3.0\n",
            "Best match: cachetools 5.3.0\n",
            "Adding cachetools 5.3.0 to easy-install.pth file\n",
            "\n",
            "Using /usr/local/lib/python3.7/site-packages\n",
            "Searching for rsa==4.9\n",
            "Best match: rsa 4.9\n",
            "Adding rsa 4.9 to easy-install.pth file\n",
            "Installing pyrsa-decrypt script to /usr/local/bin\n",
            "Installing pyrsa-encrypt script to /usr/local/bin\n",
            "Installing pyrsa-keygen script to /usr/local/bin\n",
            "Installing pyrsa-priv2pub script to /usr/local/bin\n",
            "Installing pyrsa-sign script to /usr/local/bin\n",
            "Installing pyrsa-verify script to /usr/local/bin\n",
            "\n",
            "Using /usr/local/lib/python3.7/site-packages\n",
            "Searching for six==1.14.0\n",
            "Best match: six 1.14.0\n",
            "Adding six 1.14.0 to easy-install.pth file\n",
            "\n",
            "Using /usr/local/lib/python3.7/site-packages\n",
            "Searching for pyasn1-modules==0.2.8\n",
            "Best match: pyasn1-modules 0.2.8\n",
            "Adding pyasn1-modules 0.2.8 to easy-install.pth file\n",
            "\n",
            "Using /usr/local/lib/python3.7/site-packages\n",
            "Searching for requests-oauthlib==1.3.1\n",
            "Best match: requests-oauthlib 1.3.1\n",
            "Adding requests-oauthlib 1.3.1 to easy-install.pth file\n",
            "\n",
            "Using /usr/local/lib/python3.7/site-packages\n",
            "Searching for typing-extensions==4.5.0\n",
            "Best match: typing-extensions 4.5.0\n",
            "Adding typing-extensions 4.5.0 to easy-install.pth file\n",
            "\n",
            "Using /usr/local/lib/python3.7/site-packages\n",
            "Searching for zipp==3.15.0\n",
            "Best match: zipp 3.15.0\n",
            "Adding zipp 3.15.0 to easy-install.pth file\n",
            "\n",
            "Using /usr/local/lib/python3.7/site-packages\n",
            "Searching for pyasn1==0.4.8\n",
            "Best match: pyasn1 0.4.8\n",
            "Adding pyasn1 0.4.8 to easy-install.pth file\n",
            "\n",
            "Using /usr/local/lib/python3.7/site-packages\n",
            "Searching for oauthlib==3.2.2\n",
            "Best match: oauthlib 3.2.2\n",
            "Adding oauthlib 3.2.2 to easy-install.pth file\n",
            "\n",
            "Using /usr/local/lib/python3.7/site-packages\n",
            "Finished processing dependencies for detectron2-backbone==0.0.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!python train_visdrone.py --resume --config /content/gdrive/MyDrive/QueryDet-PyTorch/work_dirs/visdrone_querydet/config.yaml MODEL.W --num-gpu 1 OUTPUT_DIR work_dirs/visdrone_querydet"
      ],
      "metadata": {
        "id": "pzdsnU40y-5x"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}